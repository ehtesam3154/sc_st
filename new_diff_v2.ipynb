{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Cell 2: force single‐threaded BLAS\n",
    "os.environ[\"OMP_NUM_THREADS\"]       = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: actually cap BLAS to 1 thread\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# 'blas' covers OpenBLAS, MKL, etc.\n",
    "threadpool_limits(limits=1, user_api='blas')\n",
    "\n",
    "# now import as usual, no more warning\n",
    "import numpy as np\n",
    "import scipy\n",
    "# … any other packages that use OpenBLAS …\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_torch(X, k, mode='connectivity', metric = 'minkowski', p=2, device='cuda'):\n",
    "    '''construct knn graph with torch and gpu\n",
    "    args:\n",
    "        X: input data containing features (torch tensor)\n",
    "        k: number of neighbors for each data point\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric (now euclidean supported for gpu knn)\n",
    "        p: param for minkowski (not used if metric is euclidean)\n",
    "    \n",
    "    Returns:\n",
    "        knn graph as a pytorch sparse tensor (coo format) or dense tensor depending on mode     \n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'.\"\n",
    "    assert metric == 'euclidean', \"for gpu knn, only 'euclidean' metric is currently supported in this implementation\"\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "        mode_knn = 'connectivity'\n",
    "    else:\n",
    "        include_self = False\n",
    "        mode_knn = 'distance'\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm='auto')\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_cpu = X.cpu().numpy()\n",
    "    else:\n",
    "        X_cpu = X.numpy()\n",
    "\n",
    "    knn.fit(X_cpu)\n",
    "    knn_graph_cpu = kneighbors_graph(knn, k, mode=mode_knn, include_self=include_self, metric=metric) #scipy sparse matrix on cpu\n",
    "    knn_graph_coo = knn_graph_cpu.tocoo()\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        knn_graph = torch.sparse_coo_tensor(torch.LongTensor([knn_graph_coo.row, knn_graph_coo.col]),\n",
    "                                            torch.FloatTensor(knn_graph_coo.data),\n",
    "                                            size = knn_graph_coo.shape).to(device)\n",
    "    elif mode == 'distance':\n",
    "        knn_graph_dense = torch.tensor(knn_graph_cpu.toarray(), dtype=torch.float32, device=device) #move to gpu as dense tensor\n",
    "        knn_graph = knn_graph_dense\n",
    "    \n",
    "    return knn_graph\n",
    "    \n",
    "def distances_cal_torch(graph, type_aware=None, aware_power =2, device='cuda'):\n",
    "    '''\n",
    "    calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (pytorch sparse or dense tensor)\n",
    "        type_aware: not implemented in this torch version for simplicity\n",
    "        aware_power: same ^^\n",
    "        device (str): 'cpu' or 'cuda' device to use\n",
    "    Returns:\n",
    "        distance matrix as a torch tensor\n",
    "    '''\n",
    "\n",
    "    if isinstance(graph, torch.Tensor) and graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().to_dense().numpy())\n",
    "    elif isinstance(graph, torch.Tensor) and not graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().numpy())\n",
    "    else:\n",
    "        graph_cpu_csr = csr_matrix(graph) #assume scipy sparse matrix if not torch tensor\n",
    "\n",
    "    shortestPath_cpu = dijkstra(csgraph = graph_cpu_csr, directed=False, return_predecessors=False) #dijkstra on cpu\n",
    "    shortestPath = torch.tensor(shortestPath_cpu, dtype=torch.float32, device=device)\n",
    "\n",
    "    # the_max = torch.nanmax(shortestPath[shortestPath != float('inf')])\n",
    "    # shortestPath[shortestPath > the_max] = the_max\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = torch.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    original_max_distance = the_max.item()\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= torch.mean(C_dis)\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_sc_torch(X_sc, k_neighbors=10, graph_mode='connectivity', device='cpu'):\n",
    "    '''calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (torch sparse or dense tensor)\n",
    "        type_aware: not implemented\n",
    "        aware_power: same ^^\n",
    "        \n",
    "    returns:\n",
    "        distanced matrix as torch tensor'''\n",
    "    \n",
    "    if not isinstance(X_sc, torch.Tensor):\n",
    "        raise TypeError('Input X_sc must be a pytorch tensor')\n",
    "    \n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_sc = X_sc.cuda(device=device)\n",
    "    else:\n",
    "        X_sc = X_sc.cpu()\n",
    "        device= 'cpu'\n",
    "\n",
    "    print(f'using device: {device}')\n",
    "    print(f'constructing knn graph...')\n",
    "    # X_normalized = normalize(X_sc.cpu().numpy(), norm='l2') #normalize on cpu for sklearn knn\n",
    "    X_normalized = X_sc\n",
    "    X_normalized_torch = torch.tensor(X_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "    Xgraph = construct_graph_torch(X_normalized_torch, k=k_neighbors, mode=graph_mode, metric='euclidean', device=device)\n",
    "\n",
    "    print('calculating distances from graph....')\n",
    "    D_sc, sc_max_distance = distances_cal_torch(Xgraph, device=device)\n",
    "\n",
    "    print('D_sc calculation complete')\n",
    "    \n",
    "    return D_sc, sc_max_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot\n",
    "\n",
    "def construct_graph_spatial(location_array, k, mode='distance', metric='euclidean', p=2):\n",
    "    '''construct KNN graph based on spatial coordinates\n",
    "    args:\n",
    "        location_array: spatial coordinates of spots (n-spots * 2)\n",
    "        k: number of neighbors for each spot\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric for knn (p=2 is euclidean)\n",
    "        p: param for minkowski if connectivity\n",
    "        \n",
    "    returns:\n",
    "        scipy.sparse.csr_matrix: knn graph in csr format\n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'\"\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "    else:\n",
    "        include_self = False\n",
    "    \n",
    "    c_graph = kneighbors_graph(location_array, k, mode=mode, metric=metric, include_self=include_self, p=p)\n",
    "    return c_graph\n",
    "\n",
    "def distances_cal_spatial(graph, spot_ids=None, spot_types=None, aware_power=2):\n",
    "    '''calculate spatial distance matrix from knn graph\n",
    "    args:\n",
    "        graph (scipy.sparse.csr_matrix): knn graph\n",
    "        spot_ids (list, optional): list of spot ids corresponding to the rows/cols of the graph. required if type_aware is used\n",
    "        spot_types (pd.Series, optinal): pandas series of spot types for type aware distance adjustment. required if type_aware is used\n",
    "        aware_power (int): power for type-aware distance adjustment\n",
    "        \n",
    "    returns:\n",
    "        sptial distance matrix'''\n",
    "    shortestPath = dijkstra(csgraph = csr_matrix(graph), directed=False, return_predecessors=False)\n",
    "    shortestPath = np.nan_to_num(shortestPath, nan=np.inf) #handle potential inf valyes after dijkstra\n",
    "\n",
    "    if spot_types is not None and spot_ids is not None:\n",
    "        shortestPath_df = pd.DataFrame(shortestPath, index=spot_ids, columns=spot_ids)\n",
    "        shortestPath_df['id1'] = shortestPath_df.index\n",
    "        shortestPath_melted = shortestPath_df.melt(id_vars=['id1'], var_name='id2', value_name='value')\n",
    "\n",
    "        type_aware_df = pd.DataFrame({'spot': spot_ids, 'spot_type': spot_types}, index=spot_ids)\n",
    "        meta1 = type_aware_df.copy()\n",
    "        meta1.columns = ['id1', 'type1']\n",
    "        meta2 = type_aware_df.copy()\n",
    "        meta2.columns = ['id2', 'type2']\n",
    "\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta1, on='id1', how='left')\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta2, on='id2', how='left')\n",
    "\n",
    "        shortestPath_melted['same_type'] = shortestPath_melted['type1'] == shortestPath_melted['type2']\n",
    "        shortestPath_melted.loc[(~shortestPath_melted.smae_type), 'value'] = shortestPath_melted.loc[(~shortestPath_melted.same_type),\n",
    "                                                                                                     'value'] * aware_power\n",
    "        shortestPath_melted.drop(['type1', 'type2', 'same_type'], axis=1, inplace=True)\n",
    "        shortestPath_pivot = shortestPath_melted.pivot(index='id1', columns='id2', values='value')\n",
    "\n",
    "        order = spot_ids\n",
    "        shortestPath = shortestPath_pivot[order].loc[order].values\n",
    "    else:\n",
    "        shortestPath = np.asarray(shortestPath) #ensure it's a numpy array\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = np.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    #store original max distance for scale reference\n",
    "    original_max_distance = the_max\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= np.mean(C_dis)\n",
    "\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_st_from_coords(spatial_coords, X_st=None, k_neighbors=10, graph_mode='distance', aware_st=False, \n",
    "                               spot_types=None, aware_power_st=2, spot_ids=None):\n",
    "    '''calculates the spatial distance matrix D_st for spatial transcriptomics data directly from coordinates and optional spot types\n",
    "    args:\n",
    "        spatial_coords: spatial coordinates of spots (n_spots * 2)\n",
    "        X_st: St gene expression data (not used for D_st calculation itself)\n",
    "        k_neighbors: number of neighbors for knn graph\n",
    "        graph_mode: 'connectivity or 'distance' for knn graph\n",
    "        aware_st: whether to use type-aware distance adjustment\n",
    "        spot_types: pandas series of spot types for type-aware adjustment\n",
    "        aware_power_st: power for type-aware distance adjustment\n",
    "        spot_ids: list or index of spot ids, required if spot_ids is provided\n",
    "        \n",
    "    returns:\n",
    "        np.ndarray: spatial disance matrix D_st'''\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        location_array = spatial_coords.values\n",
    "        if spot_ids is None:\n",
    "            spot_ids = spatial_coords.index.tolist() #use index of dataframe if available\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        location_array = spatial_coords\n",
    "        if spot_ids is None:\n",
    "            spot_ids = list(range(location_array.shape[0])) #generate default ids if not provided\n",
    "\n",
    "    else:\n",
    "        raise TypeError('spatial_coords must be a pandas dataframe or a numpy array')\n",
    "    \n",
    "    print(f'constructing {graph_mode} graph for ST data with k={k_neighbors}.....')\n",
    "    Xgraph_st = construct_graph_spatial(location_array, k=k_neighbors, mode=graph_mode)\n",
    "    \n",
    "    if aware_st:\n",
    "        if spot_types is None or spot_ids is None:\n",
    "            raise ValueError('spot_types and spot_ids must be provided when aware_st=True')\n",
    "        if not isinstance(spot_types, pd.Series):\n",
    "            spot_types = pd.Series(spot_types, idnex=spot_ids) \n",
    "        print('applying type aware distance adjustment for ST data')\n",
    "        print(f'aware power for ST: {aware_power_st}')\n",
    "    else:\n",
    "        spot_types = None \n",
    "\n",
    "    print(f'calculating spatial distances.....')\n",
    "    D_st, st_max_distance = distances_cal_spatial(Xgraph_st, spot_ids=spot_ids, spot_types=spot_types, aware_power=aware_power_st)\n",
    "\n",
    "    print('D_st calculation complete')\n",
    "    return D_st, st_max_distance\n",
    "\n",
    "\n",
    "def calculate_D_st_euclidean(spatial_coords):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance matrix for ST spots.\n",
    "    \n",
    "    Args:\n",
    "        spatial_coords: (m_spots, 2) spatial coordinates\n",
    "        \n",
    "    Returns:\n",
    "        D_st_euclid: (m_spots, m_spots) normalized Euclidean distance matrix\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        coords_array = spatial_coords.values\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        coords_array = spatial_coords\n",
    "    else:\n",
    "        coords_array = np.array(spatial_coords)\n",
    "    \n",
    "    # Compute pairwise Euclidean distances\n",
    "    D_euclid = squareform(pdist(coords_array, metric='euclidean'))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    max_dist = D_euclid.max()\n",
    "    if max_dist > 0:\n",
    "        D_euclid = D_euclid / max_dist\n",
    "    \n",
    "    return D_euclid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_D_induced_proper_scaling(T, D_st, n_sc, n_st):\n",
    "    '''compute D_induced with proper scaling'''\n",
    "    #reweight transport matrix\n",
    "    T_reweight = T * n_sc\n",
    "    D_induced_raw = T_reweight @ D_st @ T_reweight.t()\n",
    "\n",
    "    #normalize to [0,1] range\n",
    "    D_induced_max = torch.max(D_induced_raw[D_induced_raw > 0])\n",
    "    if D_induced_max > 1e-10: #avoid dvision by very small numbers\n",
    "        D_induced = D_induced_raw / D_induced_max\n",
    "    else:\n",
    "        D_induced = D_induced_raw\n",
    "\n",
    "    return D_induced\n",
    "\n",
    "def fused_gw_torch(X_sc, X_st, Y_st, alpha, k_sc=100, k_st=30, G0=None, max_iter = 100, tol=1e-9, epsilon=0.1, device='cuda', n_iter = 1, D_st_precomputed=None):\n",
    "    n = X_sc.shape[0]\n",
    "    m = X_st.shape[0]\n",
    "\n",
    "    X_sc = X_sc.to(device)\n",
    "    X_st = X_st.to(device)\n",
    "\n",
    "    if not torch.is_tensor(Y_st):\n",
    "        Y_st_tensor = torch.tensor(Y_st, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        Y_st_tensor = Y_st.to(device, dtype=torch.float32)\n",
    "\n",
    "    #calculate distance matrices\n",
    "    print('calculating SC distances with knn-dijkstra.....')\n",
    "    D_sc, sc_max_distance = calculate_D_sc_torch(X_sc, graph_mode='distance', k_neighbors=k_sc, device=device)\n",
    "\n",
    "    if D_st_precomputed is not None:\n",
    "        print(\"Using precomputed block-diagonal D_st...\")\n",
    "        D_st = D_st_precomputed.to(device)\n",
    "        st_max_distance = 1.0 #assume already normalized\n",
    "    else:\n",
    "        print('Calculating ST distances.....')\n",
    "        D_st, st_max_distance = calculate_D_st_from_coords(spatial_coords=Y_st, k_neighbors=k_st, graph_mode=\"distance\")\n",
    "        D_st = torch.tensor(D_st, dtype=torch.float32, device=device)\n",
    "\n",
    "    #get expression distance matrix\n",
    "    C_exp = torch.cdist(X_sc, X_st, p=2) #euclidean distance\n",
    "    C_exp = C_exp / (torch.max(C_exp) + 1e-16) #normalize\n",
    "\n",
    "    #ensure distance matries are C-contiguouse numpy arrays for POT\n",
    "    D_sc_np = D_sc.cpu().numpy()\n",
    "    D_st_np = D_st.cpu().numpy()\n",
    "    C_exp_np = C_exp.cpu().numpy()\n",
    "    D_sc_np = np.ascontiguousarray(D_sc_np)\n",
    "    D_st_np = np.ascontiguousarray(D_st_np)\n",
    "    C_exp_np = np.ascontiguousarray(C_exp_np)\n",
    "\n",
    "    #uniform distributions\n",
    "    p = ot.unif(n)\n",
    "    q = ot.unif(m)\n",
    "\n",
    "    #anneal the reg param over several steps\n",
    "    T_np = None\n",
    "    for i in range(n_iter):\n",
    "        T_np, log = ot.gromov.entropic_fused_gromov_wasserstein(\n",
    "            M=C_exp_np, \n",
    "            C1=D_sc_np, \n",
    "            C2=D_st_np,\n",
    "            p=p, \n",
    "            q=q, \n",
    "            loss_fun='square_loss',\n",
    "            epsilon=epsilon,\n",
    "            alpha=alpha,\n",
    "            G0=T_np if T_np is not None else (G0.cpu().numpy() if G0 is not None else None),\n",
    "            log=True,\n",
    "            verbose=True,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol\n",
    "        )\n",
    "\n",
    "    fgw_dist = log['fgw_dist']\n",
    "\n",
    "    print(f'fgw distance: {fgw_dist}')\n",
    "\n",
    "    T = torch.tensor(T_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    n_sc = X_sc.shape[0]\n",
    "    n_st = X_st.shape[0]\n",
    "\n",
    "    D_induced = compute_D_induced_proper_scaling(T, D_st, n_sc, n_st)\n",
    "\n",
    "    return T, D_sc, D_st, D_induced, fgw_dist, sc_max_distance, st_max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patient 2 data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Key innovation: Use ALL ST data points for better training.\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "\n",
    "    # Store separate coordinate lists for block-diagonal graph\n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    st_expr_combined = scaler.fit_transform(st_expr_combined)\n",
    "\n",
    "    st_coords_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "\n",
    "    sc_expr = scaler.fit_transform(sc_expr)\n",
    "\n",
    "    \n",
    "    # Create dataset labels for tracking\n",
    "    dataset_labels = (['dataset1'] * len(st1_expr) + \n",
    "                     ['dataset2'] * len(st2_expr) + \n",
    "                     ['dataset3'] * len(st3_expr))\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Prepare combined data for diffusion\n",
    "X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = prepare_combined_st_for_diffusion(\n",
    "    stadata1, stadata2, stadata3, scadata\n",
    ")\n",
    "\n",
    "print(f\"Data preparation complete!\")\n",
    "print(f\"SC cells: {X_sc.shape[0]}\")\n",
    "print(f\"Combined ST spots: {X_st_combined.shape[0]}\")\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import cKDTree\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "# =====================================================\n",
    "# PART 1: Advanced Network Components\n",
    "# =====================================================\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, n_genes, n_embedding=[512, 256, 128], dp=0):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_genes, n_embedding[0])\n",
    "        self.bn1 = nn.LayerNorm(n_embedding[0])\n",
    "        self.fc2 = nn.Linear(n_embedding[0], n_embedding[1])\n",
    "        self.bn2 = nn.LayerNorm(n_embedding[1])\n",
    "        self.fc3 = nn.Linear(n_embedding[1], n_embedding[2])\n",
    "        \n",
    "        self.dp = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self, x, isdp=False):\n",
    "        if isdp:\n",
    "            x = self.dp(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=x.device) * -emb)\n",
    "        emb = x.unsqueeze(-1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "        return emb\n",
    "\n",
    "import torch.optim as optim   \n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "# OT refinement function\n",
    "def refine_with_ot(sc_coords, st_coords, n_steps=50, lr=1e-2):\n",
    "    \"\"\"\n",
    "    Refines SC coordinates by minimizing entropic OT divergence to ST coords.\n",
    "    sc_coords: Tensor (N,2) initial SC coordinates\n",
    "    st_coords: Tensor (M,2) ST spot coordinates\n",
    "    \"\"\"\n",
    "    sinkhorn = SamplesLoss(\"sinkhorn\", p=2, blur=0.05, scaling=0.9)\n",
    "    coords = sc_coords.clone().detach().requires_grad_(True)\n",
    "    optimizer = optim.Adam([coords], lr=lr)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss_ot = sinkhorn(coords.unsqueeze(0), st_coords.unsqueeze(0))\n",
    "        loss_ot.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return coords.detach()\n",
    "    \n",
    "class OTGuidedSampler:\n",
    "    def __init__(self,\n",
    "                 T_opt: torch.Tensor,\n",
    "                 st_coords_norm: torch.Tensor,\n",
    "                 n_timesteps: int):\n",
    "        self.T_opt       = T_opt           # (n_sc, n_st)\n",
    "        self.st_coords   = st_coords_norm  # (n_st, 2)\n",
    "        self.n_timesteps = n_timesteps\n",
    "\n",
    "    def get_ot_guidance(self, sc_indices: List[int], t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the expected OT‐based target for each sc index,\n",
    "        plus a small decaying jitter.\n",
    "        \"\"\"\n",
    "        if self.T_opt is None:\n",
    "            return None\n",
    "\n",
    "        guidance = []\n",
    "        for sc_idx in sc_indices:\n",
    "            st_w = self.T_opt[sc_idx]                 # (n_st,)\n",
    "            total = st_w.sum()\n",
    "            if total <= 0:\n",
    "                guidance.append(torch.zeros(2, device=self.st_coords.device))\n",
    "                continue\n",
    "            # expected spot location (no argmax sampling)\n",
    "            w_norm = st_w / total                    # normalize\n",
    "            target_mean = (w_norm.unsqueeze(1) * self.st_coords).sum(dim=0)\n",
    "            # decaying noise: maximal when t≈T, zero at t=0\n",
    "            noise_scale = 0.02 * (t / self.n_timesteps)\n",
    "            jitter = torch.randn_like(target_mean) * noise_scale\n",
    "            guidance.append(target_mean + jitter)\n",
    "\n",
    "        return torch.stack(guidance)  # (batch_size, 2)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GeometricAttentionBlock(nn.Module):\n",
    "    \"\"\"Attention mechanism that respects spatial relationships via continuous bias.\"\"\"\n",
    "    def __init__(self, dim: int, num_heads: int = 8, init_temperature: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim       = dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Standard MHA (batch_first=True means inputs are [B, L, E], here B=1)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True,\n",
    "            bias=True,\n",
    "            add_bias_kv=False,\n",
    "            add_zero_attn=False\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # single learnable temperature\n",
    "        self.temperature = nn.Parameter(torch.tensor(init_temperature))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, coords: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:      (N, dim)   feature vectors for N cells\n",
    "        coords: (N, 2)     normalized XY coords in [-1,1]\n",
    "        \"\"\"\n",
    "        # 1) Norm + reshape to “batch of one sequence”\n",
    "        x_norm = self.norm(x)            # (N, dim)\n",
    "        seq    = x_norm.unsqueeze(0)     # (1, N, dim)\n",
    "\n",
    "        # 2) Build an additive mask from distances\n",
    "        with torch.no_grad():\n",
    "            d      = torch.cdist(coords, coords, p=2)      # (N, N)\n",
    "            d_max  = d.max().clamp(min=1e-6)\n",
    "            nd     = d / d_max                              # normalized [0,1]\n",
    "            head_d = (self.dim // self.num_heads) ** 0.5\n",
    "            # negative bias that “pulls down” far-apart pairs\n",
    "            bias2d = - nd / (self.temperature.abs() + 1e-6) / head_d  # (N, N)\n",
    "\n",
    "        # 3) Feed it in as attn_mask (float mask): it’s added into QKᵀ/√d\n",
    "        #    MHA accepts a 2D mask of shape (L, L) or 3D (B, L, L).\n",
    "        attn_out, _ = self.attn(\n",
    "            seq, seq, seq,\n",
    "            attn_mask=bias2d,      # <— continuous additive bias\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        # 4) Residual\n",
    "        return x + attn_out.squeeze(0)\n",
    "\n",
    "\n",
    "class CellTypeEmbedding(nn.Module):\n",
    "    \"\"\"Learned embeddings for cell types\"\"\"\n",
    "    def __init__(self, num_cell_types, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_cell_types, embedding_dim)\n",
    "        \n",
    "    def forward(self, cell_type_indices):\n",
    "        return self.embedding(cell_type_indices)\n",
    "\n",
    "class UncertaintyHead(nn.Module):\n",
    "    \"\"\"Predicts coordinate uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Uncertainty for x and y\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softplus(self.net(x)) + 0.01  # Ensure positive uncertainty\n",
    "\n",
    "class PhysicsInformedLayer(nn.Module):\n",
    "    \"\"\"Incorporates cell non-overlap constraints\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.radius_predictor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.repulsion_strength = nn.Parameter(torch.tensor(0.1))\n",
    "        \n",
    "    def compute_repulsion_gradient(self, coords, radii, cell_types=None):\n",
    "        \"\"\"Compute repulsion forces between cells\"\"\"\n",
    "        batch_size = coords.shape[0]\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        distances = torch.cdist(coords, coords, p=2)\n",
    "        \n",
    "        # Compute sum of radii for each pair\n",
    "        radii_sum = radii + radii.T\n",
    "        \n",
    "        # Compute overlap (positive when cells overlap)\n",
    "        overlap = F.relu(radii_sum - distances + 1e-6)\n",
    "        \n",
    "        # Mask out self-interactions\n",
    "        mask = (1 - torch.eye(batch_size, device=coords.device))\n",
    "        overlap = overlap * mask\n",
    "        \n",
    "        # Compute repulsion forces\n",
    "        coord_diff = coords.unsqueeze(1) - coords.unsqueeze(0)  # (B, B, 2)\n",
    "        distances_safe = distances + 1e-6  # Avoid division by zero\n",
    "        \n",
    "        # Normalize direction vectors\n",
    "        directions = coord_diff / distances_safe.unsqueeze(-1)\n",
    "        \n",
    "        # Apply stronger repulsion for same cell types (optional)\n",
    "        if cell_types is not None:\n",
    "            same_type_mask = (cell_types.unsqueeze(1) == cell_types.unsqueeze(0)).float()\n",
    "            repulsion_weight = 1.0 + 0.5 * same_type_mask  # 50% stronger for same type\n",
    "        else:\n",
    "            # repulsion_weight = 1.0\n",
    "            batch_size = coords.shape[0]\n",
    "            repulsion_weight = torch.ones(batch_size, batch_size, device=coords.device)\n",
    "            \n",
    "        # Compute repulsion magnitude\n",
    "        repulsion_magnitude = overlap.unsqueeze(-1) * repulsion_weight.unsqueeze(-1)\n",
    "        \n",
    "        # Sum repulsion forces from all other cells\n",
    "        repulsion_forces = (repulsion_magnitude * directions * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        \n",
    "        return repulsion_forces\n",
    "        \n",
    "    def forward(self, coords, features, cell_types=None):\n",
    "        # Predict cell radii based on features\n",
    "        radii = self.radius_predictor(features).squeeze(-1) * 0.01  # Scale to reasonable size\n",
    "        \n",
    "        # Compute repulsion gradient\n",
    "        repulsion_grad = self.compute_repulsion_gradient(coords, radii, cell_types)\n",
    "        \n",
    "        return repulsion_grad * self.repulsion_strength, radii\n",
    "    \n",
    "class SpatialBatchSampler:\n",
    "    \"\"\"Sample spatially contiguous batches for geometric attention\"\"\"\n",
    "    \n",
    "    def __init__(self, coordinates, batch_size, k_neighbors=None):\n",
    "        \"\"\"\n",
    "        coordinates: (N, 2) array of spatial coordinates\n",
    "        batch_size: size of each batch\n",
    "        k_neighbors: number of neighbors to precompute (default: batch_size)\n",
    "        \"\"\"\n",
    "        self.coordinates = coordinates\n",
    "        self.batch_size = batch_size\n",
    "        self.k_neighbors = k_neighbors or min(batch_size, len(coordinates))\n",
    "        \n",
    "        # Precompute nearest neighbors\n",
    "        self.nbrs = NearestNeighbors(\n",
    "            n_neighbors=self.k_neighbors, \n",
    "            algorithm='kd_tree'\n",
    "        ).fit(coordinates)\n",
    "        \n",
    "    def sample_spatial_batch(self):\n",
    "        \"\"\"Sample a spatially contiguous batch\"\"\"\n",
    "        # Pick random center point\n",
    "        center_idx = np.random.randint(len(self.coordinates))\n",
    "        \n",
    "        # Get k nearest neighbors\n",
    "        distances, indices = self.nbrs.kneighbors(\n",
    "            self.coordinates[center_idx:center_idx+1], \n",
    "            return_distance=True\n",
    "        )\n",
    "        \n",
    "        # Return indices as torch tensor\n",
    "        batch_indices = torch.tensor(indices.flatten()[:self.batch_size], dtype=torch.long)\n",
    "        return batch_indices\n",
    "\n",
    "# =====================================================\n",
    "# PART 2: Hierarchical Diffusion Architecture\n",
    "# =====================================================\n",
    "\n",
    "class HierarchicalDiffusionBlock(nn.Module):\n",
    "    \"\"\"Multi-scale diffusion block for coarse-to-fine generation\"\"\"\n",
    "    def __init__(self, dim, num_scales=3):\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        \n",
    "        # Coarse-level predictor (for clusters/regions)\n",
    "        self.coarse_net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Fine-level predictor (for individual cells)\n",
    "        self.fine_net = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim * 2),  # Takes both coarse and fine features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Scale mixing weights\n",
    "        self.scale_mixer = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Takes timestep\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_scales),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, coarse_context=None):\n",
    "        # Determine scale weights based on timestep\n",
    "        scale_weights = self.scale_mixer(t.unsqueeze(-1))\n",
    "        \n",
    "        # Coarse prediction\n",
    "        coarse_pred = self.coarse_net(x)\n",
    "        \n",
    "        # Fine prediction (conditioned on coarse if available)\n",
    "        if coarse_context is not None:\n",
    "            fine_input = torch.cat([x, coarse_context], dim=-1)\n",
    "        else:\n",
    "            fine_input = torch.cat([x, coarse_pred], dim=-1)\n",
    "        fine_pred = self.fine_net(fine_input)\n",
    "        \n",
    "        # Mix scales based on timestep\n",
    "        output = scale_weights[:, 0:1] * coarse_pred + scale_weights[:, 1:2] * fine_pred\n",
    "        \n",
    "        return output  \n",
    "\n",
    "def barrier_loss(coords, r_min):\n",
    "    \"\"\"\n",
    "    coords: Tensor of shape (N, 2) — final denoised coordinates for this batch\n",
    "    r_min: minimum allowed distance between any two cells\n",
    "    Returns: mean squared violation of the per‐cell nearest‐neighbor barrier\n",
    "    \"\"\"\n",
    "    # compute N×N pairwise distances\n",
    "    dists = torch.cdist(coords, coords, p=2)  # shape (N, N)\n",
    "\n",
    "    # exclude self‐distances by setting diagonal to a huge value\n",
    "    N = coords.size(0)\n",
    "    dists = dists + torch.eye(N, device=coords.device) * 1e6\n",
    "\n",
    "    # for each cell, find its nearest neighbor distance\n",
    "    min_per_cell = dists.min(dim=1)[0]        # shape (N,)\n",
    "\n",
    "    # violation is how much under r_min each cell is\n",
    "    violation = F.relu(r_min - min_per_cell)   # shape (N,)\n",
    "\n",
    "    # return mean squared violation\n",
    "    return (violation ** 2).mean()\n",
    "\n",
    "def sliced_wasserstein_loss(x_real, x_gen, n_projections=50):\n",
    "    \"\"\"\n",
    "    x_real, x_gen: (B, N, 2) real vs. generated coords\n",
    "    Returns: scalar SW loss\n",
    "    \"\"\"\n",
    "    import math\n",
    "    B, N, _ = x_real.shape\n",
    "    loss = 0.0\n",
    "\n",
    "    # sample random unit vectors in R²\n",
    "    thetas = torch.rand(n_projections, device=x_real.device) * 2*math.pi\n",
    "    dirs = torch.stack([thetas.cos(), thetas.sin()], dim=1)  # (P, 2)\n",
    "\n",
    "    for u in dirs:\n",
    "        # project to 1D: (B, N)\n",
    "        p_r = x_real @ u\n",
    "        p_g = x_gen  @ u\n",
    "\n",
    "        # sort along the N dimension\n",
    "        pr_sorted, _ = torch.sort(p_r, dim=1)\n",
    "        pg_sorted, _ = torch.sort(p_g, dim=1)\n",
    "\n",
    "        # L1 between the sorted projections\n",
    "        loss += (pr_sorted - pg_sorted).abs().mean()\n",
    "\n",
    "    return loss / n_projections\n",
    "\n",
    "# =====================================================\n",
    "# PART 3: Main Advanced Diffusion Model\n",
    "# =====================================================\n",
    "\n",
    "class AdvancedHierarchicalDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        st_gene_expr,\n",
    "        st_coords,\n",
    "        sc_gene_expr,\n",
    "        cell_types_sc=None,  # Cell type labels for SC data\n",
    "        transport_plan=None,  # Optimal transport plan from domain alignment\n",
    "        D_st=None,\n",
    "        D_induced=None,\n",
    "        n_genes=None,\n",
    "        # n_embedding=128,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=200,\n",
    "        st_max_distance=None,\n",
    "        sc_max_distance=None,\n",
    "        sigma=3.0,\n",
    "        alpha=0.9,\n",
    "        mmdbatch=0.1,\n",
    "        batch_size=64,\n",
    "        device='cuda',\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=1000,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=512,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.1,\n",
    "        outf='output'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.diffusion_losses = {\n",
    "            'total': [],\n",
    "            'diffusion': [],\n",
    "            'struct': [],\n",
    "            'physics': [],\n",
    "            'uncertainty': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.mmdbatch = mmdbatch\n",
    "        self.n_embedding = n_embedding\n",
    "        \n",
    "        # Create output directory\n",
    "        self.outf = outf\n",
    "        if not os.path.exists(outf):\n",
    "            os.makedirs(outf)\n",
    "        \n",
    "        # Store data\n",
    "        self.st_gene_expr = torch.tensor(st_gene_expr, dtype=torch.float32).to(device)\n",
    "        self.st_coords = torch.tensor(st_coords, dtype=torch.float32).to(device)\n",
    "        self.sc_gene_expr = torch.tensor(sc_gene_expr, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Setup spatial sampling for geometric attention\n",
    "        # self.setup_spatial_sampling()\n",
    "        \n",
    "        # Temperature regularization for geometric attention\n",
    "        self.temp_weight_decay = 1e-4\n",
    "        \n",
    "        # Store transport plan if provided\n",
    "        self.transport_plan = torch.tensor(transport_plan, dtype=torch.float32).to(device) if transport_plan is not None else None\n",
    "        \n",
    "        # Process cell types\n",
    "        if cell_types_sc is not None:\n",
    "            # Convert cell type strings to indices\n",
    "            unique_cell_types = np.unique(cell_types_sc)\n",
    "            self.cell_type_to_idx = {ct: i for i, ct in enumerate(unique_cell_types)}\n",
    "            self.num_cell_types = len(unique_cell_types)\n",
    "            cell_type_indices = [self.cell_type_to_idx[ct] for ct in cell_types_sc]\n",
    "            self.sc_cell_types = torch.tensor(cell_type_indices, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            self.sc_cell_types = None\n",
    "            self.num_cell_types = 0\n",
    "            \n",
    "        # Store distance matrices\n",
    "        self.D_st = torch.tensor(D_st, dtype=torch.float32).to(device) if D_st is not None else None\n",
    "        self.D_induced = torch.tensor(D_induced, dtype=torch.float32).to(device) if D_induced is not None else None\n",
    "\n",
    "        # If D_st is not provided, calculate it from spatial coordinates\n",
    "        if self.D_st is None:\n",
    "            print(\"D_st not provided, calculating from spatial coordinates...\")\n",
    "            if isinstance(st_coords, torch.Tensor):\n",
    "                st_coords_np = st_coords.cpu().numpy()\n",
    "            else:\n",
    "                st_coords_np = st_coords\n",
    "            \n",
    "            D_st_np, st_max_distance = calculate_D_st_from_coords(\n",
    "                spatial_coords=st_coords_np, \n",
    "                k_neighbors=50, \n",
    "                graph_mode=\"distance\"\n",
    "            )\n",
    "            self.D_st = torch.tensor(D_st_np, dtype=torch.float32).to(device)\n",
    "            self.st_max_distance = st_max_distance\n",
    "            print(f\"D_st calculated, shape: {self.D_st.shape}\")\n",
    "\n",
    "        # If D_induced is not provided, calculate it using fused Gromov-Wasserstein\n",
    "        if self.D_induced is None and transport_plan is None:\n",
    "            print(\"D_induced not provided, calculating using Fused Gromov-Wasserstein...\")\n",
    "            try:\n",
    "                # Calculate using fused GW if available\n",
    "                T_opt, D_sc, D_st_calc, D_induced_calc, _, sc_max_dist, st_max_dist = fused_gw_torch(\n",
    "                    X_sc=self.sc_gene_expr,\n",
    "                    X_st=self.st_gene_expr, \n",
    "                    Y_st=self.st_coords,\n",
    "                    alpha=0.9,\n",
    "                    k=100,\n",
    "                    device=device\n",
    "                )\n",
    "                self.D_induced = D_induced_calc\n",
    "                self.transport_plan = T_opt\n",
    "                if self.D_st is None:  # Use the calculated D_st if we don't have one\n",
    "                    self.D_st = D_st_calc\n",
    "                print(f\"D_induced calculated using FGW, shape: {self.D_induced.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"FGW calculation failed: {e}\")\n",
    "                print(\"Computing simple D_induced approximation...\")\n",
    "                # Simple fallback: use identity matrix scaled by D_st\n",
    "                n_sc = self.sc_gene_expr.shape[0]\n",
    "                self.D_induced = torch.eye(n_sc, device=device) * self.D_st.mean()\n",
    "\n",
    "        print(f\"Final matrices - D_st: {self.D_st.shape if self.D_st is not None else None}, \"\n",
    "            f\"D_induced: {self.D_induced.shape if self.D_induced is not None else None}\")\n",
    "        \n",
    "        # Normalize coordinates\n",
    "        self.st_coords_norm, self.coords_center, self.coords_radius = self.normalize_coordinates_isotropic(self.st_coords)\n",
    "        \n",
    "        # Model parameters\n",
    "        self.n_genes = n_genes or st_gene_expr.shape[1]\n",
    "        \n",
    "        # ========== FEATURE ENCODER ==========\n",
    "        self.netE = self.build_feature_encoder(self.n_genes, n_embedding, dp)\n",
    "\n",
    "        self.train_log = os.path.join(outf, 'train.log')\n",
    "\n",
    "        \n",
    "        # ========== CELL TYPE EMBEDDING ==========\n",
    "\n",
    "        use_cell_types = (cell_types_sc is not None)  # Check if SC data has cell types\n",
    "        self.use_cell_types = use_cell_types\n",
    "\n",
    "        if self.num_cell_types > 0:\n",
    "            self.cell_type_embedding = CellTypeEmbedding(self.num_cell_types, n_embedding[-1] // 2)\n",
    "            total_feature_dim = n_embedding[-1] + n_embedding[-1] // 2\n",
    "        else:\n",
    "            self.cell_type_embedding = None\n",
    "            total_feature_dim = n_embedding[-1]\n",
    "            \n",
    "        # ========== HIERARCHICAL DIFFUSION COMPONENTS ==========\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Feature projection (includes cell type if available)\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # ========== HIERARCHICAL DENOISING BLOCKS ==========\n",
    "        self.hierarchical_blocks = nn.ModuleList([\n",
    "            HierarchicalDiffusionBlock(hidden_dim, num_hierarchical_scales)\n",
    "            for _ in range(n_denoising_blocks)\n",
    "        ])\n",
    "        \n",
    "        # ========== GEOMETRIC ATTENTION ==========\n",
    "        self.geometric_attention_blocks = nn.ModuleList([\n",
    "            GeometricAttentionBlock(hidden_dim, num_heads)\n",
    "            for _ in range(n_denoising_blocks // 2)\n",
    "        ])\n",
    "        \n",
    "        # ========== PHYSICS-INFORMED COMPONENTS ==========\n",
    "        self.physics_layer = PhysicsInformedLayer(hidden_dim)\n",
    "        \n",
    "        # ========== UNCERTAINTY QUANTIFICATION ==========\n",
    "        self.uncertainty_head = UncertaintyHead(hidden_dim)\n",
    "        \n",
    "        # ========== OPTIMAL TRANSPORT GUIDANCE ==========\n",
    "        if self.transport_plan is not None:\n",
    "            self.ot_guidance_strength = nn.Parameter(torch.tensor(0.1))\n",
    "            \n",
    "        # ========== OUTPUT LAYERS ==========\n",
    "        self.noise_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # Create noise schedule\n",
    "        self.noise_schedule = self.create_noise_schedule()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.setup_optimizers(lr_e, lr_d)\n",
    "        \n",
    "        # MMD Loss for domain alignment\n",
    "        self.mmd_loss = MMDLoss()\n",
    "\n",
    "        # Move entire model to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def setup_spatial_sampling(self):\n",
    "        if hasattr(self, 'st_coords_norm'):\n",
    "            self.spatial_sampler = SpatialBatchSampler(\n",
    "                coordinates=self.st_coords_norm.cpu().numpy(),\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_sampler = None\n",
    "\n",
    "    def get_spatial_batch(self):\n",
    "        \"\"\"Get spatially contiguous batch for training\"\"\"\n",
    "        if self.spatial_sampler is not None:\n",
    "            return self.spatial_sampler.sample_spatial_batch()\n",
    "        else:\n",
    "            # Fallback to random sampling\n",
    "            return torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "        \n",
    "    def normalize_coordinates_isotropic(self, coords):\n",
    "        \"\"\"Normalize coordinates isotropically to [-1, 1]\"\"\"\n",
    "        center = coords.mean(dim=0)\n",
    "        centered_coords = coords - center\n",
    "        max_dist = torch.max(torch.norm(centered_coords, dim=1))\n",
    "        normalized_coords = centered_coords / (max_dist + 1e-8)\n",
    "        return normalized_coords, center, max_dist\n",
    "        \n",
    "\n",
    "    def build_feature_encoder(self, n_genes, n_embedding, dp):\n",
    "        \"\"\"Build the feature encoder network\"\"\"\n",
    "        return FeatureNet(n_genes, n_embedding=n_embedding, dp=dp).to(self.device)\n",
    "        \n",
    "    def create_noise_schedule(self):\n",
    "        \"\"\"Create the noise schedule for diffusion\"\"\"\n",
    "        betas = torch.linspace(0.0001, 0.02, self.n_timesteps, device=self.device)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'sqrt_alphas_cumprod': torch.sqrt(alphas_cumprod),\n",
    "            'sqrt_one_minus_alphas_cumprod': torch.sqrt(1 - alphas_cumprod)\n",
    "        }\n",
    "        \n",
    "    def setup_optimizers(self, lr_e, lr_d):\n",
    "        \"\"\"Setup optimizers and schedulers\"\"\"\n",
    "        # Encoder optimizer\n",
    "        self.optimizer_E = torch.optim.AdamW(self.netE.parameters(), lr=0.002)               \n",
    "        self.scheduler_E = lr_scheduler.StepLR(self.optimizer_E, step_size=200, gamma=0.5) \n",
    "\n",
    "        # MMD Loss\n",
    "        self.mmd_fn = MMDLoss()   \n",
    "        \n",
    "        # Diffusion model optimizer\n",
    "        diff_params = []\n",
    "        diff_params.extend(self.time_embed.parameters())\n",
    "        diff_params.extend(self.coord_encoder.parameters())\n",
    "        diff_params.extend(self.feat_proj.parameters())\n",
    "        diff_params.extend(self.hierarchical_blocks.parameters())\n",
    "        diff_params.extend(self.geometric_attention_blocks.parameters())\n",
    "        diff_params.extend(self.physics_layer.parameters())\n",
    "        diff_params.extend(self.uncertainty_head.parameters())\n",
    "        diff_params.extend(self.noise_predictor.parameters())\n",
    "        \n",
    "        if self.cell_type_embedding is not None:\n",
    "            diff_params.extend(self.cell_type_embedding.parameters())\n",
    "            \n",
    "        if self.transport_plan is not None:\n",
    "            diff_params.append(self.ot_guidance_strength)\n",
    "            \n",
    "        self.optimizer_diff = torch.optim.Adam(diff_params, lr=lr_d, betas=(0.9, 0.999))\n",
    "        self.scheduler_diff = lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer_diff, T_0=500)\n",
    "        \n",
    "    def add_noise(self, coords, t, noise_schedule):\n",
    "        \"\"\"Add noise to coordinates according to the diffusion schedule\"\"\"\n",
    "        noise = torch.randn_like(coords)\n",
    "        sqrt_alphas_cumprod_t = noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "        \n",
    "        noisy_coords = sqrt_alphas_cumprod_t * coords + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return noisy_coords, noise\n",
    "        \n",
    "    def compute_ot_guidance(self, coords_sc, features_sc):\n",
    "        \"\"\"Compute guidance from optimal transport plan\"\"\"\n",
    "        if self.transport_plan is None:\n",
    "            return torch.zeros_like(coords_sc)\n",
    "            \n",
    "        # Compute target positions based on transport plan\n",
    "        # T_star: (n_sc, n_st), st_coords_norm: (n_st, 2)\n",
    "        target_positions = torch.matmul(self.transport_plan, self.st_coords_norm)\n",
    "        \n",
    "        # Compute attraction force towards target positions\n",
    "        attraction = target_positions - coords_sc\n",
    "        \n",
    "        return attraction * self.ot_guidance_strength\n",
    "        \n",
    "    def forward_diffusion(self, noisy_coords, t, features, cell_types=None):\n",
    "        \"\"\"Forward pass through the advanced diffusion model\"\"\"\n",
    "        batch_size = noisy_coords.shape[0]\n",
    "        \n",
    "        # Encode inputs\n",
    "        time_emb = self.time_embed(t)\n",
    "        coord_emb = self.coord_encoder(noisy_coords)\n",
    "        \n",
    "        # Process features with optional cell type\n",
    "        if cell_types is not None and self.cell_type_embedding is not None:\n",
    "            cell_type_emb = self.cell_type_embedding(cell_types)\n",
    "            combined_features = torch.cat([features, cell_type_emb], dim=-1)\n",
    "        else:\n",
    "            #when no cell types, pad with zeros to match expected input size\n",
    "            if self.cell_type_embedding is not None:\n",
    "                #create zero padding for cell type embedding\n",
    "                cell_type_dim = self.n_embedding[-1] // 2\n",
    "                zero_padding = torch.zeros(batch_size, cell_type_dim, device=features.device)\n",
    "                combined_features = torch.cat([features, zero_padding], dim=-1)\n",
    "            else:\n",
    "                combined_features = features\n",
    "            # combined_features = features\n",
    "            \n",
    "        feat_emb = self.feat_proj(combined_features)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        h = coord_emb + time_emb + feat_emb\n",
    "        \n",
    "        # Process through hierarchical blocks with geometric attention\n",
    "        for i, block in enumerate(self.hierarchical_blocks):\n",
    "            h = block(h, t)\n",
    "                \n",
    "        # Predict noise\n",
    "        noise_pred = self.noise_predictor(h)\n",
    "        \n",
    "        # Compute physics-informed correction\n",
    "        physics_correction, cell_radii = self.physics_layer(noisy_coords, h, cell_types)\n",
    "        \n",
    "        # Compute uncertainty\n",
    "        uncertainty = self.uncertainty_head(h)\n",
    "        \n",
    "        # Apply corrections based on timestep (less physics at high noise)\n",
    "        # t_factor = 1 - t / self.n_timesteps  # 0 at start, 1 at end\n",
    "        # noise_pred = noise_pred + t_factor * physics_correction * 0.1\n",
    "        t_factor = (1 - t).unsqueeze(-1) #shape: (natch_size, 1)\n",
    "        noise_pred = noise_pred + t_factor * physics_correction * 0.1\n",
    "        \n",
    "        return noise_pred, uncertainty, cell_radii\n",
    "        \n",
    "    def train_encoder(self, n_epochs=1000, ratio_start=0, ratio_end=1.0):\n",
    "        \"\"\"Train the STEM encoder to align ST and SC data\"\"\"\n",
    "        print(\"Training STEM encoder...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting STEM encoder training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, ratio_start={ratio_start}, ratio_end={ratio_end}\\n\")\n",
    "        \n",
    "        # Calculate spatial adjacency matrix\n",
    "        if self.sigma == 0:\n",
    "            nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        else:\n",
    "            nettrue = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                self.st_coords.cpu().numpy(), \n",
    "                self.st_coords.cpu().numpy()\n",
    "            ), device=self.device).to(torch.float32)\n",
    "            \n",
    "            sigma = self.sigma\n",
    "            nettrue = torch.exp(-nettrue**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "            nettrue = F.normalize(nettrue, p=1, dim=1)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Schedule for circle loss weight\n",
    "            ratio = ratio_start + (ratio_end - ratio_start) * min(epoch / (n_epochs * 0.8), 1.0)\n",
    "            \n",
    "            # Forward pass ST data\n",
    "            e_seq_st = self.netE(self.st_gene_expr, True)\n",
    "            \n",
    "            # Sample from SC data due to large size\n",
    "            sc_idx = torch.randint(0, self.sc_gene_expr.shape[0], (min(self.batch_size, self.mmdbatch),), device=self.device)\n",
    "            sc_batch = self.sc_gene_expr[sc_idx]\n",
    "            e_seq_sc = self.netE(sc_batch, False)\n",
    "            \n",
    "            # Calculate losses\n",
    "            self.optimizer_E.zero_grad()\n",
    "            \n",
    "            # Prediction loss (equivalent to netpred in STEM)\n",
    "            netpred = e_seq_st.mm(e_seq_st.t())\n",
    "            loss_E_pred = F.cross_entropy(netpred, nettrue, reduction='mean')\n",
    "            \n",
    "            # Mapping matrices\n",
    "            st2sc = F.softmax(e_seq_st.mm(e_seq_sc.t()), dim=1)\n",
    "            sc2st = F.softmax(e_seq_sc.mm(e_seq_st.t()), dim=1)\n",
    "            \n",
    "            # Circle loss\n",
    "            st2st = torch.log(st2sc.mm(sc2st) + 1e-7)\n",
    "            loss_E_circle = F.kl_div(st2st, nettrue, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # MMD loss\n",
    "            ranidx = torch.randint(0, e_seq_sc.shape[0], (min(self.mmdbatch, e_seq_sc.shape[0]),), device=self.device)\n",
    "            loss_E_mmd = self.mmd_fn(e_seq_st, e_seq_sc[ranidx])\n",
    "            \n",
    "            # Total loss\n",
    "            loss_E = loss_E_pred + self.alpha * loss_E_mmd + ratio * loss_E_circle\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_E.backward()\n",
    "            self.optimizer_E.step()\n",
    "            self.scheduler_E.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Encoder epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss_E: {loss_E.item():.6f}, \"\n",
    "                          f\"Loss_E_pred: {loss_E_pred.item():.6f}, \"\n",
    "                          f\"Loss_E_circle: {loss_E_circle.item():.6f}, \"\n",
    "                          f\"Loss_E_mmd: {loss_E_mmd.item():.6f}, \"\n",
    "                          f\"Ratio: {ratio:.4f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'netE_state_dict': self.netE.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_E.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_E.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'encoder_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Save final encoder\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_encoder.pt'))\n",
    "        \n",
    "        print(\"Encoder training complete!\")\n",
    "                \n",
    "    def train_diffusion(self, n_epochs=2000, lambda_struct=10.0, lambda_physics=1.0, lambda_uncertainty=0.1):\n",
    "        \"\"\"Train the advanced diffusion model\"\"\"\n",
    "        print(\"Training advanced hierarchical diffusion model...\")\n",
    "        \n",
    "        # Freeze encoder\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Precompute adjacency matrix\n",
    "        def compute_adjacency_matrix(distances, sigma=3.0):\n",
    "            weights = torch.exp(-(distances ** 2) / (2 * sigma * sigma))\n",
    "            weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "            row_sums = weights.sum(dim=1, keepdim=True)\n",
    "            row_sums = torch.clamp(row_sums, min=1e-10)\n",
    "            adjacency = weights / (row_sums + 1e-8)\n",
    "            return adjacency\n",
    "            \n",
    "        st_adj = compute_adjacency_matrix(self.D_st, sigma=self.sigma)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch\n",
    "            # idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "            # Get spatially contiguous batch\n",
    "            if hasattr(self, 'spatial_sampler') and self.spatial_sampler is not None:\n",
    "                idx = self.get_spatial_batch().to(self.device)\n",
    "            else:\n",
    "                # Fallback to random sampling\n",
    "                idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size].to(self.device)\n",
    "            coords = self.st_coords_norm[idx]\n",
    "            features = self.st_gene_expr[idx]\n",
    "            sub_adj = st_adj[idx][:, idx]\n",
    "            sub_adj = sub_adj / (sub_adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            # Sample timesteps with curriculum\n",
    "            if epoch < n_epochs // 3:\n",
    "                # Early training: focus on high noise\n",
    "                t = torch.randint(int(0.7 * self.n_timesteps), self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            elif epoch < 2 * n_epochs // 3:\n",
    "                # Mid training: balanced\n",
    "                t = torch.randint(0, self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            else:\n",
    "                # Late training: focus on low noise (refinement)\n",
    "                t = torch.randint(0, int(0.3 * self.n_timesteps), (self.batch_size,), device=self.device)\n",
    "                \n",
    "            # Add noise\n",
    "            noisy_coords, target_noise = self.add_noise(coords, t, self.noise_schedule)\n",
    "            \n",
    "            # Get encoded features\n",
    "            with torch.no_grad():\n",
    "                encoded_features = self.netE(features)\n",
    "                \n",
    "            # Forward pass\n",
    "            pred_noise, uncertainty, cell_radii = self.forward_diffusion(\n",
    "                noisy_coords, \n",
    "                t.float() / self.n_timesteps, \n",
    "                encoded_features,\n",
    "                cell_types=None  # ST data doesn't have cell types\n",
    "            )\n",
    "            \n",
    "            # Compute losses\n",
    "            # 1. Diffusion loss\n",
    "            diffusion_loss = F.mse_loss(pred_noise, target_noise)\n",
    "            \n",
    "            # 2. Structure loss\n",
    "            sqrt_alphas_cumprod_t = self.noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = self.noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "            pred_coords = (noisy_coords - sqrt_one_minus_alphas_cumprod_t * pred_noise) / sqrt_alphas_cumprod_t\n",
    "            \n",
    "            pred_distances = torch.cdist(pred_coords, pred_coords, p=2)\n",
    "            pred_adj = compute_adjacency_matrix(pred_distances, sigma=self.sigma)\n",
    "            \n",
    "            struct_loss = F.kl_div(\n",
    "                torch.log(pred_adj + 1e-10),\n",
    "                sub_adj,\n",
    "                reduction='batchmean'\n",
    "            ) \n",
    "\n",
    "\n",
    "            # ——— DEBUG true nearest‐neighbor stats ———\n",
    "            N = pred_distances.size(0)     # number of points in this batch\n",
    "            # add a huge value on the diag so we exclude self‐distances\n",
    "            excl = pred_distances + torch.eye(N, device=pred_distances.device) * 1e6\n",
    "            min_per_point = excl.min(dim=1)[0]  # shape (N,)\n",
    "            min_dist = min_per_point.min().item()\n",
    "            p10      = torch.quantile(min_per_point, 0.1).item()\n",
    "            # if epoch % 50 == 0:\n",
    "            #     print(f\"[debug] epoch={epoch:4d} | min NN dist={min_dist:.4f} | 10%-ile={p10:.4f}\")\n",
    "\n",
    "            r_min = p10\n",
    "            # 3) Physics/barrier loss\n",
    "            physics_loss = barrier_loss(pred_coords, r_min)\n",
    "            \n",
    "            # 4. Uncertainty regularization\n",
    "            uncertainty_loss = uncertainty.mean()  # Encourage lower uncertainty\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = (\n",
    "                diffusion_loss + \n",
    "                lambda_struct * struct_loss + \n",
    "                lambda_physics * physics_loss +\n",
    "                lambda_uncertainty * uncertainty_loss\n",
    "            )\n",
    "\n",
    "            self.diffusion_losses['total'].append(total_loss.item())\n",
    "            self.diffusion_losses['diffusion'].append(diffusion_loss.item())\n",
    "            self.diffusion_losses['struct'].append(struct_loss.item())\n",
    "            self.diffusion_losses['physics'].append(physics_loss.item())\n",
    "            self.diffusion_losses['uncertainty'].append(uncertainty_loss.item())\n",
    "            self.diffusion_losses['epochs'].append(epoch)\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer_diff.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                [p for p in self.parameters() if p.requires_grad], \n",
    "                1.0\n",
    "            )\n",
    "            self.optimizer_diff.step()\n",
    "            self.scheduler_diff.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                best_state = self.state_dict()\n",
    "                \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Total loss = {total_loss.item():.6f}, \"\n",
    "                      f\"Diff = {diffusion_loss.item():.6f}, \"\n",
    "                      f\"Struct = {struct_loss.item():.6f}, \"\n",
    "                      f\"Physics = {physics_loss.item():.6f}, \"\n",
    "                      f\"Uncertainty = {uncertainty_loss.item():.6f}\")\n",
    "                \n",
    "        # Load best model\n",
    "        if best_state is not None:\n",
    "            self.load_state_dict(best_state)\n",
    "            \n",
    "        print(\"Advanced diffusion training complete!\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def _refine_with_ot(sc_coords, st_coords, T_opt, n_steps=30, lr=5e-3):\n",
    "        \"\"\"\n",
    "        Refine sc_coords to better match st_coords via entropic OT (Sinkhorn).\n",
    "        \"\"\"\n",
    "        # sinkhorn = SamplesLoss(\"sinkhorn\", p=2, blur=0.05, scaling=0.9)\n",
    "        coords = sc_coords.clone().detach().requires_grad_(True)\n",
    "        # optimizer = optim.Adam([coords], lr=lr)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            # optimizer.zero_grad()\n",
    "            # loss_ot = sinkhorn(coords.unsqueeze(0), st_coords.unsqueeze(0))\n",
    "            # loss_ot.backward()\n",
    "            # optimizer.step()\n",
    "            diff = coords.unsqueeze(1) - st_coords.unsqueeze(0) \n",
    "            grad = 2.0 * (T_opt.unsqueeze(2) * diff).sum(dim=1)\n",
    "            coords = coords - lr * grad\n",
    "        \n",
    "        return coords.detach()\n",
    "    \n",
    "    @staticmethod\n",
    "    def _refine_with_sinkhorn(sc_coords, st_coords, n_steps=30, lr=5e-3):\n",
    "        \"\"\"\n",
    "        Refine sc_coords to better match st_coords via entropic OT (Sinkhorn).\n",
    "        \"\"\"\n",
    "        sinkhorn = SamplesLoss(\"sinkhorn\", p=2, blur=0.05, scaling=0.9)\n",
    "        coords = sc_coords.clone().detach().requires_grad_(True)\n",
    "        optimizer = optim.Adam([coords], lr=lr)\n",
    "\n",
    "        for _ in range(n_steps):\n",
    "            optimizer.zero_grad()\n",
    "            loss_ot = sinkhorn(coords.unsqueeze(0), st_coords.unsqueeze(0))\n",
    "            loss_ot.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return coords.detach()\n",
    "\n",
    "    def sample_sc_coordinates(self, num_samples=5, n_steps=50, return_uncertainty=True):\n",
    "        \"\"\"\n",
    "        Generate SC coordinates via DDPM sampling, then optionally refine via OT.\n",
    "        \"\"\"\n",
    "        self.netE.eval()\n",
    "        all_samples       = []\n",
    "        all_uncertainties = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Pre-encode SC features once\n",
    "            sc_features = self.netE(self.sc_gene_expr.to(self.device))\n",
    "\n",
    "            for _ in range(num_samples):\n",
    "                coords = torch.randn(len(self.sc_gene_expr), 2, device=self.device)\n",
    "\n",
    "                # reverse diffusion chain\n",
    "                for t in reversed(range(self.n_timesteps)):\n",
    "                    t_batch = torch.full((len(coords),), t, device=self.device)\n",
    "                    pred_noise, uncertainty, _ = self.forward_diffusion(\n",
    "                        coords,\n",
    "                        t_batch.float() / self.n_timesteps,\n",
    "                        sc_features,\n",
    "                        self.sc_cell_types\n",
    "                    )\n",
    "                    alpha      = self.noise_schedule['alphas'][t]\n",
    "                    alpha_cp   = self.noise_schedule['alphas_cumprod'][t]\n",
    "                    beta       = self.noise_schedule['betas'][t]\n",
    "                    if t > 0:\n",
    "                        noise = torch.randn_like(coords)\n",
    "                        sigma = torch.sqrt(beta)\n",
    "                    else:\n",
    "                        noise, sigma = 0, 0\n",
    "\n",
    "                    # DDPM denoise step\n",
    "                    coords = (1.0 / torch.sqrt(alpha)) * (\n",
    "                        coords - (beta / torch.sqrt(1 - alpha_cp)) * pred_noise\n",
    "                    ) + sigma * noise\n",
    "\n",
    "                all_samples.append(coords.cpu())\n",
    "                all_uncertainties.append(uncertainty.cpu())\n",
    "\n",
    "        coords_mean      = torch.stack(all_samples).mean(0)\n",
    "        coords_std       = torch.stack(all_samples).std(0)\n",
    "        uncertainty_mean = torch.stack(all_uncertainties).mean(0)\n",
    "\n",
    "        # ——— OT refinement, if available ———\n",
    "        if getattr(self, 'transport_plan', None) is not None:\n",
    "            # get normalized spot coords\n",
    "            st_coords = self.st_coords_norm.to(self.device)\n",
    "            # refine the mean prediction\n",
    "            # refined = self._refine_with_ot(\n",
    "            #     torch.tensor(coords_mean, device=self.device),\n",
    "            #     st_coords,\n",
    "            #     n_steps=n_steps,\n",
    "            #     lr=5e-3\n",
    "            # )\n",
    "            refined = self._refine_with_ot(torch.tensor(coords_mean, device=self.device),\n",
    "                                           self.st_coords,\n",
    "                                           self.transport_plan,\n",
    "                                           n_steps = n_steps,\n",
    "                                           lr=5e-3)\n",
    "            coords_mean = refined.cpu().numpy()\n",
    "        else:\n",
    "            st_coords = self.st_coords_norm.to(self.device)\n",
    "            # refine the mean prediction\n",
    "            refined = self._refine_with_sinkhorn(\n",
    "                torch.tensor(coords_mean, device=self.device),\n",
    "                st_coords,\n",
    "                n_steps=n_steps,\n",
    "                lr=5e-3\n",
    "            )\n",
    "            coords_mean = refined.cpu().numpy()\n",
    "\n",
    "            # coords_mean = coords_mean.numpy()\n",
    "\n",
    "        coords_std = coords_std.numpy()\n",
    "        unc_mean  = uncertainty_mean.numpy()\n",
    "\n",
    "        if return_uncertainty:\n",
    "            return coords_mean, coords_std, unc_mean\n",
    "        else:\n",
    "            return coords_mean\n",
    "            \n",
    "    def train(self, encoder_epochs=1000, diffusion_epochs=2000, **kwargs):\n",
    "        \"\"\"Combined training pipeline\"\"\"\n",
    "        # Train encoder\n",
    "        self.train_encoder(n_epochs=encoder_epochs)\n",
    "        \n",
    "        # Train diffusion\n",
    "        self.train_diffusion(n_epochs=diffusion_epochs, **kwargs)\n",
    "\n",
    "\n",
    "    def plot_training_losses(self):\n",
    "        \"\"\"Plot training losses after training is complete\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Diffusion losses\n",
    "        if len(self.diffusion_losses['epochs']) > 0:\n",
    "            epochs_diff = np.array(self.diffusion_losses['epochs'])\n",
    "            ax1.plot(epochs_diff, self.diffusion_losses['total'], 'b-', label='Total Loss', linewidth=2)\n",
    "            ax1.plot(epochs_diff, self.diffusion_losses['diffusion'], 'k-', label='Diffusion Loss', linewidth=2)\n",
    "            ax1.plot(epochs_diff, self.diffusion_losses['struct'], 'r--', label='Structure Loss', alpha=0.8)\n",
    "            ax1.plot(epochs_diff, self.diffusion_losses['physics'], 'g--', label='Physics Loss', alpha=0.8)\n",
    "            ax1.plot(epochs_diff, self.diffusion_losses['uncertainty'], 'm--', label='Uncertainty Loss', alpha=0.8)\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Diffusion Training Losses')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            ax1.set_yscale('log')\n",
    "        \n",
    "        # Plot 2: Diffusion total loss smoothed\n",
    "        if len(self.diffusion_losses['epochs']) > 0:\n",
    "            window = min(50, len(self.diffusion_losses['total']) // 10)\n",
    "            if window > 1:\n",
    "                smoothed = np.convolve(self.diffusion_losses['total'], \n",
    "                                        np.ones(window)/window, mode='valid')\n",
    "                smooth_epochs = epochs_diff[window-1:]\n",
    "                ax2.plot(epochs_diff, self.diffusion_losses['total'], 'lightcoral', alpha=0.5, label='Raw')\n",
    "                ax2.plot(smooth_epochs, smoothed, 'red', linewidth=2, label=f'Smoothed (window={window})')\n",
    "            else:\n",
    "                ax2.plot(epochs_diff, self.diffusion_losses['total'], 'red', linewidth=2)\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Loss')\n",
    "            ax2.set_title('Diffusion Total Loss (Smoothed)')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final loss values\n",
    "        print(\"\\n=== Training Loss Summary ===\")\n",
    "        if len(self.diffusion_losses['total']) > 0:\n",
    "            print(f\"Diffusion - Initial Loss: {self.diffusion_losses['total'][0]:.6f}\")\n",
    "            print(f\"Diffusion - Final Loss: {self.diffusion_losses['total'][-1]:.6f}\")\n",
    "            print(f\"Diffusion - Loss Reduction: {(1 - self.diffusion_losses['total'][-1]/self.diffusion_losses['total'][0])*100:.2f}%\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# PART 4: MMD Loss Implementation\n",
    "# =====================================================\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        tmp = 0\n",
    "        for x in kernel_val:\n",
    "            tmp += x\n",
    "        return tmp\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data_individual_norm():\n",
    "    \"\"\"\n",
    "    Load and process cSCC data with individual normalization per ST dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data with individual normalization...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize expression data (same for all)\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "\n",
    "\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def normalize_coordinates_individually(coords):\n",
    "    \"\"\"\n",
    "    Normalize coordinates to [-1, 1] range individually.\n",
    "    \"\"\"\n",
    "    coords_min = coords.min(axis=0)\n",
    "    coords_max = coords.max(axis=0)\n",
    "    coords_range = coords_max - coords_min\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    coords_range[coords_range == 0] = 1.0\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    coords_normalized = 2 * (coords - coords_min) / coords_range - 1\n",
    "    \n",
    "    return coords_normalized, coords_min, coords_max, coords_range\n",
    "\n",
    "def prepare_individually_normalized_st_data(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Normalize each ST dataset individually, then combine.\n",
    "    \"\"\"\n",
    "    print(\"Preparing individually normalized ST data...\")\n",
    "    \n",
    "    # Get common genes\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates and normalize individually\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "    \n",
    "    print(\"Normalizing coordinates individually...\")\n",
    "    st1_coords_norm, st1_min, st1_max, st1_range = normalize_coordinates_individually(st1_coords)\n",
    "    st2_coords_norm, st2_min, st2_max, st2_range = normalize_coordinates_individually(st2_coords)\n",
    "    st3_coords_norm, st3_min, st3_max, st3_range = normalize_coordinates_individually(st3_coords)\n",
    "    \n",
    "    print(f\"ST1 coord range: [{st1_coords_norm.min():.3f}, {st1_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST2 coord range: [{st2_coords_norm.min():.3f}, {st2_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST3 coord range: [{st3_coords_norm.min():.3f}, {st3_coords_norm.max():.3f}]\")\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "    st_coords_combined = np.vstack([st1_coords_norm, st2_coords_norm, st3_coords_norm])\n",
    "    \n",
    "    # Create dataset metadata\n",
    "    dataset_info = {\n",
    "        'labels': (['dataset1'] * len(st1_expr) + \n",
    "                  ['dataset2'] * len(st2_expr) + \n",
    "                  ['dataset3'] * len(st3_expr)),\n",
    "        'normalization_params': {\n",
    "            'dataset1': {'min': st1_min, 'max': st1_max, 'range': st1_range},\n",
    "            'dataset2': {'min': st2_min, 'max': st2_max, 'range': st2_range},\n",
    "            'dataset3': {'min': st3_min, 'max': st3_max, 'range': st3_range}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_info, common_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data_individual_norm()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cell_interactions_advanced(scadata, coords_key='advanced_diffusion_coords_avg'):\n",
    "    \"\"\"Analyze cell-cell interactions using the advanced diffusion coordinates\"\"\"\n",
    "    \n",
    "    # Get coordinates from scadata\n",
    "    coords_mean = scadata.obsm[coords_key]\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    distances = squareform(pdist(coords_mean))\n",
    "    \n",
    "    # Get cell types\n",
    "    cell_types = scadata.obs['rough_celltype'].values\n",
    "    unique_types = np.unique(cell_types)\n",
    "    \n",
    "    # Analyze minimum distances between cell types\n",
    "    min_distances = {}\n",
    "    for i, type1 in enumerate(unique_types):\n",
    "        for j, type2 in enumerate(unique_types):\n",
    "            if i <= j:  # Include self-interactions\n",
    "                mask1 = cell_types == type1\n",
    "                mask2 = cell_types == type2\n",
    "                \n",
    "                if i == j:\n",
    "                    # Same cell type - exclude self\n",
    "                    sub_dist = distances[np.ix_(mask1, mask2)]\n",
    "                    np.fill_diagonal(sub_dist, np.inf)\n",
    "                    if sub_dist.size > 0:\n",
    "                        min_dist = np.min(sub_dist[sub_dist < np.inf])\n",
    "                    else:\n",
    "                        min_dist = np.nan\n",
    "                else:\n",
    "                    # Different cell types\n",
    "                    sub_dist = distances[np.ix_(mask1, mask2)]\n",
    "                    min_dist = np.min(sub_dist) if sub_dist.size > 0 else np.nan\n",
    "                \n",
    "                min_distances[(type1, type2)] = min_dist\n",
    "    \n",
    "    # Create interaction matrix visualization\n",
    "    interaction_matrix = np.full((len(unique_types), len(unique_types)), np.nan)\n",
    "    for i, type1 in enumerate(unique_types):\n",
    "        for j, type2 in enumerate(unique_types):\n",
    "            key = (type1, type2) if i <= j else (type2, type1)\n",
    "            if key in min_distances:\n",
    "                interaction_matrix[i, j] = min_distances[key]\n",
    "                interaction_matrix[j, i] = min_distances[key]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = ~np.isnan(interaction_matrix)\n",
    "    sns.heatmap(interaction_matrix, \n",
    "                annot=True, fmt='.3f', \n",
    "                xticklabels=unique_types,\n",
    "                yticklabels=unique_types,\n",
    "                cmap='coolwarm_r',\n",
    "                mask=~mask,\n",
    "                cbar_kws={'label': 'Minimum Distance'})\n",
    "    plt.title(f'Minimum Distances Between Cell Types ({coords_key})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return min_distances, interaction_matrix\n",
    "\n",
    "def visualize_advanced_results_multi_model(scadata):\n",
    "    \"\"\"Visualize results from multiple models with uncertainty analysis\"\"\"\n",
    "    \n",
    "    # Get coordinates from all models\n",
    "    coords_avg = scadata.obsm['diffusion_coords_avg']\n",
    "    coords_rep1 = scadata.obsm['diffusion_coords_rep1'] \n",
    "    coords_rep2 = scadata.obsm['diffusion_coords_rep2']\n",
    "    coords_rep3 = scadata.obsm['diffusion_coords_rep3']\n",
    "    \n",
    "    # Calculate uncertainty metrics across models\n",
    "    all_coords = np.stack([coords_rep1, coords_rep2, coords_rep3], axis=0)  # (3, n_cells, 2)\n",
    "    coords_std = np.std(all_coords, axis=0)  # Standard deviation across models\n",
    "    coords_var = np.var(all_coords, axis=0)  # Variance across models\n",
    "    \n",
    "    # Total variability (combining x and y dimensions)\n",
    "    total_std = np.sqrt(coords_std[:, 0]**2 + coords_std[:, 1]**2)\n",
    "    total_var = coords_var[:, 0] + coords_var[:, 1]\n",
    "    \n",
    "    # Create confidence scores (inverse of variability)\n",
    "    confidence = 1 / (1 + total_std)\n",
    "    scadata.obs['spatial_confidence'] = confidence\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Spatial coordinates colored by cell type\n",
    "    ax = axes[0, 0]\n",
    "    cell_types = scadata.obs['rough_celltype']\n",
    "    unique_types = cell_types.unique()\n",
    "    colors = sns.color_palette('tab20', n_colors=len(unique_types))\n",
    "    \n",
    "    for i, ct in enumerate(unique_types):\n",
    "        mask = cell_types == ct\n",
    "        ax.scatter(coords_avg[mask, 0], coords_avg[mask, 1], \n",
    "                  c=[colors[i]], label=ct, s=30, alpha=0.7)\n",
    "    ax.set_title('Averaged Spatial Coordinates by Cell Type', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    # 2. Model variability (standard deviation across 3 models)\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=total_std, cmap='viridis_r', \n",
    "                        s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Model Std Dev')\n",
    "    ax.set_title('Model Prediction Variability', fontsize=14)\n",
    "    \n",
    "    # 3. X vs Y coordinate uncertainty\n",
    "    ax = axes[0, 2]\n",
    "    scatter = ax.scatter(coords_std[:, 0], coords_std[:, 1], \n",
    "                        c=total_std, cmap='plasma', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Total Std')\n",
    "    ax.set_xlabel('X Coordinate Std')\n",
    "    ax.set_ylabel('Y Coordinate Std')\n",
    "    ax.set_title('Coordinate Uncertainty (X vs Y)', fontsize=14)\n",
    "    \n",
    "    # 4. Cell density heatmap\n",
    "    ax = axes[1, 0]\n",
    "    from scipy.stats import gaussian_kde\n",
    "    xy = coords_avg.T\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=z, cmap='hot', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Density')\n",
    "    ax.set_title('Cell Density (Averaged Coordinates)', fontsize=14)\n",
    "    \n",
    "    # 5. Confidence scores\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=confidence, cmap='RdYlGn', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Confidence')\n",
    "    ax.set_title('Prediction Confidence Across Models', fontsize=14)\n",
    "    \n",
    "    # 6. Model agreement visualization\n",
    "    ax = axes[1, 2]\n",
    "    # Show cells where models agree vs disagree\n",
    "    high_agreement = total_std < np.percentile(total_std, 25)  # Bottom 25%\n",
    "    low_agreement = total_std > np.percentile(total_std, 75)   # Top 25%\n",
    "    \n",
    "    ax.scatter(coords_avg[high_agreement, 0], coords_avg[high_agreement, 1], \n",
    "              c='green', s=20, alpha=0.7, label='High Agreement')\n",
    "    ax.scatter(coords_avg[low_agreement, 0], coords_avg[low_agreement, 1], \n",
    "              c='red', s=20, alpha=0.7, label='Low Agreement')\n",
    "    ax.scatter(coords_avg[~(high_agreement | low_agreement), 0], \n",
    "              coords_avg[~(high_agreement | low_agreement), 1], \n",
    "              c='gray', s=10, alpha=0.5, label='Medium Agreement')\n",
    "    ax.set_title('Model Agreement', fontsize=14)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, total_std, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_advanced_diffusion_models(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['advanced_diffusion_coords_avg']\n",
    "        models_all: All trained models for further analysis\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {dataset_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize AdvancedHierarchicalDiffusion model\n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,  # No cell type labels\n",
    "            transport_plan=None,  # No OT transport plan\n",
    "            D_st=None,           # No distance matrices\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],  # Same as STEMDiffusion\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=2.5,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=800,     # Same as STEMDiffusion\n",
    "            n_denoising_blocks=6,\n",
    "            hidden_dim=256,      # Same as STEMDiffusion\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{dataset_name}'\n",
    "        )\n",
    "        \n",
    "        print(f\"Training model for {dataset_name}...\")\n",
    "        \n",
    "        # Train the model with reduced epochs for speed\n",
    "        model.train(\n",
    "            encoder_epochs=1000,      # Reduced from 1500\n",
    "            diffusion_epochs=3000,   # Reduced from 3000\n",
    "            lambda_struct=5.0,\n",
    "            lambda_physics=2.0,\n",
    "            lambda_uncertainty=0.5\n",
    "        )\n",
    "\n",
    "        model.plot_training_losses()\n",
    "        \n",
    "        print(f\"Generating SC coordinates using model {i+1}...\")\n",
    "        \n",
    "        # Sample SC coordinates with fast sampling (fewer steps)\n",
    "        sc_coords = model.sample_sc_coordinates(\n",
    "            num_samples=1,          # Single sample for averaging\n",
    "            n_steps=30,\n",
    "            return_uncertainty=False      # Fast sampling with fewer steps\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        print(f\"Model {i+1} complete! Generated coordinates shape: {sc_coords.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average the results from all 3 models\n",
    "    print(f\"\\nAveraging results from {len(sc_coords_results)} models...\")\n",
    "    sc_coords_avg = np.mean(sc_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in sc_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    \n",
    "    # Optionally, save individual results too\n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}'] = coords\n",
    "    \n",
    "    print(f\"\\nAdvanced diffusion training complete!\")\n",
    "    print(f\"Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "    \n",
    "    return scadata, models_all\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Train individual AdvancedHierarchicalDiffusion models and get averaged results\n",
    "scadata, advanced_models = train_individual_advanced_diffusion_models(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")\n",
    "\n",
    "print(\"Advanced diffusion training complete! Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have: scadata.obsm['advanced_diffusion_coords_avg'], etc.\n",
    "\n",
    "print(\"\\n=== Advanced Analysis and Visualization ===\")\n",
    "\n",
    "# 1. Visualize advanced results with uncertainty analysis\n",
    "print(\"Creating advanced visualization plots...\")\n",
    "fig, model_uncertainty, confidence_scores = visualize_advanced_results_multi_model(scadata)\n",
    "\n",
    "# 2. Analyze cell interactions for averaged coordinates\n",
    "print(\"Analyzing cell interactions (averaged coordinates)...\")\n",
    "min_distances_avg, interaction_matrix_avg = analyze_cell_interactions_advanced(\n",
    "    scadata, coords_key='advanced_diffusion_coords_avg'\n",
    ")\n",
    "\n",
    "# 3. Optional: Analyze interactions for individual models too\n",
    "print(\"Analyzing cell interactions (individual models)...\")\n",
    "for i in range(1, 4):\n",
    "    print(f\"\\nModel {i} interactions:\")\n",
    "    min_distances, interaction_matrix = analyze_cell_interactions_advanced(\n",
    "        scadata, coords_key=f'advanced_diffusion_coords_rep{i}'\n",
    "    )\n",
    "\n",
    "# 4. Print summary statistics\n",
    "print(\"\\n=== Advanced Model Statistics ===\")\n",
    "print(f\"Total cells mapped: {len(scadata.obsm['advanced_diffusion_coords_avg'])}\")\n",
    "print(f\"Average model uncertainty: {model_uncertainty.mean():.4f}\")\n",
    "print(f\"Model uncertainty range: [{model_uncertainty.min():.4f}, {model_uncertainty.max():.4f}]\")\n",
    "print(f\"Average confidence: {confidence_scores.mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Cell Type Confidence ===\")\n",
    "for ct in scadata.obs['rough_celltype'].unique():\n",
    "    mask = scadata.obs['rough_celltype'] == ct\n",
    "    print(f\"{ct}: {mask.sum()} cells, \"\n",
    "          f\"avg confidence: {confidence_scores[mask].mean():.3f}, \"\n",
    "          f\"avg uncertainty: {model_uncertainty[mask].mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Physics Constraints (Averaged Coordinates) ===\")\n",
    "all_distances = []\n",
    "for key, dist in min_distances_avg.items():\n",
    "    if not np.isnan(dist):\n",
    "        all_distances.append(dist)\n",
    "        print(f\"Min distance {key[0]} - {key[1]}: {dist:.4f}\")\n",
    "\n",
    "if all_distances:\n",
    "    print(f\"\\nOverall minimum cell-cell distance: {np.min(all_distances):.4f}\")\n",
    "    print(f\"Cells with potential overlaps (< 0.01): {np.sum(np.array(all_distances) < 0.01)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scanpy as sc\n",
    "sc.settings.set_figure_params(dpi=100, facecolor='white')\n",
    "\n",
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='sb_coords', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep1', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep2', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep3', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata.obsm['sb_coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_advanced_diffusion_models(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset using SpaOTsc and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['diffusion_coords_avg']\n",
    "        T_all: All transport plans from SpaOTsc\n",
    "        D_induced_all: All induced distance matrices from SpaOTsc  \n",
    "        D_st_all: All ST distance matrices\n",
    "        D_sc_all: All SC distance matrices\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "\n",
    "    T_all = []\n",
    "    D_induced_all = []\n",
    "    D_st_all = []\n",
    "    D_sc_all = []\n",
    "\n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training Advanced Diffusion model {i+1}/3 for {dataset_name} using SpaOTsc\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_sc = torch.tensor(sc_expr, dtype=torch.float32).to(device)\n",
    "        X_st = torch.tensor(st_expr, dtype=torch.float32).to(device)\n",
    "        Y_st = torch.tensor(st_coords, dtype=torch.float32).to(device)\n",
    "        \n",
    "        print(f\"SC data shape: {X_sc.shape}\")\n",
    "        print(f\"ST data shape: {X_st.shape}\")\n",
    "        print(f\"ST coords shape: {Y_st.shape}\")\n",
    "        \n",
    "        # === REPLACE FUSED_GW_TORCH WITH SPAOTSC ===\n",
    "        print(f\"Running optimal transport for {dataset_name}...\")\n",
    "\n",
    "\n",
    "        T, D_sc, D_st, D_induced, fgw_dist, sc_max_dist, st_max_dist = fused_gw_torch(\n",
    "            X_sc=X_sc,\n",
    "            X_st=X_st,\n",
    "            Y_st=st_coords,\n",
    "            alpha=0.5,\n",
    "            k_sc=25, k_st=50,\n",
    "            epsilon=0.15,\n",
    "            max_iter=1000,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"FGW completed for {dataset_name}! Distance: {fgw_dist:.4f}\")\n",
    "\n",
    "        T_all.append(T.cpu().numpy())\n",
    "        D_induced_all.append(D_induced.cpu().numpy())\n",
    "        D_st_all.append(D_st.cpu().numpy())\n",
    "        D_sc_all.append(D_sc.cpu().numpy())\n",
    "\n",
    "        # === PREPARE CELL TYPE INFORMATION ===\n",
    "        # Extract cell types from scadata\n",
    "        if 'rough_celltype' in scadata.obs.columns:\n",
    "            cell_types_sc = scadata.obs['rough_celltype'].values\n",
    "            unique_cell_types = np.unique(cell_types_sc)\n",
    "            print(f\"Found {len(unique_cell_types)} unique cell types: {unique_cell_types}\")\n",
    "        else:\n",
    "            cell_types_sc = None\n",
    "            print(\"No cell type information found\")\n",
    "\n",
    "        # === INITIALIZE ADVANCED HIERARCHICAL DIFFUSION MODEL ===\n",
    "        print(f\"Initializing AdvancedHierarchicalDiffusion for {dataset_name}...\")\n",
    "        \n",
    "        output_dir = f'./cscc_advanced_diffusion_{dataset_name}'\n",
    "        \n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=X_st.cpu().numpy(),\n",
    "            st_coords=Y_st.cpu().numpy(),\n",
    "            sc_gene_expr=X_sc.cpu().numpy(),\n",
    "            cell_types_sc=cell_types_sc,  # Include cell type information\n",
    "            transport_plan=T.cpu().numpy(),  # Use transport plan from SpaOTsc\n",
    "            # transport_plan=None,  # Use transport plan from SpaOTsc\n",
    "            D_st=D_st.cpu().numpy(),     # Use distance matrices from SpaOTsc\n",
    "            D_induced=D_induced.cpu().numpy(),  # Use induced distance matrix from SpaOTsc\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],  # Same as STEMDiffusion\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=3.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=1000,     # Same as STEMDiffusion\n",
    "            n_denoising_blocks=6,\n",
    "            hidden_dim=256,      # Same as STEMDiffusion  \n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=output_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"Training model for {dataset_name}...\")\n",
    "        \n",
    "        # Train the model with reduced epochs for speed\n",
    "        model.train(\n",
    "            encoder_epochs=1000,      # Reduced from 1500\n",
    "            diffusion_epochs=3000,   # Reduced from 3000\n",
    "            lambda_struct=5.0,\n",
    "            lambda_physics=2.0,\n",
    "            lambda_uncertainty=0.5\n",
    "        )\n",
    "\n",
    "        model.plot_training_losses()\n",
    "        \n",
    "        print(f\"Generating SC coordinates using model {i+1}...\")\n",
    "        \n",
    "        # Sample SC coordinates with fast sampling (fewer steps)\n",
    "        sc_coords = model.sample_sc_coordinates(\n",
    "            num_samples=1,          # Single sample for averaging\n",
    "            n_steps=50,\n",
    "            return_uncertainty=False\n",
    "            )\n",
    "        \n",
    "        # Store results\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        print(f\"Model {i+1} complete! Generated coordinates shape: {sc_coords.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del X_sc, X_st, Y_st, D_sc, D_st, D_induced\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # === AVERAGE THE RESULTS FROM ALL 3 MODELS ===\n",
    "    print(f\"\\nAveraging results from {len(sc_coords_results)} models...\")\n",
    "    sc_coords_avg = np.mean(sc_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in sc_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['diffusion_coords_avg'] = sc_coords_avg\n",
    "    \n",
    "    # Optionally, save individual results too\n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        scadata.obsm[f'diffusion_coords_rep{i+1}'] = coords\n",
    "    \n",
    "    return scadata, T_all, D_induced_all, D_st_all, D_sc_all, models_all\n",
    "\n",
    "# === RUN THE TRAINING ===\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Train individual models using SpaOTsc and get averaged results\n",
    "scadata, T_all, D_induced_all, D_st_all, D_sc_all, models_all = train_individual_advanced_diffusion_models(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")\n",
    "\n",
    "print(\"Advanced Diffusion training complete! Results saved in scadata.obsm['diffusion_coords_avg']\")\n",
    "\n",
    "# Now you have all the matrices for testing:\n",
    "print(f\"\\nMatrices available for testing:\")\n",
    "print(f\"T_advanced_all: {len(T_all)} transport plans\")\n",
    "print(f\"D_induced_advanced_all: {len(D_induced_all)} induced distance matrices\")\n",
    "print(f\"D_st_all: {len(D_st_all)} ST distance matrices\") \n",
    "print(f\"D_sc_all: {len(D_sc_all)} SC distance matrices\")\n",
    "print(f\"Models: {len(models_all)} trained AdvancedHierarchicalDiffusion models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5)) \n",
    "    sc.pl.embedding(scadata, basis=f'diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have: scadata.obsm['advanced_diffusion_coords_avg'], etc.\n",
    "\n",
    "print(\"\\n=== Advanced Analysis and Visualization ===\")\n",
    "\n",
    "# 1. Visualize advanced results with uncertainty analysis\n",
    "print(\"Creating advanced visualization plots...\")\n",
    "fig, model_uncertainty, confidence_scores = visualize_advanced_results_multi_model(scadata)\n",
    "\n",
    "# 2. Analyze cell interactions for averaged coordinates\n",
    "print(\"Analyzing cell interactions (averaged coordinates)...\")\n",
    "min_distances_avg, interaction_matrix_avg = analyze_cell_interactions_advanced(\n",
    "    scadata, coords_key='diffusion_coords_avg'\n",
    ")\n",
    "\n",
    "# 3. Optional: Analyze interactions for individual models too\n",
    "print(\"Analyzing cell interactions (individual models)...\")\n",
    "for i in range(1, 4):\n",
    "    print(f\"\\nModel {i} interactions:\")\n",
    "    min_distances, interaction_matrix = analyze_cell_interactions_advanced(\n",
    "        scadata, coords_key=f'diffusion_coords_rep{i}'\n",
    "    )\n",
    "\n",
    "# 4. Print summary statistics\n",
    "print(\"\\n=== Advanced Model Statistics ===\")\n",
    "print(f\"Total cells mapped: {len(scadata.obsm['diffusion_coords_avg'])}\")\n",
    "print(f\"Average model uncertainty: {model_uncertainty.mean():.4f}\")\n",
    "print(f\"Model uncertainty range: [{model_uncertainty.min():.4f}, {model_uncertainty.max():.4f}]\")\n",
    "print(f\"Average confidence: {confidence_scores.mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Cell Type Confidence ===\")\n",
    "for ct in scadata.obs['rough_celltype'].unique():\n",
    "    mask = scadata.obs['rough_celltype'] == ct\n",
    "    print(f\"{ct}: {mask.sum()} cells, \"\n",
    "          f\"avg confidence: {confidence_scores[mask].mean():.3f}, \"\n",
    "          f\"avg uncertainty: {model_uncertainty[mask].mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Physics Constraints (Averaged Coordinates) ===\")\n",
    "all_distances = []\n",
    "for key, dist in min_distances_avg.items():\n",
    "    if not np.isnan(dist):\n",
    "        all_distances.append(dist)\n",
    "        print(f\"Min distance {key[0]} - {key[1]}: {dist:.4f}\")\n",
    "\n",
    "if all_distances:\n",
    "    print(f\"\\nOverall minimum cell-cell distance: {np.min(all_distances):.4f}\")\n",
    "    print(f\"Cells with potential overlaps (< 0.01): {np.sum(np.array(all_distances) < 0.01)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_coordinates_isotropic(coords):\n",
    "    '''normalize coordinates to unit circle preserving aspect ratio'''\n",
    "    if torch.is_tensor(coords):\n",
    "        center = coords.mean(dim=0)\n",
    "        centered = coords - center\n",
    "        max_radius = torch.max(torch.norm(centered, dim=1))\n",
    "        coords_norm = centered / max_radius\n",
    "        return coords_norm, center, max_radius\n",
    "    else:\n",
    "        center = coords.mean(axis=0)\n",
    "        centered = coords - center\n",
    "        max_radius = np.max(np.linalg.norm(centered, axis=1))\n",
    "        coords_norm = centered / max_radius\n",
    "        return coords_norm, center, max_radius\n",
    "\n",
    "# Load and prepare data for validation\n",
    "scadata_val, stadata1_val, stadata2_val, stadata3_val = load_and_process_cscc_data_individual_norm()\n",
    "\n",
    "# Get normalized ground truth coordinates for ST3\n",
    "st3_coords_gt = stadata3_val.obsm['spatial']\n",
    "st3_coords_gt_norm, _, _ = normalize_coordinates_isotropic(st3_coords_gt)\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT ===\")\n",
    "print(\"Training diffusion models on ST1+ST2, testing on ST3...\")\n",
    "\n",
    "# Prepare datasets for training (only first 2)\n",
    "st_datasets_train = [\n",
    "    (stadata1_val, \"dataset1\"),\n",
    "    (stadata2_val, \"dataset2\")\n",
    "]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Storage for results\n",
    "T_all = []\n",
    "D_induced_all = []\n",
    "D_st_all = []\n",
    "D_sc_all = []\n",
    "trained_models = []\n",
    "\n",
    "# Train on first two datasets\n",
    "for i, (stadata, dataset_name) in enumerate(st_datasets_train):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Advanced Diffusion model {i+1}/2 for {dataset_name} using SpaOTsc\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get common genes between SC and current ST dataset\n",
    "    sc_genes = set(scadata_val.var_names)\n",
    "    st_genes = set(stadata.var_names)\n",
    "    common_genes = sorted(list(sc_genes & st_genes))\n",
    "    \n",
    "    print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract expression data\n",
    "    sc_expr = scadata_val[:, common_genes].X\n",
    "    st_expr = stadata[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st_expr, 'toarray'):\n",
    "        st_expr = st_expr.toarray()\n",
    "        \n",
    "    # Get coordinates\n",
    "    st_coords = stadata.obsm['spatial']\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32).to(device)\n",
    "    X_st = torch.tensor(st_expr, dtype=torch.float32).to(device)\n",
    "    Y_st = torch.tensor(st_coords, dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(f\"SC data shape: {X_sc.shape}\")\n",
    "    print(f\"ST data shape: {X_st.shape}\")\n",
    "    print(f\"ST coords shape: {Y_st.shape}\")\n",
    "    \n",
    "    # === REPLACE FUSED_GW_TORCH WITH SPAOTSC ===\n",
    "    print(f\"Running optimal transport for {dataset_name}...\")\n",
    "\n",
    "    # === PREPARE CELL TYPE INFORMATION ===\n",
    "    # Extract cell types from scadata\n",
    "    if 'rough_celltype' in scadata_val.obs.columns:\n",
    "        cell_types_sc = scadata_val.obs['rough_celltype'].values\n",
    "        unique_cell_types = np.unique(cell_types_sc)\n",
    "        print(f\"Found {len(unique_cell_types)} unique cell types: {unique_cell_types}\")\n",
    "    else:\n",
    "        cell_types_sc = None\n",
    "        print(\"No cell type information found\")\n",
    "\n",
    "    # === INITIALIZE ADVANCED HIERARCHICAL DIFFUSION MODEL ===\n",
    "    print(f\"Initializing AdvancedHierarchicalDiffusion for {dataset_name}...\")\n",
    "    \n",
    "    output_dir = f'./cscc_advanced_diffusion_{dataset_name}_validation'\n",
    "    \n",
    "    model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=X_st.cpu().numpy(),\n",
    "        st_coords=Y_st.cpu().numpy(),\n",
    "        sc_gene_expr=X_sc.cpu().numpy(),\n",
    "        cell_types_sc=cell_types_sc,  # Include cell type information\n",
    "        # transport_plan=T.cpu().numpy(),  # Use transport plan from SpaOTsc\n",
    "        transport_plan=None,  # Use transport plan from SpaOTsc\n",
    "        D_st=None,     # Use distance matrices from SpaOTsc\n",
    "        D_induced=None,  # Use induced distance matrix from SpaOTsc\n",
    "        n_genes=len(common_genes),\n",
    "        n_embedding=[512, 256, 128],  # Same as STEMDiffusion\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,     # Same as STEMDiffusion\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,      # Same as STEMDiffusion  \n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=output_dir\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training model for {dataset_name}...\")\n",
    "    model.train()\n",
    "    \n",
    "    # Store the trained model\n",
    "    trained_models.append(model)\n",
    "    print(f\"Model {i+1} training completed!\")\n",
    "\n",
    "print(f\"\\nTraining completed! {len(trained_models)} models trained.\")\n",
    "\n",
    "# Test on the third dataset by creating new model instances\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Testing on ST3 dataset...\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Get common genes for testing\n",
    "sc_genes = set(scadata_val.var_names)\n",
    "st1_genes = set(stadata1_val.var_names)\n",
    "st2_genes = set(stadata2_val.var_names)\n",
    "st3_genes = set(stadata3_val.var_names)\n",
    "common_genes_test = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "\n",
    "print(f\"Common genes for testing: {len(common_genes_test)}\")\n",
    "\n",
    "# Extract ST3 expression data\n",
    "st3_expr = stadata3_val[:, common_genes_test].X\n",
    "if hasattr(st3_expr, 'toarray'):\n",
    "    st3_expr = st3_expr.toarray()\n",
    "\n",
    "# We'll create dummy models just to get predictions\n",
    "# Use ST1 as reference for coordinates (since we need some spatial reference)\n",
    "st1_coords = stadata1_val.obsm['spatial']\n",
    "st1_expr_ref = stadata1_val[:, common_genes_test].X\n",
    "if hasattr(st1_expr_ref, 'toarray'):\n",
    "    st1_expr_ref = st1_expr_ref.toarray()\n",
    "\n",
    "# Convert to tensors\n",
    "X_st1_ref = torch.tensor(st1_expr_ref, dtype=torch.float32).to(device)\n",
    "X_st3_test = torch.tensor(st3_expr, dtype=torch.float32).to(device)\n",
    "Y_st1_ref = torch.tensor(st1_coords, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get cell types\n",
    "cell_types_sc = scadata_val.obs['rough_celltype'].values if 'rough_celltype' in scadata_val.obs.columns else None\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# Get the number of cell types from the original training\n",
    "original_cell_types = scadata_val.obs['rough_celltype'].values\n",
    "unique_cell_types = np.unique(original_cell_types)\n",
    "num_original_cell_types = len(unique_cell_types)\n",
    "\n",
    "print(f\"Original model has {num_original_cell_types} cell types\")\n",
    "\n",
    "# Create dummy cell types for ST3 data to match the original number\n",
    "dummy_cell_types = np.random.choice(unique_cell_types, size=X_st3_test.shape[0])\n",
    "\n",
    "# For each trained model, create a test version\n",
    "for i, trained_model in enumerate(trained_models):\n",
    "    print(f\"Creating test model based on trained model {i+1}...\")\n",
    "    \n",
    "    # Create a minimal model instance for testing (no training)\n",
    "    test_model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=X_st1_ref.cpu().numpy(),  # Use ST1 as reference\n",
    "        st_coords=Y_st1_ref.cpu().numpy(),     # Use ST1 coords as reference\n",
    "        sc_gene_expr=X_st3_test.cpu().numpy(), # ST3 data as \"SC\" data\n",
    "        cell_types_sc=dummy_cell_types,                    # No cell types for ST3\n",
    "        transport_plan=None,               # Use transport plan from training\n",
    "        D_st=None,                      # Use distance matrices from training\n",
    "        D_induced=None,\n",
    "        n_genes=len(common_genes_test),\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=f'./temp_test_model_{i}'\n",
    "    )\n",
    "    \n",
    "    # Copy trained parameters (this is a hack, but should work)\n",
    "    state_dict = trained_model.state_dict()\n",
    "    # if 'ot_guidance_strength' in state_dict:\n",
    "    #     del state_dict['ot_guidance_strength']\n",
    "    # test_model.load_state_dict(state_dict)\n",
    "    test_model.load_state_dict(trained_model.state_dict(), strict=False)\n",
    "    \n",
    "    # Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "    print(f\"Generating predictions from test model {i+1}...\")\n",
    "    predicted_coords = test_model.sample_sc_coordinates(\n",
    "        num_samples=1,\n",
    "        return_uncertainty=False,\n",
    "        n_steps = 0\n",
    "    )\n",
    "    all_predictions.append(predicted_coords)\n",
    "\n",
    "# Continue with the rest of your evaluation code...\n",
    "\n",
    "# Average predictions from both models\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# MSE and MAE\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "\n",
    "# Correlation for each dimension\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Correlation X-dimension: {corr_x:.4f} (p={p_x:.6f})\")\n",
    "print(f\"Correlation Y-dimension: {corr_y:.4f} (p={p_y:.6f})\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Predicted coordinates\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Predicted ST3 Coordinates\\n(Averaged from 2 Models)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional distance-based evaluation\n",
    "euclidean_distances = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "median_distance = np.median(euclidean_distances)\n",
    "mean_distance = np.mean(euclidean_distances)\n",
    "\n",
    "print(f\"\\nDistance-based metrics:\")\n",
    "print(f\"Mean Euclidean distance: {mean_distance:.6f}\")\n",
    "print(f\"Median Euclidean distance: {median_distance:.6f}\")\n",
    "print(f\"Max Euclidean distance: {np.max(euclidean_distances):.6f}\")\n",
    "print(f\"Min Euclidean distance: {np.min(euclidean_distances):.6f}\")\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "print(f\"Generating predictions from test model {i+1}...\")\n",
    "all_predictions = []\n",
    "for i, trained_model in enumerate(trained_models):\n",
    "    print(f\"Creating test model based on trained model {i+1}...\")\n",
    "    \n",
    "    # Create a minimal model instance for testing (no training)\n",
    "    test_model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=X_st1_ref.cpu().numpy(),  # Use ST1 as reference\n",
    "        st_coords=Y_st1_ref.cpu().numpy(),     # Use ST1 coords as reference\n",
    "        sc_gene_expr=X_st3_test.cpu().numpy(), # ST3 data as \"SC\" data\n",
    "        cell_types_sc=dummy_cell_types,                    # No cell types for ST3\n",
    "        transport_plan=None,               # Use transport plan from training\n",
    "        D_st=None,                      # Use distance matrices from training\n",
    "        D_induced=None,\n",
    "        n_genes=len(common_genes_test),\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=f'./temp_test_model_{i}'\n",
    "    )\n",
    "    \n",
    "    # Copy trained parameters (this is a hack, but should work)\n",
    "    state_dict = trained_model.state_dict()\n",
    "    # if 'ot_guidance_strength' in state_dict:\n",
    "    #     del state_dict['ot_guidance_strength']\n",
    "    # test_model.load_state_dict(state_dict)\n",
    "    test_model.load_state_dict(trained_model.state_dict(), strict=False)\n",
    "    \n",
    "    # Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "    print(f\"Generating predictions from test model {i+1}...\")\n",
    "    predicted_coords = test_model.sample_sc_coordinates(\n",
    "        num_samples=1,\n",
    "        return_uncertainty=False,\n",
    "        n_steps = 50\n",
    "    )\n",
    "    all_predictions.append(predicted_coords)\n",
    "\n",
    "# Continue with the rest of your evaluation code...\n",
    "\n",
    "# Average predictions from both models\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# MSE and MAE\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "\n",
    "# Correlation for each dimension\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Correlation X-dimension: {corr_x:.4f} (p={p_x:.6f})\")\n",
    "print(f\"Correlation Y-dimension: {corr_y:.4f} (p={p_y:.6f})\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Predicted coordinates\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Predicted ST3 Coordinates\\n(Averaged from 2 Models)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional distance-based evaluation\n",
    "euclidean_distances = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "median_distance = np.median(euclidean_distances)\n",
    "mean_distance = np.mean(euclidean_distances)\n",
    "\n",
    "print(f\"\\nDistance-based metrics:\")\n",
    "print(f\"Mean Euclidean distance: {mean_distance:.6f}\")\n",
    "print(f\"Median Euclidean distance: {median_distance:.6f}\")\n",
    "print(f\"Max Euclidean distance: {np.max(euclidean_distances):.6f}\")\n",
    "print(f\"Min Euclidean distance: {np.min(euclidean_distances):.6f}\")\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model separately AND the average\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Evaluate individual models\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    mse_i = mean_squared_error(st3_coords_gt_norm, pred)\n",
    "    mae_i = mean_absolute_error(st3_coords_gt_norm, pred)\n",
    "    corr_x_i, p_x_i = pearsonr(st3_coords_gt_norm[:, 0], pred[:, 0])\n",
    "    corr_y_i, p_y_i = pearsonr(st3_coords_gt_norm[:, 1], pred[:, 1])\n",
    "    \n",
    "    print(f\"\\n=== MODEL {i+1} RESULTS ===\")\n",
    "    print(f\"MSE: {mse_i:.6f}, MAE: {mae_i:.6f}\")\n",
    "    print(f\"Corr X: {corr_x_i:.4f}, Corr Y: {corr_y_i:.4f}\")\n",
    "\n",
    "# Evaluate averaged results\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(f\"\\n=== AVERAGED RESULTS ===\")\n",
    "print(f\"MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"Corr X: {corr_x:.4f}, Corr Y: {corr_y:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization - individual models + average + ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Model 1 predictions\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(all_predictions[0][:, 0], all_predictions[0][:, 1], \n",
    "           c=range(len(all_predictions[0])), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Model 1 Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Model 2 predictions\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(all_predictions[1][:, 0], all_predictions[1][:, 1], \n",
    "           c=range(len(all_predictions[1])), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Model 2 Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 4: Averaged predictions\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Averaged Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation plots for each model\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    corr_x_i, _ = pearsonr(st3_coords_gt_norm[:, 0], pred[:, 0])\n",
    "    corr_y_i, _ = pearsonr(st3_coords_gt_norm[:, 1], pred[:, 1])\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(st3_coords_gt_norm[:, 0], pred[:, 0], \n",
    "               alpha=0.5, label=f'X-coord (r={corr_x_i:.3f})', s=15)\n",
    "    plt.scatter(st3_coords_gt_norm[:, 1], pred[:, 1], \n",
    "               alpha=0.5, label=f'Y-coord (r={corr_y_i:.3f})', s=15)\n",
    "    plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "             [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "             'r--', alpha=0.8, label='Perfect correlation')\n",
    "    plt.xlabel('Ground Truth Coordinates')\n",
    "    plt.ylabel('Predicted Coordinates')\n",
    "    plt.title(f'Model {i+1} vs Ground Truth')\n",
    "    plt.legend()\n",
    "\n",
    "# Averaged correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Averaged Predictions vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distance error plots for each model\n",
    "euclidean_distances_all = []\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    distances = np.sqrt(np.sum((st3_coords_gt_norm - pred)**2, axis=1))\n",
    "    euclidean_distances_all.append(distances)\n",
    "\n",
    "euclidean_distances_avg = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Distance histograms\n",
    "for i, distances in enumerate(euclidean_distances_all):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Euclidean Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Model {i+1} Error Distribution\\nMean: {np.mean(distances):.4f}')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(euclidean_distances_avg, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Averaged Model Error Distribution\\nMean: {np.mean(euclidean_distances_avg):.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print distance metrics for all models\n",
    "for i, distances in enumerate(euclidean_distances_all):\n",
    "    print(f\"\\nModel {i+1} distance metrics:\")\n",
    "    print(f\"Mean: {np.mean(distances):.6f}, Median: {np.median(distances):.6f}\")\n",
    "    print(f\"Max: {np.max(distances):.6f}, Min: {np.min(distances):.6f}\")\n",
    "\n",
    "print(f\"\\nAveraged model distance metrics:\")\n",
    "print(f\"Mean: {np.mean(euclidean_distances_avg):.6f}, Median: {np.median(euclidean_distances_avg):.6f}\")\n",
    "print(f\"Max: {np.max(euclidean_distances_avg):.6f}, Min: {np.min(euclidean_distances_avg):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
