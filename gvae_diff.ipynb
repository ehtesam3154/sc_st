{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Cell 2: force single‐threaded BLAS\n",
    "os.environ[\"OMP_NUM_THREADS\"]       = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: actually cap BLAS to 1 thread\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# 'blas' covers OpenBLAS, MKL, etc.\n",
    "threadpool_limits(limits=1, user_api='blas')\n",
    "\n",
    "# now import as usual, no more warning\n",
    "import numpy as np\n",
    "import scipy\n",
    "# … any other packages that use OpenBLAS …\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_torch(X, k, mode='connectivity', metric = 'minkowski', p=2, device='cuda'):\n",
    "    '''construct knn graph with torch and gpu\n",
    "    args:\n",
    "        X: input data containing features (torch tensor)\n",
    "        k: number of neighbors for each data point\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric (now euclidean supported for gpu knn)\n",
    "        p: param for minkowski (not used if metric is euclidean)\n",
    "    \n",
    "    Returns:\n",
    "        knn graph as a pytorch sparse tensor (coo format) or dense tensor depending on mode     \n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'.\"\n",
    "    assert metric == 'euclidean', \"for gpu knn, only 'euclidean' metric is currently supported in this implementation\"\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "        mode_knn = 'connectivity'\n",
    "    else:\n",
    "        include_self = False\n",
    "        mode_knn = 'distance'\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm='auto')\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_cpu = X.cpu().numpy()\n",
    "    else:\n",
    "        X_cpu = X.numpy()\n",
    "\n",
    "    knn.fit(X_cpu)\n",
    "    knn_graph_cpu = kneighbors_graph(knn, k, mode=mode_knn, include_self=include_self, metric=metric) #scipy sparse matrix on cpu\n",
    "    knn_graph_coo = knn_graph_cpu.tocoo()\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        knn_graph = torch.sparse_coo_tensor(torch.LongTensor([knn_graph_coo.row, knn_graph_coo.col]),\n",
    "                                            torch.FloatTensor(knn_graph_coo.data),\n",
    "                                            size = knn_graph_coo.shape).to(device)\n",
    "    elif mode == 'distance':\n",
    "        knn_graph_dense = torch.tensor(knn_graph_cpu.toarray(), dtype=torch.float32, device=device) #move to gpu as dense tensor\n",
    "        knn_graph = knn_graph_dense\n",
    "    \n",
    "    return knn_graph\n",
    "    \n",
    "def distances_cal_torch(graph, type_aware=None, aware_power =2, device='cuda'):\n",
    "    '''\n",
    "    calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (pytorch sparse or dense tensor)\n",
    "        type_aware: not implemented in this torch version for simplicity\n",
    "        aware_power: same ^^\n",
    "        device (str): 'cpu' or 'cuda' device to use\n",
    "    Returns:\n",
    "        distance matrix as a torch tensor\n",
    "    '''\n",
    "\n",
    "    if isinstance(graph, torch.Tensor) and graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().to_dense().numpy())\n",
    "    elif isinstance(graph, torch.Tensor) and not graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().numpy())\n",
    "    else:\n",
    "        graph_cpu_csr = csr_matrix(graph) #assume scipy sparse matrix if not torch tensor\n",
    "\n",
    "    shortestPath_cpu = dijkstra(csgraph = graph_cpu_csr, directed=False, return_predecessors=False) #dijkstra on cpu\n",
    "    shortestPath = torch.tensor(shortestPath_cpu, dtype=torch.float32, device=device)\n",
    "\n",
    "    # the_max = torch.nanmax(shortestPath[shortestPath != float('inf')])\n",
    "    # shortestPath[shortestPath > the_max] = the_max\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = torch.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    original_max_distance = the_max.item()\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= torch.mean(C_dis)\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_sc_torch(X_sc, k_neighbors=10, graph_mode='connectivity', device='cpu'):\n",
    "    '''calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (torch sparse or dense tensor)\n",
    "        type_aware: not implemented\n",
    "        aware_power: same ^^\n",
    "        \n",
    "    returns:\n",
    "        distanced matrix as torch tensor'''\n",
    "    \n",
    "    if not isinstance(X_sc, torch.Tensor):\n",
    "        raise TypeError('Input X_sc must be a pytorch tensor')\n",
    "    \n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_sc = X_sc.cuda(device=device)\n",
    "    else:\n",
    "        X_sc = X_sc.cpu()\n",
    "        device= 'cpu'\n",
    "\n",
    "    print(f'using device: {device}')\n",
    "    print(f'constructing knn graph...')\n",
    "    # X_normalized = normalize(X_sc.cpu().numpy(), norm='l2') #normalize on cpu for sklearn knn\n",
    "    X_normalized = X_sc\n",
    "    X_normalized_torch = torch.tensor(X_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "    Xgraph = construct_graph_torch(X_normalized_torch, k=k_neighbors, mode=graph_mode, metric='euclidean', device=device)\n",
    "\n",
    "    print('calculating distances from graph....')\n",
    "    D_sc, sc_max_distance = distances_cal_torch(Xgraph, device=device)\n",
    "\n",
    "    print('D_sc calculation complete')\n",
    "    \n",
    "    return D_sc, sc_max_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot\n",
    "\n",
    "def construct_graph_spatial(location_array, k, mode='distance', metric='euclidean', p=2):\n",
    "    '''construct KNN graph based on spatial coordinates\n",
    "    args:\n",
    "        location_array: spatial coordinates of spots (n-spots * 2)\n",
    "        k: number of neighbors for each spot\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric for knn (p=2 is euclidean)\n",
    "        p: param for minkowski if connectivity\n",
    "        \n",
    "    returns:\n",
    "        scipy.sparse.csr_matrix: knn graph in csr format\n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'\"\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "    else:\n",
    "        include_self = False\n",
    "    \n",
    "    c_graph = kneighbors_graph(location_array, k, mode=mode, metric=metric, include_self=include_self, p=p)\n",
    "    return c_graph\n",
    "\n",
    "def distances_cal_spatial(graph, spot_ids=None, spot_types=None, aware_power=2):\n",
    "    '''calculate spatial distance matrix from knn graph\n",
    "    args:\n",
    "        graph (scipy.sparse.csr_matrix): knn graph\n",
    "        spot_ids (list, optional): list of spot ids corresponding to the rows/cols of the graph. required if type_aware is used\n",
    "        spot_types (pd.Series, optinal): pandas series of spot types for type aware distance adjustment. required if type_aware is used\n",
    "        aware_power (int): power for type-aware distance adjustment\n",
    "        \n",
    "    returns:\n",
    "        sptial distance matrix'''\n",
    "    shortestPath = dijkstra(csgraph = csr_matrix(graph), directed=False, return_predecessors=False)\n",
    "    shortestPath = np.nan_to_num(shortestPath, nan=np.inf) #handle potential inf valyes after dijkstra\n",
    "\n",
    "    if spot_types is not None and spot_ids is not None:\n",
    "        shortestPath_df = pd.DataFrame(shortestPath, index=spot_ids, columns=spot_ids)\n",
    "        shortestPath_df['id1'] = shortestPath_df.index\n",
    "        shortestPath_melted = shortestPath_df.melt(id_vars=['id1'], var_name='id2', value_name='value')\n",
    "\n",
    "        type_aware_df = pd.DataFrame({'spot': spot_ids, 'spot_type': spot_types}, index=spot_ids)\n",
    "        meta1 = type_aware_df.copy()\n",
    "        meta1.columns = ['id1', 'type1']\n",
    "        meta2 = type_aware_df.copy()\n",
    "        meta2.columns = ['id2', 'type2']\n",
    "\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta1, on='id1', how='left')\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta2, on='id2', how='left')\n",
    "\n",
    "        shortestPath_melted['same_type'] = shortestPath_melted['type1'] == shortestPath_melted['type2']\n",
    "        shortestPath_melted.loc[(~shortestPath_melted.smae_type), 'value'] = shortestPath_melted.loc[(~shortestPath_melted.same_type),\n",
    "                                                                                                     'value'] * aware_power\n",
    "        shortestPath_melted.drop(['type1', 'type2', 'same_type'], axis=1, inplace=True)\n",
    "        shortestPath_pivot = shortestPath_melted.pivot(index='id1', columns='id2', values='value')\n",
    "\n",
    "        order = spot_ids\n",
    "        shortestPath = shortestPath_pivot[order].loc[order].values\n",
    "    else:\n",
    "        shortestPath = np.asarray(shortestPath) #ensure it's a numpy array\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = np.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    #store original max distance for scale reference\n",
    "    original_max_distance = the_max\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= np.mean(C_dis)\n",
    "\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_st_from_coords(spatial_coords, X_st=None, k_neighbors=10, graph_mode='distance', aware_st=False, \n",
    "                               spot_types=None, aware_power_st=2, spot_ids=None):\n",
    "    '''calculates the spatial distance matrix D_st for spatial transcriptomics data directly from coordinates and optional spot types\n",
    "    args:\n",
    "        spatial_coords: spatial coordinates of spots (n_spots * 2)\n",
    "        X_st: St gene expression data (not used for D_st calculation itself)\n",
    "        k_neighbors: number of neighbors for knn graph\n",
    "        graph_mode: 'connectivity or 'distance' for knn graph\n",
    "        aware_st: whether to use type-aware distance adjustment\n",
    "        spot_types: pandas series of spot types for type-aware adjustment\n",
    "        aware_power_st: power for type-aware distance adjustment\n",
    "        spot_ids: list or index of spot ids, required if spot_ids is provided\n",
    "        \n",
    "    returns:\n",
    "        np.ndarray: spatial disance matrix D_st'''\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        location_array = spatial_coords.values\n",
    "        if spot_ids is None:\n",
    "            spot_ids = spatial_coords.index.tolist() #use index of dataframe if available\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        location_array = spatial_coords\n",
    "        if spot_ids is None:\n",
    "            spot_ids = list(range(location_array.shape[0])) #generate default ids if not provided\n",
    "\n",
    "    else:\n",
    "        raise TypeError('spatial_coords must be a pandas dataframe or a numpy array')\n",
    "    \n",
    "    print(f'constructing {graph_mode} graph for ST data with k={k_neighbors}.....')\n",
    "    Xgraph_st = construct_graph_spatial(location_array, k=k_neighbors, mode=graph_mode)\n",
    "    \n",
    "    if aware_st:\n",
    "        if spot_types is None or spot_ids is None:\n",
    "            raise ValueError('spot_types and spot_ids must be provided when aware_st=True')\n",
    "        if not isinstance(spot_types, pd.Series):\n",
    "            spot_types = pd.Series(spot_types, idnex=spot_ids) \n",
    "        print('applying type aware distance adjustment for ST data')\n",
    "        print(f'aware power for ST: {aware_power_st}')\n",
    "    else:\n",
    "        spot_types = None \n",
    "\n",
    "    print(f'calculating spatial distances.....')\n",
    "    D_st, st_max_distance = distances_cal_spatial(Xgraph_st, spot_ids=spot_ids, spot_types=spot_types, aware_power=aware_power_st)\n",
    "\n",
    "    print('D_st calculation complete')\n",
    "    return D_st, st_max_distance\n",
    "\n",
    "\n",
    "def calculate_D_st_euclidean(spatial_coords):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance matrix for ST spots.\n",
    "    \n",
    "    Args:\n",
    "        spatial_coords: (m_spots, 2) spatial coordinates\n",
    "        \n",
    "    Returns:\n",
    "        D_st_euclid: (m_spots, m_spots) normalized Euclidean distance matrix\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        coords_array = spatial_coords.values\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        coords_array = spatial_coords\n",
    "    else:\n",
    "        coords_array = np.array(spatial_coords)\n",
    "    \n",
    "    # Compute pairwise Euclidean distances\n",
    "    D_euclid = squareform(pdist(coords_array, metric='euclidean'))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    max_dist = D_euclid.max()\n",
    "    if max_dist > 0:\n",
    "        D_euclid = D_euclid / max_dist\n",
    "    \n",
    "    return D_euclid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patient 2 data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Key innovation: Use ALL ST data points for better training.\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "\n",
    "    # Store separate coordinate lists for block-diagonal graph\n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    st_expr_combined = scaler.fit_transform(st_expr_combined)\n",
    "\n",
    "    st_coords_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "\n",
    "    sc_expr = scaler.fit_transform(sc_expr)\n",
    "\n",
    "    \n",
    "    # Create dataset labels for tracking\n",
    "    dataset_labels = (['dataset1'] * len(st1_expr) + \n",
    "                     ['dataset2'] * len(st2_expr) + \n",
    "                     ['dataset3'] * len(st3_expr))\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Prepare combined data for diffusion\n",
    "X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = prepare_combined_st_for_diffusion(\n",
    "    stadata1, stadata2, stadata3, scadata\n",
    ")\n",
    "\n",
    "print(f\"Data preparation complete!\")\n",
    "print(f\"SC cells: {X_sc.shape[0]}\")\n",
    "print(f\"Combined ST spots: {X_st_combined.shape[0]}\")\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import cKDTree\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# =====================================================\n",
    "# PART X: Graph-VAE Components\n",
    "# =====================================================\n",
    "\n",
    "class GraphVAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph encoder that learns latent representations from ST spot graphs.\n",
    "    ⚠️ Do **not** touch `train_encoder`; its aligned embeddings are the sole conditioning signal throughout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Two GraphConv layers as specified\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # MLP to output μ and log σ² FOR EACH NODE (not graph-level)\n",
    "        self.mu_head = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None, batch=None):\n",
    "        \"\"\"\n",
    "        x: node features (aligned embeddings E(X_st)) - shape (n_nodes, input_dim)\n",
    "        edge_index: graph edges from K-NN adjacency\n",
    "        edge_weight: optional edge weights\n",
    "        batch: not used since we want node-level representations\n",
    "        \n",
    "        Returns:\n",
    "        mu: (n_nodes, latent_dim)\n",
    "        logvar: (n_nodes, latent_dim)\n",
    "        \"\"\"\n",
    "        # Two GraphConv layers\n",
    "        h = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        h = torch.relu(self.conv2(h, edge_index, edge_weight))\n",
    "        \n",
    "        # NO GLOBAL POOLING - we want node-level representations\n",
    "        # Output μ and log σ² for each node\n",
    "        mu = self.mu_head(h)        # Shape: (n_nodes, latent_dim)\n",
    "        logvar = self.logvar_head(h)  # Shape: (n_nodes, latent_dim)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick - works element-wise\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "class GraphVAEDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph decoder that outputs 2D coordinates from latent z ONLY.\n",
    "    Features are NOT passed to force geometry into z.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=32, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Decoder takes ONLY latent z (no conditioning)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Output 2D coordinates\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: latent vectors (batch_size, latent_dim) ONLY\n",
    "        \"\"\"\n",
    "        coords = self.decoder(z)\n",
    "        return coords\n",
    "\n",
    "def precompute_knn_edges(coords, k=30, device='cuda'):\n",
    "    \"\"\"\n",
    "    Helper function to precompute K-NN edges for torch-geometric style layers.\n",
    "    Uses existing graph construction utilities where possible.\n",
    "    \"\"\"\n",
    "    if isinstance(coords, torch.Tensor):\n",
    "        coords_np = coords.cpu().numpy()\n",
    "    else:\n",
    "        coords_np = coords\n",
    "        \n",
    "    # Use existing construct_graph_spatial function\n",
    "    from sklearn.neighbors import kneighbors_graph\n",
    "    \n",
    "    # Build KNN graph\n",
    "    knn_graph = kneighbors_graph(\n",
    "        coords_np, \n",
    "        n_neighbors=k, \n",
    "        mode='connectivity', \n",
    "        include_self=False\n",
    "    )\n",
    "    \n",
    "    # Convert to torch-geometric format\n",
    "    from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(knn_graph)\n",
    "    \n",
    "    # CRITICAL FIX: Ensure correct dtypes\n",
    "    edge_index = edge_index.long().to(device)      # Edge indices should be long\n",
    "    edge_weight = edge_weight.float().to(device)   # Edge weights should be float32\n",
    "    \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "class LatentDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Latent-space denoiser identical to current MLP/U-Net stack but for latent dim=32.\n",
    "    ⚠️ Do **not** touch `train_encoder`; its aligned embeddings are the sole conditioning signal throughout.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=32, condition_dim=128, hidden_dim=256, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        \n",
    "        # Time embedding (reuse existing SinusoidalEmbedding)\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Latent encoder\n",
    "        self.latent_encoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Condition encoder (for aligned embeddings)\n",
    "        self.condition_encoder = nn.Sequential(\n",
    "            nn.Linear(condition_dim, hidden_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Denoising blocks (similar to existing hierarchical blocks)\n",
    "        self.denoising_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, z_noisy, t, condition):\n",
    "        \"\"\"\n",
    "        z_noisy: noisy latent vectors (batch_size, latent_dim)\n",
    "        t: timestep (batch_size,) - NOW 1D instead of 2D\n",
    "        condition: aligned embeddings E(X) (batch_size, condition_dim)\n",
    "        \"\"\"\n",
    "        batch_size = z_noisy.size(0)\n",
    "        # ENSURE inputs are 2D\n",
    "        if z_noisy.dim() > 2:\n",
    "            z_noisy = z_noisy.squeeze()\n",
    "        if condition.dim() > 2:\n",
    "            condition = condition.squeeze()\n",
    "            \n",
    "        # Handle 1D timestep input\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(1)  # Make it (batch_size, 1)\n",
    "\n",
    "        t = t.view(batch_size, 1)\n",
    "        \n",
    "        # Encode inputs\n",
    "        z_enc = self.latent_encoder(z_noisy)\n",
    "        t_enc = self.time_embed(t)\n",
    "        c_enc = self.condition_encoder(condition)\n",
    "        \n",
    "        # Combine features\n",
    "        h = z_enc + t_enc + c_enc\n",
    "        \n",
    "        # Apply denoising blocks\n",
    "        for block in self.denoising_blocks:\n",
    "            h = h + block(h)  # Residual connections\n",
    "            \n",
    "        # Output predicted noise\n",
    "        noise_pred = self.output_head(h)\n",
    "        return noise_pred\n",
    "\n",
    "# =====================================================\n",
    "# PART 1: Advanced Network Components\n",
    "# =====================================================\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, n_genes, n_embedding=[512, 256, 128], dp=0):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_genes, n_embedding[0])\n",
    "        self.bn1 = nn.LayerNorm(n_embedding[0])\n",
    "        self.fc2 = nn.Linear(n_embedding[0], n_embedding[1])\n",
    "        self.bn2 = nn.LayerNorm(n_embedding[1])\n",
    "        self.fc3 = nn.Linear(n_embedding[1], n_embedding[2])\n",
    "        \n",
    "        self.dp = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self, x, isdp=False):\n",
    "        if isdp:\n",
    "            x = self.dp(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1) or (batch_size,)\n",
    "        Returns: (batch_size, dim)\n",
    "        \"\"\"\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)  # Make (batch_size, 1)\n",
    "        \n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x * emb.unsqueeze(0)  # (batch_size, half_dim)\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)  # (batch_size, dim)\n",
    "        \n",
    "        if emb.size(1) != self.dim:\n",
    "            # Handle odd dimensions\n",
    "            emb = emb[:, :self.dim]\n",
    "            \n",
    "        return emb\n",
    "\n",
    "import torch.optim as optim   \n",
    "from geomloss import SamplesLoss\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "class CellTypeEmbedding(nn.Module):\n",
    "    \"\"\"Learned embeddings for cell types\"\"\"\n",
    "    def __init__(self, num_cell_types, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_cell_types, embedding_dim)\n",
    "        \n",
    "    def forward(self, cell_type_indices):\n",
    "        return self.embedding(cell_type_indices)\n",
    "\n",
    "class UncertaintyHead(nn.Module):\n",
    "    \"\"\"Predicts coordinate uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Uncertainty for x and y\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softplus(self.net(x)) + 0.01  # Ensure positive uncertainty\n",
    "\n",
    "class PhysicsInformedLayer(nn.Module):\n",
    "    \"\"\"Incorporates cell non-overlap constraints\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.radius_predictor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.repulsion_strength = nn.Parameter(torch.tensor(0.1))\n",
    "        \n",
    "    def compute_repulsion_gradient(self, coords, radii, cell_types=None):\n",
    "        \"\"\"Compute repulsion forces between cells\"\"\"\n",
    "        batch_size = coords.shape[0]\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        distances = torch.cdist(coords, coords, p=2)\n",
    "        \n",
    "        # Compute sum of radii for each pair\n",
    "        radii_sum = radii + radii.T\n",
    "        \n",
    "        # Compute overlap (positive when cells overlap)\n",
    "        overlap = F.relu(radii_sum - distances + 1e-6)\n",
    "        \n",
    "        # Mask out self-interactions\n",
    "        mask = (1 - torch.eye(batch_size, device=coords.device))\n",
    "        overlap = overlap * mask\n",
    "        \n",
    "        # Compute repulsion forces\n",
    "        coord_diff = coords.unsqueeze(1) - coords.unsqueeze(0)  # (B, B, 2)\n",
    "        distances_safe = distances + 1e-6  # Avoid division by zero\n",
    "        \n",
    "        # Normalize direction vectors\n",
    "        directions = coord_diff / distances_safe.unsqueeze(-1)\n",
    "        \n",
    "        # Apply stronger repulsion for same cell types (optional)\n",
    "        if cell_types is not None:\n",
    "            same_type_mask = (cell_types.unsqueeze(1) == cell_types.unsqueeze(0)).float()\n",
    "            repulsion_weight = 1.0 + 0.5 * same_type_mask  # 50% stronger for same type\n",
    "        else:\n",
    "            # repulsion_weight = 1.0\n",
    "            batch_size = coords.shape[0]\n",
    "            repulsion_weight = torch.ones(batch_size, batch_size, device=coords.device)\n",
    "            \n",
    "        # Compute repulsion magnitude\n",
    "        repulsion_magnitude = overlap.unsqueeze(-1) * repulsion_weight.unsqueeze(-1)\n",
    "        \n",
    "        # Sum repulsion forces from all other cells\n",
    "        repulsion_forces = (repulsion_magnitude * directions * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        \n",
    "        return repulsion_forces\n",
    "        \n",
    "    def forward(self, coords, features, cell_types=None):\n",
    "        # Predict cell radii based on features\n",
    "        radii = self.radius_predictor(features).squeeze(-1) * 0.01  # Scale to reasonable size\n",
    "        \n",
    "        # Compute repulsion gradient\n",
    "        repulsion_grad = self.compute_repulsion_gradient(coords, radii, cell_types)\n",
    "        \n",
    "        return repulsion_grad * self.repulsion_strength, radii\n",
    "    \n",
    "class SpatialBatchSampler:\n",
    "    \"\"\"Sample spatially contiguous batches for geometric attention\"\"\"\n",
    "    \n",
    "    def __init__(self, coordinates, batch_size, k_neighbors=None):\n",
    "        \"\"\"\n",
    "        coordinates: (N, 2) array of spatial coordinates\n",
    "        batch_size: size of each batch\n",
    "        k_neighbors: number of neighbors to precompute (default: batch_size)\n",
    "        \"\"\"\n",
    "        self.coordinates = coordinates\n",
    "        self.batch_size = batch_size\n",
    "        self.k_neighbors = k_neighbors or min(batch_size, len(coordinates))\n",
    "        \n",
    "        # Precompute nearest neighbors\n",
    "        self.nbrs = NearestNeighbors(\n",
    "            n_neighbors=self.k_neighbors, \n",
    "            algorithm='kd_tree'\n",
    "        ).fit(coordinates)\n",
    "        \n",
    "    def sample_spatial_batch(self):\n",
    "        \"\"\"Sample a spatially contiguous batch\"\"\"\n",
    "        # Pick random center point\n",
    "        center_idx = np.random.randint(len(self.coordinates))\n",
    "        \n",
    "        # Get k nearest neighbors\n",
    "        distances, indices = self.nbrs.kneighbors(\n",
    "            self.coordinates[center_idx:center_idx+1], \n",
    "            return_distance=True\n",
    "        )\n",
    "        \n",
    "        # Return indices as torch tensor\n",
    "        batch_indices = torch.tensor(indices.flatten()[:self.batch_size], dtype=torch.long)\n",
    "        return batch_indices\n",
    "\n",
    "# =====================================================\n",
    "# PART 2: Hierarchical Diffusion Architecture\n",
    "# =====================================================\n",
    "\n",
    "class HierarchicalDiffusionBlock(nn.Module):\n",
    "    \"\"\"Multi-scale diffusion block for coarse-to-fine generation\"\"\"\n",
    "    def __init__(self, dim, num_scales=3):\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        \n",
    "        # Coarse-level predictor (for clusters/regions)\n",
    "        self.coarse_net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Fine-level predictor (for individual cells)\n",
    "        self.fine_net = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim * 2),  # Takes both coarse and fine features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Scale mixing weights\n",
    "        self.scale_mixer = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Takes timestep\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_scales),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, coarse_context=None):\n",
    "        # Determine scale weights based on timestep\n",
    "        scale_weights = self.scale_mixer(t.unsqueeze(-1))\n",
    "        \n",
    "        # Coarse prediction\n",
    "        coarse_pred = self.coarse_net(x)\n",
    "        \n",
    "        # Fine prediction (conditioned on coarse if available)\n",
    "        if coarse_context is not None:\n",
    "            fine_input = torch.cat([x, coarse_context], dim=-1)\n",
    "        else:\n",
    "            fine_input = torch.cat([x, coarse_pred], dim=-1)\n",
    "        fine_pred = self.fine_net(fine_input)\n",
    "        \n",
    "        # Mix scales based on timestep\n",
    "        output = scale_weights[:, 0:1] * coarse_pred + scale_weights[:, 1:2] * fine_pred\n",
    "        \n",
    "        return output  \n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# PART 3: Main Advanced Diffusion Model\n",
    "# =====================================================\n",
    "\n",
    "class AdvancedHierarchicalDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        st_gene_expr,\n",
    "        st_coords,\n",
    "        sc_gene_expr,\n",
    "        cell_types_sc=None,  # Cell type labels for SC data\n",
    "        transport_plan=None,  # Optimal transport plan from domain alignment\n",
    "        D_st=None,\n",
    "        D_induced=None,\n",
    "        n_genes=None,\n",
    "        # n_embedding=128,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=200,\n",
    "        st_max_distance=None,\n",
    "        sc_max_distance=None,\n",
    "        sigma=3.0,\n",
    "        alpha=0.9,\n",
    "        mmdbatch=0.1,\n",
    "        batch_size=64,\n",
    "        device='cuda',\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=1000,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=512,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.1,\n",
    "        outf='output'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.diffusion_losses = {\n",
    "            'total': [],\n",
    "            'diffusion': [],\n",
    "            'struct': [],\n",
    "            'physics': [],\n",
    "            'uncertainty': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "\n",
    "        # Loss tracking for Graph-VAE training\n",
    "        self.vae_losses = {\n",
    "            'total': [],\n",
    "            'reconstruction': [],\n",
    "            'kl': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        # Loss tracking for Latent Diffusion training  \n",
    "        self.latent_diffusion_losses = {\n",
    "            'total': [],\n",
    "            'diffusion': [],\n",
    "            'struct': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        # Keep encoder losses separate (if you want to track them)\n",
    "        self.encoder_losses = {\n",
    "            'total': [],\n",
    "            'pred': [],\n",
    "            'circle': [],\n",
    "            'mmd': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.mmdbatch = mmdbatch\n",
    "        self.n_embedding = n_embedding\n",
    "        \n",
    "        # Create output directory\n",
    "        self.outf = outf\n",
    "        if not os.path.exists(outf):\n",
    "            os.makedirs(outf)\n",
    "        \n",
    "        # Store data\n",
    "        self.st_gene_expr = torch.tensor(st_gene_expr, dtype=torch.float32).to(device)\n",
    "        self.st_coords = torch.tensor(st_coords, dtype=torch.float32).to(device)\n",
    "        self.sc_gene_expr = torch.tensor(sc_gene_expr, dtype=torch.float32).to(device)\n",
    "\n",
    "        \n",
    "        # Temperature regularization for geometric attention\n",
    "        self.temp_weight_decay = 1e-4\n",
    "        \n",
    "        # Store transport plan if provided\n",
    "        self.transport_plan = torch.tensor(transport_plan, dtype=torch.float32).to(device) if transport_plan is not None else None\n",
    "        \n",
    "        # Process cell types\n",
    "        if cell_types_sc is not None:\n",
    "            # Convert cell type strings to indices\n",
    "            unique_cell_types = np.unique(cell_types_sc)\n",
    "            self.cell_type_to_idx = {ct: i for i, ct in enumerate(unique_cell_types)}\n",
    "            self.num_cell_types = len(unique_cell_types)\n",
    "            cell_type_indices = [self.cell_type_to_idx[ct] for ct in cell_types_sc]\n",
    "            self.sc_cell_types = torch.tensor(cell_type_indices, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            self.sc_cell_types = None\n",
    "            self.num_cell_types = 0\n",
    "            \n",
    "        # Store distance matrices\n",
    "        self.D_st = torch.tensor(D_st, dtype=torch.float32).to(device) if D_st is not None else None\n",
    "        self.D_induced = torch.tensor(D_induced, dtype=torch.float32).to(device) if D_induced is not None else None\n",
    "\n",
    "        # If D_st is not provided, calculate it from spatial coordinates\n",
    "        if self.D_st is None:\n",
    "            print(\"D_st not provided, calculating from spatial coordinates...\")\n",
    "            if isinstance(st_coords, torch.Tensor):\n",
    "                st_coords_np = st_coords.cpu().numpy()\n",
    "            else:\n",
    "                st_coords_np = st_coords\n",
    "            \n",
    "            D_st_np, st_max_distance = calculate_D_st_from_coords(\n",
    "                spatial_coords=st_coords_np, \n",
    "                k_neighbors=50, \n",
    "                graph_mode=\"distance\"\n",
    "            )\n",
    "            self.D_st = torch.tensor(D_st_np, dtype=torch.float32).to(device)\n",
    "            self.st_max_distance = st_max_distance\n",
    "            print(f\"D_st calculated, shape: {self.D_st.shape}\")\n",
    "\n",
    "\n",
    "        print(f\"Final matrices - D_st: {self.D_st.shape if self.D_st is not None else None}, \"\n",
    "            f\"D_induced: {self.D_induced.shape if self.D_induced is not None else None}\")\n",
    "        \n",
    "        # Normalize coordinates\n",
    "        self.st_coords_norm, self.coords_center, self.coords_radius = self.normalize_coordinates_isotropic(self.st_coords)        \n",
    "        print(f\"\\n=== NORMALIZED ST Coordinates ===\")\n",
    "        print(f\"  X range: [{self.st_coords_norm[:, 0].min():.3f}, {self.st_coords_norm[:, 0].max():.3f}]\")\n",
    "        print(f\"  Y range: [{self.st_coords_norm[:, 1].min():.3f}, {self.st_coords_norm[:, 1].max():.3f}]\")\n",
    "        print(f\"  Center: {self.coords_center}\")\n",
    "        print(f\"  Max radius: {self.coords_radius:.2f}\")\n",
    "\n",
    "        # self.st_coords_norm = self.st_coords\n",
    "\n",
    "        # Model parameters\n",
    "        self.n_genes = n_genes or st_gene_expr.shape[1]\n",
    "        \n",
    "        # ========== FEATURE ENCODER ==========\n",
    "        self.netE = self.build_feature_encoder(self.n_genes, n_embedding, dp)\n",
    "\n",
    "        self.train_log = os.path.join(outf, 'train.log')\n",
    "\n",
    "        \n",
    "        # ========== CELL TYPE EMBEDDING ==========\n",
    "\n",
    "        use_cell_types = (cell_types_sc is not None)  # Check if SC data has cell types\n",
    "        self.use_cell_types = use_cell_types\n",
    "\n",
    "        if self.num_cell_types > 0:\n",
    "            self.cell_type_embedding = CellTypeEmbedding(self.num_cell_types, n_embedding[-1] // 2)\n",
    "            total_feature_dim = n_embedding[-1] + n_embedding[-1] // 2\n",
    "        else:\n",
    "            self.cell_type_embedding = None\n",
    "            total_feature_dim = n_embedding[-1]\n",
    "            \n",
    "        # ========== HIERARCHICAL DIFFUSION COMPONENTS ==========\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Feature projection (includes cell type if available)\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # ========== GRAPH-VAE COMPONENTS (REPLACING HIERARCHICAL DIFFUSION) ==========        \n",
    "        # Graph-VAE parameters\n",
    "        self.latent_dim = 32  # As specified in instructions\n",
    "        \n",
    "        # Graph-VAE Encoder (learns latent representations from ST graphs)\n",
    "        self.graph_vae_encoder = GraphVAEEncoder(\n",
    "            input_dim=n_embedding[-1],  # Aligned embedding dimension\n",
    "            hidden_dim=128,             # GraphConv hidden dimension  \n",
    "            latent_dim=self.latent_dim\n",
    "        ).to(device)\n",
    "\n",
    "        self.graph_vae_decoder = GraphVAEDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dim=128  # Remove condition_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        # Latent Denoiser (replaces hierarchical_blocks)\n",
    "        self.latent_denoiser = LatentDenoiser(\n",
    "            latent_dim=self.latent_dim,\n",
    "            condition_dim=n_embedding[-1],\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_blocks=n_denoising_blocks\n",
    "        ).to(device)\n",
    "        \n",
    "        # ========== HIERARCHICAL DENOISING BLOCKS ==========\n",
    "        self.hierarchical_blocks = nn.ModuleList([\n",
    "            HierarchicalDiffusionBlock(hidden_dim, num_hierarchical_scales)\n",
    "            for _ in range(n_denoising_blocks)\n",
    "        ])    \n",
    "\n",
    "        # ========== PHYSICS-INFORMED COMPONENTS ==========\n",
    "        self.physics_layer = PhysicsInformedLayer(hidden_dim)\n",
    "        \n",
    "        # ========== UNCERTAINTY QUANTIFICATION ==========\n",
    "        self.uncertainty_head = UncertaintyHead(hidden_dim)\n",
    "        \n",
    "        # ========== OPTIMAL TRANSPORT GUIDANCE ==========\n",
    "        if self.transport_plan is not None:\n",
    "            self.ot_guidance_strength = nn.Parameter(torch.tensor(0.1))\n",
    "            \n",
    "        # ========== OUTPUT LAYERS ==========\n",
    "        self.noise_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # Create noise schedule\n",
    "        # self.noise_schedule = self.create_noise_schedule()\n",
    "        self.noise_schedule = self.build_noise_schedule(self.n_timesteps)\n",
    "\n",
    "        self.guidance_scale = 2.0\n",
    "        \n",
    "        # Optimizers\n",
    "        self.setup_optimizers(lr_e, lr_d)\n",
    "        \n",
    "        # MMD Loss for domain alignment\n",
    "        self.mmd_loss = MMDLoss()\n",
    "\n",
    "        # Move entire model to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def build_noise_schedule(self, T, beta_start=1e-4, beta_end=2e-2):\n",
    "        \"\"\"Rebuild noise schedule when T changes\"\"\"\n",
    "        betas = torch.linspace(beta_start, beta_end, T, device=self.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=self.device), alphas_cumprod[:-1]], dim=0)\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'alphas_cumprod_prev': alphas_cumprod_prev,\n",
    "            'posterior_variance': posterior_variance\n",
    "        }\n",
    "\n",
    "    def update_noise_schedule(self):\n",
    "        \"\"\"Update noise schedule when n_timesteps changes\"\"\"\n",
    "        self.noise_schedule = self.build_noise_schedule(self.n_timesteps)\n",
    "        print(f\"Updated noise schedule for T={self.n_timesteps}\")\n",
    "        print(f\"alpha_bar[0]={self.noise_schedule['alphas_cumprod'][0]:.6f}, alpha_bar[-1]={self.noise_schedule['alphas_cumprod'][-1]:.6f}\")\n",
    "\n",
    "    def setup_spatial_sampling(self):\n",
    "        if hasattr(self, 'st_coords_norm'):\n",
    "            self.spatial_sampler = SpatialBatchSampler(\n",
    "                coordinates=self.st_coords_norm.cpu().numpy(),\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_sampler = None\n",
    "\n",
    "    def get_spatial_batch(self):\n",
    "        \"\"\"Get spatially contiguous batch for training\"\"\"\n",
    "        if self.spatial_sampler is not None:\n",
    "            return self.spatial_sampler.sample_spatial_batch()\n",
    "        else:\n",
    "            # Fallback to random sampling\n",
    "            return torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "        \n",
    "    def _evaluate_sigma_quality(self, st_embeddings, k=10):\n",
    "        \"\"\"Evaluate how well encoder embeddings preserve spatial k-NN structure\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get k-NN from encoder similarity\n",
    "            netpred = st_embeddings.mm(st_embeddings.t())\n",
    "            pred_knn = self._get_knn_from_similarity(netpred, k=k)\n",
    "            \n",
    "            # Get k-NN from physical coordinates  \n",
    "            phys_knn = self._get_knn_from_coords(self.st_coords_norm, k=k)\n",
    "            \n",
    "            # Compute overlap\n",
    "            overlap = (pred_knn == phys_knn).float().mean().item()\n",
    "            return overlap\n",
    "\n",
    "    def _get_knn_from_similarity(self, similarity_matrix, k=10):\n",
    "        \"\"\"Extract top-k neighbors from similarity matrix\"\"\"\n",
    "        # Get top-k indices for each node\n",
    "        _, topk_indices = torch.topk(similarity_matrix, k=k+1, dim=1)  # +1 to exclude self\n",
    "        topk_indices = topk_indices[:, 1:]  # Remove self-connections\n",
    "        return topk_indices\n",
    "\n",
    "    def _get_knn_from_coords(self, coords, k=10):\n",
    "        \"\"\"Extract top-k spatial neighbors from coordinates\"\"\"\n",
    "        # Compute pairwise distances\n",
    "        distances = torch.cdist(coords, coords)\n",
    "        # Get top-k closest (smallest distances)\n",
    "        _, topk_indices = torch.topk(distances, k=k+1, dim=1, largest=False)  # +1 for self\n",
    "        topk_indices = topk_indices[:, 1:]  # Remove self-connections  \n",
    "        return topk_indices\n",
    "        \n",
    "    def normalize_coordinates_isotropic(self, coords):\n",
    "        \"\"\"Normalize coordinates isotropically to [-1, 1]\"\"\"\n",
    "        center = coords.mean(dim=0)\n",
    "        centered_coords = coords - center\n",
    "        max_dist = torch.max(torch.norm(centered_coords, dim=1))\n",
    "        normalized_coords = centered_coords / (max_dist + 1e-8)\n",
    "        return normalized_coords, center, max_dist\n",
    "        \n",
    "\n",
    "    def build_feature_encoder(self, n_genes, n_embedding, dp):\n",
    "        \"\"\"Build the feature encoder network\"\"\"\n",
    "        return FeatureNet(n_genes, n_embedding=n_embedding, dp=dp).to(self.device)\n",
    "        \n",
    "    def create_noise_schedule(self):\n",
    "        \"\"\"Create the noise schedule for diffusion\"\"\"\n",
    "        betas = torch.linspace(0.0001, 0.02, self.n_timesteps, device=self.device)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'sqrt_alphas_cumprod': torch.sqrt(alphas_cumprod),\n",
    "            'sqrt_one_minus_alphas_cumprod': torch.sqrt(1 - alphas_cumprod)\n",
    "        }\n",
    "        \n",
    "    def setup_optimizers(self, lr_e, lr_d):\n",
    "        \"\"\"Setup optimizers and schedulers\"\"\"\n",
    "        # Encoder optimizer\n",
    "        self.optimizer_E = torch.optim.AdamW(self.netE.parameters(), lr=0.002)               \n",
    "        self.scheduler_E = lr_scheduler.StepLR(self.optimizer_E, step_size=200, gamma=0.5) \n",
    "\n",
    "        # MMD Loss\n",
    "        self.mmd_fn = MMDLoss()   \n",
    "        \n",
    "        # Diffusion model optimizer\n",
    "        diff_params = []\n",
    "        diff_params.extend(self.time_embed.parameters())\n",
    "        diff_params.extend(self.coord_encoder.parameters())\n",
    "        diff_params.extend(self.feat_proj.parameters())\n",
    "        diff_params.extend(self.hierarchical_blocks.parameters())\n",
    "        # diff_params.extend(self.geometric_attention_blocks.parameters())\n",
    "        diff_params.extend(self.physics_layer.parameters())\n",
    "        diff_params.extend(self.uncertainty_head.parameters())\n",
    "        diff_params.extend(self.noise_predictor.parameters())\n",
    "        \n",
    "        if self.cell_type_embedding is not None:\n",
    "            diff_params.extend(self.cell_type_embedding.parameters())\n",
    "            \n",
    "        if self.transport_plan is not None:\n",
    "            diff_params.append(self.ot_guidance_strength)\n",
    "            \n",
    "        self.optimizer_diff = torch.optim.Adam(diff_params, lr=lr_d, betas=(0.9, 0.999))\n",
    "        self.scheduler_diff = lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer_diff, T_0=500)\n",
    "        \n",
    "    def add_noise(self, coords, t, noise_schedule):\n",
    "        \"\"\"Add noise to coordinates according to the diffusion schedule\"\"\"\n",
    "        noise = torch.randn_like(coords)\n",
    "        sqrt_alphas_cumprod_t = noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "        \n",
    "        noisy_coords = sqrt_alphas_cumprod_t * coords + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return noisy_coords, noise\n",
    "        \n",
    "        \n",
    "    def forward_diffusion(self, noisy_coords, t, features, cell_types=None):\n",
    "        \"\"\"Forward pass through the advanced diffusion model\"\"\"\n",
    "        batch_size = noisy_coords.shape[0]\n",
    "        \n",
    "        # Encode inputs\n",
    "        time_emb = self.time_embed(t)\n",
    "        coord_emb = self.coord_encoder(noisy_coords)\n",
    "        \n",
    "        # Process features with optional cell type\n",
    "        if cell_types is not None and self.cell_type_embedding is not None:\n",
    "            cell_type_emb = self.cell_type_embedding(cell_types)\n",
    "            combined_features = torch.cat([features, cell_type_emb], dim=-1)\n",
    "        else:\n",
    "            #when no cell types, pad with zeros to match expected input size\n",
    "            if self.cell_type_embedding is not None:\n",
    "                #create zero padding for cell type embedding\n",
    "                cell_type_dim = self.n_embedding[-1] // 2\n",
    "                zero_padding = torch.zeros(batch_size, cell_type_dim, device=features.device)\n",
    "                combined_features = torch.cat([features, zero_padding], dim=-1)\n",
    "            else:\n",
    "                combined_features = features\n",
    "            # combined_features = features\n",
    "            \n",
    "        feat_emb = self.feat_proj(combined_features)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        h = coord_emb + time_emb + feat_emb\n",
    "        \n",
    "        # Process through hierarchical blocks with geometric attention\n",
    "        for i, block in enumerate(self.hierarchical_blocks):\n",
    "            h = block(h, t)\n",
    "                \n",
    "        # Predict noise\n",
    "        noise_pred = self.noise_predictor(h)\n",
    "        \n",
    "        # Compute physics-informed correction\n",
    "        physics_correction, cell_radii = self.physics_layer(noisy_coords, h, cell_types)\n",
    "        \n",
    "        # Compute uncertainty\n",
    "        uncertainty = self.uncertainty_head(h)\n",
    "        \n",
    "        # Apply corrections based on timestep (less physics at high noise)\n",
    "        t_factor = (1 - t).unsqueeze(-1) #shape: (natch_size, 1)\n",
    "        noise_pred = noise_pred + t_factor * physics_correction * 0.1\n",
    "        \n",
    "        return noise_pred, uncertainty, cell_radii\n",
    "        \n",
    "    def train_encoder(self, n_epochs=1000, ratio_start=0, ratio_end=1.0):\n",
    "        \"\"\"Train the STEM encoder to align ST and SC data\"\"\"\n",
    "        print(\"Training STEM encoder...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting STEM encoder training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, ratio_start={ratio_start}, ratio_end={ratio_end}\\n\")\n",
    "        \n",
    "        # Calculate spatial adjacency matrix\n",
    "        if self.sigma == 0:\n",
    "            nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        else:\n",
    "            nettrue = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                self.st_coords.cpu().numpy(), \n",
    "                self.st_coords.cpu().numpy()\n",
    "            ), device=self.device).to(torch.float32)\n",
    "            \n",
    "            sigma = self.sigma\n",
    "            nettrue = torch.exp(-nettrue**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "            nettrue = F.normalize(nettrue, p=1, dim=1)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Schedule for circle loss weight\n",
    "            ratio = ratio_start + (ratio_end - ratio_start) * min(epoch / (n_epochs * 0.8), 1.0)\n",
    "            \n",
    "            # Forward pass ST data\n",
    "            e_seq_st = self.netE(self.st_gene_expr, True)\n",
    "            \n",
    "            # Sample from SC data due to large size\n",
    "            sc_idx = torch.randint(0, self.sc_gene_expr.shape[0], (min(self.batch_size, self.mmdbatch),), device=self.device)\n",
    "            sc_batch = self.sc_gene_expr[sc_idx]\n",
    "            e_seq_sc = self.netE(sc_batch, False)\n",
    "            \n",
    "            # Calculate losses\n",
    "            self.optimizer_E.zero_grad()\n",
    "            \n",
    "            # Prediction loss (equivalent to netpred in STEM)\n",
    "            netpred = e_seq_st.mm(e_seq_st.t())\n",
    "            loss_E_pred = F.cross_entropy(netpred, nettrue, reduction='mean')\n",
    "            \n",
    "            # Mapping matrices\n",
    "            st2sc = F.softmax(e_seq_st.mm(e_seq_sc.t()), dim=1)\n",
    "            sc2st = F.softmax(e_seq_sc.mm(e_seq_st.t()), dim=1)\n",
    "            \n",
    "            # Circle loss\n",
    "            st2st = torch.log(st2sc.mm(sc2st) + 1e-7)\n",
    "            loss_E_circle = F.kl_div(st2st, nettrue, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # MMD loss\n",
    "            ranidx = torch.randint(0, e_seq_sc.shape[0], (min(self.mmdbatch, e_seq_sc.shape[0]),), device=self.device)\n",
    "            loss_E_mmd = self.mmd_fn(e_seq_st, e_seq_sc[ranidx])\n",
    "            \n",
    "            # Total loss\n",
    "            loss_E = loss_E_pred + self.alpha * loss_E_mmd + ratio * loss_E_circle\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_E.backward()\n",
    "            self.optimizer_E.step()\n",
    "            self.scheduler_E.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 200 == 0:\n",
    "                log_msg = (f\"Encoder epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss_E: {loss_E.item():.6f}, \"\n",
    "                          f\"Loss_E_pred: {loss_E_pred.item():.6f}, \"\n",
    "                          f\"Loss_E_circle: {loss_E_circle.item():.6f}, \"\n",
    "                          f\"Loss_E_mmd: {loss_E_mmd.item():.6f}, \"\n",
    "                          f\"Ratio: {ratio:.4f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'netE_state_dict': self.netE.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_E.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_E.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'encoder_checkpoint_epoch_{epoch}.pt'))\n",
    "    \n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATING SIGMA QUALITY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Evaluate current sigma\n",
    "        with torch.no_grad():\n",
    "            self.netE.eval()\n",
    "            st_embeddings = self.netE(self.st_gene_expr, True)  # Get final ST embeddings\n",
    "            current_overlap = self._evaluate_sigma_quality(st_embeddings, k=10)\n",
    "            print(f\"Current sigma ({self.sigma:.4f}) -> kNN overlap = {current_overlap:.3f}\")\n",
    "        \n",
    "        # Test different sigma values to find optimal\n",
    "        print(\"\\nTesting different sigma values...\")\n",
    "        sigma_candidates = [\n",
    "            self.sigma * 0.5,   # Half current\n",
    "            self.sigma * 0.75,  # 3/4 current  \n",
    "            self.sigma,         # Current (baseline)\n",
    "            self.sigma * 1.25,  # 5/4 current\n",
    "            self.sigma * 1.5,   # 1.5x current\n",
    "            self.sigma * 2.0,    # Double current\n",
    "            self.sigma * 2.5,   # Double current\n",
    "            self.sigma * 3.0,    # Double current\n",
    "            self.sigma * 4.0    # Double current\n",
    "\n",
    "        ]\n",
    "        \n",
    "        overlaps = []\n",
    "        for test_sigma in sigma_candidates:\n",
    "            # Recompute adjacency with test sigma\n",
    "            if test_sigma == 0:\n",
    "                test_nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "            else:\n",
    "                distances = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                    self.st_coords.cpu().numpy(), \n",
    "                    self.st_coords.cpu().numpy()\n",
    "                ), device=self.device).to(torch.float32)\n",
    "                \n",
    "                test_nettrue = torch.exp(-distances**2/(2*test_sigma**2))/(np.sqrt(2*np.pi)*test_sigma)\n",
    "                test_nettrue = F.normalize(test_nettrue, p=1, dim=1)\n",
    "            \n",
    "            # Quick test: how well does current encoder match this adjacency?\n",
    "            with torch.no_grad():\n",
    "                netpred = st_embeddings.mm(st_embeddings.t())\n",
    "                pred_knn = self._get_knn_from_similarity(netpred, k=15)\n",
    "                true_knn = self._get_knn_from_similarity(test_nettrue, k=15)\n",
    "                overlap = (pred_knn == true_knn).float().mean().item()\n",
    "                overlaps.append(overlap)\n",
    "                \n",
    "            print(f\"  sigma = {test_sigma:.4f} -> overlap = {overlap:.5f}\")\n",
    "        \n",
    "        # Find best sigma\n",
    "        best_idx = np.argmax(overlaps)\n",
    "        best_sigma = sigma_candidates[best_idx]\n",
    "        best_overlap = overlaps[best_idx]\n",
    "\n",
    "        # print(overlaps)\n",
    "        \n",
    "        print(f\"\\nBest sigma: {best_sigma:.4f} (overlap = {best_overlap:.5f})\")\n",
    "        if best_sigma != self.sigma:\n",
    "            print(f\"⚠️  Consider using sigma = {best_sigma:.4f} instead of {self.sigma:.4f}\")\n",
    "            print(f\"   Improvement: {best_overlap:.3f} vs {current_overlap:.3f} (+{(best_overlap-current_overlap)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"✅ Current sigma is optimal!\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        # ===================================\n",
    "        \n",
    "        # Save final encoder\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_encoder.pt'))\n",
    "        \n",
    "        print(\"Encoder training complete!\")\n",
    "\n",
    "    def train_graph_vae(self, epochs=800, lr=1e-3, warmup_epochs=320,  # 40% of epochs\n",
    "                    lambda_cov_max=0.3, angle_loss_weight=0.2, \n",
    "                    radius_loss_weight=1.0, angle_warmup_epochs=0, \n",
    "                    beta_final=1e-3):  # Small β_final to prevent blow-up\n",
    "        \"\"\"\n",
    "        Train the Graph-VAE with:\n",
    "        1) normalized KL (no free-bits, mean over batch and dims)\n",
    "        2) covariance warm-up (fix axes)  \n",
    "        3) geometry-anchored angle/radius loss (fix gauge deterministically)\n",
    "        4) NO reconstruction loss, NO gradient loss (as requested)\n",
    "        \"\"\"\n",
    "        print(\"Training Graph-VAE with normalized KL and polar losses...\")\n",
    "        \n",
    "        # Freeze encoder\n",
    "        self.netE.eval()\n",
    "        for p in self.netE.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Build ST graph\n",
    "        adj_idx, adj_w = precompute_knn_edges(self.st_coords_norm, k=30, device=self.device)\n",
    "\n",
    "        # Precompute aligned features (ST only for training)\n",
    "        with torch.no_grad():\n",
    "            st_features_aligned = self.netE(self.st_gene_expr).float()\n",
    "\n",
    "        # Precompute canonical frame and angle/radius targets for ST\n",
    "        print('Computing canonical angular frame for geometry anchoring....')\n",
    "        c, a_theta, R, theta_true, r_true = self._compute_canonical_frame(self.st_coords_norm)\n",
    "\n",
    "        # Precompute true covariance of ST spots\n",
    "        with torch.no_grad():\n",
    "            centered = self.st_coords_norm - self.st_coords_norm.mean(0, keepdim=True)\n",
    "            self.cov_true = (centered.T @ centered) / (centered.shape[0] - 1)\n",
    "\n",
    "        # Optimizer + scheduler\n",
    "        vae_params = list(self.graph_vae_encoder.parameters()) + list(self.graph_vae_decoder.parameters())\n",
    "        optimizer = torch.optim.Adam(vae_params, lr=lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "        # Initialize loss logs\n",
    "        for key in ('total','reconstruction','kl','kl_raw','cov','lambda_cov','angle','radius','beta','epochs'):\n",
    "            self.vae_losses.setdefault(key, [])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss weights with warmups\n",
    "            lambda_cov = lambda_cov_max * min(epoch+1, warmup_epochs) / warmup_epochs\n",
    "            w_theta = 0.0 if epoch < angle_warmup_epochs else angle_loss_weight\n",
    "            w_radius = 0.0 if epoch < angle_warmup_epochs else radius_loss_weight\n",
    "            \n",
    "            # KL warm-up: β from 0 to β_final (small value)\n",
    "            beta = beta_final * min(epoch+1, warmup_epochs) / warmup_epochs\n",
    "\n",
    "            # Forward pass for ST spots only\n",
    "            mu_st, logvar_st = self.graph_vae_encoder(st_features_aligned, adj_idx, adj_w)\n",
    "            z_st = self.graph_vae_encoder.reparameterize(mu_st, logvar_st)\n",
    "            \n",
    "            # Decoder takes ONLY z (no features)\n",
    "            coords_pred_st = self.graph_vae_decoder(z_st)\n",
    "            \n",
    "            # 1) NO Reconstruction loss (as requested)\n",
    "            L_recon = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "            # 2) Normalized KL divergence (Option A - no free-bits)\n",
    "            # KL per dim (positive values)\n",
    "            kl_per_dim = -0.5 * (1 + logvar_st - mu_st.pow(2) - logvar_st.exp())  # [N, latent_dim]\n",
    "            \n",
    "            # Mean over batch AND latent dims (this prevents blow-up)\n",
    "            KL_raw = kl_per_dim.mean()  # O(1) scale, not O(latent_dim)\n",
    "            L_KL = beta * KL_raw\n",
    "\n",
    "            # 3) Covariance alignment (ST spots only, warm-up)\n",
    "            coords_pred_centered = coords_pred_st - coords_pred_st.mean(0, keepdim=True)\n",
    "            cov_pred = (coords_pred_centered.T @ coords_pred_centered) / (coords_pred_centered.shape[0] - 1)\n",
    "            L_cov = F.mse_loss(cov_pred, self.cov_true)\n",
    "\n",
    "            # 4) NO Gradient anchoring (as requested)\n",
    "            # L_grad = 0.0\n",
    "\n",
    "            # 5) Geometry-anchored angle/radius losses\n",
    "            # Compute predicted angles/radii using the SAME canonical frame (c, a_theta, R)\n",
    "            v_hat = coords_pred_st - c \n",
    "            cross = a_theta[0] * v_hat[:, 1] - a_theta[1] * v_hat[:, 0]\n",
    "            dot = a_theta[0] * v_hat[:, 0] + a_theta[1] * v_hat[:, 1]\n",
    "            theta_hat = torch.atan2(cross, dot)\n",
    "            r_hat = v_hat.norm(dim=1) / (R + 1e-8)\n",
    "\n",
    "            # Circular angle loss: 1 - cos(delta)\n",
    "            delta = theta_hat - theta_true\n",
    "            L_angle_raw = (1.0 - torch.cos(delta)).mean()\n",
    "            L_angle = w_theta * L_angle_raw\n",
    "\n",
    "            # Radius loss\n",
    "            L_radius_raw = F.mse_loss(r_hat, r_true)\n",
    "            L_radius = w_radius * L_radius_raw\n",
    "\n",
    "            # 6) Total loss\n",
    "            total_loss = (\n",
    "                L_recon +           # 0.0 as requested\n",
    "                L_KL +              # Small, normalized\n",
    "                lambda_cov * L_cov +\n",
    "                L_angle + \n",
    "                L_radius\n",
    "            )\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log losses\n",
    "            self.vae_losses['total'].append(total_loss.item())\n",
    "            self.vae_losses['reconstruction'].append(L_recon.item())\n",
    "            self.vae_losses['kl'].append(L_KL.item())\n",
    "            self.vae_losses['kl_raw'].append(KL_raw.item())\n",
    "            self.vae_losses['cov'].append(L_cov.item())\n",
    "            self.vae_losses['lambda_cov'].append(lambda_cov)\n",
    "            self.vae_losses['angle'].append(L_angle.item())\n",
    "            self.vae_losses['radius'].append(L_radius.item())\n",
    "            self.vae_losses['beta'].append(beta)\n",
    "            self.vae_losses['epochs'].append(epoch)\n",
    "\n",
    "            if epoch % 100 == 0 or epoch == epochs-1:\n",
    "                # Compute interpretable angle error in degrees\n",
    "                with torch.no_grad():\n",
    "                    mean_angle_err_deg = torch.mean(torch.abs(torch.rad2deg(torch.atan2(torch.sin(delta), torch.cos(delta))))).item()\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs}  \"\n",
    "                    f\"Loss={total_loss:.4f}  \"\n",
    "                    f\"L_recon={L_recon:.4f}  \"\n",
    "                    f\"L_KL={L_KL:.6f}(β={beta:.6f})  \"\n",
    "                    f\"KL_raw={KL_raw:.4f}  \"\n",
    "                    f\"L_cov={L_cov:.4f}  \"\n",
    "                    f\"L_angle={L_angle:.4f}  \"\n",
    "                    f\"L_radius={L_radius:.4f}  \"\n",
    "                    f\"AngleErr={mean_angle_err_deg:.2f}°\")\n",
    "\n",
    "        print(\"Graph-VAE training complete.\")\n",
    "        \n",
    "        # Diagnostic: Check if decoder uses z meaningfully\n",
    "        print(\"\\n=== DECODER DEPENDENCY DIAGNOSTIC ===\")\n",
    "        with torch.no_grad():\n",
    "            # Fix features, vary z\n",
    "            mu_fixed = mu_st[:5]\n",
    "            logvar_fixed = logvar_st[:5]\n",
    "            \n",
    "            coords_samples = []\n",
    "            for _ in range(5):\n",
    "                z_sample = self.graph_vae_encoder.reparameterize(mu_fixed, logvar_fixed)\n",
    "                coords_sample = self.graph_vae_decoder(z_sample)\n",
    "                coords_samples.append(coords_sample)\n",
    "            \n",
    "            coords_stack = torch.stack(coords_samples)  # (5, 5, 2)\n",
    "            coord_variance = coords_stack.var(dim=0).mean().item()  # Average variance across samples\n",
    "            \n",
    "            print(f\"Coordinate variance from z sampling: {coord_variance:.6f}\")\n",
    "            if coord_variance < 1e-4:\n",
    "                print(\"⚠️  WARNING: Low variance suggests potential posterior collapse!\")\n",
    "            else:\n",
    "                print(\"✅ Good: Decoder shows dependency on z\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    def _compute_canonical_frame(self, X_st):\n",
    "        '''compute canonical angular frame from ST coordinates for geometry anchoring\n",
    "        \n",
    "        returns:\n",
    "            c: centroid (2, )\n",
    "            a_theta: reference direction vector (2, )\n",
    "            R: max radius (scalar)\n",
    "            theta_true: true_angles (N, )\n",
    "            r_true: true normalized radii (N,)\n",
    "        '''\n",
    "        #centroid\n",
    "        c = X_st.mean(dim=0)\n",
    "\n",
    "        #find farthest point to define reference direction\n",
    "        d = torch.linalg.norm(X_st - c, dim=1)\n",
    "        A = torch.argmax(d).item()\n",
    "        a_theta = (X_st[A] - c)\n",
    "        R = d.max().clamp_min(1e-8)\n",
    "\n",
    "        #compute true angles and radii for all points\n",
    "        v = X_st - c\n",
    "        cross = a_theta[0] * v[:, 1] - a_theta[1] * v[:, 0]\n",
    "        dot = a_theta[0] * v[:, 0] + a_theta[1] * v[:, 1]\n",
    "        theta_true = torch.atan2(cross, dot)\n",
    "        r_true = (v.norm(dim=1) / R)\n",
    "\n",
    "        print(f\"Canonical frame: center=({c[0]:.3f}, {c[1]:.3f}), \"\n",
    "            f\"ref_dir=({a_theta[0]:.3f}, {a_theta[1]:.3f}), max_radius={R:.3f}\")\n",
    "        \n",
    "        return c, a_theta, R, theta_true, r_true\n",
    "    \n",
    "    def _compute_spatial_pc1(self):\n",
    "        \"\"\"\n",
    "        Compute first spatial principal component from continuous SVGs for anchoring\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        \n",
    "        # Step 1: Filter to continuous genes (5%-95% expression)\n",
    "        st_expr = self.st_gene_expr.cpu().numpy()\n",
    "        nonzero_frac = (st_expr > 0).mean(0)\n",
    "        mask = (nonzero_frac >= 0.05) & (nonzero_frac <= 0.95)\n",
    "        expr_cont = st_expr[:, mask]\n",
    "        \n",
    "        print(f\"Filtered to {expr_cont.shape[1]} continuous genes from {st_expr.shape[1]} total\")\n",
    "        \n",
    "        if expr_cont.shape[1] < 10:\n",
    "            print(\"Warning: Too few continuous genes, using all genes\")\n",
    "            expr_cont = st_expr\n",
    "        \n",
    "        # Step 2: Smooth expression and compute PCA\n",
    "        expr_smooth = gaussian_filter(expr_cont, sigma=(1, 0))  # smooth over spots only\n",
    "        \n",
    "        # Compute first PC\n",
    "        svd = TruncatedSVD(n_components=1, random_state=42)\n",
    "        pc1 = svd.fit_transform(expr_smooth).flatten()\n",
    "        \n",
    "        print(f\"PC-1 explains {svd.explained_variance_ratio_[0]:.3f} of spatial variance\")\n",
    "        \n",
    "        return torch.tensor(pc1, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def train_diffusion_latent(self, n_epochs=400, lambda_struct=10.0, p_drop=0.05, posterior_temp_floor=0.3):\n",
    "        \"\"\"\n",
    "        Train latent-space conditional DDPM as a proper conditional prior p(z|h).\n",
    "        \"\"\"\n",
    "        print(\"Training latent-space diffusion model...\")\n",
    "        \n",
    "        # Freeze encoder and Graph-VAE encoder\n",
    "        self.netE.eval()\n",
    "        self.graph_vae_encoder.eval()\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.graph_vae_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Precompute fixed ST latents as specified\n",
    "        print(\"Computing fixed ST latents...\")\n",
    "        st_adj_idx, st_adj_w = precompute_knn_edges(self.st_coords_norm, k=30, device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            st_features_aligned = self.netE(self.st_gene_expr).float()\n",
    "            st_mu, st_logvar = self.graph_vae_encoder(st_features_aligned, st_adj_idx, st_adj_w)\n",
    "            # z_st = self.graph_vae_encoder.reparameterize(st_mu, st_logvar)\n",
    "        \n",
    "        # Setup optimizer for latent denoiser\n",
    "        optimizer_latent = torch.optim.AdamW(\n",
    "            self.latent_denoiser.parameters(), \n",
    "            lr=self.optimizer_diff.param_groups[0]['lr'], \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        scheduler_latent = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer_latent, T_max=n_epochs, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Training loop - identical to old train_diffusion but in latent space\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch indices\n",
    "            idx = torch.randperm(len(st_mu))[:self.batch_size]\n",
    "            batch_mu = st_mu[idx]\n",
    "            batch_logvar = st_logvar[idx]\n",
    "            batch_h = st_features_aligned[idx]\n",
    "            \n",
    "            # Sample FRESH z0 from posterior (with temperature floor)\n",
    "            eps0 = torch.randn_like(batch_mu)\n",
    "            posterior_std = torch.sqrt(torch.exp(batch_logvar) + posterior_temp_floor**2)\n",
    "            z0 = batch_mu + posterior_std * eps0\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = torch.randint(0, self.n_timesteps, (len(z0),), device=self.device)\n",
    "            \n",
    "            # Forward diffusion: add noise to z0\n",
    "            eps = torch.randn_like(z0)\n",
    "            alpha_bar_t = self.noise_schedule['alphas_cumprod'][t].view(-1, 1)\n",
    "            z_t = torch.sqrt(alpha_bar_t) * z0 + torch.sqrt(1 - alpha_bar_t) * eps\n",
    "            \n",
    "            # Classifier-free guidance: randomly drop conditioning\n",
    "            cond = batch_h.clone()\n",
    "            drop_mask = (torch.rand(len(cond), 1, device=self.device) < p_drop).float()\n",
    "            cond = cond * (1 - drop_mask)  # Zero out dropped rows\n",
    "            \n",
    "            # Predict noise\n",
    "            t_norm = (t.float().unsqueeze(1) / max(self.n_timesteps - 1, 1)).clamp(0, 1)\n",
    "            # t_norm = torch.full((B, 1), t / max(self.n_timesteps - 1, 1), device=self.device)\n",
    "\n",
    "            eps_pred = self.latent_denoiser(z_t, t_norm, cond)\n",
    "\n",
    "            # v_tgt = torch.sqrt(alpha_bar_t) * eps - torch.sqrt(1 - alpha_bar_t) * z0\n",
    "            # v_pred = self.latent_denoiser(z_t, t_norm, cond)\n",
    "            \n",
    "            # Diffusion loss\n",
    "            loss_diffusion = F.mse_loss(eps_pred, eps)\n",
    "            # loss_diffusion = F.mse_loss(v_pred, v_tgt)\n",
    "\n",
    "            \n",
    "            # Structure loss on x0 (denoised latent)\n",
    "            loss_struct = torch.tensor(0.0, device=self.device)\n",
    "            if lambda_struct > 0:\n",
    "                # Predict x0 from eps_pred\n",
    "                x0_pred = (z_t - torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t + 1e-8)\n",
    "                \n",
    "                # Preserve latent geometry\n",
    "                D_target = torch.cdist(z0, z0, p=2)\n",
    "                D_pred = torch.cdist(x0_pred, x0_pred, p=2)\n",
    "                loss_struct = F.mse_loss(D_pred, D_target)\n",
    "            \n",
    "            # Total loss\n",
    "            # total_loss = loss_diffusion + lambda_struct * loss_struct\n",
    "            total_loss = loss_diffusion\n",
    "\n",
    "            # Record losses for plotting\n",
    "            self.latent_diffusion_losses['total'].append(total_loss.item())\n",
    "            self.latent_diffusion_losses['diffusion'].append(loss_diffusion.item())\n",
    "            self.latent_diffusion_losses['struct'].append(loss_struct.item() if isinstance(loss_struct, torch.Tensor) else loss_struct)\n",
    "            self.latent_diffusion_losses['epochs'].append(epoch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer_latent.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.latent_denoiser.parameters(), 1.0)\n",
    "            optimizer_latent.step()\n",
    "            scheduler_latent.step()\n",
    "            \n",
    "            # Logging\n",
    "            if epoch % 500 == 0:\n",
    "                ls = float(loss_struct) if isinstance(loss_struct, torch.Tensor) else loss_struct\n",
    "                log_msg = (\n",
    "                    f\"Latent Diffusion epoch {epoch}/{n_epochs}, \"\n",
    "                    f\"Total: {total_loss.item():.6f}, \"\n",
    "                    f\"Diffusion: {loss_diffusion.item():.6f}, \"\n",
    "                    f\"Struct: {ls:.6f}\"\n",
    "                )\n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if epoch % 500 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'latent_denoiser_state_dict': self.latent_denoiser.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer_latent.state_dict(),\n",
    "                }, os.path.join(self.outf, f'latent_diffusion_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'latent_denoiser_state_dict': self.latent_denoiser.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_latent_diffusion.pt'))\n",
    "        \n",
    "        print(\"Latent diffusion training complete!\")\n",
    "                        \n",
    "\n",
    "    def train(self, encoder_epochs=1000, vae_epochs=800, diffusion_epochs=400, **kwargs):\n",
    "        \"\"\"\n",
    "        Combined training pipeline: encoder → graph_vae → diffusion_latent\n",
    "        ⚠️ Do **not** touch `train_encoder`; its aligned embeddings are the sole conditioning signal throughout.\n",
    "        \"\"\"\n",
    "        print(\"Starting Graph-VAE + Latent Diffusion training pipeline...\")\n",
    "        \n",
    "        # Stage 1: Train encoder (DO NOT MODIFY - keep existing train_encoder)\n",
    "        print(\"Stage 1: Training domain alignment encoder...\")\n",
    "        self.train_encoder(n_epochs=encoder_epochs)\n",
    "        \n",
    "        # Stage 2: Train Graph-VAE\n",
    "        print(\"Stage 2: Training Graph-VAE...\")\n",
    "        self.train_graph_vae(epochs=vae_epochs)\n",
    "        \n",
    "        # Stage 3: Train latent diffusion\n",
    "        print(\"Stage 3: Training latent diffusion...\")\n",
    "        self.train_diffusion_latent(n_epochs=diffusion_epochs, **kwargs)\n",
    "        \n",
    "        print(\"Complete training pipeline finished!\")\n",
    "\n",
    "    def refine_coordinates_scale_free(self, coords, tau=0.9):\n",
    "        '''scale free coordinate refinement using relative min seperation\n",
    "        \n",
    "        tau: factor for minimum seperation (0.8-1.0) where 1.0 is tight'''\n",
    "\n",
    "        device = coords.device\n",
    "        n = coords.shape[0]\n",
    "\n",
    "        def compute_nearest_neighbor_distances(X):\n",
    "            '''compute distance to nearest enighbor for each point'''\n",
    "            distances = torch.cdist(X, X)\n",
    "            distances.fill_diagonal_(float('inf'))\n",
    "            nn_distances, _ = distances.min(dim=1)\n",
    "            return nn_distances\n",
    "        \n",
    "        def pairwise_distances_squared(X):\n",
    "            '''compute pairwise squared distances'''\n",
    "            s = (X * X).sum(1, keepdim=True)\n",
    "            S = s + s.T - 2 * X @ X.T\n",
    "            S = S.clamp_min_(0.0)\n",
    "            S.fill_diagonal_(0.0)\n",
    "            return S\n",
    "        \n",
    "        def gram_matrix(S):\n",
    "            '''convert gram matrix back to distance matrix'''\n",
    "            r = S.mean(1, keepdim=True)\n",
    "            c = S.mean(0, keepdim=True)\n",
    "            m = S.mean()\n",
    "            B = -0.5 * ( S - r - c + m)\n",
    "            return B\n",
    "        \n",
    "        def distance_from_gram(B):\n",
    "            \"\"\"Convert Gram matrix back to distance matrix\"\"\"\n",
    "            d = torch.diag(B)\n",
    "            S = d[:, None] + d[None, :] - 2 * B\n",
    "            S = S.clamp_min_(0.0)\n",
    "            S.fill_diagonal_(0.0)\n",
    "            return S\n",
    "        \n",
    "        def project_to_euclidean(S):\n",
    "            \"\"\"Project distance matrix to Euclidean space\"\"\"\n",
    "            B = gram_matrix(S)\n",
    "            vals, vecs = torch.linalg.eigh(B)\n",
    "            vals = vals.clamp_min(0.0)\n",
    "            \n",
    "            # Keep top 2 eigenvalues for 2D\n",
    "            idx = torch.argsort(vals, descending=True)[:2]\n",
    "            vals = vals[idx]\n",
    "            vecs = vecs[:, idx]\n",
    "            \n",
    "            B_proj = (vecs * vals.sqrt()) @ (vecs * vals.sqrt()).T\n",
    "            return distance_from_gram(B_proj)\n",
    "        \n",
    "        def enforce_min_separation(S, min_dist):\n",
    "            \"\"\"Enforce minimum distance constraints\"\"\"\n",
    "            S = S.clone()\n",
    "            min_dist_sq = min_dist ** 2\n",
    "            mask = ~torch.eye(n, dtype=torch.bool, device=device)\n",
    "            S[mask] = torch.maximum(S[mask], torch.full_like(S[mask], min_dist_sq))\n",
    "            S.fill_diagonal_(0.0)\n",
    "            return 0.5 * (S + S.T)\n",
    "        \n",
    "        def coords_from_distances(S):\n",
    "            \"\"\"Extract 2D coordinates from distance matrix\"\"\"\n",
    "            B = gram_matrix(S)\n",
    "            vals, vecs = torch.linalg.eigh(B)\n",
    "            idx = torch.argsort(vals, descending=True)[:2]\n",
    "            L = torch.diag(vals[idx].clamp_min(0.0).sqrt())\n",
    "            U = vecs[:, idx] @ L\n",
    "            return U\n",
    "\n",
    "        \n",
    "        def align_to_original(U, X_orig):\n",
    "            Um = U.mean(0)\n",
    "            Xm = X_orig.mean(0)\n",
    "            Uc = U - Um\n",
    "            Xc = X_orig - Xm  # Fixed: was X_orig - Xc\n",
    "            M = Uc.T @ Xc\n",
    "            U_svd, _, Vt = torch.linalg.svd(M, full_matrices=False)\n",
    "            R = U_svd @ Vt\n",
    "            # avoid reflection\n",
    "            if torch.linalg.det(R) < 0:\n",
    "                U_svd[:, -1] *= -1\n",
    "                R = U_svd @ Vt\n",
    "            t = Xm - (Um @ R)\n",
    "            return U @ R + t\n",
    "\n",
    "\n",
    "        \n",
    "        # Step 1: Compute scale-free minimum separation\n",
    "        nn_distances = compute_nearest_neighbor_distances(coords)\n",
    "        median_nn_dist = torch.median(nn_distances)\n",
    "        min_separation = tau * median_nn_dist\n",
    "        \n",
    "        print(f\"Nearest neighbor distances: min={nn_distances.min():.4f}, \"\n",
    "            f\"median={median_nn_dist:.4f}, max={nn_distances.max():.4f}\")\n",
    "        print(f\"Using minimum separation: {min_separation:.4f} (tau={tau})\")\n",
    "        \n",
    "        # Step 2: Refinement loop\n",
    "        S = pairwise_distances_squared(coords)\n",
    "        \n",
    "        for iteration in range(100):\n",
    "            S_prev = S.clone()\n",
    "            \n",
    "            # Project to Euclidean distance matrix\n",
    "            S = project_to_euclidean(S)\n",
    "            \n",
    "            # Enforce minimum separation\n",
    "            S = enforce_min_separation(S, min_separation)\n",
    "            \n",
    "            # Check convergence\n",
    "            change = (S - S_prev).norm() / (S_prev.norm() + 1e-12)\n",
    "            if change < 1e-6:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        # Step 3: Extract refined coordinates\n",
    "        coords_refined = coords_from_distances(S)\n",
    "        coords_final = align_to_original(coords_refined, coords)\n",
    "        # coords_final = lock_orientation(coords_refined, coords)\n",
    "        \n",
    "        # Compute final statistics\n",
    "        final_nn_distances = compute_nearest_neighbor_distances(coords_final)\n",
    "        print(f\"After refinement - min distance: {final_nn_distances.min():.4f}, \"\n",
    "            f\"median: {torch.median(final_nn_distances):.4f}\")\n",
    "        \n",
    "        return coords_final\n",
    "\n",
    "    def sample_sc_coordinates_batched(self, batch_size=512, guidance_scale=1.0, refine_coords=True, refinement_tau=0.9):\n",
    "        \"\"\"\n",
    "        Sample SC coordinates using pure noise → guided denoising → decode.\n",
    "        No Graph-VAE latents used during sampling.\n",
    "        \"\"\"\n",
    "        n_total = len(self.sc_gene_expr)\n",
    "        print(f\"Sampling {n_total} SC coordinates using pure noise → guided diffusion → decode...\")\n",
    "        print(f\"Using guidance scale: {guidance_scale}\")\n",
    "        \n",
    "        # Set models to eval mode\n",
    "        self.netE.eval()\n",
    "        self.graph_vae_decoder.eval()\n",
    "        self.latent_denoiser.eval()\n",
    "        \n",
    "        # Update noise schedule for current n_timesteps\n",
    "        self.update_noise_schedule()\n",
    "        \n",
    "        all_coords = []\n",
    "        n_batches = (n_total + batch_size - 1) // batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, n_total)\n",
    "                batch_sc_expr = self.sc_gene_expr[start_idx:end_idx]\n",
    "                \n",
    "                print(f\"Processing batch {batch_idx + 1}/{n_batches} ({len(batch_sc_expr)} cells)...\")\n",
    "                \n",
    "                # Get aligned SC embeddings\n",
    "                h_sc = self.netE(batch_sc_expr).float()\n",
    "                B = h_sc.size(0)\n",
    "                \n",
    "                # Start from PURE NOISE in latent space\n",
    "                z_t = torch.randn(B, self.latent_dim, device=self.device)\n",
    "                \n",
    "                # Reverse diffusion with classifier-free guidance\n",
    "                for t in reversed(range(self.n_timesteps)):\n",
    "                    # t_norm = torch.full((B, 1), t / (self.n_timesteps - 1), device=self.device)\n",
    "                    t_norm = torch.full((B, 1), t / max(self.n_timesteps - 1, 1), device=self.device)\n",
    "                    \n",
    "                    # Classifier-free guidance: conditional and unconditional predictions\n",
    "                    eps_c = self.latent_denoiser(z_t, t_norm, h_sc)\n",
    "                    eps_u = self.latent_denoiser(z_t, t_norm, torch.zeros_like(h_sc))\n",
    "                    eps_pred = (1 + guidance_scale) * eps_c - guidance_scale * eps_u\n",
    "\n",
    "                    # eps_pred = eps_pred.view(B, self.latent_dim)\n",
    "                    \n",
    "                    # DDPM reverse step\n",
    "                    alpha_t = self.noise_schedule['alphas'][t]\n",
    "                    alpha_bar_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                    beta_t = self.noise_schedule['betas'][t]\n",
    "                    \n",
    "                    # Compute mean\n",
    "                    mu = (1 / torch.sqrt(alpha_t)) * (\n",
    "                        z_t - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * eps_pred\n",
    "                    )\n",
    "\n",
    "                    # mu = mu.view(B, self.latent_dim)\n",
    "                    \n",
    "                    if t > 0:\n",
    "                        # Add noise (use posterior variance for better sampling)\n",
    "                        sigma_t = torch.sqrt(self.noise_schedule['posterior_variance'][t])\n",
    "                        noise = torch.randn_like(z_t)\n",
    "                        z_t = mu + sigma_t * noise\n",
    "                    else:\n",
    "                        z_t = mu\n",
    "                \n",
    "                # Decode final latent to coordinates (z ONLY, no features)\n",
    "                batch_coords = self.graph_vae_decoder(z_t)\n",
    "\n",
    "                \n",
    "                # Move to CPU and store\n",
    "                all_coords.append(batch_coords.cpu()) \n",
    "                \n",
    "                # Clear GPU cache\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine all batches\n",
    "        final_coords = torch.cat(all_coords, dim=0)\n",
    "\n",
    "        # Scale-free coordinate refinement on ALL coordinates\n",
    "        if refine_coords:\n",
    "            print(\"Applying scale-free refinement to all coordinates...\")\n",
    "            # Convert back to tensor on device for refinement\n",
    "            coords_tensor = final_coords.to(self.device)\n",
    "            refined_coords = self.refine_coordinates_scale_free(coords_tensor, tau=refinement_tau)\n",
    "            final_coords = refined_coords.cpu()\n",
    "        \n",
    "        print(\"Pure noise → guided diffusion → decode sampling complete!\")\n",
    "        return final_coords.cpu().numpy()\n",
    "    \n",
    "    def sample_sc_coordinates_with_geometry_guidance(\n",
    "        self,\n",
    "        batch_size=512,\n",
    "        guidance_scale=1.0,\n",
    "        geometry_guidance=True,\n",
    "        k_nn=15, margin=0.05,\n",
    "        lambda_trip=0.05, lambda_rep=0.08, lambda_ang=0.01,\n",
    "        gamma=0.05,\n",
    "        repulsion_sigma=None,\n",
    "        temp_sigma=1.0,          # optional diversity on reverse noise (1.0 = off)\n",
    "        denoiser_chunk=None,     # e.g., 256 if denoiser is big; None = full batch\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ε-prediction sampler with geometry guidance and tight memory use.\n",
    "        Uses the denoising logic from `sample_sc_coordinates_batched`.\n",
    "        \"\"\"\n",
    "        n_total = len(self.sc_gene_expr)\n",
    "        print(f\"Sampling {n_total} SC coordinates (geom_guidance={geometry_guidance}, k={k_nn})\")\n",
    "        # eval + schedule\n",
    "        self.netE.eval(); self.graph_vae_encoder.eval()\n",
    "        self.graph_vae_decoder.eval(); self.latent_denoiser.eval()\n",
    "        self.update_noise_schedule()\n",
    "\n",
    "        # ---- repulsion scale from ST once (cheap; subsample if ST is huge) ----\n",
    "        if geometry_guidance and repulsion_sigma is None:\n",
    "            with torch.no_grad():\n",
    "                n_sub = min(1024, self.st_coords_norm.shape[0])\n",
    "                idx = torch.randperm(self.st_coords_norm.shape[0], device=self.device)[:n_sub]\n",
    "                st_sub = self.st_coords_norm[idx]\n",
    "                D = torch.cdist(st_sub, st_sub)                              # (n_sub, n_sub)\n",
    "                # 5-NN distance (k=6 incl. self)\n",
    "                kth = torch.kthvalue(D, 6, dim=1).values\n",
    "                repulsion_sigma = (kth.median().item() / 2.5) or 0.1\n",
    "            # free temp tensor\n",
    "            del D\n",
    "\n",
    "        # optional micro-batched denoiser to cap peak activation memory\n",
    "        def _denoise_eps(z, tnorm, cond):\n",
    "            if denoiser_chunk is None:\n",
    "                return self.latent_denoiser(z, tnorm, cond)\n",
    "            outs = []\n",
    "            for i in range(0, z.size(0), denoiser_chunk):\n",
    "                outs.append(self.latent_denoiser(z[i:i+denoiser_chunk], tnorm[i:i+denoiser_chunk], cond[i:i+denoiser_chunk]))\n",
    "            return torch.cat(outs, dim=0)\n",
    "\n",
    "        all_coords = []\n",
    "        n_batches = (n_total + batch_size - 1) // batch_size\n",
    "\n",
    "        with torch.no_grad():  # keep denoiser & DDPM math gradient-free\n",
    "            for b in range(n_batches):\n",
    "                s = b * batch_size\n",
    "                e = min(s + batch_size, n_total)\n",
    "                batch_sc_expr = self.sc_gene_expr[s:e]\n",
    "\n",
    "                # conditioning embedding\n",
    "                h_sc = self.netE(batch_sc_expr).float()           # (B, d_embed)\n",
    "                B = h_sc.size(0)\n",
    "\n",
    "                # --- kNN once per batch (embedding space) ---\n",
    "                D_h = torch.cdist(h_sc, h_sc)                     # (B, B) ~ 1 MB when B=512\n",
    "                nn_idx = D_h.topk(k=k_nn+1, largest=False).indices[:, 1:]    # (B, k)\n",
    "                far_idx = D_h.topk(k=1, largest=True).indices.squeeze(1)     # (B,)\n",
    "\n",
    "                # --- baseline for tiny angle anchor (SC kNN graph; cheap) ---\n",
    "                sc_adj_idx, sc_adj_w = precompute_knn_edges(h_sc, k=30, device=self.device)\n",
    "                mu_sc, _ = self.graph_vae_encoder(h_sc, sc_adj_idx, sc_adj_w)  # (B, d_z)\n",
    "                y_base = self.graph_vae_decoder(mu_sc)                         # (B, 2)\n",
    "\n",
    "                # init from noise in latent space\n",
    "                z_t = torch.randn(B, self.latent_dim, device=self.device)\n",
    "\n",
    "                # -------- reverse diffusion (ε-pred, classifier-free) --------\n",
    "                for t in reversed(range(self.n_timesteps)):\n",
    "                    # normalized timestep\n",
    "                    t_norm = torch.full((B, 1), t / max(self.n_timesteps - 1, 1), device=self.device)\n",
    "\n",
    "                    # ε-pred with CF guidance (denoiser in inference mode → tiny memory)\n",
    "                    with torch.inference_mode():\n",
    "                        eps_c = _denoise_eps(z_t, t_norm, h_sc)\n",
    "                        eps_u = _denoise_eps(z_t, t_norm, torch.zeros_like(h_sc))\n",
    "                        eps_pred = (1.0 + guidance_scale) * eps_c - guidance_scale * eps_u\n",
    "\n",
    "                    # DDPM reverse mean (same as your old code)\n",
    "                    alpha_t = self.noise_schedule['alphas'][t]\n",
    "                    alpha_bar_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                    mu = (1.0 / torch.sqrt(alpha_t)) * (\n",
    "                        z_t - ((1.0 - alpha_t) / torch.sqrt(1.0 - alpha_bar_t)) * eps_pred\n",
    "                    )\n",
    "\n",
    "                    # ---- geometry guidance: tiny grad block on decoder only ----\n",
    "                    if geometry_guidance and t > 0:\n",
    "                        with torch.enable_grad():\n",
    "                            z_tmp = mu.detach().requires_grad_(True)\n",
    "                            y_t = self.graph_vae_decoder(z_tmp)               # (B,2)\n",
    "\n",
    "                            # Triplet hinge on decoded coords (neighbors only)\n",
    "                            yi = y_t\n",
    "                            yj = yi[nn_idx]                                   # (B,k,2)\n",
    "                            yk = yi[far_idx].unsqueeze(1).expand_as(yj)       # (B,k,2)\n",
    "                            d_pos = (yi.unsqueeze(1) - yj).norm(dim=2)        # (B,k)\n",
    "                            d_neg = (yi.unsqueeze(1) - yk).norm(dim=2)        # (B,k)\n",
    "                            L_trip = torch.relu(d_pos + margin - d_neg).mean()\n",
    "\n",
    "                            # Sparse repulsion: only neighbor pairs (avoid O(B^2))\n",
    "                            rows = torch.arange(B, device=self.device).unsqueeze(1).expand(-1, k_nn)\n",
    "                            yi_n = yi[rows.reshape(-1)]                       # (B*k, 2)\n",
    "                            yj_n = yi[nn_idx.reshape(-1)]                     # (B*k, 2)\n",
    "                            d2_n = (yi_n - yj_n).pow(2).sum(dim=1)            # (B*k,)\n",
    "                            E_rep = torch.exp(-d2_n / (2 * (repulsion_sigma ** 2))).sum()\n",
    "\n",
    "                            # Light angular anchor vs baseline\n",
    "                            theta  = torch.atan2(y_t[:, 1], y_t[:, 0])\n",
    "                            theta0 = torch.atan2(y_base[:, 1], y_base[:, 0])\n",
    "                            E_ang = (1 - torch.cos(theta - theta0)).mean()\n",
    "\n",
    "                            E_geo = lambda_trip * L_trip + lambda_rep * E_rep + lambda_ang * E_ang\n",
    "                            g = torch.autograd.grad(E_geo, z_tmp, retain_graph=False, create_graph=False)[0]\n",
    "                            g = g / (g.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "                        # subtract grad (minimize geometry energy) and detach to cut graph\n",
    "                        mu = (mu - gamma * g).detach()\n",
    "\n",
    "                    # add noise with posterior variance (optionally temperature-scaled)\n",
    "                    if t > 0:\n",
    "                        sigma_post = torch.sqrt(self.noise_schedule['posterior_variance'][t])\n",
    "                        z_t = mu + sigma_post * temp_sigma * torch.randn_like(z_t)\n",
    "                    else:\n",
    "                        z_t = mu\n",
    "\n",
    "                # decode final latent → coords (z only)\n",
    "                batch_coords = self.graph_vae_decoder(z_t)\n",
    "                all_coords.append(batch_coords.cpu())\n",
    "\n",
    "                # free short-lived tensors\n",
    "                del D_h, nn_idx, far_idx, sc_adj_idx, sc_adj_w, mu_sc, y_base, z_t\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        coords_final = torch.cat(all_coords, dim=0)\n",
    "        print(f\"Generated {coords_final.shape[0]} SC coordinates.\")\n",
    "        return coords_final\n",
    "        # return self._denorm_self(coords_final)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _denorm_self(self, coords_norm: torch.Tensor, target_center=None, target_radius=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Recenter on SC itself; scale by target_radius (default=1.0).\n",
    "        No ST dependency unless you choose target_radius tied to ST elsewhere.\n",
    "        \"\"\"\n",
    "        c_sc = coords_norm.mean(dim=0, keepdim=True)                  # (1,2)\n",
    "        x0 = coords_norm - c_sc                                       # recenter\n",
    "        R = 1.0 if target_radius is None else float(target_radius)\n",
    "        t = torch.zeros_like(c_sc) if target_center is None else torch.as_tensor(target_center, device=coords_norm.device).view(1,2)\n",
    "        return x0 * R + t\n",
    "    \n",
    "\n",
    "    def denormalize_coordinates(self, normalized_coords):\n",
    "        '''convert normalized coords back to original scale'''\n",
    "        if isinstance(normalized_coords, torch.Tensor):\n",
    "            coords_radius = self.coords_radius.to(normalized_coords.device)\n",
    "            coords_center = self.coords_center.to(normalized_coords.device)\n",
    "            original_coords = normalized_coords * coords_radius + coords_center\n",
    "            return original_coords\n",
    "        else:\n",
    "            coords_radius = self.coords_radius.cpu().numpy()\n",
    "            coords_center = self.coords_center.cpu().numpy()\n",
    "            original_coords = normalized_coords * coords_radius + coords_center\n",
    "            return original_coords\n",
    "\n",
    "\n",
    "    def _compute_geometry_metrics(self, sc_coords, h_sc):\n",
    "        \"\"\"\n",
    "        Compute geometry preservation metrics for evaluation.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            metrics = {}\n",
    "            \n",
    "            # 1. kNN agreement between embedding and coordinates\n",
    "            k_values = [5, 10]\n",
    "            for k in k_values:\n",
    "                # Embedding kNN\n",
    "                D_h = torch.cdist(h_sc, h_sc)\n",
    "                emb_knn = D_h.topk(k=k+1, largest=False).indices[:,1:]  # Exclude self\n",
    "                \n",
    "                # Coordinate kNN  \n",
    "                D_coord = torch.cdist(sc_coords, sc_coords)\n",
    "                coord_knn = D_coord.topk(k=k+1, largest=False).indices[:,1:]\n",
    "                \n",
    "                # Jaccard similarity\n",
    "                intersection = (emb_knn.unsqueeze(2) == coord_knn.unsqueeze(1)).any(dim=2).sum(dim=1)\n",
    "                jaccard = intersection.float() / (2 * k - intersection.float())\n",
    "                metrics[f'knn_jaccard_k{k}'] = jaccard.mean().item()\n",
    "            \n",
    "            # 2. Collision detection (minimum distances)\n",
    "            min_distances = D_coord.fill_diagonal_(float('inf')).min(dim=1).values\n",
    "            metrics['min_distance_mean'] = min_distances.mean().item()\n",
    "            metrics['min_distance_std'] = min_distances.std().item()\n",
    "            metrics['collision_rate'] = (min_distances < 0.01).float().mean().item()\n",
    "            \n",
    "            # 3. Radial distribution compared to ST\n",
    "            sc_radii = torch.norm(sc_coords, dim=1)\n",
    "            st_radii = torch.norm(self.st_coords_norm, dim=1)\n",
    "            \n",
    "            # KS test statistic (approximation)\n",
    "            sc_sorted = torch.sort(sc_radii).values\n",
    "            st_sorted = torch.sort(st_radii).values\n",
    "            \n",
    "            # Interpolate to same length for comparison\n",
    "            from scipy import stats\n",
    "            ks_stat, ks_pvalue = stats.ks_2samp(\n",
    "                sc_sorted.cpu().numpy(), \n",
    "                st_sorted.cpu().numpy()\n",
    "            )\n",
    "            metrics['radial_ks_statistic'] = ks_stat\n",
    "            metrics['radial_ks_pvalue'] = ks_pvalue\n",
    "            \n",
    "            return metrics\n",
    "\n",
    "    # Add this to your diffusion model class\n",
    "    def evaluate_geometry_preservation(self, sc_coords, print_results=True):\n",
    "        \"\"\"\n",
    "        Evaluate how well the generated coordinates preserve geometric structure.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            sc_coords = sc_coords.to(self.device)\n",
    "            h_sc = self.netE(self.sc_gene_expr).float()\n",
    "            metrics = self._compute_geometry_metrics(sc_coords, h_sc)\n",
    "            \n",
    "            if print_results:\n",
    "                print(\"\\n=== Geometry Preservation Metrics ===\")\n",
    "                print(f\"kNN Jaccard (k=5):  {metrics['knn_jaccard_k5']:.4f}\")\n",
    "                print(f\"kNN Jaccard (k=10): {metrics['knn_jaccard_k10']:.4f}\")\n",
    "                print(f\"Min distance (mean ± std): {metrics['min_distance_mean']:.4f} ± {metrics['min_distance_std']:.4f}\")\n",
    "                print(f\"Collision rate (< 0.01): {metrics['collision_rate']:.4f}\")\n",
    "                print(f\"Radial KS statistic: {metrics['radial_ks_statistic']:.4f} (p={metrics['radial_ks_pvalue']:.4f})\")\n",
    "                \n",
    "            return metrics\n",
    "\n",
    "\n",
    "    def plot_training_losses(self):\n",
    "        \"\"\"Plot training losses for Graph-VAE + Latent Diffusion pipeline\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        # Determine how many subplots we need\n",
    "        n_plots = 0\n",
    "        if len(self.vae_losses['epochs']) > 0:\n",
    "            n_plots += 2  # VAE losses and VAE smoothed\n",
    "        if len(self.latent_diffusion_losses['epochs']) > 0:\n",
    "            n_plots += 2  # Latent diffusion losses and smoothed\n",
    "        \n",
    "        if n_plots == 0:\n",
    "            print(\"No training losses to plot.\")\n",
    "            return\n",
    "        \n",
    "        # Create figure with appropriate number of subplots\n",
    "        if n_plots == 2:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            axes = [axes] if n_plots == 2 else axes\n",
    "        elif n_plots == 4:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, n_plots, figsize=(7*n_plots, 5))\n",
    "            if n_plots == 1:\n",
    "                axes = [axes]\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        # Plot 1: Graph-VAE losses\n",
    "        if len(self.vae_losses['epochs']) > 0:\n",
    "            epochs_vae = np.array(self.vae_losses['epochs'])\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            ax.plot(epochs_vae, self.vae_losses['total'], 'b-', label='Total VAE Loss', linewidth=2)\n",
    "            ax.plot(epochs_vae, self.vae_losses['reconstruction'], 'g-', label='Reconstruction Loss', linewidth=2)\n",
    "            ax.plot(epochs_vae, np.array(self.vae_losses['kl']) * 0.01, 'r--', label='KL Loss (×0.01)', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_title('Graph-VAE Training Losses')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_yscale('log')\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Plot 2: Graph-VAE smoothed\n",
    "            if len(self.vae_losses['total']) > 1:\n",
    "                ax = axes[plot_idx]\n",
    "                window = min(50, len(self.vae_losses['total']) // 10)\n",
    "                if window > 1:\n",
    "                    smoothed = np.convolve(self.vae_losses['total'], \n",
    "                                        np.ones(window)/window, mode='valid')\n",
    "                    smooth_epochs = epochs_vae[window-1:]\n",
    "                    ax.plot(epochs_vae, self.vae_losses['total'], 'lightblue', alpha=0.5, label='Raw')\n",
    "                    ax.plot(smooth_epochs, smoothed, 'blue', linewidth=2, label=f'Smoothed (window={window})')\n",
    "                else:\n",
    "                    ax.plot(epochs_vae, self.vae_losses['total'], 'blue', linewidth=2)\n",
    "                \n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Loss')\n",
    "                ax.set_title('Graph-VAE Total Loss (Smoothed)')\n",
    "                if window > 1:\n",
    "                    ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_yscale('log')\n",
    "                plot_idx += 1\n",
    "        \n",
    "        # Plot 3: Latent Diffusion losses\n",
    "        if len(self.latent_diffusion_losses['epochs']) > 0:\n",
    "            epochs_diff = np.array(self.latent_diffusion_losses['epochs'])\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            ax.plot(epochs_diff, self.latent_diffusion_losses['total'], 'b-', label='Total Loss', linewidth=2)\n",
    "            ax.plot(epochs_diff, self.latent_diffusion_losses['diffusion'], 'k-', label='Diffusion Loss', linewidth=2)\n",
    "            \n",
    "            # Only plot struct loss if it's non-zero\n",
    "            struct_losses = np.array(self.latent_diffusion_losses['struct'])\n",
    "            if np.any(struct_losses > 0):\n",
    "                ax.plot(epochs_diff, struct_losses, 'r--', label='Structure Loss', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_title('Latent Diffusion Training Losses')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_yscale('log')\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Plot 4: Latent Diffusion smoothed\n",
    "            if len(self.latent_diffusion_losses['total']) > 1:\n",
    "                ax = axes[plot_idx]\n",
    "                window = min(50, len(self.latent_diffusion_losses['total']) // 10)\n",
    "                if window > 1:\n",
    "                    smoothed = np.convolve(self.latent_diffusion_losses['total'],\n",
    "                                        np.ones(window)/window, mode='valid')\n",
    "                    smooth_epochs = epochs_diff[window-1:]\n",
    "                    ax.plot(epochs_diff, self.latent_diffusion_losses['total'], 'lightcoral', alpha=0.5, label='Raw')\n",
    "                    ax.plot(smooth_epochs, smoothed, 'red', linewidth=2, label=f'Smoothed (window={window})')\n",
    "                else:\n",
    "                    ax.plot(epochs_diff, self.latent_diffusion_losses['total'], 'red', linewidth=2)\n",
    "                \n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Loss')\n",
    "                ax.set_title('Latent Diffusion Total Loss (Smoothed)')\n",
    "                if window > 1:\n",
    "                    ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_yscale('log')\n",
    "                plot_idx += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final loss values\n",
    "        print(\"\\n=== Training Loss Summary ===\")\n",
    "        \n",
    "        if len(self.vae_losses['total']) > 0:\n",
    "            print(f\"Graph-VAE - Initial Loss: {self.vae_losses['total'][0]:.6f}\")\n",
    "            print(f\"Graph-VAE - Final Loss: {self.vae_losses['total'][-1]:.6f}\")\n",
    "            print(f\"Graph-VAE - Loss Reduction: {(1 - self.vae_losses['total'][-1]/self.vae_losses['total'][0])*100:.2f}%\")\n",
    "            print(f\"Graph-VAE - Final Reconstruction Loss: {self.vae_losses['reconstruction'][-1]:.6f}\")\n",
    "            print(f\"Graph-VAE - Final KL Loss: {self.vae_losses['kl'][-1]:.6f}\")\n",
    "        \n",
    "        if len(self.latent_diffusion_losses['total']) > 0:\n",
    "            print(f\"Latent Diffusion - Initial Loss: {self.latent_diffusion_losses['total'][0]:.6f}\")\n",
    "            print(f\"Latent Diffusion - Final Loss: {self.latent_diffusion_losses['total'][-1]:.6f}\")\n",
    "            print(f\"Latent Diffusion - Loss Reduction: {(1 - self.latent_diffusion_losses['total'][-1]/self.latent_diffusion_losses['total'][0])*100:.2f}%\")\n",
    "            print(f\"Latent Diffusion - Final Diffusion Loss: {self.latent_diffusion_losses['diffusion'][-1]:.6f}\")\n",
    "            if np.any(np.array(self.latent_diffusion_losses['struct']) > 0):\n",
    "                print(f\"Latent Diffusion - Final Structure Loss: {self.latent_diffusion_losses['struct'][-1]:.6f}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# PART 4: MMD Loss Implementation\n",
    "# =====================================================\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        tmp = 0\n",
    "        for x in kernel_val:\n",
    "            tmp += x\n",
    "        return tmp\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "def analyze_sc_st_patterns(model):\n",
    "    \"\"\"Run the complete analysis\"\"\"\n",
    "    \n",
    "    print(\"Analyzing SC vs ST expression patterns...\")\n",
    "    \n",
    "    # Main comparison plot\n",
    "    common_genes = model.compare_sc_st_expression_patterns(n_genes=20)\n",
    "    \n",
    "    # Detailed gene-by-gene analysis\n",
    "    print(f\"\\nDetailed analysis for top {len(common_genes)} variable genes...\")\n",
    "    model.plot_detailed_gene_comparison(common_genes, n_genes=10)\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(\"\\nExpression Statistics:\")\n",
    "    print(f\"SC data shape: {model.sc_gene_expr.shape}\")\n",
    "    print(f\"ST data shape: {model.st_gene_expr.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sc_mean = model.sc_gene_expr.mean(0)\n",
    "        st_mean = model.st_gene_expr.mean(0)\n",
    "        \n",
    "        print(f\"SC mean expression: {sc_mean.mean():.3f} ± {sc_mean.std():.3f}\")\n",
    "        print(f\"ST mean expression: {st_mean.mean():.3f} ± {st_mean.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data_individual_norm():\n",
    "    \"\"\"\n",
    "    Load and process cSCC data with individual normalization per ST dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data with individual normalization...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize expression data (same for all)\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "\n",
    "\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def normalize_coordinates_individually(coords):\n",
    "    \"\"\"\n",
    "    Normalize coordinates to [-1, 1] range individually.\n",
    "    \"\"\"\n",
    "    coords_min = coords.min(axis=0)\n",
    "    coords_max = coords.max(axis=0)\n",
    "    coords_range = coords_max - coords_min\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    coords_range[coords_range == 0] = 1.0\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    coords_normalized = 2 * (coords - coords_min) / coords_range - 1\n",
    "    \n",
    "    return coords_normalized, coords_min, coords_max, coords_range\n",
    "\n",
    "def prepare_individually_normalized_st_data(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Normalize each ST dataset individually, then combine.\n",
    "    \"\"\"\n",
    "    print(\"Preparing individually normalized ST data...\")\n",
    "    \n",
    "    # Get common genes\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates and normalize individually\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "    \n",
    "    print(\"Normalizing coordinates individually...\")\n",
    "    st1_coords_norm, st1_min, st1_max, st1_range = normalize_coordinates_individually(st1_coords)\n",
    "    st2_coords_norm, st2_min, st2_max, st2_range = normalize_coordinates_individually(st2_coords)\n",
    "    st3_coords_norm, st3_min, st3_max, st3_range = normalize_coordinates_individually(st3_coords)\n",
    "    \n",
    "    print(f\"ST1 coord range: [{st1_coords_norm.min():.3f}, {st1_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST2 coord range: [{st2_coords_norm.min():.3f}, {st2_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST3 coord range: [{st3_coords_norm.min():.3f}, {st3_coords_norm.max():.3f}]\")\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "    st_coords_combined = np.vstack([st1_coords_norm, st2_coords_norm, st3_coords_norm])\n",
    "    \n",
    "    # Create dataset metadata\n",
    "    dataset_info = {\n",
    "        'labels': (['dataset1'] * len(st1_expr) + \n",
    "                  ['dataset2'] * len(st2_expr) + \n",
    "                  ['dataset3'] * len(st3_expr)),\n",
    "        'normalization_params': {\n",
    "            'dataset1': {'min': st1_min, 'max': st1_max, 'range': st1_range},\n",
    "            'dataset2': {'min': st2_min, 'max': st2_max, 'range': st2_range},\n",
    "            'dataset3': {'min': st3_min, 'max': st3_max, 'range': st3_range}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_info, common_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data_individual_norm()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cell_interactions_advanced(scadata, coords_key='advanced_diffusion_coords_avg'):\n",
    "    \"\"\"Analyze cell-cell interactions using the advanced diffusion coordinates\"\"\"\n",
    "    \n",
    "    # Get coordinates from scadata\n",
    "    coords_mean = scadata.obsm[coords_key]\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    distances = squareform(pdist(coords_mean))\n",
    "    \n",
    "    # Get cell types\n",
    "    cell_types = scadata.obs['rough_celltype'].values\n",
    "    unique_types = np.unique(cell_types)\n",
    "    \n",
    "    # Analyze minimum distances between cell types\n",
    "    min_distances = {}\n",
    "    for i, type1 in enumerate(unique_types):\n",
    "        for j, type2 in enumerate(unique_types):\n",
    "            if i <= j:  # Include self-interactions\n",
    "                mask1 = cell_types == type1\n",
    "                mask2 = cell_types == type2\n",
    "                \n",
    "                if i == j:\n",
    "                    # Same cell type - exclude self\n",
    "                    sub_dist = distances[np.ix_(mask1, mask2)]\n",
    "                    np.fill_diagonal(sub_dist, np.inf)\n",
    "                    if sub_dist.size > 0:\n",
    "                        min_dist = np.min(sub_dist[sub_dist < np.inf])\n",
    "                    else:\n",
    "                        min_dist = np.nan\n",
    "                else:\n",
    "                    # Different cell types\n",
    "                    sub_dist = distances[np.ix_(mask1, mask2)]\n",
    "                    min_dist = np.min(sub_dist) if sub_dist.size > 0 else np.nan\n",
    "                \n",
    "                min_distances[(type1, type2)] = min_dist\n",
    "    \n",
    "    # Create interaction matrix visualization\n",
    "    interaction_matrix = np.full((len(unique_types), len(unique_types)), np.nan)\n",
    "    for i, type1 in enumerate(unique_types):\n",
    "        for j, type2 in enumerate(unique_types):\n",
    "            key = (type1, type2) if i <= j else (type2, type1)\n",
    "            if key in min_distances:\n",
    "                interaction_matrix[i, j] = min_distances[key]\n",
    "                interaction_matrix[j, i] = min_distances[key]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = ~np.isnan(interaction_matrix)\n",
    "    sns.heatmap(interaction_matrix, \n",
    "                annot=True, fmt='.3f', \n",
    "                xticklabels=unique_types,\n",
    "                yticklabels=unique_types,\n",
    "                cmap='coolwarm_r',\n",
    "                mask=~mask,\n",
    "                cbar_kws={'label': 'Minimum Distance'})\n",
    "    plt.title(f'Minimum Distances Between Cell Types ({coords_key})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return min_distances, interaction_matrix\n",
    "\n",
    "def visualize_advanced_results_multi_model(scadata):\n",
    "    \"\"\"Visualize results from multiple models with uncertainty analysis\"\"\"\n",
    "    \n",
    "    # Get coordinates from all models\n",
    "    coords_avg = scadata.obsm['advanced_diffusion_coords_avg']\n",
    "    coords_rep1 = scadata.obsm['advanced_diffusion_coords_rep1'] \n",
    "    coords_rep2 = scadata.obsm['advanced_diffusion_coords_rep2']\n",
    "    coords_rep3 = scadata.obsm['advanced_diffusion_coords_rep3']\n",
    "    \n",
    "    # Calculate uncertainty metrics across models\n",
    "    all_coords = np.stack([coords_rep1, coords_rep2, coords_rep3], axis=0)  # (3, n_cells, 2)\n",
    "    coords_std = np.std(all_coords, axis=0)  # Standard deviation across models\n",
    "    coords_var = np.var(all_coords, axis=0)  # Variance across models\n",
    "    \n",
    "    # Total variability (combining x and y dimensions)\n",
    "    total_std = np.sqrt(coords_std[:, 0]**2 + coords_std[:, 1]**2)\n",
    "    total_var = coords_var[:, 0] + coords_var[:, 1]\n",
    "    \n",
    "    # Create confidence scores (inverse of variability)\n",
    "    confidence = 1 / (1 + total_std)\n",
    "    scadata.obs['spatial_confidence'] = confidence\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Spatial coordinates colored by cell type\n",
    "    ax = axes[0, 0]\n",
    "    cell_types = scadata.obs['rough_celltype']\n",
    "    unique_types = cell_types.unique()\n",
    "    colors = sns.color_palette('tab20', n_colors=len(unique_types))\n",
    "    \n",
    "    for i, ct in enumerate(unique_types):\n",
    "        mask = cell_types == ct\n",
    "        ax.scatter(coords_avg[mask, 0], coords_avg[mask, 1], \n",
    "                  c=[colors[i]], label=ct, s=30, alpha=0.7)\n",
    "    ax.set_title('Averaged Spatial Coordinates by Cell Type', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    # 2. Model variability (standard deviation across 3 models)\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=total_std, cmap='viridis_r', \n",
    "                        s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Model Std Dev')\n",
    "    ax.set_title('Model Prediction Variability', fontsize=14)\n",
    "    \n",
    "    # 3. X vs Y coordinate uncertainty\n",
    "    ax = axes[0, 2]\n",
    "    scatter = ax.scatter(coords_std[:, 0], coords_std[:, 1], \n",
    "                        c=total_std, cmap='plasma', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Total Std')\n",
    "    ax.set_xlabel('X Coordinate Std')\n",
    "    ax.set_ylabel('Y Coordinate Std')\n",
    "    ax.set_title('Coordinate Uncertainty (X vs Y)', fontsize=14)\n",
    "    \n",
    "    # 4. Cell density heatmap\n",
    "    ax = axes[1, 0]\n",
    "    from scipy.stats import gaussian_kde\n",
    "    xy = coords_avg.T\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=z, cmap='hot', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Density')\n",
    "    ax.set_title('Cell Density (Averaged Coordinates)', fontsize=14)\n",
    "    \n",
    "    # 5. Confidence scores\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=confidence, cmap='RdYlGn', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Confidence')\n",
    "    ax.set_title('Prediction Confidence Across Models', fontsize=14)\n",
    "    \n",
    "    # 6. Model agreement visualization\n",
    "    ax = axes[1, 2]\n",
    "    # Show cells where models agree vs disagree\n",
    "    high_agreement = total_std < np.percentile(total_std, 25)  # Bottom 25%\n",
    "    low_agreement = total_std > np.percentile(total_std, 75)   # Top 25%\n",
    "    \n",
    "    ax.scatter(coords_avg[high_agreement, 0], coords_avg[high_agreement, 1], \n",
    "              c='green', s=20, alpha=0.7, label='High Agreement')\n",
    "    ax.scatter(coords_avg[low_agreement, 0], coords_avg[low_agreement, 1], \n",
    "              c='red', s=20, alpha=0.7, label='Low Agreement')\n",
    "    ax.scatter(coords_avg[~(high_agreement | low_agreement), 0], \n",
    "              coords_avg[~(high_agreement | low_agreement), 1], \n",
    "              c='gray', s=10, alpha=0.5, label='Medium Agreement')\n",
    "    ax.set_title('Model Agreement', fontsize=14)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, total_std, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def procrustes_alignment_2d(coords_to_align, reference_coords):\n",
    "    \"\"\"\n",
    "    Align coords_to_align to reference_coords using 2D Procrustes analysis\n",
    "    (rotation + translation only, no scaling)\n",
    "    \n",
    "    Args:\n",
    "        coords_to_align: (n_points, 2) coordinates to be aligned\n",
    "        reference_coords: (n_points, 2) reference coordinates\n",
    "    \n",
    "    Returns:\n",
    "        aligned_coords: (n_points, 2) aligned coordinates\n",
    "        rotation_matrix: (2, 2) rotation matrix used\n",
    "        translation: (2,) translation vector used\n",
    "    \"\"\"\n",
    "    assert coords_to_align.shape == reference_coords.shape, \"Coordinate shapes must match\"\n",
    "    assert coords_to_align.shape[1] == 2, \"Only 2D coordinates supported\"\n",
    "    \n",
    "    # Center both coordinate sets\n",
    "    coords_centered = coords_to_align - np.mean(coords_to_align, axis=0)\n",
    "    ref_centered = reference_coords - np.mean(reference_coords, axis=0)\n",
    "    \n",
    "    # Compute cross-covariance matrix\n",
    "    H = coords_centered.T @ ref_centered\n",
    "    \n",
    "    # SVD to find optimal rotation\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    \n",
    "    # Compute rotation matrix\n",
    "    R = Vt.T @ U.T\n",
    "    \n",
    "    # Ensure proper rotation (det(R) = 1)\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    \n",
    "    # Apply rotation to centered coordinates\n",
    "    coords_rotated = coords_centered @ R.T\n",
    "    \n",
    "    # Compute translation to align centroids\n",
    "    translation = np.mean(reference_coords, axis=0) - np.mean(coords_rotated, axis=0)\n",
    "    \n",
    "    # Apply translation\n",
    "    aligned_coords = coords_rotated + translation\n",
    "    \n",
    "    return aligned_coords, R, translation\n",
    "\n",
    "def plot_alignment_comparison(coords_list, cell_types, labels, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot coordinates before and after alignment for comparison\n",
    "    \n",
    "    Args:\n",
    "        coords_list: List of coordinate arrays [(n_points, 2), ...]\n",
    "        cell_types: Array of cell type labels\n",
    "        labels: List of labels for each coordinate set\n",
    "        title_prefix: Prefix for plot titles\n",
    "    \"\"\"\n",
    "    n_models = len(coords_list)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=(5*n_models, 10))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    # Get unique cell types and colors\n",
    "    unique_types = np.unique(cell_types)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_types)))\n",
    "    color_map = dict(zip(unique_types, colors))\n",
    "    \n",
    "    for i, (coords, label) in enumerate(zip(coords_list, labels)):\n",
    "        # Plot individual models\n",
    "        for j, cell_type in enumerate(unique_types):\n",
    "            mask = cell_types == cell_type\n",
    "            axes[0, i].scatter(coords[mask, 0], coords[mask, 1], \n",
    "                             c=[color_map[cell_type]], label=cell_type, \n",
    "                             alpha=0.6, s=20)\n",
    "        \n",
    "        axes[0, i].set_title(f'{title_prefix}{label}')\n",
    "        axes[0, i].set_xlabel('X coordinate')\n",
    "        axes[0, i].set_ylabel('Y coordinate')\n",
    "        axes[0, i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot overlay of all models\n",
    "    for i, (coords, label) in enumerate(zip(coords_list, labels)):\n",
    "        axes[1, 0].scatter(coords[:, 0], coords[:, 1], \n",
    "                          label=label, alpha=0.4, s=15)\n",
    "    \n",
    "    axes[1, 0].set_title(f'{title_prefix}All Models Overlay')\n",
    "    axes[1, 0].set_xlabel('X coordinate')\n",
    "    axes[1, 0].set_ylabel('Y coordinate')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(1, n_models):\n",
    "        axes[1, i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_individual_advanced_diffusion_models_with_alignment(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset, \n",
    "    apply Procrustes alignment, and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['advanced_diffusion_coords_avg']\n",
    "        models_all: All trained models for further analysis\n",
    "        alignment_info: Dictionary containing alignment details\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {dataset_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize AdvancedHierarchicalDiffusion model\n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,\n",
    "            transport_plan=None,\n",
    "            D_st=None,\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=3.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=600,\n",
    "            n_denoising_blocks=4,\n",
    "            hidden_dim=256,\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{dataset_name}'\n",
    "        )\n",
    "        \n",
    "        print(f\"Training model for {dataset_name}...\")\n",
    "        \n",
    "        # Train using new Graph-VAE + Latent Diffusion pipeline\n",
    "        model.train(\n",
    "            encoder_epochs=1000,\n",
    "            vae_epochs=1000,\n",
    "            diffusion_epochs=2500,\n",
    "            lambda_struct=2.0\n",
    "        )\n",
    "        \n",
    "        model.plot_training_losses()\n",
    "        \n",
    "        print(f\"Generating SC coordinates using model {i+1}...\")\n",
    "        sc_coords = model.sample_sc_coordinates_batched(\n",
    "            batch_size=512\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        print(f\"Model {i+1} complete! Generated coordinates shape: {sc_coords.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # ========================\n",
    "    # PROCRUSTES ALIGNMENT\n",
    "    # ========================\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"APPLYING PROCRUSTES ALIGNMENT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Use second model (index 1) as reference\n",
    "    reference_idx = 1\n",
    "    reference_coords = sc_coords_results[reference_idx].copy()\n",
    "    \n",
    "    print(f\"Using model {reference_idx + 1} (dataset{reference_idx + 1}) as reference\")\n",
    "    \n",
    "    # Plot BEFORE alignment\n",
    "    print(\"Plotting coordinates BEFORE alignment...\")\n",
    "    plot_alignment_comparison(\n",
    "        coords_list=sc_coords_results.copy(),\n",
    "        cell_types=scadata.obs['rough_celltype'].values,\n",
    "        labels=[f\"Model {i+1}\" for i in range(len(sc_coords_results))],\n",
    "        title_prefix=\"BEFORE Alignment - \"\n",
    "    )\n",
    "    \n",
    "    # Apply Procrustes alignment\n",
    "    aligned_coords_results = []\n",
    "    alignment_info = {\n",
    "        'reference_model': reference_idx + 1,\n",
    "        'rotations': [],\n",
    "        'translations': [],\n",
    "        'rmse_before': [],\n",
    "        'rmse_after': []\n",
    "    }\n",
    "    \n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        if i == reference_idx:\n",
    "            # Reference model - no alignment needed\n",
    "            aligned_coords = coords.copy()\n",
    "            alignment_info['rotations'].append(np.eye(2))\n",
    "            alignment_info['translations'].append(np.zeros(2))\n",
    "            rmse_before = 0.0\n",
    "            rmse_after = 0.0\n",
    "        else:\n",
    "            # Calculate RMSE before alignment\n",
    "            rmse_before = np.sqrt(np.mean((coords - reference_coords)**2))\n",
    "            \n",
    "            # Apply Procrustes alignment\n",
    "            aligned_coords, rotation_matrix, translation = procrustes_alignment_2d(\n",
    "                coords, reference_coords\n",
    "            )\n",
    "            \n",
    "            # Calculate RMSE after alignment\n",
    "            rmse_after = np.sqrt(np.mean((aligned_coords - reference_coords)**2))\n",
    "            \n",
    "            # Store alignment info\n",
    "            alignment_info['rotations'].append(rotation_matrix)\n",
    "            alignment_info['translations'].append(translation)\n",
    "            \n",
    "            print(f\"Model {i+1} -> Reference alignment:\")\n",
    "            print(f\"  RMSE before: {rmse_before:.6f}\")\n",
    "            print(f\"  RMSE after:  {rmse_after:.6f}\")\n",
    "            print(f\"  Improvement: {rmse_before - rmse_after:.6f}\")\n",
    "        \n",
    "        alignment_info['rmse_before'].append(rmse_before)\n",
    "        alignment_info['rmse_after'].append(rmse_after)\n",
    "        aligned_coords_results.append(aligned_coords)\n",
    "    \n",
    "    # Plot AFTER alignment\n",
    "    print(\"Plotting coordinates AFTER alignment...\")\n",
    "    plot_alignment_comparison(\n",
    "        coords_list=aligned_coords_results.copy(),\n",
    "        cell_types=scadata.obs['rough_celltype'].values,\n",
    "        labels=[f\"Model {i+1}\" for i in range(len(aligned_coords_results))],\n",
    "        title_prefix=\"AFTER Alignment - \"\n",
    "    )\n",
    "    \n",
    "    # ========================\n",
    "    # AVERAGING\n",
    "    # ========================\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"AVERAGING ALIGNED RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Average the aligned results\n",
    "    sc_coords_avg = np.mean(aligned_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in aligned_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    scadata.obsm['advanced_diffusion_coords_avg_aligned'] = sc_coords_avg  # For clarity\n",
    "    \n",
    "    # Save individual results (both original and aligned)\n",
    "    for i, (orig_coords, aligned_coords) in enumerate(zip(sc_coords_results, aligned_coords_results)):\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}_original'] = orig_coords\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}_aligned'] = aligned_coords\n",
    "    \n",
    "    # Plot final averaged result\n",
    "    print(\"Plotting final averaged coordinates...\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Get unique cell types and colors\n",
    "    unique_types = np.unique(scadata.obs['rough_celltype'].values)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_types)))\n",
    "    color_map = dict(zip(unique_types, colors))\n",
    "    \n",
    "    cell_types = scadata.obs['rough_celltype'].values\n",
    "    \n",
    "    # Plot before averaging (overlay of aligned models)\n",
    "    for i, coords in enumerate(aligned_coords_results):\n",
    "        ax1.scatter(coords[:, 0], coords[:, 1], \n",
    "                   label=f\"Model {i+1}\", alpha=0.4, s=15)\n",
    "    ax1.set_title('Aligned Models (Before Averaging)')\n",
    "    ax1.set_xlabel('X coordinate')\n",
    "    ax1.set_ylabel('Y coordinate')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot final averaged result by cell type\n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        ax2.scatter(sc_coords_avg[mask, 0], sc_coords_avg[mask, 1], \n",
    "                   c=[color_map[cell_type]], label=cell_type, \n",
    "                   alpha=0.6, s=20)\n",
    "    ax2.set_title('Final Averaged Coordinates (by Cell Type)')\n",
    "    ax2.set_xlabel('X coordinate')\n",
    "    ax2.set_ylabel('Y coordinate')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAdvanced diffusion training with alignment complete!\")\n",
    "    print(f\"Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "    print(f\"Individual results saved with '_original' and '_aligned' suffixes\")\n",
    "    \n",
    "    return scadata, models_all, alignment_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata, models_all, alignment_info = train_individual_advanced_diffusion_models_with_alignment(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_advanced_diffusion_models(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset and average the results.\n",
    "    MODIFIED: Run stadata1 three times to test for SC cluster rotation/sliding\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # STEP 1: Build canonical angular frame from ST slide (ONCE)\n",
    "    # st_coords_raw = stadata1.obsm['spatial']  # Use raw ST coordinates\n",
    "    # angular_frame = _build_canonical_angular_frame(st_coords_raw)\n",
    "    \n",
    "    # List of ST datasets for iteration - Use stadata1 three times\n",
    "    st_datasets = [\n",
    "        (stadata1, \"run1\"),\n",
    "        (stadata2, \"run2\"), \n",
    "        (stadata3, \"run3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, run_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {run_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {run_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize model with different random seed for each run\n",
    "        torch.manual_seed(42 + i)\n",
    "        np.random.seed(42 + i)\n",
    "        \n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,\n",
    "            transport_plan=None,\n",
    "            D_st=None,\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=0.75,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=800,\n",
    "            n_denoising_blocks=6,\n",
    "            hidden_dim=256,\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{run_name}'\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Training model for {run_name}...\")\n",
    "        model.train(\n",
    "            encoder_epochs=1000,\n",
    "            vae_epochs=1200,\n",
    "            diffusion_epochs=2500,\n",
    "            lambda_struct=10.0\n",
    "        )\n",
    "\n",
    "        st_coords_raw = model.st_coords_norm.cpu().numpy()  # Use normalized coords from model\n",
    "        angular_frame = _build_canonical_angular_frame(st_coords_raw)\n",
    "        \n",
    "        # Generate SC coordinates\n",
    "        print(f\"Generating SC coordinates using {run_name} model...\")\n",
    "        # sc_coords = model.generate_sc_coordinates()\n",
    "        # sc_coords = model.sample_sc_coordinates_batched(\n",
    "        #     batch_size=512,  # Even smaller batches\n",
    "        #     refine_coords=False\n",
    "        # )\n",
    "\n",
    "        # Sample with geometry guidance\n",
    "        sc_coords = model.sample_sc_coordinates_with_geometry_guidance(\n",
    "            batch_size=512,\n",
    "            guidance_scale=1.0,\n",
    "            geometry_guidance=False,\n",
    "            k_nn=5,\n",
    "            lambda_trip=0.05,\n",
    "            lambda_rep=0.08,\n",
    "            gamma=0.05\n",
    "        )\n",
    "\n",
    "        print(f\"\\n=== Generated SC Coordinates ({run_name}) ===\")\n",
    "        print(f\"  X range: [{sc_coords[:, 0].min():.3f}, {sc_coords[:, 0].max():.3f}]\")\n",
    "        print(f\"  Y range: [{sc_coords[:, 1].min():.3f}, {sc_coords[:, 1].max():.3f}]\")\n",
    "        # print(f\"  Max radius from center: {np.max(np.linalg.norm(sc_coords, axis=1)):.3f}\")\n",
    "        # print(f\"  % points outside unit circle: {(np.linalg.norm(sc_coords, axis=1) > 1).mean()*100:.1f}%\")\n",
    "\n",
    "        # Evaluate geometry preservation\n",
    "        metrics = model.evaluate_geometry_preservation(sc_coords)\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        # STEP 2: Plot SC cells colored by angle (using ST-derived frame)\n",
    "        _plot_sc_angle_analysis(sc_coords, scadata.obs['rough_celltype'].values, \n",
    "                               angular_frame, st_coords_raw, run_name, i+1)\n",
    "    \n",
    "    # STEP 3: Comparative analysis across runs\n",
    "    _plot_comparative_sc_angle_analysis(sc_coords_results, scadata.obs['rough_celltype'].values,\n",
    "                                       angular_frame, st_coords_raw)\n",
    "    \n",
    "    # Compute averaged SC coordinates\n",
    "    sc_coords_results_np = [coords.cpu().numpy() for coords in sc_coords_results]  # Convert to numpy\n",
    "    sc_coords_avg = np.mean(sc_coords_results_np, axis=0)\n",
    "    sc_coords_std = np.std(sc_coords_results_np, axis=0)\n",
    "\n",
    "    # Store results in scadata\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    scadata.obsm['advanced_diffusion_coords_std'] = sc_coords_std\n",
    "\n",
    "    # Store individual results\n",
    "    for i, coords in enumerate(sc_coords_results_np):  # Use numpy arrays\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}'] = coords\n",
    "\n",
    "    print(f\"\\nTraining complete. Results stored in scadata.obsm\")\n",
    "    return scadata, models_all\n",
    "\n",
    "def _build_canonical_angular_frame(st_coords):\n",
    "    \"\"\"Build canonical angular frame from ST coordinates (dataset-specific, run-independent)\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute centroid\n",
    "    centroid = st_coords.mean(axis=0)\n",
    "    \n",
    "    # Find farthest spot from centroid (deterministic 0° direction)\n",
    "    distances = np.linalg.norm(st_coords - centroid, axis=1)\n",
    "    farthest_idx = np.argmax(distances)\n",
    "    a0 = st_coords[farthest_idx] - centroid  # 0° direction vector\n",
    "    \n",
    "    def angle_fn(x):\n",
    "        \"\"\"Compute angle from canonical frame\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        \n",
    "        v = x - centroid\n",
    "        cross = a0[0] * v[:, 1] - a0[1] * v[:, 0]  # z-component of 2D cross\n",
    "        dot = a0[0] * v[:, 0] + a0[1] * v[:, 1]\n",
    "        angles = np.arctan2(cross, dot)\n",
    "        angles = np.where(angles < 0, angles + 2*np.pi, angles)  # Map to [0, 2π)\n",
    "        return angles\n",
    "    \n",
    "    return {\n",
    "        'centroid': centroid,\n",
    "        'zero_direction': a0,\n",
    "        'farthest_idx': farthest_idx,\n",
    "        'angle_fn': angle_fn\n",
    "    }\n",
    "\n",
    "def _plot_sc_angle_analysis(sc_coords, cell_types, angular_frame, st_coords_bg, run_name, run_num):\n",
    "    \"\"\"Plot SC cells colored by angle from ST-derived frame\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute angles for SC cells using ST-derived frame\n",
    "    sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "    sc_angles_degrees = np.degrees(sc_angles)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: SC cells colored by angle (with ST outline in background)\n",
    "    ax1.scatter(st_coords_bg[:, 0], st_coords_bg[:, 1], \n",
    "               c='lightgray', s=10, alpha=0.3, label='ST outline')\n",
    "    \n",
    "    scatter = ax1.scatter(sc_coords[:, 0], sc_coords[:, 1], \n",
    "                         c=sc_angles_degrees, cmap='hsv', s=30, alpha=0.8)\n",
    "    \n",
    "    # Mark centroid and 0° direction\n",
    "    centroid = angular_frame['centroid']\n",
    "    zero_dir = angular_frame['zero_direction']\n",
    "    ax1.scatter(centroid[0], centroid[1], c='black', s=100, marker='x', linewidth=3)\n",
    "    ax1.arrow(centroid[0], centroid[1], zero_dir[0]*0.3, zero_dir[1]*0.3, \n",
    "              head_width=0.05, head_length=0.05, fc='red', ec='red', linewidth=2)\n",
    "    \n",
    "    ax1.set_title(f'{run_name}: SC Cells Colored by Angle θ')\n",
    "    ax1.set_xlabel('X coordinate')\n",
    "    ax1.set_ylabel('Y coordinate')\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Angle (degrees)')\n",
    "    \n",
    "    # Plot 2: Per-cell-type angle distribution\n",
    "    unique_types = np.unique(cell_types)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n",
    "    \n",
    "    for i, cell_type in enumerate(unique_types):\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 0:\n",
    "            angles_subset = sc_angles_degrees[mask]\n",
    "            ax2.hist(angles_subset, bins=36, alpha=0.6, label=cell_type, \n",
    "                    color=colors[i], density=True)\n",
    "    \n",
    "    ax2.set_title(f'{run_name}: Angle Distribution by Cell Type')\n",
    "    ax2.set_xlabel('Angle (degrees)')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_xlim(0, 360)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'sc_angle_analysis_{run_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print circular statistics per cell type\n",
    "    print(f\"\\n{run_name} - Circular statistics per cell type:\")\n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 5:  # Only if enough cells\n",
    "            angles_rad = sc_angles[mask]\n",
    "            # Circular mean\n",
    "            mean_cos = np.mean(np.cos(angles_rad))\n",
    "            mean_sin = np.mean(np.sin(angles_rad))\n",
    "            circular_mean = np.arctan2(mean_sin, mean_cos)\n",
    "            if circular_mean < 0:\n",
    "                circular_mean += 2*np.pi\n",
    "            \n",
    "            print(f\"  {cell_type}: mean={np.degrees(circular_mean):.1f}°, n={np.sum(mask)}\")\n",
    "\n",
    "def _plot_comparative_sc_angle_analysis(sc_coords_list, cell_types, angular_frame, st_coords_bg):\n",
    "    \"\"\"Plot comparative SC angle analysis across all runs\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    n_runs = len(sc_coords_list)\n",
    "    unique_types = np.unique(cell_types)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_runs, figsize=(5*n_runs, 10))\n",
    "    if n_runs == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Top row: SC scatter plots per run\n",
    "    for i, sc_coords in enumerate(sc_coords_list):\n",
    "        ax = axes[0, i]\n",
    "        \n",
    "        # ST background\n",
    "        ax.scatter(st_coords_bg[:, 0], st_coords_bg[:, 1], \n",
    "                  c='lightgray', s=5, alpha=0.3)\n",
    "        \n",
    "        # SC cells colored by angle\n",
    "        sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "        sc_angles_degrees = np.degrees(sc_angles)\n",
    "        \n",
    "        scatter = ax.scatter(sc_coords[:, 0], sc_coords[:, 1], \n",
    "                           c=sc_angles_degrees, cmap='hsv', s=20, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'Run {i+1}: SC Cells by Angle')\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        if i == n_runs-1:  # Add colorbar to last plot\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('Angle (degrees)')\n",
    "    \n",
    "    # Bottom row: Cell type angle distributions per run  \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n",
    "    \n",
    "    for i, sc_coords in enumerate(sc_coords_list):\n",
    "        ax = axes[1, i]\n",
    "        \n",
    "        sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "        sc_angles_degrees = np.degrees(sc_angles)\n",
    "        \n",
    "        for j, cell_type in enumerate(unique_types):\n",
    "            mask = cell_types == cell_type\n",
    "            if np.sum(mask) > 5:\n",
    "                angles_subset = sc_angles_degrees[mask]\n",
    "                ax.hist(angles_subset, bins=36, alpha=0.6, \n",
    "                       label=cell_type if i == 0 else \"\", \n",
    "                       color=colors[j], density=True)\n",
    "        \n",
    "        ax.set_title(f'Run {i+1}: Cell Type Angles')\n",
    "        ax.set_xlabel('Angle (degrees)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_xlim(0, 360)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparative_sc_angle_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for sector sliding\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"SECTOR SLIDING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 10:  # Only analyze cell types with enough cells\n",
    "            circular_means = []\n",
    "            \n",
    "            for i, sc_coords in enumerate(sc_coords_list):\n",
    "                sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "                angles_subset = sc_angles[mask]\n",
    "                \n",
    "                # Circular mean\n",
    "                mean_cos = np.mean(np.cos(angles_subset))\n",
    "                mean_sin = np.mean(np.sin(angles_subset))\n",
    "                circular_mean = np.arctan2(mean_sin, mean_cos)\n",
    "                if circular_mean < 0:\n",
    "                    circular_mean += 2*np.pi\n",
    "                \n",
    "                circular_means.append(np.degrees(circular_mean))\n",
    "            \n",
    "            # Check for large differences between runs\n",
    "            max_diff = max(circular_means) - min(circular_means)\n",
    "            if max_diff > 180:  # Handle wraparound\n",
    "                max_diff = 360 - max_diff\n",
    "            \n",
    "            print(f\"{cell_type}:\")\n",
    "            print(f\"  Run means: {[f'{m:.1f}°' for m in circular_means]}\")\n",
    "            print(f\"  Max difference: {max_diff:.1f}°\")\n",
    "            \n",
    "            if max_diff > 30:  # Significant sliding\n",
    "                print(f\"  ⚠️  SECTOR SLIDING DETECTED!\")\n",
    "            else:\n",
    "                print(f\"  ✅ Consistent placement\")\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# ADD THESE LINES:\n",
    "for i, stdata in enumerate([stadata1, stadata2, stadata3], 1):\n",
    "    coords = stdata.obsm['spatial']\n",
    "    print(f\"ST{i}: X[{coords[:, 0].min():.2f}, {coords[:, 0].max():.2f}], Y[{coords[:, 1].min():.2f}, {coords[:, 1].max():.2f}]\")\n",
    "\n",
    "\n",
    "# Train individual AdvancedHierarchicalDiffusion models and get averaged results\n",
    "scadata, advanced_models = train_individual_advanced_diffusion_models(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")\n",
    "\n",
    "print(\"Advanced diffusion training complete! Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawstdata = sc.read_csv('/home/ehtesamul/sc_st/data/cSCC/processed/GSM4284316_P2_ST_rep1_stdata.tsv.gz',delimiter='\\t')\n",
    "\n",
    "def normalize_coordinates_isotropic(coords):\n",
    "    \"\"\"Normalize coordinates isotropically to [-1, 1]\"\"\"\n",
    "    center = coords.mean(axis=0)\n",
    "    centered_coords = coords - center\n",
    "    max_dist = np.max(np.linalg.norm(centered_coords, axis=1))\n",
    "    normalized_coords = centered_coords / (max_dist + 1e-8)\n",
    "    return normalized_coords, center, max_dist\n",
    "\n",
    "# Load metadata FIRST to know which spots to keep\n",
    "rawstmeta = pd.read_csv('/home/ehtesamul/sc_st/data/cSCC/processed/GSM4284316_spot_data-selection-P2_ST_rep1.tsv.gz',delimiter='\\t')\n",
    "\n",
    "# Normalize the filtered coordinates\n",
    "stindex=[]\n",
    "for i in range(len(rawstmeta.x.tolist())):\n",
    "    stindex.append(str(rawstmeta.x[i])+'x'+str(rawstmeta.y[i]))\n",
    "rawstmeta.index = stindex\n",
    "\n",
    "# Filter FIRST, then extract and normalize coordinates\n",
    "rawstdata = rawstdata[stindex,:]\n",
    "rawstdata.obs = rawstmeta\n",
    "\n",
    "# NOW extract coordinates from the filtered data\n",
    "coord = np.array([x.split('x') for x in rawstdata.obs_names.tolist()],dtype='int')\n",
    "print(f\"Coordinates shape after filtering: {coord.shape}\")  # Should be (666, 2)\n",
    "\n",
    "# Normalize the filtered coordinates\n",
    "coord_norm, _, _ = normalize_coordinates_isotropic(coord)\n",
    "rawstdata.obsm['spatial'] = coord_norm\n",
    "\n",
    "# Continue with preprocessing\n",
    "sc.pp.normalize_total(rawstdata, target_sum=1e4)\n",
    "sc.pp.log1p(rawstdata)\n",
    "rawstdata.layers[\"log1p\"] = rawstdata.X.copy()   # keep positive values\n",
    "rawstdata.raw = rawstdata[:, :]                  # optional: make this the .raw snapshot\n",
    "sc.pp.scale(rawstdata)                           # z-score only for downstream PCA etc.\n",
    "\n",
    "\n",
    "print(rawstdata)  # Should show 666 × 17138\n",
    "print(f\"Spatial coordinates shape: {rawstdata.obsm['spatial'].shape}\")  # Should be (666, 2)\n",
    "\n",
    "sc.pl.spatial(rawstdata,color=['BST2','NRP1','JCHAIN'],show=True,basis='spatial',na_in_legend=False,spot_size=0.05, save='all_three_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "\n",
    "sccooravg = scadata.obsm['advanced_diffusion_coords_rep1']\n",
    "PDCcoor = sccooravg[scadata.obs.level2_celltype=='PDC',:]\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# estimate ST spot pitch from nearest-neighbor distance in ST space\n",
    "_nn_st = NearestNeighbors(n_neighbors=2).fit(rawstdata.obsm['spatial'])\n",
    "_d2, _ = _nn_st.kneighbors(rawstdata.obsm['spatial'])\n",
    "spot_pitch = np.median(_d2[:, 1])\n",
    "\n",
    "radius = 1.5 * spot_pitch  # <- toggle this (1.2–2.0× are common)\n",
    "\n",
    "\n",
    "nbrs_rad = NearestNeighbors(radius=radius).fit(rawstdata.obsm['spatial'])\n",
    "ind = nbrs_rad.radius_neighbors(PDCcoor, return_distance=False)\n",
    "nearST = sorted(set(np.concatenate(ind)))\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# nbrs = NearestNeighbors(n_neighbors=1, algorithm='ball_tree').fit(rawstdata.obsm['spatial'])\n",
    "# nearST = nbrs.kneighbors(PDCcoor,return_distance=False)\n",
    "# nearST = list(set(nearST.flatten()))\n",
    "\n",
    "# k = 1  # <- toggle (3–10)\n",
    "# nbrs_k = NearestNeighbors(n_neighbors=k).fit(rawstdata.obsm['spatial'])\n",
    "# idx = nbrs_k.kneighbors(PDCcoor, return_distance=False)\n",
    "# nearST = sorted(set(idx.flatten()))\n",
    "\n",
    "# rawstdata.obs['pDCnear'] = 'Others'\n",
    "# rawstdata.obs.iloc[nearST,-1]='pDC'\n",
    "\n",
    "rawstdata.obs['pDCnear'] = 'Others'\n",
    "spot_idx = rawstdata.obs.index[nearST]   # index names corresponding to nearST positions\n",
    "rawstdata.obs.loc[spot_idx, 'pDCnear'] = 'pDC'\n",
    "\n",
    "\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "sc.pl.spatial(rawstdata,color=['pDCnear'],show=True,basis='spatial',na_in_legend=False,spot_size=0.04,save='pDCenrich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nn_st.kneighbors(rawstdata.obsm['spatial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) how big is the region?\n",
    "frac_st_labeled = len(nearST) / rawstdata.n_obs\n",
    "print(\"labeled ST fraction:\", f\"{100*frac_st_labeled:.1f}%\")\n",
    "\n",
    "# 2) coverage: how many pDCs found ≥1 ST within radius?\n",
    "cov = np.mean([len(ix)>0 for ix in ind]) * 100\n",
    "print(\"PDC with ≥1 ST in radius:\", f\"{cov:.1f}%\")\n",
    "\n",
    "# 3) neighbors-per-pDC distribution (shouldn’t explode)\n",
    "lens = [len(ix) for ix in ind]\n",
    "print(\"neighbors per pDC — median/IQR:\",\n",
    "      np.median(lens), np.percentile(lens,25), \"-\", np.percentile(lens,75))\n",
    "\n",
    "# 4) report the actual radius you used\n",
    "print(\"spot_pitch:\", spot_pitch, \"  radius:\", radius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "muls = [1.2, 1.3, 1.4, 1.5]\n",
    "print(\"mul | frac_labeled | coverage>=1 | neighbors_med | BST2 Δmean  p | NRP1 Δmean  p\")\n",
    "for c in muls:\n",
    "    r = c * spot_pitch\n",
    "    nn = NearestNeighbors(radius=r).fit(rawstdata.obsm['spatial'])\n",
    "    ind = nn.radius_neighbors(PDCcoor, return_distance=False)\n",
    "    nearST = sorted(set(np.concatenate(ind)))\n",
    "    mask = np.zeros(rawstdata.n_obs, bool); mask[nearST] = True\n",
    "    frac = mask.mean()\n",
    "    cov = np.mean([len(ix)>0 for ix in ind])\n",
    "    lens = [len(ix) for ix in ind]\n",
    "    def _m_p(g):\n",
    "        j = rawstdata.var_names.get_loc(g)\n",
    "        x = np.asarray(rawstdata.layers['log1p'][:, j]).ravel()\n",
    "        xin, xout = x[mask], x[~mask]\n",
    "        from scipy.stats import mannwhitneyu\n",
    "        return (xin.mean()-xout.mean(), mannwhitneyu(xin, xout, alternative='two-sided', method='asymptotic').pvalue)\n",
    "    d1,p1 = _m_p('BST2'); d2,p2 = _m_p('NRP1')\n",
    "    print(f\"{c:>3} | {100*frac:6.1f}%     | {100*cov:6.1f}%    | {np.median(lens):2.0f}           | {d1:6.3f} {p1:6.2e} | {d2:6.3f} {p2:6.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def _mean_log1p(gene, mask):\n",
    "    j = rawstdata.var_names.get_loc(gene)\n",
    "    x = np.asarray(rawstdata.layers['log1p'][:, j]).ravel()\n",
    "    return x[mask].mean()\n",
    "\n",
    "mask = np.zeros(rawstdata.n_obs, dtype=bool); mask[nearST] = True\n",
    "for g in ['BST2','NRP1','JCHAIN']:\n",
    "    m_in  = _mean_log1p(g, mask)\n",
    "    m_out = _mean_log1p(g, ~mask)\n",
    "    print(g, \"Δ(mean log1p) =\", (m_in - m_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute spot pitch\n",
    "_nn_st = NearestNeighbors(n_neighbors=2).fit(rawstdata.obsm['spatial'])\n",
    "_d2, _ = _nn_st.kneighbors(rawstdata.obsm['spatial'])\n",
    "spot_pitch = np.median(_d2[:,1])\n",
    "radius = 1.5 * spot_pitch   # choose 1.2–1.8 as sensitivity\n",
    "# radius-based pDC label\n",
    "nbrs_rad = NearestNeighbors(radius=radius).fit(rawstdata.obsm['spatial'])\n",
    "ind = nbrs_rad.radius_neighbors(PDCcoor, return_distance=False)\n",
    "nearST = sorted(set(np.concatenate(ind)))\n",
    "\n",
    "# k = 1  # <- toggle (3–10)\n",
    "# nbrs_k = NearestNeighbors(n_neighbors=k).fit(rawstdata.obsm['spatial'])\n",
    "# idx = nbrs_k.kneighbors(PDCcoor, return_distance=False)\n",
    "# nearST = sorted(set(idx.flatten()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rawstdata.obs['pDCnear_radius'] = 'Others'\n",
    "rawstdata.obs.loc[rawstdata.obs.index[nearST],'pDCnear_radius'] = 'pDC'\n",
    "\n",
    "# run Scanpy DE (genome-wide)\n",
    "sc.tl.rank_genes_groups(rawstdata, groupby='pDCnear_radius', method='wilcoxon',\n",
    "                        layer='log1p', use_raw=False, key_added='deg_pdc_radius')\n",
    "\n",
    "degdf = sc.get.rank_genes_groups_df(rawstdata, group='pDC', key='deg_pdc_radius', log2fc_min=0)\n",
    "# degdf includes: names, logfoldchanges (Scanpy style), scores, pvals, pvals_adj\n",
    "\n",
    "\n",
    "# Print specific genes (BST2, NRP1) in a clean line format\n",
    "for g in ['BST2','NRP1']:\n",
    "    row = degdf.loc[degdf['names'] == g]\n",
    "    if not row.empty:\n",
    "        r = row.iloc[0]\n",
    "        print(f\"{g}: log2FC={r.logfoldchanges:.3f}, scores= {r.scores}, p={r.pvals:.2e}, padj={r.pvals_adj:.2e}\")\n",
    "    else:\n",
    "        print(f\"{g}: not found in DE results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE on your positive layer\n",
    "sc.tl.rank_genes_groups(rawstdata, groupby='pDCnear',\n",
    "                        method='wilcoxon', layer='log1p',\n",
    "                        use_raw=False, key_added='deg_pdc')\n",
    "\n",
    "# Get results for the 'pDC' group\n",
    "degdf = sc.get.rank_genes_groups_df(rawstdata, group='pDC', key='deg_pdc', log2fc_min=0)\n",
    "\n",
    "valid = rawstdata.obs['pDCnear'].isin(['pDC','Others']).values\n",
    "print(\"sizes:\", valid.sum(), \"/\", rawstdata.n_obs)\n",
    "print(rawstdata.obs['pDCnear'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# Print top hits\n",
    "print(degdf[['names','logfoldchanges','pvals','pvals_adj']].head(5).to_string(index=False))\n",
    "\n",
    "# Print specific genes (BST2, NRP1) in a clean line format\n",
    "for g in ['BST2','NRP1']:\n",
    "    row = degdf.loc[degdf['names'] == g]\n",
    "    if not row.empty:\n",
    "        r = row.iloc[0]\n",
    "        print(f\"{g}: log2FC={r.logfoldchanges:.3f}, scores= {r.scores}, p={r.pvals:.2e}, padj={r.pvals_adj:.2e}\")\n",
    "    else:\n",
    "        print(f\"{g}: not found in DE results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degdf is your DataFrame from scanpy\n",
    "print(\"number of genes tested:\", len(degdf))\n",
    "print(\"raw p < 0.01:\", (degdf['pvals'] < 0.01).sum())\n",
    "print(\"padj < 0.05:\", (degdf['pvals_adj'] < 0.05).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, scanpy as sc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 1) radius = 1.5 × spot pitch\n",
    "ST = rawstdata.obsm['spatial']\n",
    "_nn = NearestNeighbors(n_neighbors=2).fit(ST)\n",
    "_d2,_ = _nn.kneighbors(ST)\n",
    "pitch = np.median(_d2[:,1])\n",
    "radius = 1.5 * pitch\n",
    "\n",
    "PDC = scadata.obsm['advanced_diffusion_coords_rep1'][scadata.obs.level2_celltype=='PDC', :]\n",
    "nnr = NearestNeighbors(radius=radius).fit(ST)\n",
    "ind = nnr.radius_neighbors(PDC, return_distance=False)\n",
    "near = sorted(set(np.concatenate(ind))) if len(ind) else []\n",
    "\n",
    "rawstdata.obs['pDCnear'] = 'Others'\n",
    "rawstdata.obs.iloc[near, rawstdata.obs.columns.get_loc('pDCnear')] = 'pDC'\n",
    "\n",
    "# 2) genome-wide DE on log1p layer\n",
    "sc.tl.rank_genes_groups(rawstdata, groupby='pDCnear',\n",
    "                        method='wilcoxon', layer='log1p',\n",
    "                        use_raw=False, key_added='deg_pdc')\n",
    "\n",
    "deg = sc.get.rank_genes_groups_df(rawstdata, group='pDC', key='deg_pdc', log2fc_min=0)\n",
    "\n",
    "# 3) optional detection filter + effect gate + FDR\n",
    "# (names vary by Scanpy version; keep robust)\n",
    "for a,b in [('pct_nz_group','pct_in_group'), ('pct_nz_reference','pct_in_reference')]:\n",
    "    if a in deg.columns: break\n",
    "pct_g = a if a in deg.columns else None\n",
    "pct_r = 'pct_nz_reference' if 'pct_nz_reference' in deg.columns else 'pct_in_reference'\n",
    "\n",
    "if pct_g and pct_r:\n",
    "    deg = deg[(deg[pct_g] >= 0.10) & (deg[pct_r] >= 0.10)]\n",
    "\n",
    "keepers = deg[(deg['pvals_adj'] < 0.05) & (deg['logfoldchanges'].abs() >= 0.6)]\n",
    "print(\"FDR<0.05 & |log2FC|≥0.6:\", len(keepers))\n",
    "print(keepers[['names','logfoldchanges','pvals_adj']].head(10).to_string(index=False))\n",
    "\n",
    "# 4) for your two markers (optional targeted readout)\n",
    "for g in ['BST2','NRP1']:\n",
    "    if g in deg['names'].values:\n",
    "        r = deg.loc[deg['names']==g].iloc[0]\n",
    "        print(f\"{g}: scanpy_logFC={r.logfoldchanges:.2f}, FDR={r.pvals_adj:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "p = degdf['pvals'].values\n",
    "rej, pvals_bh, _, _ = multipletests(p, alpha=0.05, method='fdr_bh')\n",
    "degdf['p_bh'] = pvals_bh\n",
    "print(\"BH-significant:\", rej.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(degdf['logfoldchanges'], -np.log10(degdf['pvals_adj']+1e-300), s=4)\n",
    "plt.xlabel('log2FC'); plt.ylabel('-log10(padj)')\n",
    "plt.axhline(-np.log10(0.01), color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n",
    "# --- 1) Ensure DE is computed (your params) ---\n",
    "sc.tl.rank_genes_groups(\n",
    "    rawstdata,\n",
    "    groupby='pDCnear',\n",
    "    method='wilcoxon',\n",
    "    layer='log1p',\n",
    "    use_raw=False,\n",
    "    key_added='deg_pdc'\n",
    ")\n",
    "\n",
    "# --- 2) Pull both directions ---\n",
    "deg_pdc  = sc.get.rank_genes_groups_df(rawstdata, group='pDC',    key='deg_pdc', log2fc_min=0)\n",
    "deg_oth  = sc.get.rank_genes_groups_df(rawstdata, group='Others', key='deg_pdc', log2fc_min=0)\n",
    "\n",
    "# mark direction and flip the \"Others\" side to negative log2FC so it plots on the left\n",
    "deg_pdc['direction'] = 'up_in_pDC'\n",
    "deg_oth['direction'] = 'down_in_pDC'\n",
    "deg_oth['logfoldchanges'] = -deg_oth['logfoldchanges']  # put \"up in Others\" on the left\n",
    "\n",
    "# --- 3) Harmonize detection columns if present (names vary by scanpy version) ---\n",
    "def pick(colnames, candidates):\n",
    "    for c in candidates:\n",
    "        if c in colnames: return c\n",
    "    return None\n",
    "\n",
    "def add_detection_cols(df, group_is_pdc: bool):\n",
    "    cols = df.columns\n",
    "    cg = pick(cols, ['pct_nz_group','pct_in_group','pct_group','pct_group_nz'])\n",
    "    cr = pick(cols, ['pct_nz_reference','pct_in_reference','pct_reference','pct_ref_nz'])\n",
    "    if cg is not None and cr is not None:\n",
    "        if group_is_pdc:\n",
    "            df = df.rename(columns={cg:'pct_pDC', cr:'pct_Other'})\n",
    "        else:\n",
    "            # when group == Others, swap meaning\n",
    "            df = df.rename(columns={cg:'pct_Other', cr:'pct_pDC'})\n",
    "    else:\n",
    "        # if not available, create placeholders to skip detection filtering later\n",
    "        df['pct_pDC'] = np.nan\n",
    "        df['pct_Other'] = np.nan\n",
    "    return df\n",
    "\n",
    "deg_pdc = add_detection_cols(deg_pdc, group_is_pdc=True)\n",
    "deg_oth = add_detection_cols(deg_oth, group_is_pdc=False)\n",
    "\n",
    "both = pd.concat([deg_pdc, deg_oth], ignore_index=True)\n",
    "\n",
    "# If adjusted p-values missing for some reason, recompute BH (safety)\n",
    "if 'pvals_adj' not in both.columns or both['pvals_adj'].isna().all():\n",
    "    try:\n",
    "        from statsmodels.stats.multitest import multipletests\n",
    "        rej, p_bh, _, _ = multipletests(both['pvals'].values, alpha=0.05, method='fdr_bh')\n",
    "        both['pvals_adj'] = p_bh\n",
    "    except Exception as e:\n",
    "        print(\"Could not recompute BH:\", e)\n",
    "\n",
    "# --- 4) Volcano (RAW) ---\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(both['logfoldchanges'], -np.log10(both['pvals_adj'] + 1e-300), s=8, alpha=0.6)\n",
    "plt.axhline(-np.log10(0.05), linestyle='--', linewidth=1, color='red')\n",
    "plt.xlabel('log2FC  (right: up in pDC,  left: up in Others)')\n",
    "plt.ylabel('-log10(FDR)')\n",
    "plt.title('Volcano (raw)')\n",
    "plt.show()\n",
    "\n",
    "# --- 5) Optional: filter low-detection artifacts, then effect-size gate ---\n",
    "# Use detection if available; if not, skip this filter\n",
    "have_detection = not both['pct_pDC'].isna().all() and not both['pct_Other'].isna().all()\n",
    "if have_detection:\n",
    "    filt = (both['pct_pDC'] >= 0.10) & (both['pct_Other'] >= 0.10)  # ≥10% nonzero in both groups\n",
    "    both_filt = both.loc[filt].copy()\n",
    "else:\n",
    "    both_filt = both.copy()\n",
    "\n",
    "keepers = both_filt[(both_filt['pvals_adj'] < 0.05) & (both_filt['logfoldchanges'].abs() >= 0.6)]\n",
    "print(f\"Keepers (FDR<0.05 & |log2FC|≥0.6): {len(keepers)}\")\n",
    "\n",
    "# --- 6) Volcano (FILTERED) ---\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(both_filt['logfoldchanges'], -np.log10(both_filt['pvals_adj'] + 1e-300), s=8, alpha=0.6)\n",
    "plt.axhline(-np.log10(0.05), linestyle='--', linewidth=1, color='red')\n",
    "plt.xlabel('log2FC  (right: up in pDC,  left: up in Others)')\n",
    "plt.ylabel('-log10(FDR)')\n",
    "ttl = 'Volcano (filtered by detection ≥10%)' if have_detection else 'Volcano (no detection filter available)'\n",
    "plt.title(ttl)\n",
    "plt.show()\n",
    "\n",
    "# --- 7) Show top genes (both directions) ---\n",
    "top_up    = keepers[keepers['logfoldchanges'] > 0].sort_values('pvals_adj').head(10)\n",
    "top_down  = keepers[keepers['logfoldchanges'] < 0].sort_values('pvals_adj').head(10)\n",
    "\n",
    "print(\"\\nTop up in pDC (keep):\")\n",
    "print(top_up[['names','logfoldchanges','pvals_adj']].to_string(index=False))\n",
    "\n",
    "print(\"\\nTop up in Others (keep):\")\n",
    "print(top_down[['names','logfoldchanges','pvals_adj']].to_string(index=False))\n",
    "\n",
    "# --- 8) Optionally highlight specific markers on the RAW volcano ---\n",
    "markers = ['BST2','NRP1']\n",
    "m = both[both['names'].isin(markers)].copy()\n",
    "if len(m):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(both['logfoldchanges'], -np.log10(both['pvals_adj'] + 1e-300), s=8, alpha=0.2)\n",
    "    plt.scatter(m['logfoldchanges'], -np.log10(m['pvals_adj'] + 1e-300), s=30)\n",
    "    for _, r in m.iterrows():\n",
    "        plt.text(r['logfoldchanges'], -np.log10(r['pvals_adj'] + 1e-300) + 0.2, r['names'], fontsize=9)\n",
    "    plt.axhline(-np.log10(0.05), linestyle='--', linewidth=1, color='red')\n",
    "    plt.xlabel('log2FC  (right: up in pDC,  left: up in Others)')\n",
    "    plt.ylabel('-log10(FDR)')\n",
    "    plt.title('Markers highlighted')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Markers not found in DE table (BST2/NRP1).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import scanpy as sc\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 0) assume you already built rawstdata (with layer 'log1p') and have scadata with pDC coords ---\n",
    "# ST coords (2D, already normalized to [-1,1] in rawstdata.obsm['spatial'])\n",
    "ST = rawstdata.obsm['spatial']\n",
    "\n",
    "# Single-cell pDC coords (whatever space your STEM/advanced diffusion produced)\n",
    "SC_all = scadata.obsm['advanced_diffusion_coords_rep1']\n",
    "pdc_mask = (scadata.obs.level2_celltype == 'PDC').values\n",
    "PDC = SC_all[pdc_mask, :]\n",
    "\n",
    "# (Optional but recommended) quick numeric sanity check of scales\n",
    "print(\"ST range:\", ST.min(0), ST.max(0))\n",
    "print(\"pDC range:\", PDC.min(0), PDC.max(0))\n",
    "\n",
    "# If the two spaces are already aligned by STEM, good. If not, a quick isotropic rescale of PDC to match ST:\n",
    "def iso_norm(X):\n",
    "    c = X.mean(0, keepdims=True)\n",
    "    Y = X - c\n",
    "    r = np.linalg.norm(Y, axis=1).max() + 1e-8\n",
    "    return Y / r\n",
    "# # comment the next line out if your spaces are already in the same gauge\n",
    "# PDC_norm = iso_norm(PDC)\n",
    "# # do the same to ST so both are centered/unit radius (keeps geometry)\n",
    "# ST_norm = iso_norm(ST)\n",
    "\n",
    "PDC_norm = PDC\n",
    "ST_norm = ST\n",
    "\n",
    "# --- 1) k=1 nearest spot for each pDC cell ---\n",
    "nbrs_k = NearestNeighbors(n_neighbors=1).fit(ST_norm)\n",
    "idx = nbrs_k.kneighbors(PDC_norm, return_distance=False).flatten()\n",
    "nearST = sorted(set(idx))  # unique ST spots touched by any pDC cell\n",
    "\n",
    "\n",
    "_nn_st = NearestNeighbors(n_neighbors=2).fit(rawstdata.obsm['spatial'])\n",
    "_d2, _ = _nn_st.kneighbors(rawstdata.obsm['spatial'])\n",
    "spot_pitch = np.median(_d2[:, 1])\n",
    "\n",
    "radius = 1.5 * spot_pitch  # <- toggle this (1.2–2.0× are common)\n",
    "\n",
    "\n",
    "nbrs_rad = NearestNeighbors(radius=radius).fit(rawstdata.obsm['spatial'])\n",
    "ind = nbrs_rad.radius_neighbors(PDCcoor, return_distance=False)\n",
    "nearST = sorted(set(np.concatenate(ind)))\n",
    "\n",
    "# --- 2) make the binary label on ST spots ---\n",
    "rawstdata.obs['pDCnear'] = 'Others'\n",
    "spot_idx = rawstdata.obs.index[nearST]\n",
    "rawstdata.obs.loc[spot_idx, 'pDCnear'] = 'pDC'\n",
    "\n",
    "# sanity: group sizes\n",
    "vc = rawstdata.obs['pDCnear'].value_counts()\n",
    "print(\"\\nGroup sizes:\\n\", vc)\n",
    "\n",
    "# --- 3) targeted tests for prespecified markers (BST2, NRP1, JCHAIN) ---\n",
    "genes = ['BST2','NRP1']\n",
    "group = rawstdata.obs['pDCnear'].values\n",
    "mask_p = (group == 'pDC')\n",
    "mask_o = (group == 'Others')\n",
    "\n",
    "def gene_vec(adata, gene, layer='log1p'):\n",
    "    X = adata[:, gene].layers[layer] if layer in adata.layers else adata[:, gene].X\n",
    "    v = X.toarray().ravel() if hasattr(X, \"toarray\") else np.asarray(X).ravel()\n",
    "    return v\n",
    "\n",
    "rows = []\n",
    "for g in genes:\n",
    "    if g not in rawstdata.var_names:\n",
    "        rows.append(dict(gene=g, present=False))\n",
    "        continue\n",
    "    v = gene_vec(rawstdata, g, layer='log1p')\n",
    "    x_p, x_o = v[mask_p], v[mask_o]\n",
    "\n",
    "    # two-sided Wilcoxon rank-sum (if you prespecified \"higher in pDC\", use alternative='greater')\n",
    "    stat, p = mannwhitneyu(x_p, x_o, alternative='two-sided')\n",
    "\n",
    "    # effect sizes: log2FC on de-logged means; and Cliff's delta (rank-based effect)\n",
    "    mean_p = np.expm1(x_p).mean(); mean_o = np.expm1(x_o).mean()\n",
    "    log2fc = np.log2((mean_p + 1e-8) / (mean_o + 1e-8))\n",
    "\n",
    "    rows.append(dict(gene=g, present=True,\n",
    "                     n_p=mask_p.sum(), n_o=mask_o.sum(),\n",
    "                     mean_p=mean_p, mean_o=mean_o,\n",
    "                     log2FC=log2fc, p=p))\n",
    "\n",
    "out = pd.DataFrame(rows)\n",
    "# OPTIONAL: adjust only across these 3 tests (for completeness; you can still report raw p)\n",
    "rej_bh, q_bh, _, _ = multipletests(out['p'].fillna(1.0), alpha=0.05, method='fdr_bh')\n",
    "out['padj_over_3'] = q_bh\n",
    "out['sig_raw_p<0.01'] = out['p'] < 0.01  # matches paper’s threshold\n",
    "print(\"\\nTargeted marker test (k=1 NN mapping):\")\n",
    "print(out[['gene','present','n_p','n_o','log2FC','p','padj_over_3','sig_raw_p<0.01']])\n",
    "\n",
    "# --- 4) quick visual confirmation of the label and markers ---\n",
    "sc.pl.spatial(rawstdata, color=['pDCnear'], basis='spatial', spot_size=0.05, na_in_legend=False, show=True, save='pDCnear_p2_v1_r15')\n",
    "sc.pl.spatial(rawstdata, color=genes, basis='spatial', spot_size=0.05, na_in_legend=False, show=True, save='p2_bst_nrp_v1')\n",
    "\n",
    "# Optional: simple box/violin for the markers across the two groups\n",
    "fig, axes = plt.subplots(1, len(genes), figsize=(4*len(genes), 4), sharey=False)\n",
    "axes = np.atleast_1d(axes)\n",
    "for ax, g in zip(axes, genes):\n",
    "    if g not in rawstdata.var_names: \n",
    "        ax.set_title(f\"{g} (missing)\"); continue\n",
    "    v = gene_vec(rawstdata, g, 'log1p')\n",
    "    ax.boxplot([v[mask_p], v[mask_o]], labels=['pDC','Others'])\n",
    "    ax.set_title(g); ax.set_ylabel('log1p expr')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Scanpy's DE table\n",
    "scanpy_rows = degdf.set_index('names').loc[['BST2','NRP1'], ['scores','logfoldchanges','pvals']].rename(\n",
    "    columns={'scores':'scanpy_z','logfoldchanges':'scanpy_log2FC','pvals':'scanpy_p'}\n",
    ")\n",
    "\n",
    "# From targeted test results you computed earlier\n",
    "target_rows = out.set_index('gene').loc[['BST2','NRP1'], ['log2FC','p']].rename(\n",
    "    columns={'log2FC':'target_log2FC','p':'target_p'}\n",
    ")\n",
    "\n",
    "print(scanpy_rows.join(target_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# --- helpers ---\n",
    "def gene_vec(adata, gene, layer='log1p'):\n",
    "    X = adata[:, gene].layers[layer] if layer in adata.layers else adata[:, gene].X\n",
    "    return (X.toarray().ravel() if hasattr(X, \"toarray\") else np.asarray(X).ravel())\n",
    "\n",
    "def test_markers(adata, label_col, genes=('BST2','NRP1')):\n",
    "    g = adata.obs[label_col].values\n",
    "    m_p, m_o = (g=='pDC'), (g=='Others')\n",
    "    rows = []\n",
    "    for gene in genes:\n",
    "        if gene not in adata.var_names:\n",
    "            rows.append(dict(gene=gene, present=False)); continue\n",
    "        v = gene_vec(adata, gene, 'log1p')\n",
    "        stat, p = mannwhitneyu(v[m_p], v[m_o], alternative='two-sided')\n",
    "        # effect size on de-logged means\n",
    "        log2fc = np.log2( (np.expm1(v[m_p]).mean()+1e-8) / (np.expm1(v[m_o]).mean()+1e-8) )\n",
    "        rows.append(dict(gene=gene, present=True, n_p=int(m_p.sum()), n_o=int(m_o.sum()),\n",
    "                         log2FC=log2fc, p=p))\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- inputs you already have ---\n",
    "ST = rawstdata.obsm['spatial']                 # predicted ST coord frame\n",
    "PDCcoor = scadata.obsm['advanced_diffusion_coords_rep1'][scadata.obs.level2_celltype=='PDC', :]\n",
    "\n",
    "# (Optional) If both are already centered/scaled similarly (your ranges look fine), skip normalization.\n",
    "\n",
    "# --- k = 1 (paper style) ---\n",
    "nn1 = NearestNeighbors(n_neighbors=1).fit(ST)\n",
    "idx1 = nn1.kneighbors(PDCcoor, return_distance=False).flatten()\n",
    "near_knn = sorted(set(idx1))\n",
    "rawstdata.obs['pDCnear_k1'] = 'Others'\n",
    "rawstdata.obs.iloc[near_knn, rawstdata.obs.columns.get_loc('pDCnear_k1')] = 'pDC'\n",
    "\n",
    "# --- radius = 1.5 × spot pitch (robustness) ---\n",
    "_nn_st = NearestNeighbors(n_neighbors=2).fit(ST)\n",
    "_d2, _ = _nn_st.kneighbors(ST)\n",
    "spot_pitch = np.median(_d2[:,1])\n",
    "radius = 1.5 * spot_pitch\n",
    "nnr = NearestNeighbors(radius=radius).fit(ST)\n",
    "ind = nnr.radius_neighbors(PDCcoor, return_distance=False)\n",
    "near_rad = sorted(set(np.concatenate(ind)))\n",
    "rawstdata.obs['pDCnear_r15'] = 'Others'\n",
    "rawstdata.obs.iloc[near_rad, rawstdata.obs.columns.get_loc('pDCnear_r15')] = 'pDC'\n",
    "\n",
    "# --- compare targeted tests ---\n",
    "res_k1  = test_markers(rawstdata, 'pDCnear_k1', genes=('BST2','NRP1'))\n",
    "res_r15 = test_markers(rawstdata, 'pDCnear_r15', genes=('BST2','NRP1'))\n",
    "\n",
    "print(\"k=1 groups:\\n\", rawstdata.obs['pDCnear_k1'].value_counts().to_string(), \"\\n\")\n",
    "print(res_k1[['gene','n_p','n_o','log2FC','p']])\n",
    "\n",
    "print(\"\\nradius=1.5×pitch groups:\\n\", rawstdata.obs['pDCnear_r15'].value_counts().to_string(), \"\\n\")\n",
    "print(res_r15[['gene','n_p','n_o','log2FC','p']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def add_q2(df_with_p):\n",
    "    df = df_with_p.copy()\n",
    "    # BH-FDR across just these rows (here: 2 genes)\n",
    "    _, qvals, _, _ = multipletests(df['p'].values, alpha=0.05, method='fdr_bh')\n",
    "    df['q_two'] = qvals\n",
    "    return df\n",
    "\n",
    "res_k1  = add_q2(res_k1)    # your k=1 table\n",
    "res_r15 = add_q2(res_r15)   # your radius=1.5× table\n",
    "print(\"k=1:\\n\",  res_k1[['gene','n_p','n_o','log2FC','p','q_two']])\n",
    "print(\"\\nradius=1.5×:\\n\", res_r15[['gene','n_p','n_o','log2FC','p','q_two']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# --- user-adjustable ---\n",
    "genes = ['BST2', 'NRP1']          # change if you want to plot different genes\n",
    "label_col = 'pDCnear_r15'         # the column in rawstdata.obs with 'pDC'/'Others'\n",
    "out_dir = 'figures'\n",
    "out_fname = 'violin_pdc_markers.png'\n",
    "palette = ['#800000', '#bdbdbd']  # [pDC, Others] - change colors if you want\n",
    "\n",
    "# --- helpers ---\n",
    "def gene_vec(adata, gene, layer='log1p'):\n",
    "    \"\"\"Return 1D numpy array of gene expression from layer or X.\"\"\"\n",
    "    if layer in adata.layers:\n",
    "        X = adata[:, gene].layers[layer]\n",
    "    else:\n",
    "        X = adata[:, gene].X\n",
    "    return (X.toarray().ravel() if hasattr(X, \"toarray\") else np.asarray(X).ravel())\n",
    "\n",
    "def targeted_stats_and_df(adata, gene, label_col, layer='log1p'):\n",
    "    \"\"\"Return (p-value, log2FC on count-scale, dataframe for plotting).\"\"\"\n",
    "    if gene not in adata.var_names:\n",
    "        raise ValueError(f\"Gene {gene} not in adata.var_names\")\n",
    "    if label_col not in adata.obs.columns:\n",
    "        raise ValueError(f\"Label column {label_col} not in adata.obs\")\n",
    "    grp = adata.obs[label_col].values\n",
    "    mask_p = (grp == 'pDC')\n",
    "    mask_o = (grp == 'Others')\n",
    "    v = gene_vec(adata, gene, layer=layer)\n",
    "    # Mann-Whitney two-sided\n",
    "    stat, p = mannwhitneyu(v[mask_p], v[mask_o], alternative='two-sided')\n",
    "    # log2FC computed on de-logged means (count-scale approx)\n",
    "    mean_p = np.expm1(v[mask_p]).mean() if mask_p.sum() > 0 else 0.0\n",
    "    mean_o = np.expm1(v[mask_o]).mean() if mask_o.sum() > 0 else 0.0\n",
    "    log2fc = np.log2((mean_p + 1e-8) / (mean_o + 1e-8))\n",
    "    # build plotting dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'Group': np.where(grp == 'pDC', 'pDC', 'Others'),\n",
    "        'Expression': v\n",
    "    })\n",
    "    return p, log2fc, df\n",
    "\n",
    "# --- make sure output dir exists ---\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "out_path = os.path.join(out_dir, out_fname)\n",
    "\n",
    "# --- plotting ---\n",
    "sns.set(style='whitegrid')\n",
    "fig, axes = plt.subplots(1, len(genes), figsize=(10, 4), sharey=True)\n",
    "\n",
    "for ax, g in zip(axes.flat, genes):\n",
    "    p, l2fc, df = targeted_stats_and_df(rawstdata, g, label_col)\n",
    "    sns.violinplot(data=df, x='Group', y='Expression',\n",
    "                   order=['pDC', 'Others'],\n",
    "                   cut=0, scale='width', inner=None,\n",
    "                   palette=palette, ax=ax)\n",
    "    sns.stripplot(data=df, x='Group', y='Expression',\n",
    "                  order=['pDC', 'Others'],\n",
    "                  size=3, color='k', alpha=0.6, jitter=True, ax=ax)\n",
    "    # medians\n",
    "    medians = df.groupby('Group')['Expression'].median().reindex(['pDC', 'Others']).values\n",
    "    xpositions = np.arange(len(medians))\n",
    "    ax.scatter(xpositions, medians, s=60, facecolors='white', edgecolors='black', zorder=10)\n",
    "    ax.set_title(f\"{g}    P={p:.1e}    Log2FC={l2fc:.2f}\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Expression (log1p)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Saved violin figure to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# choose which label to show\n",
    "label_col = 'pDCnear_r15'   # or 'pDCnear_r15'\n",
    "\n",
    "genes = ['BST2','NRP1']\n",
    "\n",
    "# color palette: orange for pDC, blue for Others\n",
    "palette = {'pDC': '#FF7F0E', 'Others': '#1F77B4'}\n",
    "\n",
    "def gene_vec(adata, gene, layer='log1p'):\n",
    "    X = adata[:, gene].layers[layer] if layer in adata.layers else adata[:, gene].X\n",
    "    return X.toarray().ravel() if hasattr(X, \"toarray\") else np.asarray(X).ravel()\n",
    "\n",
    "def targeted_stats_and_df(adata, gene, label_col):\n",
    "    v = gene_vec(adata, gene, 'log1p')\n",
    "    grp = adata.obs[label_col].values\n",
    "    m_p, m_o = (grp == 'pDC'), (grp == 'Others')\n",
    "    # Mann-Whitney (two-sided, asymptotic with continuity) to be close to Scanpy style behavior\n",
    "    stat, p = mannwhitneyu(v[m_p], v[m_o], alternative='two-sided', method='asymptotic', use_continuity=True)\n",
    "    # log2FC on de-logged means (interpretable fold-change)\n",
    "    mean_p = np.expm1(v[m_p]).mean()\n",
    "    mean_o = np.expm1(v[m_o]).mean()\n",
    "    log2fc = np.log2((mean_p + 1e-8) / (mean_o + 1e-8))\n",
    "    df = pd.DataFrame({'Expression': v, 'Group': grp})\n",
    "    return p, log2fc, df\n",
    "\n",
    "###############################################################################\n",
    "# Variant A: show median (and quartiles) inside violin (informative, like paper)\n",
    "###############################################################################\n",
    "fig, axes = plt.subplots(1, len(genes), figsize=(10,4), sharey=True)\n",
    "for ax, g in zip(axes.flat, genes):\n",
    "    p, l2fc, df = targeted_stats_and_df(rawstdata, g, label_col)\n",
    "    sns.violinplot(data=df, x='Group', y='Expression',\n",
    "                   order=['pDC','Others'],\n",
    "                   cut=0, scale='width',\n",
    "                   inner='quartile',    # shows median + quartiles (middle line is median)\n",
    "                   palette=palette,\n",
    "                   ax=ax)\n",
    "    sns.stripplot(data=df, x='Group', y='Expression',\n",
    "                  order=['pDC','Others'],\n",
    "                  size=3, color='k', alpha=0.6, jitter=True, ax=ax)\n",
    "    ax.set_title(f\"{g}    P={p:.1e}    Log2FC={l2fc:.2f}\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Expression (log1p)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Variant B: no internal lines (cleaner), but we add a separate median marker\n",
    "###############################################################################\n",
    "fig, axes = plt.subplots(1, len(genes), figsize=(10,4), sharey=True)\n",
    "for ax, g in zip(axes.flat, genes):\n",
    "    p, l2fc, df = targeted_stats_and_df(rawstdata, g, label_col)\n",
    "    sns.violinplot(data=df, x='Group', y='Expression',\n",
    "                   order=['pDC','Others'],\n",
    "                   cut=0, scale='width',\n",
    "                   inner=None,         # NO median/box/lines inside the violin\n",
    "                   palette=palette,\n",
    "                   ax=ax)\n",
    "    sns.stripplot(data=df, x='Group', y='Expression',\n",
    "                  order=['pDC','Others'],\n",
    "                  size=3, color='k', alpha=0.6, jitter=True, ax=ax)\n",
    "    # draw median points manually (white dot with black edge for visibility)\n",
    "    medians = df.groupby('Group')['Expression'].median().reindex(['pDC','Others']).values\n",
    "    xpositions = np.arange(len(medians))\n",
    "    ax.scatter(xpositions, medians, s=60, facecolors='white', edgecolors='black', zorder=10)\n",
    "    ax.set_title(f\"{g}    P={p:.1e}    Log2FC={l2fc:.2f}\")\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Expression (log1p)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_asym, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "ST = rawstdata.obsm['spatial']\n",
    "PDCcoor = scadata.obsm['advanced_diffusion_coords_rep1'][scadata.obs.level2_celltype=='PDC', :]\n",
    "genes = ['BST2','NRP1']\n",
    "\n",
    "def gvec(adata, gene, layer='log1p'):\n",
    "    X = adata[:, gene].layers[layer] if layer in adata.layers else adata[:, gene].X\n",
    "    return (X.toarray().ravel() if hasattr(X, \"toarray\") else np.asarray(X).ravel())\n",
    "\n",
    "def test_set(near_idx, label):\n",
    "    col = f'pDCnear_{label}'\n",
    "    rawstdata.obs[col] = 'Others'\n",
    "    rawstdata.obs.iloc[near_idx, rawstdata.obs.columns.get_loc(col)] = 'pDC'\n",
    "    m_p = (rawstdata.obs[col].values=='pDC'); m_o = ~m_p\n",
    "    rows=[]\n",
    "    for g in genes:\n",
    "        if g not in rawstdata.var_names: rows.append(dict(defn=label, gene=g, present=False)); continue\n",
    "        v = gvec(rawstdata, g, 'log1p')\n",
    "        stat,p = mannwhitneyu(v[m_p], v[m_o], alternative='two-sided')\n",
    "        log2fc = np.log2((np.expm1(v[m_p]).mean()+1e-8)/(np.expm1(v[m_o]).mean()+1e-8))\n",
    "        rows.append(dict(defn=label, gene=g, n_p=int(m_p.sum()), n_o=int(m_o.sum()), log2FC=log2fc, p=p))\n",
    "    return rows\n",
    "\n",
    "# spot pitch\n",
    "_nn = NearestNeighbors(n_neighbors=2).fit(ST)\n",
    "_d2,_ = _nn.kneighbors(ST)\n",
    "pitch = np.median(_d2[:,1])\n",
    "\n",
    "results = []\n",
    "\n",
    "# k sweep\n",
    "for k in [1,3,5,10]:\n",
    "    nn = NearestNeighbors(n_neighbors=k).fit(ST)\n",
    "    idx = nn.kneighbors(PDCcoor, return_distance=False)\n",
    "    near = sorted(set(idx.flatten()))\n",
    "    results += test_set(near, f'k{k}')\n",
    "\n",
    "# radius sweep\n",
    "for c in [1.2,1.5,1.8,2.0]:\n",
    "    nnr = NearestNeighbors(radius=c*pitch).fit(ST)\n",
    "    ind = nnr.radius_neighbors(PDCcoor, return_distance=False)\n",
    "    near = sorted(set(np.concatenate(ind))) if len(ind) else []\n",
    "    results += test_set(near, f'r{c:.1f}x')\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(['gene','defn'])\n",
    "print(df[['gene','defn','n_p','n_o','log2FC','p']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawstdata.obs['pDCnear'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(PDCcoor.mean()-rawstdata.obsm['spatial'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "genes = ['BST2','NRP1']\n",
    "mask = (rawstdata.obs['pDCnear'].values == 'pDC')  # same 67 vs 599 split\n",
    "X = np.asarray(rawstdata.layers['log1p'])          # matrix used for DE\n",
    "eps = 1e-9\n",
    "\n",
    "for g in genes:\n",
    "    j = rawstdata.var_names.get_loc(g)\n",
    "    mu_in  = X[mask,  j].mean()\n",
    "    mu_out = X[~mask, j].mean()\n",
    "    l2fc_manual = np.log2((mu_in + eps) / (mu_out + eps))\n",
    "    l2fc_scanpy = float(degdf.loc[degdf.names == g, 'logfoldchanges'].iloc[0])\n",
    "    print(f\"{g}: mu_in={mu_in:.4f} mu_out={mu_out:.4f}  manual_log1p={l2fc_manual:.3f}  scanpy={l2fc_scanpy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rawstdata.uns['deg_pdc']['params'])  # should show layer='log1p', use_raw=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "mask = (rawstdata.obs['pDCnear'].values == 'pDC')\n",
    "\n",
    "for g in ['BST2','NRP1']:\n",
    "    j = rawstdata.var_names.get_loc(g)\n",
    "    x = np.asarray(rawstdata.layers['log1p'][:, j]).ravel()\n",
    "\n",
    "    l2fc_log1p  = np.log2((x[mask].mean()+eps)/(x[~mask].mean()+eps))\n",
    "    l2fc_counts = np.log2((np.expm1(x[mask]).mean()+eps)/(np.expm1(x[~mask]).mean()+eps))\n",
    "    scanpy_l2fc = float(degdf.loc[degdf.names==g, 'logfoldchanges'])\n",
    "\n",
    "    print(g, \"manual_log1p:\", round(l2fc_log1p,3),\n",
    "              \"manual_counts:\", round(l2fc_counts,3),\n",
    "              \"scanpy:\", round(scanpy_l2fc,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "ks = [1,2,3, 4,5,6,7,8,9,10]\n",
    "markers = ['BST2','NRP1','JCHAIN']\n",
    "\n",
    "# helper to get mean(log1p) and two-sided MWU p\n",
    "def _mean_log1p(gene, mask):\n",
    "    j = rawstdata.var_names.get_loc(gene)\n",
    "    x = np.asarray(rawstdata.layers['log1p'][:, j]).ravel()\n",
    "    return x[mask].mean(), x[~mask].mean(), x[mask], x[~mask]\n",
    "\n",
    "print(\"k | frac_labeled | BST2 Δmean  p    | NRP1 Δmean  p    | JCHAIN Δmean  p\")\n",
    "for k in ks:\n",
    "    idx = NearestNeighbors(n_neighbors=k).fit(rawstdata.obsm['spatial']).kneighbors(PDCcoor, return_distance=False)\n",
    "    nearST = sorted(set(idx.flatten()))\n",
    "    mask = np.zeros(rawstdata.n_obs, dtype=bool); mask[nearST] = True\n",
    "    frac = mask.mean()\n",
    "\n",
    "    line = [f\"{k:2d}\", f\"{100*frac:6.1f}%\"]\n",
    "    for g in markers:\n",
    "        m_in, m_out, xin, xout = _mean_log1p(g, mask)\n",
    "        p = mannwhitneyu(xin, xout, alternative='two-sided', method='asymptotic').pvalue\n",
    "        line += [f\"{(m_in-m_out):7.3f}\", f\"{p:6.2e}\"]\n",
    "    print(\" \".join(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(rawstdata,color=['NRP1','BST2','pDCnear'],show=True,basis='spatial',na_in_legend=False,spot_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you set .raw right after log1p\n",
    "# sc.pl.violin(rawstdata, ['BST2','NRP1'], groupby='pDCnear',\n",
    "#              multi_panel=True, order=['pDC','Others'], use_raw=True)\n",
    "\n",
    "# or use the saved layer explicitly\n",
    "sc.pl.violin(rawstdata, ['BST2','NRP1'], groupby='pDCnear',\n",
    "             multi_panel=True, order=['pDC','Others'],\n",
    "             use_raw=False, layer='log1p', size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X min/max (current):\", rawstdata.X.min(), rawstdata.X.max())        # likely negative (scaled)\n",
    "print(\"raw min/max:\", rawstdata.raw.X.min(), rawstdata.raw.X.max())        # should be ≥0 if set pre-scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def gene_values_from_adata(adata, gene, use_raw=True):\n",
    "    \"\"\"Return a 1-D numpy array of expression values for `gene`.\n",
    "       Works whether X is sparse or dense and whether .raw exists.\"\"\"\n",
    "    # choose var names / X matrix\n",
    "    if use_raw and getattr(adata, \"raw\", None) is not None:\n",
    "        var_names = adata.raw.var_names\n",
    "        X = adata.raw.X\n",
    "    else:\n",
    "        var_names = adata.var_names\n",
    "        X = adata.X\n",
    "\n",
    "    # get index robustly\n",
    "    try:\n",
    "        # if var_names is an Index, this is efficient\n",
    "        idx = var_names.get_loc(gene)\n",
    "    except Exception:\n",
    "        # fallback to numpy search\n",
    "        idx = np.where(np.asarray(var_names) == gene)[0][0]\n",
    "\n",
    "    col = X[:, idx]\n",
    "\n",
    "    # if indexing returns a 2D sparse matrix, convert to 1D array\n",
    "    if sparse.issparse(col):\n",
    "        vals = col.toarray().ravel()\n",
    "    else:\n",
    "        vals = np.asarray(col).ravel()\n",
    "\n",
    "    return vals\n",
    "\n",
    "# usage\n",
    "bst2_vals = gene_values_from_adata(rawstdata, \"BST2\", use_raw=True)\n",
    "nrp1_vals = gene_values_from_adata(rawstdata, \"NRP1\", use_raw=True)\n",
    "\n",
    "# basic ranges & summary\n",
    "print(\"BST2: min\", bst2_vals.min(), \"max\", bst2_vals.max())\n",
    "print(\"NRP1: min\", nrp1_vals.min(), \"max\", nrp1_vals.max())\n",
    "\n",
    "# extra useful diagnostics\n",
    "def summarize(vals, name):\n",
    "    print(f\"\\n{name} summary:\")\n",
    "    print(\"  min/max      :\", vals.min(), \"/\", vals.max())\n",
    "    print(\"  median       :\", np.median(vals))\n",
    "    print(\"  mean         :\", np.mean(vals))\n",
    "    print(\"  5/95 percent.:\", np.percentile(vals, 5), \"/\", np.percentile(vals, 95))\n",
    "    print(\"  zeros fraction:\", np.mean(vals == 0))\n",
    "    # simple heuristic for log-transform detection\n",
    "    if vals.max() > 100:\n",
    "        print(\"  NOTE: values look like raw counts (max > 100).\")\n",
    "    elif vals.max() <= 20 and np.any(vals < 0) is False:\n",
    "        print(\"  NOTE: values might be log1p or normalized (max <= 20).\")\n",
    "\n",
    "summarize(bst2_vals, \"BST2\")\n",
    "summarize(nrp1_vals, \"NRP1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_plot = rawstdata.copy()\n",
    "sc.pp.normalize_total(ad_plot, target_sum=1e4)  # or the paper’s target_sum\n",
    "sc.pp.log1p(ad_plot)\n",
    "\n",
    "# plot from ad_plot (not scaled), so values are ≥ 0\n",
    "sc.pl.violin(ad_plot, ['BST2','NRP1'], groupby='pDCnear',\n",
    "             size=2.0, multi_panel=True, order=['pDC','Others'], use_raw=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst2 = gene_values_from_adata(ad_plot, \"BST2\", use_raw=False)\n",
    "nrp1 = gene_values_from_adata(ad_plot, \"NRP1\", use_raw=False)\n",
    "print(bst2.min(), bst2.max(), nrp1.min(), nrp1.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(rawstdata, ['BST2','NRP1'], groupby='pDCnear',\n",
    "             size=2.0, multi_panel=True, order=['pDC','Others'],\n",
    "             use_raw=False, layer='log1p')  # change to your layer name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "# sc.settings.figdir='./revise/'\n",
    "fig = sc.pl.violin(rawstdata,['BST2','NRP1'],groupby='pDCnear',size=2.0,multi_panel=True,order =['pDC','Others'],use_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDCcoor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import stats\n",
    "\n",
    "# Set plot parameters\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['pdf.fonttype'] = 42\n",
    "rcParams['ps.fonttype'] = 42\n",
    "\n",
    "\n",
    "def calculate_log2fc_safe(group1_expr, group2_expr, pseudocount=1e-9, layer='log1p'):\n",
    "    \"\"\"\n",
    "    Calculate log2 fold change with pseudocount to avoid log(0)\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    group1_expr = group1_expr[~np.isnan(group1_expr)]\n",
    "    group2_expr = group2_expr[~np.isnan(group2_expr)]\n",
    "    \n",
    "    # Calculate means with pseudocount\n",
    "    mean1 = np.mean(group1_expr) + pseudocount\n",
    "    mean2 = np.mean(group2_expr) + pseudocount\n",
    "    \n",
    "    # Calculate log2FC\n",
    "    log2fc = np.log2(mean1 / mean2)\n",
    "    \n",
    "    return log2fc\n",
    "\n",
    "def add_stat_annotation(adata, genes, groupby='pDCnear', groups=['pDC', 'Others'], \n",
    "                        use_raw=False, pseudocount=1e-9, layer='log1p'):\n",
    "    \"\"\"\n",
    "    Calculate statistics and create annotated violin plots\n",
    "    \"\"\"\n",
    "    # Get the expression data\n",
    "    if use_raw and adata.raw is not None:\n",
    "        expr_matrix = adata.raw.X\n",
    "        var_names = adata.raw.var_names\n",
    "    else:\n",
    "        # expr_matrix = adata.X\n",
    "        expr_matrix = adata.layers['log1p']\n",
    "        var_names = adata.var_names\n",
    "\n",
    "    valid = adata.obs[groupby].isin(groups) & adata.obs[groupby].notna()\n",
    "    expr_matrix = expr_matrix[valid, :]\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(expr_matrix, 'toarray'):\n",
    "        expr_matrix = expr_matrix.toarray()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, len(genes), figsize=(4*len(genes), 5))\n",
    "    if len(genes) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, gene in enumerate(genes):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Get gene index\n",
    "        try:\n",
    "            gene_idx = var_names.get_loc(gene)\n",
    "        except KeyError:\n",
    "            print(f\"Gene {gene} not found in data\")\n",
    "            continue\n",
    "        \n",
    "        # Get expression values for this gene\n",
    "        gene_expr = expr_matrix[:, gene_idx]\n",
    "        \n",
    "        # Split by groups\n",
    "        # group_labels = adata.obs[groupby].values\n",
    "        group_labels = adata.obs[groupby].values[valid]\n",
    "        group1_mask = group_labels == groups[0]\n",
    "        group2_mask = group_labels == groups[1]\n",
    "        \n",
    "        group1_expr = gene_expr[group1_mask]\n",
    "        group2_expr = gene_expr[group2_mask]\n",
    "        \n",
    "        # Remove NaN values for statistics\n",
    "        group1_expr_clean = group1_expr[~np.isnan(group1_expr)]\n",
    "        group2_expr_clean = group2_expr[~np.isnan(group2_expr)]\n",
    "        \n",
    "        # Check if we have data\n",
    "        if len(group1_expr_clean) == 0 or len(group2_expr_clean) == 0:\n",
    "            print(f\"No valid data for {gene} in one of the groups\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate statistics\n",
    "        try:\n",
    "            # Mann-Whitney U test\n",
    "            # statistic, pvalue = stats.mannwhitneyu(group1_expr_clean, group2_expr_clean, \n",
    "            #                                        alternative='two-sided')\n",
    "            statistic, pvalue = stats.mannwhitneyu(group1_expr_clean, group2_expr_clean,\n",
    "                                       alternative='two-sided', method='asymptotic')\n",
    "\n",
    "        except:\n",
    "            # If test fails, use t-test\n",
    "            statistic, pvalue = stats.ttest_ind(group1_expr_clean, group2_expr_clean)\n",
    "        \n",
    "        # Calculate log2 fold change with pseudocount\n",
    "        log2fc = calculate_log2fc_safe(group1_expr, group2_expr, pseudocount=pseudocount)\n",
    "        \n",
    "        # Format p-value\n",
    "        if pvalue < 0.001:\n",
    "            p_text = f\"P={pvalue:.1e}\"\n",
    "        else:\n",
    "            p_text = f\"P={pvalue:.3f}\"\n",
    "        \n",
    "        # Create violin plot using scanpy\n",
    "        # sc.pl.violin(adata, gene, groupby=groupby, ax=ax, show=False, \n",
    "        #             order=groups, use_raw=use_raw, stripplot=True, layer='log1p')\n",
    "        sc.pl.violin(adata[valid], gene, groupby=groupby, ax=ax, show=False,\n",
    "             order=groups, use_raw=use_raw, stripplot=True, layer='log1p')\n",
    "\n",
    "        \n",
    "        # Add statistical annotation as title\n",
    "        title = f\"{gene} {p_text} Log2FC={log2fc:.2f}\"\n",
    "        ax.set_title(title, fontsize=12, pad=10)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Expression')\n",
    "        \n",
    "        print(f\"{gene}: {p_text}, Log2FC={log2fc:.2f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Method 1: Using the safer function\n",
    "fig = add_stat_annotation(rawstdata, genes=['BST2', 'NRP1'], \n",
    "                          groupby='pDCnear', groups=['pDC', 'Others'], \n",
    "                          use_raw=False, pseudocount=1e-9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(rawstdata,groupby='pDCnear')\n",
    "degdf = sc.get.rank_genes_groups_df(rawstdata,group='pDC',log2fc_min=0)\n",
    "degdf.loc[degdf.names.isin(['BST2','NRP1']),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check if 'pDCnear' column exists and has the right values\n",
    "if 'pDCnear' not in rawstdata.obs.columns:\n",
    "    # Create it - you need to define which spots are near pDC\n",
    "    # This is an example - adjust based on your criteria\n",
    "    rawstdata.obs['pDCnear'] = 'Others'  # Set all as Others first\n",
    "    # Then set specific spots as 'pDC' based on your criteria\n",
    "    # rawstdata.obs.loc[some_condition, 'pDCnear'] = 'pDC'\n",
    "    \n",
    "# Check the groups\n",
    "print(\"Groups in pDCnear:\", rawstdata.obs['pDCnear'].value_counts())\n",
    "\n",
    "# IMPORTANT: Run rank_genes_groups on RAW data, not scaled data\n",
    "# Use the .raw attribute which has the log-normalized but unscaled data\n",
    "sc.tl.rank_genes_groups(rawstdata, groupby='pDCnear', use_raw=True, method='wilcoxon')\n",
    "\n",
    "# Now get the results\n",
    "degdf = sc.get.rank_genes_groups_df(rawstdata, group='pDC')\n",
    "print(degdf.loc[degdf.names.isin(['BST2','NRP1']),])\n",
    "\n",
    "# If you want to see top genes\n",
    "print(\"\\nTop 10 genes:\")\n",
    "print(degdf.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Get the expression data from raw\n",
    "genes = ['BST2', 'NRP1']\n",
    "results = []\n",
    "\n",
    "for gene in genes:\n",
    "    # Get gene expression from raw data\n",
    "    gene_idx = rawstdata.raw.var_names.get_loc(gene)\n",
    "    expr = rawstdata.raw.X[:, gene_idx]\n",
    "    \n",
    "    # Convert to array if sparse\n",
    "    if hasattr(expr, 'toarray'):\n",
    "        expr = expr.toarray().flatten()\n",
    "    \n",
    "    # Split by groups\n",
    "    pdc_expr = expr[rawstdata.obs['pDCnear'] == 'pDC']\n",
    "    others_expr = expr[rawstdata.obs['pDCnear'] == 'Others']\n",
    "    \n",
    "    # Calculate means with pseudocount to avoid log(0)\n",
    "    pseudocount = 0.1  # Small value to avoid log(0)\n",
    "    mean_pdc = np.mean(pdc_expr) + pseudocount\n",
    "    mean_others = np.mean(others_expr) + pseudocount\n",
    "    \n",
    "    # Calculate log2FC\n",
    "    log2fc = np.log2(mean_pdc / mean_others)\n",
    "    \n",
    "    # Calculate p-value using Mann-Whitney U test\n",
    "    stat, pval = stats.mannwhitneyu(pdc_expr, others_expr)\n",
    "    \n",
    "    results.append({\n",
    "        'gene': gene,\n",
    "        'mean_pDC': mean_pdc - pseudocount,  # Show actual mean\n",
    "        'mean_Others': mean_others - pseudocount,\n",
    "        'log2FC': log2fc,\n",
    "        'pvalue': pval\n",
    "    })\n",
    "    \n",
    "    print(f\"{gene}: P={pval:.1e}, Log2FC={log2fc:.2f}\")\n",
    "\n",
    "# Create dataframe\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\", results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "eps = 1e-9\n",
    "X = rawstdata.layers['log1p']  # same data you pass to rank_genes_groups(layer='log1p')\n",
    "\n",
    "genes = ['BST2','NRP1']\n",
    "mask = (rawstdata.obs['pDCnear'] == 'pDC').values\n",
    "results = []\n",
    "\n",
    "for g in genes:\n",
    "    j = rawstdata.var_names.get_loc(g)\n",
    "    x1 = np.asarray(X[mask, j]).ravel()\n",
    "    x0 = np.asarray(X[~mask, j]).ravel()\n",
    "\n",
    "    # log2FC on the SAME (log1p) scale used by Scanpy\n",
    "    l2fc = np.log2((x1.mean() + eps) / (x0.mean() + eps))\n",
    "\n",
    "    # Wilcoxon/MWU similar to Scanpy\n",
    "    p = mannwhitneyu(x1, x0, alternative='two-sided', method='asymptotic').pvalue\n",
    "\n",
    "    results.append((g, l2fc, p))\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2fc_counts = np.log2((np.expm1(x1).mean() + eps) / (np.expm1(x0).mean() + eps))\n",
    "l2fc_counts\n",
    "l2fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ST coordinates range:\")\n",
    "print(f\"  X: [{rawstdata.obsm['spatial'][:, 0].min():.3f}, {rawstdata.obsm['spatial'][:, 0].max():.3f}]\")\n",
    "print(f\"  Y: [{rawstdata.obsm['spatial'][:, 1].min():.3f}, {rawstdata.obsm['spatial'][:, 1].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use t-test method which handles zeros better\n",
    "sc.tl.rank_genes_groups(rawstdata, groupby='pDCnear', \n",
    "                       use_raw=False, method='t-test')\n",
    "degdf = sc.get.rank_genes_groups_df(rawstdata, group='pDC')\n",
    "print(degdf.loc[degdf.names.isin(['BST2','NRP1']),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSC pDC coordinates range:\")\n",
    "print(f\"  X: [{PDCcoor[:, 0].min():.3f}, {PDCcoor[:, 0].max():.3f}]\")\n",
    "print(f\"  Y: [{PDCcoor[:, 1].min():.3f}, {PDCcoor[:, 1].max():.3f}]\")\n",
    "print(f\"  Number of pDC cells: {len(PDCcoor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create the violin plots\n",
    "axes = sc.pl.violin(rawstdata, ['BST2', 'NRP1'], groupby='pDCnear', \n",
    "                    size=2.0, multi_panel=True, order=['pDC', 'Others'], \n",
    "                    use_raw=True, show=False)\n",
    "\n",
    "# Calculate and add statistics for each gene\n",
    "genes = ['BST2', 'NRP1']\n",
    "for idx, (ax, gene) in enumerate(zip(axes, genes)):\n",
    "    # Get expression data\n",
    "    if rawstdata.raw:\n",
    "        gene_idx = rawstdata.raw.var_names.get_loc(gene)\n",
    "        expr_data = rawstdata.raw.X if hasattr(rawstdata.raw.X, 'toarray') else rawstdata.raw[:, gene_idx].X\n",
    "        if hasattr(expr_data, 'toarray'):\n",
    "            expr_data = expr_data.toarray().flatten()\n",
    "    else:\n",
    "        gene_idx = rawstdata.var_names.get_loc(gene)\n",
    "        expr_data = rawstdata[:, gene_idx].X\n",
    "        if hasattr(expr_data, 'toarray'):\n",
    "            expr_data = expr_data.toarray().flatten()\n",
    "    \n",
    "    # Split by groups\n",
    "    group_labels = rawstdata.obs['pDCnear'].values\n",
    "    pdc_expr = expr_data[group_labels == 'pDC']\n",
    "    others_expr = expr_data[group_labels == 'Others']\n",
    "    \n",
    "    # Calculate statistics\n",
    "    _, pvalue = stats.mannwhitneyu(pdc_expr, others_expr, alternative='two-sided')\n",
    "    \n",
    "    # Calculate log2 fold change\n",
    "    epsilon = 1e-9\n",
    "    mean_pdc = np.mean(pdc_expr) + epsilon\n",
    "    mean_others = np.mean(others_expr) + epsilon\n",
    "    log2fc = np.log2(mean_pdc / mean_others)\n",
    "    \n",
    "    # Format text\n",
    "    if pvalue < 0.001:\n",
    "        p_text = f\"P={pvalue:.1e}\"\n",
    "    else:\n",
    "        p_text = f\"P={pvalue:.3f}\"\n",
    "    \n",
    "    # Add as title\n",
    "    ax.set_title(f\"{gene} {p_text} Log2FC={log2fc:.2f}\", fontsize=11, pad=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Alternative: Add text annotation within the plot area\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for idx, gene in enumerate(['BST2', 'NRP1']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create violin plot\n",
    "    sc.pl.violin(rawstdata, gene, groupby='pDCnear', ax=ax, show=False,\n",
    "                order=['pDC', 'Others'], use_raw=True, stripplot=True)\n",
    "    \n",
    "    # Calculate stats (same as above)\n",
    "    # ... (statistics calculation code)\n",
    "    \n",
    "    # Add text annotation in the plot area\n",
    "    ax.text(0.5, 0.95, f\"{gene} P={pvalue:.1e} Log2FC={log2fc:.2f}\", \n",
    "            transform=ax.transAxes, ha='center', va='top', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata.obsm['advanced_diffusion_coords_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata.obsm['advanced_diffusion_coords_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (6, 6)\n",
    "# import scanpy as sc\n",
    "# sc.settings.set_figure_params(figsize=(4,4), dpi=100)\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep1_original', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep1_aligned', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep2_original', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep2_aligned', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=12).as_hex()\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep3_original', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep3_aligned', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have: scadata.obsm['advanced_diffusion_coords_avg'], etc.\n",
    "\n",
    "print(\"\\n=== Advanced Analysis and Visualization ===\")\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "\n",
    "# 1. Visualize advanced results with uncertainty analysis\n",
    "print(\"Creating advanced visualization plots...\")\n",
    "fig, model_uncertainty, confidence_scores = visualize_advanced_results_multi_model(scadata)\n",
    "\n",
    "# 2. Analyze cell interactions for averaged coordinates\n",
    "print(\"Analyzing cell interactions (averaged coordinates)...\")\n",
    "min_distances_avg, interaction_matrix_avg = analyze_cell_interactions_advanced(\n",
    "    scadata, coords_key='advanced_diffusion_coords_avg'\n",
    ")\n",
    "\n",
    "# 3. Optional: Analyze interactions for individual models too\n",
    "print(\"Analyzing cell interactions (individual models)...\")\n",
    "for i in range(1, 4):\n",
    "    print(f\"\\nModel {i} interactions:\")\n",
    "    min_distances, interaction_matrix = analyze_cell_interactions_advanced(\n",
    "        scadata, coords_key=f'advanced_diffusion_coords_rep{i}'\n",
    "    )\n",
    "\n",
    "# 4. Print summary statistics\n",
    "print(\"\\n=== Advanced Model Statistics ===\")\n",
    "print(f\"Total cells mapped: {len(scadata.obsm['advanced_diffusion_coords_avg'])}\")\n",
    "print(f\"Average model uncertainty: {model_uncertainty.mean():.4f}\")\n",
    "print(f\"Model uncertainty range: [{model_uncertainty.min():.4f}, {model_uncertainty.max():.4f}]\")\n",
    "print(f\"Average confidence: {confidence_scores.mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Cell Type Confidence ===\")\n",
    "for ct in scadata.obs['rough_celltype'].unique():\n",
    "    mask = scadata.obs['rough_celltype'] == ct\n",
    "    print(f\"{ct}: {mask.sum()} cells, \"\n",
    "          f\"avg confidence: {confidence_scores[mask].mean():.3f}, \"\n",
    "          f\"avg uncertainty: {model_uncertainty[mask].mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Physics Constraints (Averaged Coordinates) ===\")\n",
    "all_distances = []\n",
    "for key, dist in min_distances_avg.items():\n",
    "    if not np.isnan(dist):\n",
    "        all_distances.append(dist)\n",
    "        print(f\"Min distance {key[0]} - {key[1]}: {dist:.4f}\")\n",
    "\n",
    "if all_distances:\n",
    "    print(f\"\\nOverall minimum cell-cell distance: {np.min(all_distances):.4f}\")\n",
    "    print(f\"Cells with potential overlaps (< 0.01): {np.sum(np.array(all_distances) < 0.01)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "mpl.rcParams['figure.figsize'] = (6, 6)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=100, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5)) \n",
    "    sc.pl.embedding(scadata, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=100, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "print(f\"Generating predictions from test model {i+1}...\")\n",
    "all_predictions = []\n",
    "for i, trained_model in enumerate(trained_models):\n",
    "    print(f\"Creating test model based on trained model {i+1}...\")\n",
    "    \n",
    "    # Create a minimal model instance for testing (no training)\n",
    "    test_model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=X_st1_ref.cpu().numpy(),  # Use ST1 as reference\n",
    "        st_coords=Y_st1_ref.cpu().numpy(),     # Use ST1 coords as reference\n",
    "        sc_gene_expr=X_st3_test.cpu().numpy(), # ST3 data as \"SC\" data\n",
    "        cell_types_sc=dummy_cell_types,                    # No cell types for ST3\n",
    "        transport_plan=None,               # Use transport plan from training\n",
    "        D_st=None,                      # Use distance matrices from training\n",
    "        D_induced=None,\n",
    "        n_genes=len(common_genes_test),\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=f'./temp_test_model_{i}'\n",
    "    )\n",
    "    \n",
    "    # Copy trained parameters (this is a hack, but should work)\n",
    "    state_dict = trained_model.state_dict()\n",
    "    # if 'ot_guidance_strength' in state_dict:\n",
    "    #     del state_dict['ot_guidance_strength']\n",
    "    # test_model.load_state_dict(state_dict)\n",
    "    test_model.load_state_dict(trained_model.state_dict(), strict=False)\n",
    "    \n",
    "    # Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "    print(f\"Generating predictions from test model {i+1}...\")\n",
    "    predicted_coords = test_model.sample_sc_coordinates_batched(\n",
    "            batch_size=512  # Even smaller batches\n",
    "    )\n",
    "    all_predictions.append(predicted_coords)\n",
    "\n",
    "# Continue with the rest of your evaluation code...\n",
    "\n",
    "# Average predictions from both models\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# MSE and MAE\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "\n",
    "# Correlation for each dimension\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Correlation X-dimension: {corr_x:.4f} (p={p_x:.6f})\")\n",
    "print(f\"Correlation Y-dimension: {corr_y:.4f} (p={p_y:.6f})\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Predicted coordinates\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Predicted ST3 Coordinates\\n(Averaged from 2 Models)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional distance-based evaluation\n",
    "euclidean_distances = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "median_distance = np.median(euclidean_distances)\n",
    "mean_distance = np.mean(euclidean_distances)\n",
    "\n",
    "print(f\"\\nDistance-based metrics:\")\n",
    "print(f\"Mean Euclidean distance: {mean_distance:.6f}\")\n",
    "print(f\"Median Euclidean distance: {median_distance:.6f}\")\n",
    "print(f\"Max Euclidean distance: {np.max(euclidean_distances):.6f}\")\n",
    "print(f\"Min Euclidean distance: {np.min(euclidean_distances):.6f}\")\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model separately AND the average\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Evaluate individual models\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    mse_i = mean_squared_error(st3_coords_gt_norm, pred)\n",
    "    mae_i = mean_absolute_error(st3_coords_gt_norm, pred)\n",
    "    corr_x_i, p_x_i = pearsonr(st3_coords_gt_norm[:, 0], pred[:, 0])\n",
    "    corr_y_i, p_y_i = pearsonr(st3_coords_gt_norm[:, 1], pred[:, 1])\n",
    "    \n",
    "    print(f\"\\n=== MODEL {i+1} RESULTS ===\")\n",
    "    print(f\"MSE: {mse_i:.6f}, MAE: {mae_i:.6f}\")\n",
    "    print(f\"Corr X: {corr_x_i:.4f}, Corr Y: {corr_y_i:.4f}\")\n",
    "\n",
    "# Evaluate averaged results\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(f\"\\n=== AVERAGED RESULTS ===\")\n",
    "print(f\"MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"Corr X: {corr_x:.4f}, Corr Y: {corr_y:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization - individual models + average + ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Model 1 predictions\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(all_predictions[0][:, 0], all_predictions[0][:, 1], \n",
    "           c=range(len(all_predictions[0])), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Model 1 Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Model 2 predictions\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(all_predictions[1][:, 0], all_predictions[1][:, 1], \n",
    "           c=range(len(all_predictions[1])), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Model 2 Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 4: Averaged predictions\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Averaged Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation plots for each model\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    corr_x_i, _ = pearsonr(st3_coords_gt_norm[:, 0], pred[:, 0])\n",
    "    corr_y_i, _ = pearsonr(st3_coords_gt_norm[:, 1], pred[:, 1])\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(st3_coords_gt_norm[:, 0], pred[:, 0], \n",
    "               alpha=0.5, label=f'X-coord (r={corr_x_i:.3f})', s=15)\n",
    "    plt.scatter(st3_coords_gt_norm[:, 1], pred[:, 1], \n",
    "               alpha=0.5, label=f'Y-coord (r={corr_y_i:.3f})', s=15)\n",
    "    plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "             [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "             'r--', alpha=0.8, label='Perfect correlation')\n",
    "    plt.xlabel('Ground Truth Coordinates')\n",
    "    plt.ylabel('Predicted Coordinates')\n",
    "    plt.title(f'Model {i+1} vs Ground Truth')\n",
    "    plt.legend()\n",
    "\n",
    "# Averaged correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Averaged Predictions vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distance error plots for each model\n",
    "euclidean_distances_all = []\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    distances = np.sqrt(np.sum((st3_coords_gt_norm - pred)**2, axis=1))\n",
    "    euclidean_distances_all.append(distances)\n",
    "\n",
    "euclidean_distances_avg = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Distance histograms\n",
    "for i, distances in enumerate(euclidean_distances_all):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Euclidean Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Model {i+1} Error Distribution\\nMean: {np.mean(distances):.4f}')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(euclidean_distances_avg, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Averaged Model Error Distribution\\nMean: {np.mean(euclidean_distances_avg):.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print distance metrics for all models\n",
    "for i, distances in enumerate(euclidean_distances_all):\n",
    "    print(f\"\\nModel {i+1} distance metrics:\")\n",
    "    print(f\"Mean: {np.mean(distances):.6f}, Median: {np.median(distances):.6f}\")\n",
    "    print(f\"Max: {np.max(distances):.6f}, Min: {np.min(distances):.6f}\")\n",
    "\n",
    "print(f\"\\nAveraged model distance metrics:\")\n",
    "print(f\"Mean: {np.mean(euclidean_distances_avg):.6f}, Median: {np.median(euclidean_distances_avg):.6f}\")\n",
    "print(f\"Max: {np.max(euclidean_distances_avg):.6f}, Min: {np.min(euclidean_distances_avg):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(format='svg') \n",
    "mpl.rcParams['figure.figsize'] = (5, 5)\n",
    "\n",
    "\n",
    "sc.pl.spatial(scadata,color=\"rough_celltype\",spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep2',title='reconstructed', save='P2_final__rep2_v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(format='svg') \n",
    "mpl.rcParams['figure.figsize'] = (5, 5)\n",
    "\n",
    "\n",
    "sc.pl.spatial(scadata,color=\"rough_celltype\",spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep3',title='reconstructed', save='P2_final_rep3_v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n",
    "import seaborn as sns\n",
    "n_groups = scadata_p10.obs[\"rough_celltype\"].nunique()\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=n_groups).as_hex()\n",
    "\n",
    "# my_tab20 = sns.color_palette(\"tab10\", n_colors=20).as_hex()\n",
    "\n",
    "\n",
    "# ---------- user preferences ----------\n",
    "sc.settings.set_figure_params(format='svg')      # keep default SVG\n",
    "mpl.rcParams['figure.figsize'] = (5, 5)          # default figsize (will be overridden for this fig)\n",
    "os.makedirs(\"figures\", exist_ok=True)            # create folder on the go\n",
    "outpath = \"figures/P2_final_v4.svg\"                 # final save path\n",
    "dpi_save = 600                                   # desired DPI for export\n",
    "# --------------------------------------\n",
    "\n",
    "# (optional) temporarily silence the FutureWarning about squidpy alternative\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Plot with Scanpy but DON'T save yet (show=False), so we can customize the legend\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"rough_celltype\",\n",
    "    spot_size=0.06,\n",
    "    show=False,                         # prevent automatic display/save so we can customize\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    save=None,                           # make sure Scanpy doesn't auto-save; we handle saving below\n",
    "    palette = my_tab20\n",
    ")\n",
    "\n",
    "# grab current Axes & Figure\n",
    "ax = plt.gca()\n",
    "fig = ax.get_figure()\n",
    "\n",
    "# --- Optional: make the figure wider so the horizontal legend fits comfortably ---\n",
    "# You can change these numbers (width, height) to taste.\n",
    "fig.set_size_inches(10, 6)   # increase width (was 5x5); user said wider is fine\n",
    "\n",
    "# --- Build a clean horizontal legend BELOW the plot ---\n",
    "# Remove any existing legend first (Scanpy sometimes creates one)\n",
    "old_leg = ax.get_legend()\n",
    "if old_leg:\n",
    "    old_leg.remove()\n",
    "\n",
    "# obtain handles & labels from the scatter artist(s)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# If Scanpy produced extra \"title-like\" legend entries, you might get duplicate/empty labels;\n",
    "# if so, filter them out (uncomment if needed)\n",
    "# pairs = [(h, l) for h, l in zip(handles, labels) if l not in (None, '')]\n",
    "# if pairs:\n",
    "#     handles, labels = zip(*pairs)\n",
    "# else:\n",
    "#     handles, labels = [], []\n",
    "\n",
    "# place legend as a horizontal strip below the main axes\n",
    "# - loc='upper center' with bbox_to_anchor centers the legend beneath the axes\n",
    "# - bbox_to_anchor: (0.5, -0.15) -> x=0.5 center, y negative moves it below the axes; tweak y to move further down\n",
    "# - ncol: set how many columns you want; set to len(labels) for a single-row legend (will be wide)\n",
    "# - frameon: False removes the border if you want a clean strip\n",
    "ncol = len(labels)                 # puts all entries in a single row; change to e.g. 6 for multiple rows\n",
    "legend = ax.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.13),\n",
    "    ncol=ncol,\n",
    "    frameon=False,\n",
    "    handlelength=2.5,\n",
    "    columnspacing=1.0,\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# after you create `legend`:\n",
    "for h in legend.legendHandles:\n",
    "    if isinstance(h, matplotlib.collections.PathCollection):\n",
    "        # size is in points^2 (try values like 50, 100, 200, 400)\n",
    "        h.set_sizes([200.0])     # <- increase this number to make markers bigger\n",
    "    else:\n",
    "        try:\n",
    "            h.set_markersize(10)  # for Line2D handles (if any)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "# Reserve space at the bottom so the legend isn't clipped\n",
    "# Increase the bottom margin proportionally to how far below you placed the legend\n",
    "fig.subplots_adjust(bottom=0.22)   # increase if you move legend further down (or more rows)\n",
    "\n",
    "# If you prefer the legend to be drawn across the whole figure, you can use fig.legend(...) instead.\n",
    "# Example: fig.legend(handles, labels, loc='lower center', ncol=ncol, bbox_to_anchor=(0.5, 0.02))\n",
    "# But ax.legend above keeps alignment with the axes more predictably.\n",
    "\n",
    "# --- Save the figure to the folder as SVG at 600 DPI ---\n",
    "# fig.savefig(outpath, format=\"svg\", dpi=dpi_save, bbox_inches=\"tight\")\n",
    "\n",
    "# show the final adjusted figure in the notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (5, 5)\n",
    "\n",
    "# Generate the plot but do NOT save it yet\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"PDC\"],\n",
    "    spot_size=0.055,\n",
    "    show=False,  # Prevents figure auto-closing\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"]   # maroon / blood-red\n",
    ")\n",
    "\n",
    "# Save manually with high resolution\n",
    "# plt.savefig(\"P2_pdc_rep2_v3.png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams['pdf.fonttype'] = 42\n",
    "# rcParams['ps.fonttype'] = 42\n",
    "# figsize(4,4)\n",
    "mpl.rcParams['figure.figsize'] = (5, 5)\n",
    "# sc.pl.spatial(scadata,color=\"level3_celltype\",groups=[\"TSK\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep3',title='reconstructed',na_in_legend=False, save='P2_rep3_tsk_v2')\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"level3_celltype\",\n",
    "    groups=[\"TSK\"],\n",
    "    spot_size=0.06,\n",
    "    show=False,  # <- Important: prevent auto-show\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False\n",
    ")\n",
    "\n",
    "# Save manually with high resolution\n",
    "plt.savefig(\"P2_rep2_tsk_v3.svg\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "#save='TSK',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize(4,4)\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"Tumor_KC_Cyc\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep2',title='reconstructed',na_in_legend=False,save='P2cyc')\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"Tumor_KC_Basal\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep2',title='reconstructed',na_in_legend=False,save='P2bas')\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"Tumor_KC_Diff\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep2',title='reconstructed',na_in_legend=False,save='P2diff')\n",
    "#save='nonTSK',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import scanpy as sc \n",
    "\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "sc.settings.figdir = \"figures\"   # optional, used by scanpy save\n",
    "\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"Tumor_KC_Cyc\"],\n",
    "    spot_size=0.06,\n",
    "    show=True,\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"],   # maroon / blood-red\n",
    "    save='P2_Tumor_KC_Cyc_v1.svg'\n",
    ")\n",
    "\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"Tumor_KC_Basal\"],\n",
    "    spot_size=0.06,\n",
    "    show=True,\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"],  # maroon / blood-red\n",
    "    save=\"P2_Tumor_KC_Basal_v1.svg\"\n",
    ")\n",
    "\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"Tumor_KC_Diff\"],\n",
    "    spot_size=0.06,\n",
    "    show=True,\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"],   # maroon / blood-red\n",
    "    save = \"P2_Tumor_KC_Diff_v1.svg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "sq.gr.spatial_neighbors(scadata,spatial_key='advanced_diffusion_coords_rep3')\n",
    "sq.gr.nhood_enrichment(scadata,cluster_key='rough_celltype')\n",
    "sq.gr.interaction_matrix(scadata,cluster_key='rough_celltype')\n",
    "kscadata = scadata[ scadata.obs.level2_celltype.isin(['Tumor_KC_Cyc','Tumor_KC_Basal','Tumor_KC_Diff','TSK'])].copy()\n",
    "sq.gr.spatial_neighbors(kscadata,spatial_key='advanced_diffusion_coords_rep3')\n",
    "sq.gr.nhood_enrichment(kscadata,cluster_key='level2_celltype')\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',save='TSKKC_new_good.png',figsize=(3,5))\n",
    "sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',figsize=(3,5), save='TSKKC_P2_rep3_final.svg', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import QuadMesh\n",
    "\n",
    "sq.gr.spatial_neighbors(kscadata,spatial_key='advanced_diffusion_coords_rep3')\n",
    "\n",
    "\n",
    "# draw squidpy plot\n",
    "sq.pl.nhood_enrichment(\n",
    "    kscadata,\n",
    "    cluster_key=\"level2_celltype\",\n",
    "    cmap=\"coolwarm\",\n",
    "    figsize=(4, 3)\n",
    ")\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "# find heatmap mappable and its axes (AxesImage or QuadMesh)\n",
    "mappable = None\n",
    "heat_ax = None\n",
    "\n",
    "\n",
    "for ax in fig.axes:\n",
    "    if ax.images:\n",
    "        mappable = ax.images[0]\n",
    "        heat_ax = ax\n",
    "        break\n",
    "    for coll in ax.collections:\n",
    "        if isinstance(coll, QuadMesh):\n",
    "            mappable = coll\n",
    "            heat_ax = ax\n",
    "            break\n",
    "    if mappable is not None:\n",
    "        break\n",
    "\n",
    "if mappable is None:\n",
    "    raise RuntimeError(\"Couldn't find heatmap mappable in the figure.\")\n",
    "\n",
    "# remove any old colorbar axes attached to mappable (and any existing colorbar artists)\n",
    "old_cb = getattr(mappable, \"colorbar\", None)\n",
    "if old_cb is not None:\n",
    "    try: old_cb.remove()\n",
    "    except Exception: pass\n",
    "\n",
    "# also remove any Axes that look like a colorbar (optional safe cleanup)\n",
    "# (we check for very narrow axes to avoid removing legitimate axes)\n",
    "to_delete = []\n",
    "for ax in fig.axes:\n",
    "    pos = ax.get_position()\n",
    "    if pos.width < 0.02 and ax is not heat_ax:   # heuristic for colorbar axes\n",
    "        to_delete.append(ax)\n",
    "for ax in to_delete:\n",
    "    try: fig.delaxes(ax)\n",
    "    except Exception: pass\n",
    "\n",
    "# --- create a new colorbar axis in FIGURE COORDS (doesn't resize heatmap) ---\n",
    "# get heatmap bbox in figure coordinates\n",
    "hb = heat_ax.get_position()\n",
    "\n",
    "# pad_fig shifts the colorbar to the right of the heatmap (fraction of figure width)\n",
    "# width_fig controls the colorbar width (fraction of figure width)\n",
    "# height will be same as heatmap height (hb.height)\n",
    "pad_fig = 0.02   # try 0.02, 0.03, 0.04 ... -> moves bar further right\n",
    "width_fig = 0.035  # try 0.03..0.08 -> makes bar fatter\n",
    "cax_rect = [hb.x1 + pad_fig, hb.y0, width_fig, hb.height]\n",
    "\n",
    "# create the axis (if it would overflow the figure, reduce pad or width)\n",
    "cax = fig.add_axes(cax_rect)\n",
    "\n",
    "# draw colorbar into that fixed axis\n",
    "cb = fig.colorbar(mappable, cax=cax)\n",
    "\n",
    "# label only top and bottom with \"High\"/\"Low\" and title \"Score\"\n",
    "if getattr(mappable, \"norm\", None) is not None:\n",
    "    vmin, vmax = mappable.norm.vmin, mappable.norm.vmax\n",
    "else:\n",
    "    try:\n",
    "        arr = mappable.get_array()\n",
    "        vmin, vmax = float(arr.min()), float(arr.max())\n",
    "    except Exception:\n",
    "        vmin, vmax = 0.0, 1.0\n",
    "\n",
    "cb.set_ticks([vmin, vmax])\n",
    "cb.set_ticklabels([\"Low\", \"High\"])\n",
    "cb.ax.tick_params(length=0)\n",
    "cb.ax.set_title(\"Score\", pad=6)\n",
    "\n",
    "# if your color scale looks inverted (High at bottom), invert axis:\n",
    "# cb.ax.invert_yaxis()\n",
    "\n",
    "# avoid tight_layout conflicts; adjust if you need margin room\n",
    "fig.subplots_adjust(right=min(0.98, hb.x1 + pad_fig + width_fig + 0.02))\n",
    "\n",
    "import os; \n",
    "os.makedirs(\"figures\", exist_ok=True); \n",
    "fig.savefig(\"figures/nbhd_enrich_p2.savg\", format=\"svg\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patient 10 stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load all 3 ST datasets\n",
    "stadata1_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep1.h5ad')\n",
    "stadata2_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep2.h5ad')\n",
    "stadata3_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep3.h5ad')\n",
    "\n",
    "datasets = [stadata1_p10, stadata2_p10, stadata3_p10]\n",
    "names = ['ST_P10_Rep1', 'ST_P10_Rep2', 'ST_P10_Rep3']\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset Basic Info:\")\n",
    "for i, (data, name) in enumerate(zip(datasets, names)):\n",
    "    print(f\"{name}: {data.shape[0]} spots, {data.shape[1]} genes\")\n",
    "    print(f\"  Spatial coords range: X[{data.obsm['spatial'][:,0].min():.2f}, {data.obsm['spatial'][:,0].max():.2f}], Y[{data.obsm['spatial'][:,1].min():.2f}, {data.obsm['spatial'][:,1].max():.2f}]\")\n",
    "\n",
    "# Plot spatial coordinates\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Individual plots\n",
    "for i, (data, name) in enumerate(zip(datasets, names)):\n",
    "    coords = data.obsm['spatial']\n",
    "    row = 0 if i < 2 else 1\n",
    "    col = i if i < 2 else 0\n",
    "    axes[row, col].scatter(coords[:, 0], coords[:, 1], alpha=0.6, s=20)\n",
    "    axes[row, col].set_title(f'{name}\\n{data.shape[0]} spots')\n",
    "    axes[row, col].set_xlabel('X coordinate')\n",
    "    axes[row, col].set_ylabel('Y coordinate')\n",
    "\n",
    "# Overlay plot\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (data, name, color) in enumerate(zip(datasets, names, colors)):\n",
    "    coords = data.obsm['spatial']\n",
    "    axes[1, 1].scatter(coords[:, 0], coords[:, 1], alpha=0.5, s=15, c=color, label=name)\n",
    "axes[1, 1].set_title('All Datasets Overlay')\n",
    "axes[1, 1].set_xlabel('X coordinate')\n",
    "axes[1, 1].set_ylabel('Y coordinate')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find common genes\n",
    "all_genes = [set(data.var_names) for data in datasets]\n",
    "common_genes = sorted(list(all_genes[0] & all_genes[1] & all_genes[2]))\n",
    "print(f\"\\nCommon genes across all datasets: {len(common_genes)}\")\n",
    "\n",
    "# Coordinate overlap analysis\n",
    "print(\"\\nCoordinate Overlap Analysis:\")\n",
    "tolerance = 1.0  # Distance tolerance for \"overlap\"\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        coords_i = datasets[i].obsm['spatial']\n",
    "        coords_j = datasets[j].obsm['spatial']\n",
    "        \n",
    "        # Calculate pairwise distances\n",
    "        distances = cdist(coords_i, coords_j)\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        \n",
    "        # Count overlaps within tolerance\n",
    "        overlaps = np.sum(min_distances < tolerance)\n",
    "        \n",
    "        print(f\"{names[i]} vs {names[j]}:\")\n",
    "        print(f\"  Spots within {tolerance} units: {overlaps}/{len(coords_i)} ({overlaps/len(coords_i)*100:.1f}%)\")\n",
    "        print(f\"  Mean min distance: {np.mean(min_distances):.2f}\")\n",
    "\n",
    "# Gene expression similarity for closest spots\n",
    "print(\"\\nGene Expression Similarity (for closest coordinate pairs):\")\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        # Get common genes data\n",
    "        expr_i = datasets[i][:, common_genes].X\n",
    "        expr_j = datasets[j][:, common_genes].X\n",
    "        \n",
    "        if hasattr(expr_i, 'toarray'):\n",
    "            expr_i = expr_i.toarray()\n",
    "        if hasattr(expr_j, 'toarray'):\n",
    "            expr_j = expr_j.toarray()\n",
    "        \n",
    "        coords_i = datasets[i].obsm['spatial']\n",
    "        coords_j = datasets[j].obsm['spatial']\n",
    "        \n",
    "        # Find closest pairs\n",
    "        distances = cdist(coords_i, coords_j)\n",
    "        closest_j_indices = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Calculate correlations for closest pairs\n",
    "        correlations = []\n",
    "        for spot_i in range(len(expr_i)):\n",
    "            closest_j = closest_j_indices[spot_i]\n",
    "            corr = np.corrcoef(expr_i[spot_i], expr_j[closest_j])[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        print(f\"{names[i]} vs {names[j]}:\")\n",
    "        print(f\"  Mean gene expression correlation: {np.mean(correlations):.4f}\")\n",
    "        print(f\"  Median correlation: {np.median(correlations):.4f}\")\n",
    "        print(f\"  Correlations > 0.5: {np.sum(np.array(correlations) > 0.5)}/{len(correlations)} ({np.sum(np.array(correlations) > 0.5)/len(correlations)*100:.1f}%)\")\n",
    "\n",
    "# Distance distribution plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "pair_idx = 0\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        coords_i = datasets[i].obsm['spatial']\n",
    "        coords_j = datasets[j].obsm['spatial']\n",
    "        \n",
    "        distances = cdist(coords_i, coords_j)\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        \n",
    "        axes[pair_idx].hist(min_distances, bins=50, alpha=0.7)\n",
    "        axes[pair_idx].set_title(f'{names[i]} vs {names[j]}\\nMin Distance Distribution')\n",
    "        axes[pair_idx].set_xlabel('Distance to closest spot')\n",
    "        axes[pair_idx].set_ylabel('Frequency')\n",
    "        axes[pair_idx].axvline(tolerance, color='red', linestyle='--', label=f'Tolerance={tolerance}')\n",
    "        axes[pair_idx].legend()\n",
    "        \n",
    "        pair_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "# Get common genes and prepare data\n",
    "all_genes = [set(data.var_names) for data in datasets]\n",
    "common_genes = sorted(list(all_genes[0] & all_genes[1] & all_genes[2]))\n",
    "\n",
    "print(f\"Running comprehensive analysis on {len(common_genes)} common genes...\")\n",
    "\n",
    "# Prepare expression matrices\n",
    "expr_matrices = []\n",
    "coord_matrices = []\n",
    "for data in datasets:\n",
    "    expr = data[:, common_genes].X\n",
    "    if hasattr(expr, 'toarray'):\n",
    "        expr = expr.toarray()\n",
    "    expr_matrices.append(expr)\n",
    "    coord_matrices.append(data.obsm['spatial'])\n",
    "\n",
    "# 1. FIXED SPATIAL GENE EXPRESSION GRADIENTS\n",
    "print(\"\\n1. Calculating spatial gradients...\")\n",
    "\n",
    "def calculate_spatial_gradients_fixed(expr, coords, top_n_genes=20):\n",
    "    \"\"\"Calculate spatial gradients using local neighborhood differences\"\"\"\n",
    "    tree = cKDTree(coords)\n",
    "    gradients = {}\n",
    "    \n",
    "    for gene_idx in range(min(top_n_genes, expr.shape[1])):\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        \n",
    "        grad_magnitudes = []\n",
    "        \n",
    "        for i in range(len(coords)):\n",
    "            # Find 5 nearest neighbors\n",
    "            distances, indices = tree.query(coords[i], k=6)  # Include self\n",
    "            neighbors = indices[1:]  # Exclude self\n",
    "            \n",
    "            if len(neighbors) > 0:\n",
    "                # Calculate gradient as max difference with neighbors\n",
    "                expr_diffs = []\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor < len(gene_expr):\n",
    "                        coord_diff = coords[neighbor] - coords[i]\n",
    "                        expr_diff = gene_expr[neighbor] - gene_expr[i]\n",
    "                        if np.linalg.norm(coord_diff) > 0:\n",
    "                            # Directional derivative\n",
    "                            grad_component = expr_diff / np.linalg.norm(coord_diff)\n",
    "                            expr_diffs.append(abs(grad_component))\n",
    "                \n",
    "                if expr_diffs:\n",
    "                    grad_magnitudes.append(max(expr_diffs))\n",
    "                else:\n",
    "                    grad_magnitudes.append(0)\n",
    "            else:\n",
    "                grad_magnitudes.append(0)\n",
    "        \n",
    "        grad_magnitudes = np.array(grad_magnitudes)\n",
    "        gradients[common_genes[gene_idx]] = {\n",
    "            'magnitude': grad_magnitudes,\n",
    "            'mean_magnitude': np.mean(grad_magnitudes)\n",
    "        }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Calculate gradients for each dataset\n",
    "gradient_results = []\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    print(f\"  Processing {name}...\")\n",
    "    gradients = calculate_spatial_gradients_fixed(expr, coords, top_n_genes=20)\n",
    "    gradient_results.append(gradients)\n",
    "\n",
    "# Plot top gradient genes\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "for i, (gradients, name) in enumerate(zip(gradient_results, names)):\n",
    "    # Get top 3 genes with highest gradient magnitude\n",
    "    top_genes = sorted(gradients.items(), key=lambda x: x[1]['mean_magnitude'], reverse=True)[:3]\n",
    "    \n",
    "    for j, (gene, grad_data) in enumerate(top_genes):\n",
    "        coords = coord_matrices[i]\n",
    "        scatter = axes[i, j].scatter(coords[:, 0], coords[:, 1], c=grad_data['magnitude'], \n",
    "                          cmap='viridis', s=20, alpha=0.7)\n",
    "        axes[i, j].set_title(f'{name}\\n{gene} (grad: {grad_data[\"mean_magnitude\"]:.3f})')\n",
    "        axes[i, j].set_xlabel('X')\n",
    "        axes[i, j].set_ylabel('Y')\n",
    "        plt.colorbar(scatter, ax=axes[i, j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PCA/UMAP ANALYSIS\n",
    "print(\"\\n2. Performing PCA and UMAP analysis...\")\n",
    "\n",
    "# Combine all datasets for joint analysis\n",
    "all_expr = np.vstack(expr_matrices)\n",
    "all_coords = np.vstack(coord_matrices)\n",
    "dataset_labels = np.concatenate([np.full(len(expr), i) for i, expr in enumerate(expr_matrices)])\n",
    "\n",
    "# Standardize expression data\n",
    "scaler = StandardScaler()\n",
    "all_expr_scaled = scaler.fit_transform(all_expr)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca_result = pca.fit_transform(all_expr_scaled)\n",
    "\n",
    "# UMAP\n",
    "umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_result = umap_reducer.fit_transform(all_expr_scaled)\n",
    "\n",
    "# Plot PCA and UMAP\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# PCA by dataset\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (name, color) in enumerate(zip(names, colors)):\n",
    "    mask = dataset_labels == i\n",
    "    axes[0, 0].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                      c=color, label=name, alpha=0.6, s=15)\n",
    "axes[0, 0].set_title('PCA by Dataset')\n",
    "axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# UMAP by dataset\n",
    "for i, (name, color) in enumerate(zip(names, colors)):\n",
    "    mask = dataset_labels == i\n",
    "    axes[0, 1].scatter(umap_result[mask, 0], umap_result[mask, 1], \n",
    "                      c=color, label=name, alpha=0.6, s=15)\n",
    "axes[0, 1].set_title('UMAP by Dataset')\n",
    "axes[0, 1].set_xlabel('UMAP1')\n",
    "axes[0, 1].set_ylabel('UMAP2')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# PCA variance explained\n",
    "axes[1, 0].plot(range(1, 21), pca.explained_variance_ratio_[:20], 'bo-')\n",
    "axes[1, 0].set_title('PCA Variance Explained')\n",
    "axes[1, 0].set_xlabel('Principal Component')\n",
    "axes[1, 0].set_ylabel('Variance Explained')\n",
    "\n",
    "# Spatial coordinates colored by first PC\n",
    "scatter = axes[1, 1].scatter(all_coords[:, 0], all_coords[:, 1], c=pca_result[:, 0], \n",
    "                  cmap='viridis', s=15, alpha=0.7)\n",
    "axes[1, 1].set_title('Spatial Distribution (colored by PC1)')\n",
    "axes[1, 1].set_xlabel('X coordinate')\n",
    "axes[1, 1].set_ylabel('Y coordinate')\n",
    "plt.colorbar(scatter, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. HIGHLY VARIABLE GENES\n",
    "print(\"\\n3. Finding highly variable genes...\")\n",
    "\n",
    "def find_highly_variable_genes(expr, gene_names, top_n=20):\n",
    "    \"\"\"Find genes with highest coefficient of variation\"\"\"\n",
    "    cv_scores = []\n",
    "    for i in range(expr.shape[1]):\n",
    "        mean_expr = np.mean(expr[:, i])\n",
    "        std_expr = np.std(expr[:, i])\n",
    "        cv = std_expr / (mean_expr + 1e-8)\n",
    "        cv_scores.append(cv)\n",
    "    \n",
    "    top_indices = np.argsort(cv_scores)[-top_n:][::-1]\n",
    "    return [(gene_names[i], cv_scores[i]) for i in top_indices]\n",
    "\n",
    "hvg_results = []\n",
    "for i, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    hvgs = find_highly_variable_genes(expr, common_genes)\n",
    "    hvg_results.append(hvgs)\n",
    "    print(f\"\\nTop 10 HVGs in {name}:\")\n",
    "    for gene, cv in hvgs[:10]:\n",
    "        print(f\"  {gene}: CV = {cv:.3f}\")\n",
    "\n",
    "# 4. SPATIAL AUTOCORRELATION\n",
    "print(\"\\n4. Calculating spatial autocorrelation...\")\n",
    "\n",
    "def moran_i(expr, coords, k=8):\n",
    "    \"\"\"Calculate Moran's I for spatial autocorrelation\"\"\"\n",
    "    tree = cKDTree(coords)\n",
    "    n = len(expr)\n",
    "    \n",
    "    distances, indices = tree.query(coords, k=min(k+1, n))\n",
    "    \n",
    "    mean_expr = np.mean(expr)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    w_sum = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        neighbors = indices[i, 1:min(k+1, len(indices[i]))]\n",
    "        for j in neighbors:\n",
    "            if j < n:\n",
    "                numerator += (expr[i] - mean_expr) * (expr[j] - mean_expr)\n",
    "                w_sum += 1\n",
    "        denominator += (expr[i] - mean_expr) ** 2\n",
    "    \n",
    "    if w_sum > 0 and denominator > 0:\n",
    "        moran_i = (n / w_sum) * (numerator / denominator)\n",
    "    else:\n",
    "        moran_i = 0\n",
    "    \n",
    "    return moran_i\n",
    "\n",
    "autocorr_results = []\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    print(f\"  Processing {name}...\")\n",
    "    autocorr_genes = []\n",
    "    \n",
    "    # Test top 50 most variable genes\n",
    "    hvgs = find_highly_variable_genes(expr, common_genes, top_n=50)\n",
    "    \n",
    "    for gene, _ in hvgs:\n",
    "        gene_idx = common_genes.index(gene)\n",
    "        moran = moran_i(expr[:, gene_idx], coords)\n",
    "        autocorr_genes.append((gene, moran))\n",
    "    \n",
    "    autocorr_genes.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    autocorr_results.append(autocorr_genes)\n",
    "    \n",
    "    print(f\"Top 5 spatially autocorrelated genes in {name}:\")\n",
    "    for gene, moran in autocorr_genes[:5]:\n",
    "        print(f\"  {gene}: Moran's I = {moran:.3f}\")\n",
    "\n",
    "# 5. FIXED CROSS-DATASET GENE CORRELATION\n",
    "print(\"\\n5. Cross-dataset gene correlation analysis...\")\n",
    "\n",
    "def calculate_cross_dataset_correlation_fixed(expr1, expr2, coords1, coords2, gene_names):\n",
    "    \"\"\"Calculate gene-wise correlation using spatially matched spots\"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    # Find spatially closest spots between datasets\n",
    "    tree = cKDTree(coords2)\n",
    "    distances, indices = tree.query(coords1)\n",
    "    \n",
    "    # Only use matches within reasonable distance\n",
    "    good_matches = distances < 5.0  # Adjust threshold as needed\n",
    "    \n",
    "    if np.sum(good_matches) < 10:\n",
    "        print(f\"    Warning: Only {np.sum(good_matches)} good spatial matches found\")\n",
    "        return correlations\n",
    "    \n",
    "    matched_expr1 = expr1[good_matches]\n",
    "    matched_expr2 = expr2[indices[good_matches]]\n",
    "    \n",
    "    for i in range(len(gene_names)):\n",
    "        if len(matched_expr1) > 1:  # Need at least 2 points for correlation\n",
    "            corr, _ = pearsonr(matched_expr1[:, i], matched_expr2[:, i])\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append((gene_names[i], corr))\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Calculate pairwise correlations\n",
    "corr_results = {}\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        pair_name = f\"{names[i]}_vs_{names[j]}\"\n",
    "        correlations = calculate_cross_dataset_correlation_fixed(\n",
    "            expr_matrices[i], expr_matrices[j], \n",
    "            coord_matrices[i], coord_matrices[j], \n",
    "            common_genes\n",
    "        )\n",
    "        correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        corr_results[pair_name] = correlations\n",
    "        \n",
    "        print(f\"\\nTop correlated genes between {names[i]} and {names[j]}:\")\n",
    "        for gene, corr in correlations[:10]:\n",
    "            print(f\"  {gene}: r = {corr:.3f}\")\n",
    "\n",
    "# 6. EXPRESSION CLUSTERING\n",
    "print(\"\\n6. Performing expression clustering...\")\n",
    "\n",
    "n_clusters = 5\n",
    "clustering_results = []\n",
    "\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    expr_scaled = StandardScaler().fit_transform(expr)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(expr_scaled)\n",
    "    \n",
    "    clustering_results.append((cluster_labels, kmeans))\n",
    "    \n",
    "    print(f\"\\nCluster sizes in {name}:\")\n",
    "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        print(f\"  Cluster {cluster}: {count} spots\")\n",
    "\n",
    "# Plot clustering results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, ((cluster_labels, _), coords, name) in enumerate(zip(clustering_results, coord_matrices, names)):\n",
    "    scatter = axes[i].scatter(coords[:, 0], coords[:, 1], c=cluster_labels, \n",
    "                             cmap='tab10', s=20, alpha=0.7)\n",
    "    axes[i].set_title(f'{name}\\nExpression Clusters')\n",
    "    axes[i].set_xlabel('X coordinate')\n",
    "    axes[i].set_ylabel('Y coordinate')\n",
    "    plt.colorbar(scatter, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. DIFFERENTIAL EXPRESSION BY SPATIAL REGIONS\n",
    "print(\"\\n7. Differential expression by spatial regions...\")\n",
    "\n",
    "def spatial_differential_expression(expr, coords, gene_names, n_regions=4):\n",
    "    \"\"\"Find differentially expressed genes across spatial regions\"\"\"\n",
    "    x_median = np.median(coords[:, 0])\n",
    "    y_median = np.median(coords[:, 1])\n",
    "    \n",
    "    regions = []\n",
    "    regions.append((coords[:, 0] <= x_median) & (coords[:, 1] <= y_median))\n",
    "    regions.append((coords[:, 0] > x_median) & (coords[:, 1] <= y_median))\n",
    "    regions.append((coords[:, 0] <= x_median) & (coords[:, 1] > y_median))\n",
    "    regions.append((coords[:, 0] > x_median) & (coords[:, 1] > y_median))\n",
    "    \n",
    "    de_results = []\n",
    "    \n",
    "    for gene_idx in range(min(100, expr.shape[1])):\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        region_means = [np.mean(gene_expr[region]) for region in regions]\n",
    "        \n",
    "        cv_regions = np.std(region_means) / (np.mean(region_means) + 1e-8)\n",
    "        de_results.append((gene_names[gene_idx], cv_regions, region_means))\n",
    "    \n",
    "    de_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return de_results\n",
    "\n",
    "de_results = []\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    de_genes = spatial_differential_expression(expr, coords, common_genes)\n",
    "    de_results.append(de_genes)\n",
    "    \n",
    "    print(f\"\\nTop spatially DE genes in {name}:\")\n",
    "    for gene, cv, means in de_genes[:10]:\n",
    "        print(f\"  {gene}: CV = {cv:.3f}\")\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE ANALYSIS COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the expression patterns of these suspicious HVGs\n",
    "print(\"INVESTIGATING SUSPICIOUS HVG PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get the top HVG genes for each dataset\n",
    "top_hvg_genes = []\n",
    "for i, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    hvgs = find_highly_variable_genes(expr, common_genes, top_n=10)\n",
    "    top_hvg_genes.append([gene for gene, cv in hvgs])\n",
    "\n",
    "# Check expression patterns for the first few genes in each dataset\n",
    "for dataset_idx, (expr, name, top_genes) in enumerate(zip(expr_matrices, names, top_hvg_genes)):\n",
    "    print(f\"\\n{name} - Examining top 3 HVG genes:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for gene_idx in range(3):  # Check first 3 genes\n",
    "        gene_name = top_genes[gene_idx]\n",
    "        gene_position = common_genes.index(gene_name)\n",
    "        gene_expr = expr[:, gene_position]\n",
    "        \n",
    "        print(f\"\\nGene: {gene_name}\")\n",
    "        print(f\"Expression values (first 20 spots): {gene_expr[:20]}\")\n",
    "        print(f\"Unique values: {np.unique(gene_expr)}\")\n",
    "        print(f\"Number of unique values: {len(np.unique(gene_expr))}\")\n",
    "        print(f\"Min: {np.min(gene_expr):.4f}, Max: {np.max(gene_expr):.4f}\")\n",
    "        print(f\"Mean: {np.mean(gene_expr):.4f}, Std: {np.std(gene_expr):.4f}\")\n",
    "        print(f\"CV: {np.std(gene_expr) / (np.mean(gene_expr) + 1e-8):.4f}\")\n",
    "        \n",
    "        # Count how many zeros vs non-zeros\n",
    "        zeros = np.sum(gene_expr == 0)\n",
    "        non_zeros = np.sum(gene_expr != 0)\n",
    "        print(f\"Zeros: {zeros}, Non-zeros: {non_zeros}\")\n",
    "        \n",
    "        # Show value distribution\n",
    "        unique_vals, counts = np.unique(gene_expr, return_counts=True)\n",
    "        print(f\"Value distribution:\")\n",
    "        for val, count in zip(unique_vals[:10], counts[:10]):  # Show first 10 most common\n",
    "            print(f\"  {val:.4f}: {count} spots\")\n",
    "        if len(unique_vals) > 10:\n",
    "            print(f\"  ... and {len(unique_vals)-10} more unique values\")\n",
    "\n",
    "# Let's also check if this is a pattern across ALL genes, not just HVGs\n",
    "print(f\"\\n\\nCHECKING OVERALL GENE EXPRESSION PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for dataset_idx, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    print(f\"\\n{name} - Overall statistics:\")\n",
    "    \n",
    "    # Check how many genes have very few unique values\n",
    "    genes_with_few_values = 0\n",
    "    genes_with_binary = 0\n",
    "    genes_with_identical_cv = 0\n",
    "    \n",
    "    cv_values = []\n",
    "    \n",
    "    for gene_idx in range(min(100, expr.shape[1])):  # Check first 100 genes\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        unique_vals = np.unique(gene_expr)\n",
    "        cv = np.std(gene_expr) / (np.mean(gene_expr) + 1e-8)\n",
    "        cv_values.append(cv)\n",
    "        \n",
    "        if len(unique_vals) <= 3:\n",
    "            genes_with_few_values += 1\n",
    "        if len(unique_vals) == 2:\n",
    "            genes_with_binary += 1\n",
    "    \n",
    "    # Check for identical CV values\n",
    "    unique_cvs = np.unique(cv_values)\n",
    "    print(f\"  Genes with ≤3 unique values: {genes_with_few_values}/100\")\n",
    "    print(f\"  Genes with binary expression: {genes_with_binary}/100\") \n",
    "    print(f\"  Number of unique CV values: {len(unique_cvs)}\")\n",
    "    print(f\"  Most common CV values: {unique_cvs[:10]}\")\n",
    "    \n",
    "    # Show distribution of CV values\n",
    "    cv_values = np.array(cv_values)\n",
    "    print(f\"  CV range: {np.min(cv_values):.4f} to {np.max(cv_values):.4f}\")\n",
    "    print(f\"  CV mean: {np.mean(cv_values):.4f}\")\n",
    "\n",
    "# Create a histogram of CV values to visualize the pattern\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for dataset_idx, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    cv_values = []\n",
    "    for gene_idx in range(min(200, expr.shape[1])):  # Check first 200 genes\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        cv = np.std(gene_expr) / (np.mean(gene_expr) + 1e-8)\n",
    "        cv_values.append(cv)\n",
    "    \n",
    "    axes[dataset_idx].hist(cv_values, bins=50, alpha=0.7)\n",
    "    axes[dataset_idx].set_title(f'{name}\\nCV Distribution')\n",
    "    axes[dataset_idx].set_xlabel('Coefficient of Variation')\n",
    "    axes[dataset_idx].set_ylabel('Number of Genes')\n",
    "    axes[dataset_idx].set_yscale('log')  # Log scale to see patterns better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data_p10():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP10.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep1.h5ad')\n",
    "    stadata2_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep2.h5ad')\n",
    "    stadata3_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata_p10.obs['rough_celltype'] = scadata_p10.obs['level1_celltype'].astype(str)\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10\n",
    "\n",
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Key innovation: Use ALL ST data points for better training.\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "\n",
    "    # Store separate coordinate lists for block-diagonal graph\n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    st_expr_combined = scaler.fit_transform(st_expr_combined)\n",
    "\n",
    "    st_coords_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "\n",
    "    sc_expr = scaler.fit_transform(sc_expr)\n",
    "\n",
    "\n",
    "    \n",
    "    # Create dataset labels for tracking\n",
    "    dataset_labels = (['dataset1'] * len(st1_expr) + \n",
    "                     ['dataset2'] * len(st2_expr) + \n",
    "                     ['dataset3'] * len(st3_expr))\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list\n",
    "\n",
    "# Load and process data\n",
    "scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10 = load_and_process_cscc_data_p10()\n",
    "\n",
    "# Prepare combined data for diffusion\n",
    "X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = prepare_combined_st_for_diffusion(\n",
    "    stadata1_p10, stadata2_p10, stadata3_p10, scadata_p10\n",
    ")\n",
    "\n",
    "print(f\"Data preparation complete!\")\n",
    "print(f\"SC cells: {X_sc.shape[0]}\")\n",
    "print(f\"Combined ST spots: {X_st_combined.shape[0]}\")\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code block to your notebook BEFORE the training loop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "def analyze_sc_st_patterns(model, n_genes=20, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Comprehensive comparison of gene expression patterns between SC and ST data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get data\n",
    "    sc_expr = model.sc_gene_expr\n",
    "    st_expr = model.st_gene_expr\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if torch.is_tensor(sc_expr):\n",
    "        sc_expr = sc_expr.cpu().numpy()\n",
    "    if torch.is_tensor(st_expr):\n",
    "        st_expr = st_expr.cpu().numpy()\n",
    "    \n",
    "    # Find common highly variable genes for comparison\n",
    "    common_genes = find_common_variable_genes(sc_expr, st_expr, n_genes)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 1. Expression distribution comparison (violin plots)\n",
    "    plot_expression_distributions(sc_expr, st_expr, common_genes, fig, 1)\n",
    "    \n",
    "    # 2. Correlation heatmap between SC and ST\n",
    "    plot_sc_st_correlation(sc_expr, st_expr, common_genes, fig, 2)\n",
    "    \n",
    "    # 3. Joint embedding (t-SNE) of SC and ST\n",
    "    plot_joint_embedding(sc_expr, st_expr, fig, 3)\n",
    "    \n",
    "    # 4. Spatial expression patterns for ST\n",
    "    plot_spatial_expression_patterns(model, common_genes, fig, 4)\n",
    "    \n",
    "    # 5. Domain alignment quality\n",
    "    plot_domain_alignment(model, fig, 5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return common_genes\n",
    "\n",
    "def find_common_variable_genes(sc_expr, st_expr, n_genes=20):\n",
    "    \"\"\"Find genes that are variable in both SC and ST\"\"\"\n",
    "    \n",
    "    # Calculate CV for SC data\n",
    "    sc_mean = np.mean(sc_expr, axis=0) + 1e-8\n",
    "    sc_std = np.std(sc_expr, axis=0)\n",
    "    sc_cv = sc_std / sc_mean\n",
    "    \n",
    "    # Calculate CV for ST data  \n",
    "    st_mean = np.mean(st_expr, axis=0) + 1e-8\n",
    "    st_std = np.std(st_expr, axis=0)\n",
    "    st_cv = st_std / st_mean\n",
    "    \n",
    "    # Find genes variable in both (avoid extreme single-spot genes)\n",
    "    sc_nonzero_frac = (sc_expr > 0).mean(0)\n",
    "    st_nonzero_frac = (st_expr > 0).mean(0)\n",
    "    \n",
    "    # Filter for genes expressed in reasonable fraction of cells/spots\n",
    "    valid_mask = (sc_nonzero_frac >= 0.1) & (sc_nonzero_frac <= 0.9) & \\\n",
    "                 (st_nonzero_frac >= 0.1) & (st_nonzero_frac <= 0.9)\n",
    "    \n",
    "    # Rank by combined variability\n",
    "    combined_cv = sc_cv + st_cv\n",
    "    combined_cv[~valid_mask] = 0\n",
    "    \n",
    "    top_gene_indices = np.argsort(combined_cv)[-n_genes:]\n",
    "    \n",
    "    print(f\"Selected {len(top_gene_indices)} variable genes for analysis\")\n",
    "    \n",
    "    return top_gene_indices\n",
    "\n",
    "def plot_expression_distributions(sc_expr, st_expr, gene_indices, fig, subplot_num):\n",
    "    \"\"\"Plot violin plots comparing expression distributions\"\"\"\n",
    "    \n",
    "    n_genes = min(len(gene_indices), 10)  # Limit to 10 genes for clarity\n",
    "    n_cols = 5\n",
    "    n_rows = 2\n",
    "    \n",
    "    for i in range(n_genes):\n",
    "        gene_idx = gene_indices[i]\n",
    "        ax = fig.add_subplot(5, n_cols, i + 1)\n",
    "        \n",
    "        # Prepare data\n",
    "        sc_values = sc_expr[:, gene_idx]\n",
    "        st_values = st_expr[:, gene_idx]\n",
    "        \n",
    "        data_df = pd.DataFrame({\n",
    "            'Expression': np.concatenate([sc_values, st_values]),\n",
    "            'Data_Type': ['SC'] * len(sc_values) + ['ST'] * len(st_values)\n",
    "        })\n",
    "        \n",
    "        # Plot violin plot\n",
    "        sns.violinplot(data=data_df, x='Data_Type', y='Expression', ax=ax)\n",
    "        ax.set_title(f'Gene {gene_idx}', fontsize=8)\n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "        if i % n_cols != 0:\n",
    "            ax.set_ylabel('')\n",
    "\n",
    "def plot_sc_st_correlation(sc_expr, st_expr, gene_indices, fig, subplot_num):\n",
    "    \"\"\"Plot correlation between SC and ST expression levels\"\"\"\n",
    "    \n",
    "    ax = fig.add_subplot(5, 2, 3)\n",
    "    \n",
    "    # Calculate mean expression for each gene\n",
    "    sc_means = np.mean(sc_expr[:, gene_indices], axis=0)\n",
    "    st_means = np.mean(st_expr[:, gene_indices], axis=0)\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(sc_means, st_means, alpha=0.7)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr, p_val = stats.pearsonr(sc_means, st_means)\n",
    "    ax.set_title(f'SC vs ST Mean Expression\\nCorr = {corr:.3f}, p = {p_val:.3e}')\n",
    "    ax.set_xlabel('SC Mean Expression')\n",
    "    ax.set_ylabel('ST Mean Expression')\n",
    "    \n",
    "    # Add diagonal line\n",
    "    min_val, max_val = 0, max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "\n",
    "def plot_joint_embedding(sc_expr, st_expr, fig, subplot_num):\n",
    "    \"\"\"Create joint t-SNE embedding of SC and ST data\"\"\"\n",
    "    \n",
    "    ax = fig.add_subplot(5, 2, 4)\n",
    "    \n",
    "    # Sample data for faster computation\n",
    "    n_sample = min(2000, sc_expr.shape[0])\n",
    "    sc_sample_idx = np.random.choice(sc_expr.shape[0], n_sample, replace=False)\n",
    "    \n",
    "    n_st_sample = min(500, st_expr.shape[0])\n",
    "    st_sample_idx = np.random.choice(st_expr.shape[0], n_st_sample, replace=False)\n",
    "    \n",
    "    # Combine data\n",
    "    combined_data = np.vstack([\n",
    "        sc_expr[sc_sample_idx],\n",
    "        st_expr[st_sample_idx]\n",
    "    ])\n",
    "    \n",
    "    # t-SNE embedding\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embedding = tsne.fit_transform(combined_data)\n",
    "    \n",
    "    # Plot\n",
    "    colors = ['blue'] * n_sample + ['red'] * n_st_sample\n",
    "    labels = ['SC'] * n_sample + ['ST'] * n_st_sample\n",
    "    \n",
    "    for label, color in [('SC', 'blue'), ('ST', 'red')]:\n",
    "        mask = np.array(labels) == label\n",
    "        ax.scatter(embedding[mask, 0], embedding[mask, 1], \n",
    "                  c=color, alpha=0.6, s=20, label=label)\n",
    "    \n",
    "    ax.set_title('Joint t-SNE: SC vs ST')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "\n",
    "def plot_spatial_expression_patterns(model, gene_indices, fig, subplot_num):\n",
    "    \"\"\"Plot spatial expression patterns for ST data\"\"\"\n",
    "    \n",
    "    n_genes_show = min(4, len(gene_indices))\n",
    "    \n",
    "    for i in range(n_genes_show):\n",
    "        ax = fig.add_subplot(5, 4, 8 + i + 1)\n",
    "        gene_idx = gene_indices[-(i+1)]  # Take top variable genes\n",
    "        \n",
    "        # Get expression values\n",
    "        if torch.is_tensor(model.st_gene_expr):\n",
    "            expr_values = model.st_gene_expr[:, gene_idx].cpu().numpy()\n",
    "        else:\n",
    "            expr_values = model.st_gene_expr[:, gene_idx]\n",
    "            \n",
    "        if torch.is_tensor(model.st_coords):\n",
    "            coords = model.st_coords.cpu().numpy()\n",
    "        else:\n",
    "            coords = model.st_coords\n",
    "        \n",
    "        # Spatial scatter plot\n",
    "        scatter = ax.scatter(coords[:, 0], coords[:, 1], \n",
    "                           c=expr_values, cmap='viridis', \n",
    "                           s=30, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'ST Spatial: Gene {gene_idx}', fontsize=8)\n",
    "        ax.set_aspect('equal')\n",
    "        plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "\n",
    "def plot_domain_alignment(model, fig, subplot_num):\n",
    "    \"\"\"Plot domain alignment quality using encoder embeddings\"\"\"\n",
    "    \n",
    "    ax = fig.add_subplot(5, 1, 5)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get aligned embeddings\n",
    "        if torch.is_tensor(model.sc_gene_expr):\n",
    "            sc_data = model.sc_gene_expr\n",
    "        else:\n",
    "            sc_data = torch.tensor(model.sc_gene_expr, device=model.device)\n",
    "            \n",
    "        if torch.is_tensor(model.st_gene_expr):\n",
    "            st_data = model.st_gene_expr\n",
    "        else:\n",
    "            st_data = torch.tensor(model.st_gene_expr, device=model.device)\n",
    "        \n",
    "        sc_embedding = model.netE(sc_data).cpu().numpy()\n",
    "        st_embedding = model.netE(st_data).cpu().numpy()\n",
    "        \n",
    "        # Sample for visualization\n",
    "        n_sample = min(1000, sc_embedding.shape[0])\n",
    "        sc_idx = np.random.choice(sc_embedding.shape[0], n_sample, replace=False)\n",
    "        st_idx = np.random.choice(st_embedding.shape[0], min(300, st_embedding.shape[0]), replace=False)\n",
    "        \n",
    "        # PCA for visualization\n",
    "        combined_embedding = np.vstack([sc_embedding[sc_idx], st_embedding[st_idx]])\n",
    "        pca = PCA(n_components=2)\n",
    "        embedding_2d = pca.fit_transform(combined_embedding)\n",
    "        \n",
    "        # Plot\n",
    "        n_sc = len(sc_idx)\n",
    "        ax.scatter(embedding_2d[:n_sc, 0], embedding_2d[:n_sc, 1], \n",
    "                  c='blue', alpha=0.6, s=20, label='SC')\n",
    "        ax.scatter(embedding_2d[n_sc:, 0], embedding_2d[n_sc:, 1], \n",
    "                  c='red', alpha=0.6, s=20, label='ST')\n",
    "        \n",
    "        ax.set_title('Domain Alignment (Encoder Embeddings)')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "\n",
    "print(\"Analysis functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_advanced_diffusion_models(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset and average the results.\n",
    "    MODIFIED: Run stadata1 three times to test for SC cluster rotation/sliding\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # STEP 1: Build canonical angular frame from ST slide (ONCE)\n",
    "    # st_coords_raw = stadata1.obsm['spatial']  # Use raw ST coordinates\n",
    "    # angular_frame = _build_canonical_angular_frame(st_coords_raw)\n",
    "    \n",
    "    # List of ST datasets for iteration - Use stadata1 three times\n",
    "    st_datasets = [\n",
    "        (stadata1, \"run1\"),\n",
    "        (stadata2, \"run2\"), \n",
    "        (stadata3, \"run3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, run_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {run_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {run_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize model with different random seed for each run\n",
    "        torch.manual_seed(42 + i)\n",
    "        np.random.seed(42 + i)\n",
    "        \n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,\n",
    "            transport_plan=None,\n",
    "            D_st=None,\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=0.75,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=800,\n",
    "            n_denoising_blocks=6,\n",
    "            hidden_dim=256,\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{run_name}'\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Training model for {run_name}...\")\n",
    "        model.train(\n",
    "            encoder_epochs=1000,\n",
    "            vae_epochs=1200,\n",
    "            diffusion_epochs=3000,\n",
    "            lambda_struct=10.0\n",
    "        )\n",
    "\n",
    "        st_coords_raw = model.st_coords_norm.cpu().numpy()  # Use normalized coords from model\n",
    "        angular_frame = _build_canonical_angular_frame(st_coords_raw)\n",
    "        \n",
    "        # Generate SC coordinates\n",
    "        print(f\"Generating SC coordinates using {run_name} model...\")\n",
    "        # sc_coords = model.generate_sc_coordinates()\n",
    "        # sc_coords = model.sample_sc_coordinates_batched(\n",
    "        #     batch_size=512,  # Even smaller batches\n",
    "        #     refine_coords=False\n",
    "        # )\n",
    "\n",
    "        # Sample with geometry guidance\n",
    "        sc_coords = model.sample_sc_coordinates_with_geometry_guidance(\n",
    "            batch_size=512,\n",
    "            guidance_scale=1.0,\n",
    "            geometry_guidance=False,\n",
    "            k_nn=5,\n",
    "            lambda_trip=0.05,\n",
    "            lambda_rep=0.08,\n",
    "            gamma=0.05\n",
    "        )\n",
    "\n",
    "        # Evaluate geometry preservation\n",
    "        metrics = model.evaluate_geometry_preservation(sc_coords)\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        # STEP 2: Plot SC cells colored by angle (using ST-derived frame)\n",
    "        _plot_sc_angle_analysis(sc_coords, scadata.obs['rough_celltype'].values, \n",
    "                               angular_frame, st_coords_raw, run_name, i+1)\n",
    "    \n",
    "    # STEP 3: Comparative analysis across runs\n",
    "    _plot_comparative_sc_angle_analysis(sc_coords_results, scadata.obs['rough_celltype'].values,\n",
    "                                       angular_frame, st_coords_raw)\n",
    "    \n",
    "    # Compute averaged SC coordinates\n",
    "    sc_coords_results_np = [coords.cpu().numpy() for coords in sc_coords_results]  # Convert to numpy\n",
    "    sc_coords_avg = np.mean(sc_coords_results_np, axis=0)\n",
    "    sc_coords_std = np.std(sc_coords_results_np, axis=0)\n",
    "\n",
    "    # Store results in scadata\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    scadata.obsm['advanced_diffusion_coords_std'] = sc_coords_std\n",
    "\n",
    "    # Store individual results\n",
    "    for i, coords in enumerate(sc_coords_results_np):  # Use numpy arrays\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}'] = coords\n",
    "\n",
    "    print(f\"\\nTraining complete. Results stored in scadata.obsm\")\n",
    "    return scadata, models_all\n",
    "\n",
    "def _build_canonical_angular_frame(st_coords):\n",
    "    \"\"\"Build canonical angular frame from ST coordinates (dataset-specific, run-independent)\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute centroid\n",
    "    centroid = st_coords.mean(axis=0)\n",
    "    \n",
    "    # Find farthest spot from centroid (deterministic 0° direction)\n",
    "    distances = np.linalg.norm(st_coords - centroid, axis=1)\n",
    "    farthest_idx = np.argmax(distances)\n",
    "    a0 = st_coords[farthest_idx] - centroid  # 0° direction vector\n",
    "    \n",
    "    def angle_fn(x):\n",
    "        \"\"\"Compute angle from canonical frame\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        \n",
    "        v = x - centroid\n",
    "        cross = a0[0] * v[:, 1] - a0[1] * v[:, 0]  # z-component of 2D cross\n",
    "        dot = a0[0] * v[:, 0] + a0[1] * v[:, 1]\n",
    "        angles = np.arctan2(cross, dot)\n",
    "        angles = np.where(angles < 0, angles + 2*np.pi, angles)  # Map to [0, 2π)\n",
    "        return angles\n",
    "    \n",
    "    return {\n",
    "        'centroid': centroid,\n",
    "        'zero_direction': a0,\n",
    "        'farthest_idx': farthest_idx,\n",
    "        'angle_fn': angle_fn\n",
    "    }\n",
    "\n",
    "def _plot_sc_angle_analysis(sc_coords, cell_types, angular_frame, st_coords_bg, run_name, run_num):\n",
    "    \"\"\"Plot SC cells colored by angle from ST-derived frame\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute angles for SC cells using ST-derived frame\n",
    "    sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "    sc_angles_degrees = np.degrees(sc_angles)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: SC cells colored by angle (with ST outline in background)\n",
    "    ax1.scatter(st_coords_bg[:, 0], st_coords_bg[:, 1], \n",
    "               c='lightgray', s=10, alpha=0.3, label='ST outline')\n",
    "    \n",
    "    scatter = ax1.scatter(sc_coords[:, 0], sc_coords[:, 1], \n",
    "                         c=sc_angles_degrees, cmap='hsv', s=30, alpha=0.8)\n",
    "    \n",
    "    # Mark centroid and 0° direction\n",
    "    centroid = angular_frame['centroid']\n",
    "    zero_dir = angular_frame['zero_direction']\n",
    "    ax1.scatter(centroid[0], centroid[1], c='black', s=100, marker='x', linewidth=3)\n",
    "    ax1.arrow(centroid[0], centroid[1], zero_dir[0]*0.3, zero_dir[1]*0.3, \n",
    "              head_width=0.05, head_length=0.05, fc='red', ec='red', linewidth=2)\n",
    "    \n",
    "    ax1.set_title(f'{run_name}: SC Cells Colored by Angle θ')\n",
    "    ax1.set_xlabel('X coordinate')\n",
    "    ax1.set_ylabel('Y coordinate')\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Angle (degrees)')\n",
    "    \n",
    "    # Plot 2: Per-cell-type angle distribution\n",
    "    unique_types = np.unique(cell_types)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n",
    "    \n",
    "    for i, cell_type in enumerate(unique_types):\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 0:\n",
    "            angles_subset = sc_angles_degrees[mask]\n",
    "            ax2.hist(angles_subset, bins=36, alpha=0.6, label=cell_type, \n",
    "                    color=colors[i], density=True)\n",
    "    \n",
    "    ax2.set_title(f'{run_name}: Angle Distribution by Cell Type')\n",
    "    ax2.set_xlabel('Angle (degrees)')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_xlim(0, 360)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'sc_angle_analysis_{run_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print circular statistics per cell type\n",
    "    print(f\"\\n{run_name} - Circular statistics per cell type:\")\n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 5:  # Only if enough cells\n",
    "            angles_rad = sc_angles[mask]\n",
    "            # Circular mean\n",
    "            mean_cos = np.mean(np.cos(angles_rad))\n",
    "            mean_sin = np.mean(np.sin(angles_rad))\n",
    "            circular_mean = np.arctan2(mean_sin, mean_cos)\n",
    "            if circular_mean < 0:\n",
    "                circular_mean += 2*np.pi\n",
    "            \n",
    "            print(f\"  {cell_type}: mean={np.degrees(circular_mean):.1f}°, n={np.sum(mask)}\")\n",
    "\n",
    "def _plot_comparative_sc_angle_analysis(sc_coords_list, cell_types, angular_frame, st_coords_bg):\n",
    "    \"\"\"Plot comparative SC angle analysis across all runs\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    n_runs = len(sc_coords_list)\n",
    "    unique_types = np.unique(cell_types)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_runs, figsize=(5*n_runs, 10))\n",
    "    if n_runs == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Top row: SC scatter plots per run\n",
    "    for i, sc_coords in enumerate(sc_coords_list):\n",
    "        ax = axes[0, i]\n",
    "        \n",
    "        # ST background\n",
    "        ax.scatter(st_coords_bg[:, 0], st_coords_bg[:, 1], \n",
    "                  c='lightgray', s=5, alpha=0.3)\n",
    "        \n",
    "        # SC cells colored by angle\n",
    "        sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "        sc_angles_degrees = np.degrees(sc_angles)\n",
    "        \n",
    "        scatter = ax.scatter(sc_coords[:, 0], sc_coords[:, 1], \n",
    "                           c=sc_angles_degrees, cmap='hsv', s=20, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'Run {i+1}: SC Cells by Angle')\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        if i == n_runs-1:  # Add colorbar to last plot\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('Angle (degrees)')\n",
    "    \n",
    "    # Bottom row: Cell type angle distributions per run  \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n",
    "    \n",
    "    for i, sc_coords in enumerate(sc_coords_list):\n",
    "        ax = axes[1, i]\n",
    "        \n",
    "        sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "        sc_angles_degrees = np.degrees(sc_angles)\n",
    "        \n",
    "        for j, cell_type in enumerate(unique_types):\n",
    "            mask = cell_types == cell_type\n",
    "            if np.sum(mask) > 5:\n",
    "                angles_subset = sc_angles_degrees[mask]\n",
    "                ax.hist(angles_subset, bins=36, alpha=0.6, \n",
    "                       label=cell_type if i == 0 else \"\", \n",
    "                       color=colors[j], density=True)\n",
    "        \n",
    "        ax.set_title(f'Run {i+1}: Cell Type Angles')\n",
    "        ax.set_xlabel('Angle (degrees)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_xlim(0, 360)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparative_sc_angle_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for sector sliding\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"SECTOR SLIDING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 10:  # Only analyze cell types with enough cells\n",
    "            circular_means = []\n",
    "            \n",
    "            for i, sc_coords in enumerate(sc_coords_list):\n",
    "                sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "                angles_subset = sc_angles[mask]\n",
    "                \n",
    "                # Circular mean\n",
    "                mean_cos = np.mean(np.cos(angles_subset))\n",
    "                mean_sin = np.mean(np.sin(angles_subset))\n",
    "                circular_mean = np.arctan2(mean_sin, mean_cos)\n",
    "                if circular_mean < 0:\n",
    "                    circular_mean += 2*np.pi\n",
    "                \n",
    "                circular_means.append(np.degrees(circular_mean))\n",
    "            \n",
    "            # Check for large differences between runs\n",
    "            max_diff = max(circular_means) - min(circular_means)\n",
    "            if max_diff > 180:  # Handle wraparound\n",
    "                max_diff = 360 - max_diff\n",
    "            \n",
    "            print(f\"{cell_type}:\")\n",
    "            print(f\"  Run means: {[f'{m:.1f}°' for m in circular_means]}\")\n",
    "            print(f\"  Max difference: {max_diff:.1f}°\")\n",
    "            \n",
    "            if max_diff > 30:  # Significant sliding\n",
    "                print(f\"  ⚠️  SECTOR SLIDING DETECTED!\")\n",
    "            else:\n",
    "                print(f\"  ✅ Consistent placement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10 = load_and_process_cscc_data_p10()\n",
    "\n",
    "# Train individual AdvancedHierarchicalDiffusion models and get averaged results\n",
    "scadata_p10, advanced_models_p10 = train_individual_advanced_diffusion_models(\n",
    "    scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata_p10, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (5, 5)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata_p10, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "\n",
    "import seaborn as sns\n",
    "n_groups = scadata_p10.obs[\"rough_celltype\"].nunique()\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=n_groups).as_hex()\n",
    "\n",
    "# my_tab20 = sns.color_palette(\"tab10\", n_colors=20).as_hex()\n",
    "\n",
    "\n",
    "# ---------- user preferences ----------\n",
    "sc.settings.set_figure_params(format='svg')      # keep default SVG\n",
    "mpl.rcParams['figure.figsize'] = (6, 6)          # default figsize (will be overridden for this fig)\n",
    "os.makedirs(\"figures\", exist_ok=True)            # create folder on the go\n",
    "outpath = \"figures/P10_rep1_v1.svg\"                 # final save path\n",
    "dpi_save = 600                                   # desired DPI for export\n",
    "# --------------------------------------\n",
    "\n",
    "# (optional) temporarily silence the FutureWarning about squidpy alternative\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Plot with Scanpy but DON'T save yet (show=False), so we can customize the legend\n",
    "sc.pl.spatial(\n",
    "    scadata_p10,\n",
    "    color=\"rough_celltype\",\n",
    "    spot_size=0.06,\n",
    "    show=False,                         # prevent automatic display/save so we can customize\n",
    "    basis='advanced_diffusion_coords_avg',\n",
    "    title='reconstructed',\n",
    "    save=None,                           # make sure Scanpy doesn't auto-save; we handle saving below\n",
    "    palette = my_tab20\n",
    ")\n",
    "\n",
    "# grab current Axes & Figure\n",
    "ax = plt.gca()\n",
    "fig = ax.get_figure()\n",
    "\n",
    "# --- Optional: make the figure wider so the horizontal legend fits comfortably ---\n",
    "# You can change these numbers (width, height) to taste.\n",
    "fig.set_size_inches(10, 6)   # increase width (was 5x5); user said wider is fine\n",
    "\n",
    "# --- Build a clean horizontal legend BELOW the plot ---\n",
    "# Remove any existing legend first (Scanpy sometimes creates one)\n",
    "old_leg = ax.get_legend()\n",
    "if old_leg:\n",
    "    old_leg.remove()\n",
    "\n",
    "# obtain handles & labels from the scatter artist(s)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "# If Scanpy produced extra \"title-like\" legend entries, you might get duplicate/empty labels;\n",
    "# if so, filter them out (uncomment if needed)\n",
    "# pairs = [(h, l) for h, l in zip(handles, labels) if l not in (None, '')]\n",
    "# if pairs:\n",
    "#     handles, labels = zip(*pairs)\n",
    "# else:\n",
    "#     handles, labels = [], []\n",
    "\n",
    "# place legend as a horizontal strip below the main axes\n",
    "# - loc='upper center' with bbox_to_anchor centers the legend beneath the axes\n",
    "# - bbox_to_anchor: (0.5, -0.15) -> x=0.5 center, y negative moves it below the axes; tweak y to move further down\n",
    "# - ncol: set how many columns you want; set to len(labels) for a single-row legend (will be wide)\n",
    "# - frameon: False removes the border if you want a clean strip\n",
    "ncol = len(labels)                 # puts all entries in a single row; change to e.g. 6 for multiple rows\n",
    "legend = ax.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, -0.13),\n",
    "    ncol=ncol,\n",
    "    frameon=False,\n",
    "    handlelength=2.5,\n",
    "    columnspacing=1.0,\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "# after you create `legend`:\n",
    "for h in legend.legendHandles:\n",
    "    if isinstance(h, matplotlib.collections.PathCollection):\n",
    "        # size is in points^2 (try values like 50, 100, 200, 400)\n",
    "        h.set_sizes([200.0])     # <- increase this number to make markers bigger\n",
    "    else:\n",
    "        try:\n",
    "            h.set_markersize(10)  # for Line2D handles (if any)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "# Reserve space at the bottom so the legend isn't clipped\n",
    "# Increase the bottom margin proportionally to how far below you placed the legend\n",
    "fig.subplots_adjust(bottom=0.22)   # increase if you move legend further down (or more rows)\n",
    "\n",
    "# If you prefer the legend to be drawn across the whole figure, you can use fig.legend(...) instead.\n",
    "# Example: fig.legend(handles, labels, loc='lower center', ncol=ncol, bbox_to_anchor=(0.5, 0.02))\n",
    "# But ax.legend above keeps alignment with the axes more predictably.\n",
    "\n",
    "# --- Save the figure to the folder as SVG at 600 DPI ---\n",
    "fig.savefig(outpath, format=\"svg\", dpi=dpi_save, bbox_inches=\"tight\")\n",
    "\n",
    "# show the final adjusted figure in the notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "# mpl.rcParams.update({\"font.family\": \"Tahoma\", \"font.weight\": \"bold\"})\n",
    "# mpl.rcParams.update({\"font.family\": \"Arial\", \"font.weight\": \"bold\"})\n",
    "mpl.rcParams.update({\"font.family\": \"sans-serif\", \"font.sans-serif\": [\"Arial\", \"Liberation Sans\", \"DejaVu Sans\"], \"font.weight\": \"bold\"})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Setup ----------\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sc.settings.set_figure_params(format='svg')\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "outpath = \"figures/p2_p10_v5.svg\"\n",
    "dpi_save = 600\n",
    "\n",
    "# ---------- Ensure Consistent Category Order ----------\n",
    "# Get sorted unique categories from both datasets combined\n",
    "all_categories = sorted(\n",
    "    list(set(scadata.obs[\"rough_celltype\"].unique()) | set(scadata_p10.obs[\"rough_celltype\"].unique()))\n",
    ")\n",
    "\n",
    "# Enforce the same category order for both AnnData objects\n",
    "scadata.obs[\"rough_celltype\"] = scadata.obs[\"rough_celltype\"].astype(\"category\")\n",
    "scadata.obs[\"rough_celltype\"] = scadata.obs[\"rough_celltype\"].cat.set_categories(all_categories)\n",
    "\n",
    "scadata_p10.obs[\"rough_celltype\"] = scadata_p10.obs[\"rough_celltype\"].astype(\"category\")\n",
    "scadata_p10.obs[\"rough_celltype\"] = scadata_p10.obs[\"rough_celltype\"].cat.set_categories(all_categories)\n",
    "\n",
    "# Create a consistent palette based on unified categories\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=len(all_categories)).as_hex()\n",
    "palette_dict = dict(zip(all_categories, my_tab20))\n",
    "\n",
    "# ---------- Create Subplots ----------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Plot scadata on the left\n",
    "sc.pl.spatial(\n",
    "    scadata,\n",
    "    color=\"rough_celltype\",\n",
    "    spot_size=0.06,\n",
    "    ax=axes[0],\n",
    "    show=False,\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title=None,\n",
    "    palette=palette_dict\n",
    ")\n",
    "\n",
    "\n",
    "# Plot scadata_p10 on the right\n",
    "sc.pl.spatial(\n",
    "    scadata_p10,\n",
    "    color=\"rough_celltype\",\n",
    "    spot_size=0.06,\n",
    "    ax=axes[1],\n",
    "    show=False,\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title=None,\n",
    "    palette=palette_dict\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Create One Unified Legend ----------\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "for ax in axes:\n",
    "    old_leg = ax.get_legend()\n",
    "    if old_leg:\n",
    "        old_leg.remove()\n",
    "\n",
    "legend = fig.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='lower center',\n",
    "    bbox_to_anchor=(0.5, 0.02),\n",
    "    ncol=len(labels),\n",
    "    frameon=False,\n",
    "    fontsize=12,\n",
    "    handlelength=2,\n",
    "    columnspacing=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "# ncol = 7  # instead of len(labels)\n",
    "\n",
    "for h in legend.legendHandles:\n",
    "    if isinstance(h, matplotlib.collections.PathCollection):\n",
    "        h.set_sizes([200])\n",
    "    else:\n",
    "        try:\n",
    "            h.set_markersize(10)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Adjust layout to make space for the bottom legend\n",
    "fig.subplots_adjust(bottom=0.18, wspace=0.5)\n",
    "\n",
    "for ax in axes:\n",
    "    # Remove axis labels & ticks\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove plot title if somehow still present\n",
    "    ax.set_title(\"\")\n",
    "\n",
    "    # Remove colorbar titles (Scanpy auto-adds these)\n",
    "    for cbar in ax.figure.axes:\n",
    "        if cbar != ax:  # skip the main plot axis\n",
    "            cbar.set_title(\"\")  # removes \"rough_celltype\"\n",
    "\n",
    "\n",
    "# ---------- Save Figure ----------\n",
    "# fig.savefig(outpath, format=\"svg\", dpi=dpi_save, bbox_inches=\"tight\")\n",
    "fig.savefig(outpath, format=\"svg\", dpi=dpi_save, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install ttf-mscorefonts-installer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(format='svg') \n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "\n",
    "\n",
    "sc.pl.spatial(scadata_p10,color=\"rough_celltype\",spot_size=0.04, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams['pdf.fonttype'] = 42\n",
    "# rcParams['ps.fonttype'] = 42\n",
    "# figsize(4,4)\n",
    "mpl.rcParams['figure.figsize'] = (5, 5)\n",
    "# sc.pl.spatial(scadata,color=\"level3_celltype\",groups=[\"TSK\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_rep3',title='reconstructed',na_in_legend=False, save='P2_rep3_tsk_v2')\n",
    "sc.pl.spatial(\n",
    "    scadata_p10,\n",
    "    color=\"level3_celltype\",\n",
    "    groups=[\"TSK\"],\n",
    "    spot_size=0.06,\n",
    "    show=False,  # <- Important: prevent auto-show\n",
    "    basis='advanced_diffusion_coords_rep2',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False\n",
    ")\n",
    "\n",
    "# Save manually with high resolution\n",
    "plt.savefig(\"P10_rep2_tsk_v10.svg\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "#save='TSK',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata_p10.obsm['advanced_diffusion_coords_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "# import scanpy as sc\n",
    "# sc.settings.set_figure_params(figsize=(4,4), dpi=100)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_rep1', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_rep2', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=12).as_hex()\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_rep3', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata_p10.obs['selection'] = (scadata_p10.obs['level2_celltype']=='TSK').astype(int)\n",
    "scadata_p10.obs['selection2'] = (scadata_p10.obs['level1_celltype']=='Fibroblast').astype(int)\n",
    "scadata_p10.obs['selection3'] = (scadata_p10.obs['rough_celltype']=='Epithelial').astype(int)\n",
    "\n",
    "# figsize(6,5)\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "sc.pl.spatial(scadata_p10, color=['selection','selection2','selection3','level3_celltype'], spot_size=0.025,cmap='bwr',basis='advanced_diffusion_coords_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(scadata_p10,color=\"level3_celltype\",groups=[\"TSK\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "sq.gr.spatial_neighbors(scadata_p10,spatial_key='advanced_diffusion_coords_rep2')\n",
    "sq.gr.nhood_enrichment(scadata_p10,cluster_key='rough_celltype')\n",
    "sq.gr.interaction_matrix(scadata_p10,cluster_key='rough_celltype')\n",
    "kscadata_p10 = scadata_p10[ scadata_p10.obs.level2_celltype.isin(['Tumor_KC_Cyc','Tumor_KC_Basal','Tumor_KC_Diff','TSK'])].copy()\n",
    "sq.gr.spatial_neighbors(kscadata_p10,spatial_key='advanced_diffusion_coords_rep2')\n",
    "sq.gr.nhood_enrichment(kscadata_p10,cluster_key='level2_celltype')\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',save='TSKKC_new_best_p10.svg',figsize=(3,5), title=None)\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\", cmap='coolwarm', save='TSKKC_new_best_p10.svg', figsize=(3,5), ylabel='')\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',figsize=(3,5))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,5))\n",
    "sq.pl.nhood_enrichment(kscadata_p10, cluster_key=\"level2_celltype\", cmap='coolwarm', ax=ax)\n",
    "ax.set_ylabel('')\n",
    "# plt.savefig('TSKKC_new_best_p10.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import QuadMesh\n",
    "\n",
    "sq.gr.spatial_neighbors(kscadata_p10,spatial_key='advanced_diffusion_coords_rep2')\n",
    "\n",
    "\n",
    "# draw squidpy plot\n",
    "sq.pl.nhood_enrichment(\n",
    "    kscadata_p10,\n",
    "    cluster_key=\"level2_celltype\",\n",
    "    cmap=\"coolwarm\",\n",
    "    figsize=(4, 3)\n",
    ")\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "# find heatmap mappable and its axes (AxesImage or QuadMesh)\n",
    "mappable = None\n",
    "heat_ax = None\n",
    "\n",
    "\n",
    "for ax in fig.axes:\n",
    "    if ax.images:\n",
    "        mappable = ax.images[0]\n",
    "        heat_ax = ax\n",
    "        break\n",
    "    for coll in ax.collections:\n",
    "        if isinstance(coll, QuadMesh):\n",
    "            mappable = coll\n",
    "            heat_ax = ax\n",
    "            break\n",
    "    if mappable is not None:\n",
    "        break\n",
    "\n",
    "if mappable is None:\n",
    "    raise RuntimeError(\"Couldn't find heatmap mappable in the figure.\")\n",
    "\n",
    "# remove any old colorbar axes attached to mappable (and any existing colorbar artists)\n",
    "old_cb = getattr(mappable, \"colorbar\", None)\n",
    "if old_cb is not None:\n",
    "    try: old_cb.remove()\n",
    "    except Exception: pass\n",
    "\n",
    "# also remove any Axes that look like a colorbar (optional safe cleanup)\n",
    "# (we check for very narrow axes to avoid removing legitimate axes)\n",
    "to_delete = []\n",
    "for ax in fig.axes:\n",
    "    pos = ax.get_position()\n",
    "    if pos.width < 0.02 and ax is not heat_ax:   # heuristic for colorbar axes\n",
    "        to_delete.append(ax)\n",
    "for ax in to_delete:\n",
    "    try: fig.delaxes(ax)\n",
    "    except Exception: pass\n",
    "\n",
    "# --- create a new colorbar axis in FIGURE COORDS (doesn't resize heatmap) ---\n",
    "# get heatmap bbox in figure coordinates\n",
    "hb = heat_ax.get_position()\n",
    "\n",
    "# pad_fig shifts the colorbar to the right of the heatmap (fraction of figure width)\n",
    "# width_fig controls the colorbar width (fraction of figure width)\n",
    "# height will be same as heatmap height (hb.height)\n",
    "pad_fig = 0.08   # try 0.02, 0.03, 0.04 ... -> moves bar further right\n",
    "width_fig = 0.035  # try 0.03..0.08 -> makes bar fatter\n",
    "# extra = 0.2  # fraction of figure height\n",
    "# cax_rect = [hb.x1 + pad_fig, hb.y0 - extra/2, width_fig, hb.height + extra]\n",
    "\n",
    "cax_rect = [hb.x1 + pad_fig, hb.y0, width_fig, hb.height]\n",
    "# cax_rect = [hb.x1 + pad_fig, hb.y0, width_fig, hb.height * 1.2]\n",
    "\n",
    "\n",
    "# create the axis (if it would overflow the figure, reduce pad or width)\n",
    "# cax = fig.add_axes(cax_rect)\n",
    "# cb = fig.colorbar(mappable, ax=heat_ax, fraction=0.046, pad=0.02)\n",
    "fig.canvas.draw()\n",
    "hb = heat_ax.get_position()                      # measure after draw()\n",
    "cax_rect = [hb.x1 + pad_fig, hb.y0, width_fig, hb.height]\n",
    "cax = fig.add_axes(cax_rect)\n",
    "cb = fig.colorbar(mappable, cax=cax)\n",
    "\n",
    "\n",
    "\n",
    "# draw colorbar into that fixed axis\n",
    "cb = fig.colorbar(mappable, cax=cax, aspect=100)\n",
    "\n",
    "# label only top and bottom with \"High\"/\"Low\" and title \"Score\"\n",
    "if getattr(mappable, \"norm\", None) is not None:\n",
    "    vmin, vmax = mappable.norm.vmin, mappable.norm.vmax\n",
    "else:\n",
    "    try:\n",
    "        arr = mappable.get_array()\n",
    "        vmin, vmax = float(arr.min()), float(arr.max())\n",
    "    except Exception:\n",
    "        vmin, vmax = 0.0, 1.0\n",
    "\n",
    "cb.set_ticks([vmin, vmax])\n",
    "cb.set_ticklabels([\"Low\", \"High\"])\n",
    "cb.ax.tick_params(length=0)\n",
    "cb.ax.set_title(\"Score\", pad=8)\n",
    "\n",
    "# if your color scale looks inverted (High at bottom), invert axis:\n",
    "# cb.ax.invert_yaxis()\n",
    "\n",
    "# avoid tight_layout conflicts; adjust if you need margin room\n",
    "fig.subplots_adjust(right=min(0.98, hb.x1 + pad_fig + width_fig + 0.02))\n",
    "\n",
    "import os; \n",
    "os.makedirs(\"figures\", exist_ok=True); \n",
    "fig.savefig(\"figures/nbhd_enrich_p10_v3.svg\", format=\"svg\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize(4,4)\n",
    "# sc.settings.file_format_figs = 'svg'\n",
    "\n",
    "# sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Cyc\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P10_cyc')\n",
    "# sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Basal\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P10_bas')\n",
    "# sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Diff\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P10_diff')\n",
    "# sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Cyc\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_rep1',title='reconstructed',na_in_legend=False)\n",
    "# sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Basal\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_rep1',title='reconstructed',na_in_legend=False)\n",
    "# sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Diff\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_rep1',title='reconstructed',na_in_legend=False)\n",
    "#save='nonTSK',\n",
    "\n",
    "sc.pl.spatial(\n",
    "    scadata_p10,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"Tumor_KC_Cyc\"],\n",
    "    spot_size=0.05,\n",
    "    show=True,\n",
    "    basis='advanced_diffusion_coords_rep1',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"]   # maroon / blood-red\n",
    ")\n",
    "\n",
    "sc.pl.spatial(\n",
    "    scadata_p10,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"Tumor_KC_Basal\"],\n",
    "    spot_size=0.05,\n",
    "    show=True,\n",
    "    basis='advanced_diffusion_coords_rep1',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"]  # maroon / blood-red\n",
    ")\n",
    "\n",
    "sc.pl.spatial(\n",
    "    scadata_p10,\n",
    "    color=\"level2_celltype\",\n",
    "    groups=[\"Tumor_KC_Diff\"],\n",
    "    spot_size=0.05,\n",
    "    show=True,\n",
    "    basis='advanced_diffusion_coords_rep1',\n",
    "    title='reconstructed',\n",
    "    na_in_legend=False,\n",
    "    palette=[\"#800000\"]   # maroon / blood-red\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"PDC\"],spot_size=0.04, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=['Tumor_KC_Cyc','Tumor_KC_Basal','Tumor_KC_Diff'],spot_size=0.025, \n",
    "              show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load SC data\n",
    "scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "\n",
    "# Normalize and log transform\n",
    "sc.pp.normalize_total(scadata)\n",
    "sc.pp.log1p(scadata)\n",
    "\n",
    "# Find highly variable genes and compute PCA\n",
    "sc.pp.highly_variable_genes(scadata, n_top_genes=2000)\n",
    "scadata.raw = scadata\n",
    "scadata = scadata[:, scadata.var.highly_variable]\n",
    "sc.pp.scale(scadata, max_value=10)\n",
    "sc.tl.pca(scadata, svd_solver='arpack')\n",
    "\n",
    "# Compute neighbors and UMAP with more spread\n",
    "sc.pp.neighbors(scadata, n_neighbors=15, n_pcs=40)\n",
    "sc.tl.umap(scadata, min_dist=0.5, spread=2.0)\n",
    "\n",
    "# Use detailed cell types instead of rough categories\n",
    "cell_type_column = 'level2_celltype'  # This has more detailed cell types\n",
    "n_groups = len(scadata.obs[cell_type_column].unique())\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=n_groups).as_hex()\n",
    "\n",
    "print(f\"Number of cell types: {n_groups}\")\n",
    "print(f\"Cell types: {list(scadata.obs[cell_type_column].unique())}\")\n",
    "\n",
    "# Plot UMAP colored by detailed cell types\n",
    "plt.figure(figsize=(12, 8))\n",
    "sc.pl.umap(scadata, color=cell_type_column, legend_loc='right margin', \n",
    "           legend_fontsize=10, size=50, palette=my_tab20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load SC data\n",
    "scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP10.h5ad')\n",
    "\n",
    "# Normalize and log transform\n",
    "sc.pp.normalize_total(scadata)\n",
    "sc.pp.log1p(scadata)\n",
    "\n",
    "# Find highly variable genes and compute PCA\n",
    "sc.pp.highly_variable_genes(scadata, n_top_genes=3000)\n",
    "scadata.raw = scadata\n",
    "scadata = scadata[:, scadata.var.highly_variable]\n",
    "sc.pp.scale(scadata, max_value=10)\n",
    "sc.tl.pca(scadata, svd_solver='arpack')\n",
    "\n",
    "# Compute neighbors and t-SNE with better separation\n",
    "sc.pp.neighbors(scadata, n_neighbors=5, n_pcs=40)\n",
    "sc.tl.tsne(scadata, perplexity=30, early_exaggeration=12, learning_rate=1000)\n",
    "\n",
    "# Use detailed cell types\n",
    "cell_type_column = 'level2_celltype'\n",
    "n_groups = len(scadata.obs[cell_type_column].unique())\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=n_groups).as_hex()\n",
    "\n",
    "print(f\"Number of cell types: {n_groups}\")\n",
    "print(f\"Cell types: {list(scadata.obs[cell_type_column].unique())}\")\n",
    "\n",
    "# Plot t-SNE colored by detailed cell types\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.tsne(scadata, color=cell_type_column, legend_loc='right margin', \n",
    "           legend_fontsize=10, size=50, palette=my_tab20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "adata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP10.h5ad')\n",
    "\n",
    "# standard preproc (if already done you can skip) \n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=3000)\n",
    "adata.raw = adata\n",
    "adata = adata[:, adata.var.highly_variable]\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "sc.tl.pca(adata, svd_solver='arpack')\n",
    "\n",
    "# compute neighbors (you already do this; n_pcs influences structure)\n",
    "sc.pp.neighbors(adata, n_neighbors=5, n_pcs=40)\n",
    "\n",
    "# --- compute t-SNE: try a few variants (pick one you like) ---\n",
    "# Variant A: more spread, random init, smaller learning rate, more iters\n",
    "sc.tl.tsne(adata,\n",
    "          perplexity=30,\n",
    "          early_exaggeration=12,\n",
    "          learning_rate=200,\n",
    "          n_pcs=40,\n",
    "        #   init='random',\n",
    "          random_state=0,\n",
    "          use_fast_tsne=False)  # let Scanpy choose backend\n",
    "# compute UMAP (after neighbors already set)\n",
    "# sc.tl.umap(adata, min_dist=0.3, n_components=2, spread=1.0, random_state=0)\n",
    "# then plot with the same seaborn approach: use adata.obsm['X_umap']\n",
    "\n",
    "\n",
    "# Variant B: try cosine metric (uncomment if supported / desired)\n",
    "# sc.tl.tsne(adata, perplexity=30, metric='cosine', learning_rate=200, init='random', n_iter=2000)\n",
    "\n",
    "# Now plot manually with seaborn/matplotlib for full control\n",
    "cell_col = 'level2_celltype'\n",
    "coords = adata.obsm['X_tsne']\n",
    "df = pd.DataFrame(coords, columns=['tsne1','tsne2'], index=adata.obs_names)\n",
    "df[cell_col] = adata.obs[cell_col].values\n",
    "\n",
    "# palette: tab20 may not be enough if many categories — try colorblind or husl variants\n",
    "unique = df[cell_col].unique()\n",
    "palette = dict(zip(unique, sns.color_palette(\"tab20\", n_colors=len(unique))))\n",
    "\n",
    "plt.figure(figsize=(10,8), dpi=150)\n",
    "ax = sns.scatterplot(\n",
    "    data=df, x='tsne1', y='tsne2', hue=cell_col, palette=palette,\n",
    "    s=20, alpha=0.7, linewidth=0, edgecolor=None, legend='full'\n",
    ")\n",
    "\n",
    "# crop axes to data with small padding to remove huge white margins\n",
    "xpad = (df.tsne1.max() - df.tsne1.min()) * 0.05\n",
    "ypad = (df.tsne2.max() - df.tsne2.min()) * 0.05\n",
    "ax.set_xlim(df.tsne1.min()-xpad, df.tsne1.max()+xpad)\n",
    "ax.set_ylim(df.tsne2.min()-ypad, df.tsne2.max()+ypad)\n",
    "\n",
    "# nicer legend outside\n",
    "ax.legend(bbox_to_anchor=(1.01, 1), loc='upper left', borderaxespad=0, frameon=False, fontsize=8)\n",
    "ax.set_xlabel('t-SNE 1'); ax.set_ylabel('t-SNE 2')\n",
    "plt.tight_layout()\n",
    "plt.savefig('p10_tsne.pdf', bbox_inches='tight', dpi=600)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
