{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Cell 2: force single‐threaded BLAS\n",
    "os.environ[\"OMP_NUM_THREADS\"]       = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: actually cap BLAS to 1 thread\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# 'blas' covers OpenBLAS, MKL, etc.\n",
    "threadpool_limits(limits=1, user_api='blas')\n",
    "\n",
    "# now import as usual, no more warning\n",
    "import numpy as np\n",
    "import scipy\n",
    "# … any other packages that use OpenBLAS …\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_torch(X, k, mode='connectivity', metric = 'minkowski', p=2, device='cuda'):\n",
    "    '''construct knn graph with torch and gpu\n",
    "    args:\n",
    "        X: input data containing features (torch tensor)\n",
    "        k: number of neighbors for each data point\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric (now euclidean supported for gpu knn)\n",
    "        p: param for minkowski (not used if metric is euclidean)\n",
    "    \n",
    "    Returns:\n",
    "        knn graph as a pytorch sparse tensor (coo format) or dense tensor depending on mode     \n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'.\"\n",
    "    assert metric == 'euclidean', \"for gpu knn, only 'euclidean' metric is currently supported in this implementation\"\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "        mode_knn = 'connectivity'\n",
    "    else:\n",
    "        include_self = False\n",
    "        mode_knn = 'distance'\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm='auto')\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_cpu = X.cpu().numpy()\n",
    "    else:\n",
    "        X_cpu = X.numpy()\n",
    "\n",
    "    knn.fit(X_cpu)\n",
    "    knn_graph_cpu = kneighbors_graph(knn, k, mode=mode_knn, include_self=include_self, metric=metric) #scipy sparse matrix on cpu\n",
    "    knn_graph_coo = knn_graph_cpu.tocoo()\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        knn_graph = torch.sparse_coo_tensor(torch.LongTensor([knn_graph_coo.row, knn_graph_coo.col]),\n",
    "                                            torch.FloatTensor(knn_graph_coo.data),\n",
    "                                            size = knn_graph_coo.shape).to(device)\n",
    "    elif mode == 'distance':\n",
    "        knn_graph_dense = torch.tensor(knn_graph_cpu.toarray(), dtype=torch.float32, device=device) #move to gpu as dense tensor\n",
    "        knn_graph = knn_graph_dense\n",
    "    \n",
    "    return knn_graph\n",
    "    \n",
    "def distances_cal_torch(graph, type_aware=None, aware_power =2, device='cuda'):\n",
    "    '''\n",
    "    calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (pytorch sparse or dense tensor)\n",
    "        type_aware: not implemented in this torch version for simplicity\n",
    "        aware_power: same ^^\n",
    "        device (str): 'cpu' or 'cuda' device to use\n",
    "    Returns:\n",
    "        distance matrix as a torch tensor\n",
    "    '''\n",
    "\n",
    "    if isinstance(graph, torch.Tensor) and graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().to_dense().numpy())\n",
    "    elif isinstance(graph, torch.Tensor) and not graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().numpy())\n",
    "    else:\n",
    "        graph_cpu_csr = csr_matrix(graph) #assume scipy sparse matrix if not torch tensor\n",
    "\n",
    "    shortestPath_cpu = dijkstra(csgraph = graph_cpu_csr, directed=False, return_predecessors=False) #dijkstra on cpu\n",
    "    shortestPath = torch.tensor(shortestPath_cpu, dtype=torch.float32, device=device)\n",
    "\n",
    "    # the_max = torch.nanmax(shortestPath[shortestPath != float('inf')])\n",
    "    # shortestPath[shortestPath > the_max] = the_max\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = torch.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    original_max_distance = the_max.item()\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= torch.mean(C_dis)\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_sc_torch(X_sc, k_neighbors=10, graph_mode='connectivity', device='cpu'):\n",
    "    '''calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (torch sparse or dense tensor)\n",
    "        type_aware: not implemented\n",
    "        aware_power: same ^^\n",
    "        \n",
    "    returns:\n",
    "        distanced matrix as torch tensor'''\n",
    "    \n",
    "    if not isinstance(X_sc, torch.Tensor):\n",
    "        raise TypeError('Input X_sc must be a pytorch tensor')\n",
    "    \n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_sc = X_sc.cuda(device=device)\n",
    "    else:\n",
    "        X_sc = X_sc.cpu()\n",
    "        device= 'cpu'\n",
    "\n",
    "    print(f'using device: {device}')\n",
    "    print(f'constructing knn graph...')\n",
    "    # X_normalized = normalize(X_sc.cpu().numpy(), norm='l2') #normalize on cpu for sklearn knn\n",
    "    X_normalized = X_sc\n",
    "    X_normalized_torch = torch.tensor(X_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "    Xgraph = construct_graph_torch(X_normalized_torch, k=k_neighbors, mode=graph_mode, metric='euclidean', device=device)\n",
    "\n",
    "    print('calculating distances from graph....')\n",
    "    D_sc, sc_max_distance = distances_cal_torch(Xgraph, device=device)\n",
    "\n",
    "    print('D_sc calculation complete')\n",
    "    \n",
    "    return D_sc, sc_max_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot\n",
    "\n",
    "def construct_graph_spatial(location_array, k, mode='distance', metric='euclidean', p=2):\n",
    "    '''construct KNN graph based on spatial coordinates\n",
    "    args:\n",
    "        location_array: spatial coordinates of spots (n-spots * 2)\n",
    "        k: number of neighbors for each spot\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric for knn (p=2 is euclidean)\n",
    "        p: param for minkowski if connectivity\n",
    "        \n",
    "    returns:\n",
    "        scipy.sparse.csr_matrix: knn graph in csr format\n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'\"\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "    else:\n",
    "        include_self = False\n",
    "    \n",
    "    c_graph = kneighbors_graph(location_array, k, mode=mode, metric=metric, include_self=include_self, p=p)\n",
    "    return c_graph\n",
    "\n",
    "def distances_cal_spatial(graph, spot_ids=None, spot_types=None, aware_power=2):\n",
    "    '''calculate spatial distance matrix from knn graph\n",
    "    args:\n",
    "        graph (scipy.sparse.csr_matrix): knn graph\n",
    "        spot_ids (list, optional): list of spot ids corresponding to the rows/cols of the graph. required if type_aware is used\n",
    "        spot_types (pd.Series, optinal): pandas series of spot types for type aware distance adjustment. required if type_aware is used\n",
    "        aware_power (int): power for type-aware distance adjustment\n",
    "        \n",
    "    returns:\n",
    "        sptial distance matrix'''\n",
    "    shortestPath = dijkstra(csgraph = csr_matrix(graph), directed=False, return_predecessors=False)\n",
    "    shortestPath = np.nan_to_num(shortestPath, nan=np.inf) #handle potential inf valyes after dijkstra\n",
    "\n",
    "    if spot_types is not None and spot_ids is not None:\n",
    "        shortestPath_df = pd.DataFrame(shortestPath, index=spot_ids, columns=spot_ids)\n",
    "        shortestPath_df['id1'] = shortestPath_df.index\n",
    "        shortestPath_melted = shortestPath_df.melt(id_vars=['id1'], var_name='id2', value_name='value')\n",
    "\n",
    "        type_aware_df = pd.DataFrame({'spot': spot_ids, 'spot_type': spot_types}, index=spot_ids)\n",
    "        meta1 = type_aware_df.copy()\n",
    "        meta1.columns = ['id1', 'type1']\n",
    "        meta2 = type_aware_df.copy()\n",
    "        meta2.columns = ['id2', 'type2']\n",
    "\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta1, on='id1', how='left')\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta2, on='id2', how='left')\n",
    "\n",
    "        shortestPath_melted['same_type'] = shortestPath_melted['type1'] == shortestPath_melted['type2']\n",
    "        shortestPath_melted.loc[(~shortestPath_melted.smae_type), 'value'] = shortestPath_melted.loc[(~shortestPath_melted.same_type),\n",
    "                                                                                                     'value'] * aware_power\n",
    "        shortestPath_melted.drop(['type1', 'type2', 'same_type'], axis=1, inplace=True)\n",
    "        shortestPath_pivot = shortestPath_melted.pivot(index='id1', columns='id2', values='value')\n",
    "\n",
    "        order = spot_ids\n",
    "        shortestPath = shortestPath_pivot[order].loc[order].values\n",
    "    else:\n",
    "        shortestPath = np.asarray(shortestPath) #ensure it's a numpy array\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = np.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    #store original max distance for scale reference\n",
    "    original_max_distance = the_max\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= np.mean(C_dis)\n",
    "\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_st_from_coords(spatial_coords, X_st=None, k_neighbors=10, graph_mode='distance', aware_st=False, \n",
    "                               spot_types=None, aware_power_st=2, spot_ids=None):\n",
    "    '''calculates the spatial distance matrix D_st for spatial transcriptomics data directly from coordinates and optional spot types\n",
    "    args:\n",
    "        spatial_coords: spatial coordinates of spots (n_spots * 2)\n",
    "        X_st: St gene expression data (not used for D_st calculation itself)\n",
    "        k_neighbors: number of neighbors for knn graph\n",
    "        graph_mode: 'connectivity or 'distance' for knn graph\n",
    "        aware_st: whether to use type-aware distance adjustment\n",
    "        spot_types: pandas series of spot types for type-aware adjustment\n",
    "        aware_power_st: power for type-aware distance adjustment\n",
    "        spot_ids: list or index of spot ids, required if spot_ids is provided\n",
    "        \n",
    "    returns:\n",
    "        np.ndarray: spatial disance matrix D_st'''\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        location_array = spatial_coords.values\n",
    "        if spot_ids is None:\n",
    "            spot_ids = spatial_coords.index.tolist() #use index of dataframe if available\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        location_array = spatial_coords\n",
    "        if spot_ids is None:\n",
    "            spot_ids = list(range(location_array.shape[0])) #generate default ids if not provided\n",
    "\n",
    "    else:\n",
    "        raise TypeError('spatial_coords must be a pandas dataframe or a numpy array')\n",
    "    \n",
    "    print(f'constructing {graph_mode} graph for ST data with k={k_neighbors}.....')\n",
    "    Xgraph_st = construct_graph_spatial(location_array, k=k_neighbors, mode=graph_mode)\n",
    "    \n",
    "    if aware_st:\n",
    "        if spot_types is None or spot_ids is None:\n",
    "            raise ValueError('spot_types and spot_ids must be provided when aware_st=True')\n",
    "        if not isinstance(spot_types, pd.Series):\n",
    "            spot_types = pd.Series(spot_types, idnex=spot_ids) \n",
    "        print('applying type aware distance adjustment for ST data')\n",
    "        print(f'aware power for ST: {aware_power_st}')\n",
    "    else:\n",
    "        spot_types = None \n",
    "\n",
    "    print(f'calculating spatial distances.....')\n",
    "    D_st, st_max_distance = distances_cal_spatial(Xgraph_st, spot_ids=spot_ids, spot_types=spot_types, aware_power=aware_power_st)\n",
    "\n",
    "    print('D_st calculation complete')\n",
    "    return D_st, st_max_distance\n",
    "\n",
    "\n",
    "def calculate_D_st_euclidean(spatial_coords):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance matrix for ST spots.\n",
    "    \n",
    "    Args:\n",
    "        spatial_coords: (m_spots, 2) spatial coordinates\n",
    "        \n",
    "    Returns:\n",
    "        D_st_euclid: (m_spots, m_spots) normalized Euclidean distance matrix\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        coords_array = spatial_coords.values\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        coords_array = spatial_coords\n",
    "    else:\n",
    "        coords_array = np.array(spatial_coords)\n",
    "    \n",
    "    # Compute pairwise Euclidean distances\n",
    "    D_euclid = squareform(pdist(coords_array, metric='euclidean'))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    max_dist = D_euclid.max()\n",
    "    if max_dist > 0:\n",
    "        D_euclid = D_euclid / max_dist\n",
    "    \n",
    "    return D_euclid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patient 2 data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Key innovation: Use ALL ST data points for better training.\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "\n",
    "    # Store separate coordinate lists for block-diagonal graph\n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    st_expr_combined = scaler.fit_transform(st_expr_combined)\n",
    "\n",
    "    st_coords_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "\n",
    "    sc_expr = scaler.fit_transform(sc_expr)\n",
    "\n",
    "    \n",
    "    # Create dataset labels for tracking\n",
    "    dataset_labels = (['dataset1'] * len(st1_expr) + \n",
    "                     ['dataset2'] * len(st2_expr) + \n",
    "                     ['dataset3'] * len(st3_expr))\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Prepare combined data for diffusion\n",
    "X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = prepare_combined_st_for_diffusion(\n",
    "    stadata1, stadata2, stadata3, scadata\n",
    ")\n",
    "\n",
    "print(f\"Data preparation complete!\")\n",
    "print(f\"SC cells: {X_sc.shape[0]}\")\n",
    "print(f\"Combined ST spots: {X_st_combined.shape[0]}\")\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import cKDTree\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# =====================================================\n",
    "# PART X: Graph-VAE Components\n",
    "# =====================================================\n",
    "\n",
    "class GraphVAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph encoder that learns latent representations from ST spot graphs.\n",
    "    ⚠️ Do **not** touch `train_encoder`; its aligned embeddings are the sole conditioning signal throughout.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=128, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Two GraphConv layers as specified\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # MLP to output μ and log σ² FOR EACH NODE (not graph-level)\n",
    "        self.mu_head = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index, edge_weight=None, batch=None):\n",
    "        \"\"\"\n",
    "        x: node features (aligned embeddings E(X_st)) - shape (n_nodes, input_dim)\n",
    "        edge_index: graph edges from K-NN adjacency\n",
    "        edge_weight: optional edge weights\n",
    "        batch: not used since we want node-level representations\n",
    "        \n",
    "        Returns:\n",
    "        mu: (n_nodes, latent_dim)\n",
    "        logvar: (n_nodes, latent_dim)\n",
    "        \"\"\"\n",
    "        # Two GraphConv layers\n",
    "        h = torch.relu(self.conv1(x, edge_index, edge_weight))\n",
    "        h = torch.relu(self.conv2(h, edge_index, edge_weight))\n",
    "        \n",
    "        # NO GLOBAL POOLING - we want node-level representations\n",
    "        # Output μ and log σ² for each node\n",
    "        mu = self.mu_head(h)        # Shape: (n_nodes, latent_dim)\n",
    "        logvar = self.logvar_head(h)  # Shape: (n_nodes, latent_dim)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick - works element-wise\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "class GraphVAEDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph decoder that outputs 2D coordinates from latent z ONLY.\n",
    "    Features are NOT passed to force geometry into z.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=32, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Decoder takes ONLY latent z (no conditioning)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Output 2D coordinates\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: latent vectors (batch_size, latent_dim) ONLY\n",
    "        \"\"\"\n",
    "        coords = self.decoder(z)\n",
    "        return coords\n",
    "\n",
    "def precompute_knn_edges(coords, k=30, device='cuda'):\n",
    "    \"\"\"\n",
    "    Helper function to precompute K-NN edges for torch-geometric style layers.\n",
    "    Uses existing graph construction utilities where possible.\n",
    "    \"\"\"\n",
    "    if isinstance(coords, torch.Tensor):\n",
    "        coords_np = coords.cpu().numpy()\n",
    "    else:\n",
    "        coords_np = coords\n",
    "        \n",
    "    # Use existing construct_graph_spatial function\n",
    "    from sklearn.neighbors import kneighbors_graph\n",
    "    \n",
    "    # Build KNN graph\n",
    "    knn_graph = kneighbors_graph(\n",
    "        coords_np, \n",
    "        n_neighbors=k, \n",
    "        mode='connectivity', \n",
    "        include_self=False\n",
    "    )\n",
    "    \n",
    "    # Convert to torch-geometric format\n",
    "    from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "    edge_index, edge_weight = from_scipy_sparse_matrix(knn_graph)\n",
    "    \n",
    "    # CRITICAL FIX: Ensure correct dtypes\n",
    "    edge_index = edge_index.long().to(device)      # Edge indices should be long\n",
    "    edge_weight = edge_weight.float().to(device)   # Edge weights should be float32\n",
    "    \n",
    "    return edge_index, edge_weight\n",
    "\n",
    "class LatentDenoiser(nn.Module):\n",
    "    \"\"\"\n",
    "    Latent-space denoiser identical to current MLP/U-Net stack but for latent dim=32.\n",
    "    ⚠️ Do **not** touch `train_encoder`; its aligned embeddings are the sole conditioning signal throughout.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=32, condition_dim=128, hidden_dim=256, n_blocks=6):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.condition_dim = condition_dim\n",
    "        \n",
    "        # Time embedding (reuse existing SinusoidalEmbedding)\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Latent encoder\n",
    "        self.latent_encoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Condition encoder (for aligned embeddings)\n",
    "        self.condition_encoder = nn.Sequential(\n",
    "            nn.Linear(condition_dim, hidden_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Denoising blocks (similar to existing hierarchical blocks)\n",
    "        self.denoising_blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim)\n",
    "            ) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output head\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, z_noisy, t, condition):\n",
    "        \"\"\"\n",
    "        z_noisy: noisy latent vectors (batch_size, latent_dim)\n",
    "        t: timestep (batch_size,) - NOW 1D instead of 2D\n",
    "        condition: aligned embeddings E(X) (batch_size, condition_dim)\n",
    "        \"\"\"\n",
    "        batch_size = z_noisy.size(0)\n",
    "        # ENSURE inputs are 2D\n",
    "        if z_noisy.dim() > 2:\n",
    "            z_noisy = z_noisy.squeeze()\n",
    "        if condition.dim() > 2:\n",
    "            condition = condition.squeeze()\n",
    "            \n",
    "        # Handle 1D timestep input\n",
    "        if t.dim() == 1:\n",
    "            t = t.unsqueeze(1)  # Make it (batch_size, 1)\n",
    "\n",
    "        t = t.view(batch_size, 1)\n",
    "        \n",
    "        # Encode inputs\n",
    "        z_enc = self.latent_encoder(z_noisy)\n",
    "        t_enc = self.time_embed(t)\n",
    "        c_enc = self.condition_encoder(condition)\n",
    "        \n",
    "        # Combine features\n",
    "        h = z_enc + t_enc + c_enc\n",
    "        \n",
    "        # Apply denoising blocks\n",
    "        for block in self.denoising_blocks:\n",
    "            h = h + block(h)  # Residual connections\n",
    "            \n",
    "        # Output predicted noise\n",
    "        noise_pred = self.output_head(h)\n",
    "        return noise_pred\n",
    "\n",
    "# =====================================================\n",
    "# PART 1: Advanced Network Components\n",
    "# =====================================================\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, n_genes, n_embedding=[512, 256, 128], dp=0):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_genes, n_embedding[0])\n",
    "        self.bn1 = nn.LayerNorm(n_embedding[0])\n",
    "        self.fc2 = nn.Linear(n_embedding[0], n_embedding[1])\n",
    "        self.bn2 = nn.LayerNorm(n_embedding[1])\n",
    "        self.fc3 = nn.Linear(n_embedding[1], n_embedding[2])\n",
    "        \n",
    "        self.dp = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self, x, isdp=False):\n",
    "        if isdp:\n",
    "            x = self.dp(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, 1) or (batch_size,)\n",
    "        Returns: (batch_size, dim)\n",
    "        \"\"\"\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)  # Make (batch_size, 1)\n",
    "        \n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x * emb.unsqueeze(0)  # (batch_size, half_dim)\n",
    "        emb = torch.cat([emb.sin(), emb.cos()], dim=-1)  # (batch_size, dim)\n",
    "        \n",
    "        if emb.size(1) != self.dim:\n",
    "            # Handle odd dimensions\n",
    "            emb = emb[:, :self.dim]\n",
    "            \n",
    "        return emb\n",
    "\n",
    "import torch.optim as optim   \n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "# OT refinement function\n",
    "def refine_with_ot(sc_coords, st_coords, n_steps=50, lr=1e-2):\n",
    "    \"\"\"\n",
    "    Refines SC coordinates by minimizing entropic OT divergence to ST coords.\n",
    "    sc_coords: Tensor (N,2) initial SC coordinates\n",
    "    st_coords: Tensor (M,2) ST spot coordinates\n",
    "    \"\"\"\n",
    "    sinkhorn = SamplesLoss(\"sinkhorn\", p=2, blur=0.05, scaling=0.9)\n",
    "    coords = sc_coords.clone().detach().requires_grad_(True)\n",
    "    optimizer = optim.Adam([coords], lr=lr)\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss_ot = sinkhorn(coords.unsqueeze(0), st_coords.unsqueeze(0))\n",
    "        loss_ot.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return coords.detach()\n",
    "    \n",
    "class OTGuidedSampler:\n",
    "    def __init__(self,\n",
    "                 T_opt: torch.Tensor,\n",
    "                 st_coords_norm: torch.Tensor,\n",
    "                 n_timesteps: int):\n",
    "        self.T_opt       = T_opt           # (n_sc, n_st)\n",
    "        self.st_coords   = st_coords_norm  # (n_st, 2)\n",
    "        self.n_timesteps = n_timesteps\n",
    "\n",
    "    def get_ot_guidance(self, sc_indices: List[int], t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the expected OT‐based target for each sc index,\n",
    "        plus a small decaying jitter.\n",
    "        \"\"\"\n",
    "        if self.T_opt is None:\n",
    "            return None\n",
    "\n",
    "        guidance = []\n",
    "        for sc_idx in sc_indices:\n",
    "            st_w = self.T_opt[sc_idx]                 # (n_st,)\n",
    "            total = st_w.sum()\n",
    "            if total <= 0:\n",
    "                guidance.append(torch.zeros(2, device=self.st_coords.device))\n",
    "                continue\n",
    "            # expected spot location (no argmax sampling)\n",
    "            w_norm = st_w / total                    # normalize\n",
    "            target_mean = (w_norm.unsqueeze(1) * self.st_coords).sum(dim=0)\n",
    "            # decaying noise: maximal when t≈T, zero at t=0\n",
    "            noise_scale = 0.02 * (t / self.n_timesteps)\n",
    "            jitter = torch.randn_like(target_mean) * noise_scale\n",
    "            guidance.append(target_mean + jitter)\n",
    "\n",
    "        return torch.stack(guidance)  # (batch_size, 2)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "class CellTypeEmbedding(nn.Module):\n",
    "    \"\"\"Learned embeddings for cell types\"\"\"\n",
    "    def __init__(self, num_cell_types, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_cell_types, embedding_dim)\n",
    "        \n",
    "    def forward(self, cell_type_indices):\n",
    "        return self.embedding(cell_type_indices)\n",
    "\n",
    "class UncertaintyHead(nn.Module):\n",
    "    \"\"\"Predicts coordinate uncertainty\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)  # Uncertainty for x and y\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.softplus(self.net(x)) + 0.01  # Ensure positive uncertainty\n",
    "\n",
    "class PhysicsInformedLayer(nn.Module):\n",
    "    \"\"\"Incorporates cell non-overlap constraints\"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.radius_predictor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.repulsion_strength = nn.Parameter(torch.tensor(0.1))\n",
    "        \n",
    "    def compute_repulsion_gradient(self, coords, radii, cell_types=None):\n",
    "        \"\"\"Compute repulsion forces between cells\"\"\"\n",
    "        batch_size = coords.shape[0]\n",
    "        \n",
    "        # Compute pairwise distances\n",
    "        distances = torch.cdist(coords, coords, p=2)\n",
    "        \n",
    "        # Compute sum of radii for each pair\n",
    "        radii_sum = radii + radii.T\n",
    "        \n",
    "        # Compute overlap (positive when cells overlap)\n",
    "        overlap = F.relu(radii_sum - distances + 1e-6)\n",
    "        \n",
    "        # Mask out self-interactions\n",
    "        mask = (1 - torch.eye(batch_size, device=coords.device))\n",
    "        overlap = overlap * mask\n",
    "        \n",
    "        # Compute repulsion forces\n",
    "        coord_diff = coords.unsqueeze(1) - coords.unsqueeze(0)  # (B, B, 2)\n",
    "        distances_safe = distances + 1e-6  # Avoid division by zero\n",
    "        \n",
    "        # Normalize direction vectors\n",
    "        directions = coord_diff / distances_safe.unsqueeze(-1)\n",
    "        \n",
    "        # Apply stronger repulsion for same cell types (optional)\n",
    "        if cell_types is not None:\n",
    "            same_type_mask = (cell_types.unsqueeze(1) == cell_types.unsqueeze(0)).float()\n",
    "            repulsion_weight = 1.0 + 0.5 * same_type_mask  # 50% stronger for same type\n",
    "        else:\n",
    "            # repulsion_weight = 1.0\n",
    "            batch_size = coords.shape[0]\n",
    "            repulsion_weight = torch.ones(batch_size, batch_size, device=coords.device)\n",
    "            \n",
    "        # Compute repulsion magnitude\n",
    "        repulsion_magnitude = overlap.unsqueeze(-1) * repulsion_weight.unsqueeze(-1)\n",
    "        \n",
    "        # Sum repulsion forces from all other cells\n",
    "        repulsion_forces = (repulsion_magnitude * directions * mask.unsqueeze(-1)).sum(dim=1)\n",
    "        \n",
    "        return repulsion_forces\n",
    "        \n",
    "    def forward(self, coords, features, cell_types=None):\n",
    "        # Predict cell radii based on features\n",
    "        radii = self.radius_predictor(features).squeeze(-1) * 0.01  # Scale to reasonable size\n",
    "        \n",
    "        # Compute repulsion gradient\n",
    "        repulsion_grad = self.compute_repulsion_gradient(coords, radii, cell_types)\n",
    "        \n",
    "        return repulsion_grad * self.repulsion_strength, radii\n",
    "    \n",
    "class SpatialBatchSampler:\n",
    "    \"\"\"Sample spatially contiguous batches for geometric attention\"\"\"\n",
    "    \n",
    "    def __init__(self, coordinates, batch_size, k_neighbors=None):\n",
    "        \"\"\"\n",
    "        coordinates: (N, 2) array of spatial coordinates\n",
    "        batch_size: size of each batch\n",
    "        k_neighbors: number of neighbors to precompute (default: batch_size)\n",
    "        \"\"\"\n",
    "        self.coordinates = coordinates\n",
    "        self.batch_size = batch_size\n",
    "        self.k_neighbors = k_neighbors or min(batch_size, len(coordinates))\n",
    "        \n",
    "        # Precompute nearest neighbors\n",
    "        self.nbrs = NearestNeighbors(\n",
    "            n_neighbors=self.k_neighbors, \n",
    "            algorithm='kd_tree'\n",
    "        ).fit(coordinates)\n",
    "        \n",
    "    def sample_spatial_batch(self):\n",
    "        \"\"\"Sample a spatially contiguous batch\"\"\"\n",
    "        # Pick random center point\n",
    "        center_idx = np.random.randint(len(self.coordinates))\n",
    "        \n",
    "        # Get k nearest neighbors\n",
    "        distances, indices = self.nbrs.kneighbors(\n",
    "            self.coordinates[center_idx:center_idx+1], \n",
    "            return_distance=True\n",
    "        )\n",
    "        \n",
    "        # Return indices as torch tensor\n",
    "        batch_indices = torch.tensor(indices.flatten()[:self.batch_size], dtype=torch.long)\n",
    "        return batch_indices\n",
    "\n",
    "# =====================================================\n",
    "# PART 2: Hierarchical Diffusion Architecture\n",
    "# =====================================================\n",
    "\n",
    "class HierarchicalDiffusionBlock(nn.Module):\n",
    "    \"\"\"Multi-scale diffusion block for coarse-to-fine generation\"\"\"\n",
    "    def __init__(self, dim, num_scales=3):\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        \n",
    "        # Coarse-level predictor (for clusters/regions)\n",
    "        self.coarse_net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Fine-level predictor (for individual cells)\n",
    "        self.fine_net = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim * 2),  # Takes both coarse and fine features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 2, dim)\n",
    "        )\n",
    "        \n",
    "        # Scale mixing weights\n",
    "        self.scale_mixer = nn.Sequential(\n",
    "            nn.Linear(1, 64),  # Takes timestep\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_scales),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, coarse_context=None):\n",
    "        # Determine scale weights based on timestep\n",
    "        scale_weights = self.scale_mixer(t.unsqueeze(-1))\n",
    "        \n",
    "        # Coarse prediction\n",
    "        coarse_pred = self.coarse_net(x)\n",
    "        \n",
    "        # Fine prediction (conditioned on coarse if available)\n",
    "        if coarse_context is not None:\n",
    "            fine_input = torch.cat([x, coarse_context], dim=-1)\n",
    "        else:\n",
    "            fine_input = torch.cat([x, coarse_pred], dim=-1)\n",
    "        fine_pred = self.fine_net(fine_input)\n",
    "        \n",
    "        # Mix scales based on timestep\n",
    "        output = scale_weights[:, 0:1] * coarse_pred + scale_weights[:, 1:2] * fine_pred\n",
    "        \n",
    "        return output  \n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# PART 3: Main Advanced Diffusion Model\n",
    "# =====================================================\n",
    "\n",
    "class AdvancedHierarchicalDiffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        st_gene_expr,\n",
    "        st_coords,\n",
    "        sc_gene_expr,\n",
    "        cell_types_sc=None,  # Cell type labels for SC data\n",
    "        transport_plan=None,  # Optimal transport plan from domain alignment\n",
    "        D_st=None,\n",
    "        D_induced=None,\n",
    "        n_genes=None,\n",
    "        # n_embedding=128,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=200,\n",
    "        st_max_distance=None,\n",
    "        sc_max_distance=None,\n",
    "        sigma=3.0,\n",
    "        alpha=0.9,\n",
    "        mmdbatch=0.1,\n",
    "        batch_size=64,\n",
    "        device='cuda',\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=1000,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=512,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.1,\n",
    "        outf='output'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.diffusion_losses = {\n",
    "            'total': [],\n",
    "            'diffusion': [],\n",
    "            'struct': [],\n",
    "            'physics': [],\n",
    "            'uncertainty': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "\n",
    "        # Loss tracking for Graph-VAE training\n",
    "        self.vae_losses = {\n",
    "            'total': [],\n",
    "            'reconstruction': [],\n",
    "            'kl': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        # Loss tracking for Latent Diffusion training  \n",
    "        self.latent_diffusion_losses = {\n",
    "            'total': [],\n",
    "            'diffusion': [],\n",
    "            'struct': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        # Keep encoder losses separate (if you want to track them)\n",
    "        self.encoder_losses = {\n",
    "            'total': [],\n",
    "            'pred': [],\n",
    "            'circle': [],\n",
    "            'mmd': [],\n",
    "            'epochs': []\n",
    "        }\n",
    "        \n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.mmdbatch = mmdbatch\n",
    "        self.n_embedding = n_embedding\n",
    "        \n",
    "        # Create output directory\n",
    "        self.outf = outf\n",
    "        if not os.path.exists(outf):\n",
    "            os.makedirs(outf)\n",
    "        \n",
    "        # Store data\n",
    "        self.st_gene_expr = torch.tensor(st_gene_expr, dtype=torch.float32).to(device)\n",
    "        self.st_coords = torch.tensor(st_coords, dtype=torch.float32).to(device)\n",
    "        self.sc_gene_expr = torch.tensor(sc_gene_expr, dtype=torch.float32).to(device)\n",
    "\n",
    "        \n",
    "        # Temperature regularization for geometric attention\n",
    "        self.temp_weight_decay = 1e-4\n",
    "        \n",
    "        # Store transport plan if provided\n",
    "        self.transport_plan = torch.tensor(transport_plan, dtype=torch.float32).to(device) if transport_plan is not None else None\n",
    "        \n",
    "        # Process cell types\n",
    "        if cell_types_sc is not None:\n",
    "            # Convert cell type strings to indices\n",
    "            unique_cell_types = np.unique(cell_types_sc)\n",
    "            self.cell_type_to_idx = {ct: i for i, ct in enumerate(unique_cell_types)}\n",
    "            self.num_cell_types = len(unique_cell_types)\n",
    "            cell_type_indices = [self.cell_type_to_idx[ct] for ct in cell_types_sc]\n",
    "            self.sc_cell_types = torch.tensor(cell_type_indices, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            self.sc_cell_types = None\n",
    "            self.num_cell_types = 0\n",
    "            \n",
    "        # Store distance matrices\n",
    "        self.D_st = torch.tensor(D_st, dtype=torch.float32).to(device) if D_st is not None else None\n",
    "        self.D_induced = torch.tensor(D_induced, dtype=torch.float32).to(device) if D_induced is not None else None\n",
    "\n",
    "        # If D_st is not provided, calculate it from spatial coordinates\n",
    "        if self.D_st is None:\n",
    "            print(\"D_st not provided, calculating from spatial coordinates...\")\n",
    "            if isinstance(st_coords, torch.Tensor):\n",
    "                st_coords_np = st_coords.cpu().numpy()\n",
    "            else:\n",
    "                st_coords_np = st_coords\n",
    "            \n",
    "            D_st_np, st_max_distance = calculate_D_st_from_coords(\n",
    "                spatial_coords=st_coords_np, \n",
    "                k_neighbors=50, \n",
    "                graph_mode=\"distance\"\n",
    "            )\n",
    "            self.D_st = torch.tensor(D_st_np, dtype=torch.float32).to(device)\n",
    "            self.st_max_distance = st_max_distance\n",
    "            print(f\"D_st calculated, shape: {self.D_st.shape}\")\n",
    "\n",
    "\n",
    "        print(f\"Final matrices - D_st: {self.D_st.shape if self.D_st is not None else None}, \"\n",
    "            f\"D_induced: {self.D_induced.shape if self.D_induced is not None else None}\")\n",
    "        \n",
    "        # Normalize coordinates\n",
    "        self.st_coords_norm, self.coords_center, self.coords_radius = self.normalize_coordinates_isotropic(self.st_coords)        \n",
    "        # Model parameters\n",
    "        self.n_genes = n_genes or st_gene_expr.shape[1]\n",
    "        \n",
    "        # ========== FEATURE ENCODER ==========\n",
    "        self.netE = self.build_feature_encoder(self.n_genes, n_embedding, dp)\n",
    "\n",
    "        self.train_log = os.path.join(outf, 'train.log')\n",
    "\n",
    "        \n",
    "        # ========== CELL TYPE EMBEDDING ==========\n",
    "\n",
    "        use_cell_types = (cell_types_sc is not None)  # Check if SC data has cell types\n",
    "        self.use_cell_types = use_cell_types\n",
    "\n",
    "        if self.num_cell_types > 0:\n",
    "            self.cell_type_embedding = CellTypeEmbedding(self.num_cell_types, n_embedding[-1] // 2)\n",
    "            total_feature_dim = n_embedding[-1] + n_embedding[-1] // 2\n",
    "        else:\n",
    "            self.cell_type_embedding = None\n",
    "            total_feature_dim = n_embedding[-1]\n",
    "            \n",
    "        # ========== HIERARCHICAL DIFFUSION COMPONENTS ==========\n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Feature projection (includes cell type if available)\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(total_feature_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        # ========== GRAPH-VAE COMPONENTS (REPLACING HIERARCHICAL DIFFUSION) ==========        \n",
    "        # Graph-VAE parameters\n",
    "        self.latent_dim = 32  # As specified in instructions\n",
    "        \n",
    "        # Graph-VAE Encoder (learns latent representations from ST graphs)\n",
    "        self.graph_vae_encoder = GraphVAEEncoder(\n",
    "            input_dim=n_embedding[-1],  # Aligned embedding dimension\n",
    "            hidden_dim=128,             # GraphConv hidden dimension  \n",
    "            latent_dim=self.latent_dim\n",
    "        ).to(device)\n",
    "\n",
    "        self.graph_vae_decoder = GraphVAEDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            hidden_dim=128  # Remove condition_dim\n",
    "        ).to(device)\n",
    "        \n",
    "        # Latent Denoiser (replaces hierarchical_blocks)\n",
    "        self.latent_denoiser = LatentDenoiser(\n",
    "            latent_dim=self.latent_dim,\n",
    "            condition_dim=n_embedding[-1],\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_blocks=n_denoising_blocks\n",
    "        ).to(device)\n",
    "        \n",
    "        # ========== HIERARCHICAL DENOISING BLOCKS ==========\n",
    "        self.hierarchical_blocks = nn.ModuleList([\n",
    "            HierarchicalDiffusionBlock(hidden_dim, num_hierarchical_scales)\n",
    "            for _ in range(n_denoising_blocks)\n",
    "        ])    \n",
    "\n",
    "        # ========== PHYSICS-INFORMED COMPONENTS ==========\n",
    "        self.physics_layer = PhysicsInformedLayer(hidden_dim)\n",
    "        \n",
    "        # ========== UNCERTAINTY QUANTIFICATION ==========\n",
    "        self.uncertainty_head = UncertaintyHead(hidden_dim)\n",
    "        \n",
    "        # ========== OPTIMAL TRANSPORT GUIDANCE ==========\n",
    "        if self.transport_plan is not None:\n",
    "            self.ot_guidance_strength = nn.Parameter(torch.tensor(0.1))\n",
    "            \n",
    "        # ========== OUTPUT LAYERS ==========\n",
    "        self.noise_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "        \n",
    "        # Create noise schedule\n",
    "        # self.noise_schedule = self.create_noise_schedule()\n",
    "        self.noise_schedule = self.build_noise_schedule(self.n_timesteps)\n",
    "\n",
    "        self.guidance_scale = 2.0\n",
    "        \n",
    "        # Optimizers\n",
    "        self.setup_optimizers(lr_e, lr_d)\n",
    "        \n",
    "        # MMD Loss for domain alignment\n",
    "        self.mmd_loss = MMDLoss()\n",
    "\n",
    "        # Move entire model to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def build_noise_schedule(self, T, beta_start=1e-4, beta_end=2e-2):\n",
    "        \"\"\"Rebuild noise schedule when T changes\"\"\"\n",
    "        betas = torch.linspace(beta_start, beta_end, T, device=self.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=self.device), alphas_cumprod[:-1]], dim=0)\n",
    "        posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'alphas_cumprod_prev': alphas_cumprod_prev,\n",
    "            'posterior_variance': posterior_variance\n",
    "        }\n",
    "\n",
    "    def update_noise_schedule(self):\n",
    "        \"\"\"Update noise schedule when n_timesteps changes\"\"\"\n",
    "        self.noise_schedule = self.build_noise_schedule(self.n_timesteps)\n",
    "        print(f\"Updated noise schedule for T={self.n_timesteps}\")\n",
    "        print(f\"alpha_bar[0]={self.noise_schedule['alphas_cumprod'][0]:.6f}, alpha_bar[-1]={self.noise_schedule['alphas_cumprod'][-1]:.6f}\")\n",
    "\n",
    "    def setup_spatial_sampling(self):\n",
    "        if hasattr(self, 'st_coords_norm'):\n",
    "            self.spatial_sampler = SpatialBatchSampler(\n",
    "                coordinates=self.st_coords_norm.cpu().numpy(),\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "        else:\n",
    "            self.spatial_sampler = None\n",
    "\n",
    "    def get_spatial_batch(self):\n",
    "        \"\"\"Get spatially contiguous batch for training\"\"\"\n",
    "        if self.spatial_sampler is not None:\n",
    "            return self.spatial_sampler.sample_spatial_batch()\n",
    "        else:\n",
    "            # Fallback to random sampling\n",
    "            return torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "        \n",
    "    def _evaluate_sigma_quality(self, st_embeddings, k=10):\n",
    "        \"\"\"Evaluate how well encoder embeddings preserve spatial k-NN structure\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get k-NN from encoder similarity\n",
    "            netpred = st_embeddings.mm(st_embeddings.t())\n",
    "            pred_knn = self._get_knn_from_similarity(netpred, k=k)\n",
    "            \n",
    "            # Get k-NN from physical coordinates  \n",
    "            phys_knn = self._get_knn_from_coords(self.st_coords_norm, k=k)\n",
    "            \n",
    "            # Compute overlap\n",
    "            overlap = (pred_knn == phys_knn).float().mean().item()\n",
    "            return overlap\n",
    "\n",
    "    def _get_knn_from_similarity(self, similarity_matrix, k=10):\n",
    "        \"\"\"Extract top-k neighbors from similarity matrix\"\"\"\n",
    "        # Get top-k indices for each node\n",
    "        _, topk_indices = torch.topk(similarity_matrix, k=k+1, dim=1)  # +1 to exclude self\n",
    "        topk_indices = topk_indices[:, 1:]  # Remove self-connections\n",
    "        return topk_indices\n",
    "\n",
    "    def _get_knn_from_coords(self, coords, k=10):\n",
    "        \"\"\"Extract top-k spatial neighbors from coordinates\"\"\"\n",
    "        # Compute pairwise distances\n",
    "        distances = torch.cdist(coords, coords)\n",
    "        # Get top-k closest (smallest distances)\n",
    "        _, topk_indices = torch.topk(distances, k=k+1, dim=1, largest=False)  # +1 for self\n",
    "        topk_indices = topk_indices[:, 1:]  # Remove self-connections  \n",
    "        return topk_indices\n",
    "        \n",
    "    def normalize_coordinates_isotropic(self, coords):\n",
    "        \"\"\"Normalize coordinates isotropically to [-1, 1]\"\"\"\n",
    "        center = coords.mean(dim=0)\n",
    "        centered_coords = coords - center\n",
    "        max_dist = torch.max(torch.norm(centered_coords, dim=1))\n",
    "        normalized_coords = centered_coords / (max_dist + 1e-8)\n",
    "        return normalized_coords, center, max_dist\n",
    "        \n",
    "\n",
    "    def build_feature_encoder(self, n_genes, n_embedding, dp):\n",
    "        \"\"\"Build the feature encoder network\"\"\"\n",
    "        return FeatureNet(n_genes, n_embedding=n_embedding, dp=dp).to(self.device)\n",
    "        \n",
    "    def create_noise_schedule(self):\n",
    "        \"\"\"Create the noise schedule for diffusion\"\"\"\n",
    "        betas = torch.linspace(0.0001, 0.02, self.n_timesteps, device=self.device)\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'sqrt_alphas_cumprod': torch.sqrt(alphas_cumprod),\n",
    "            'sqrt_one_minus_alphas_cumprod': torch.sqrt(1 - alphas_cumprod)\n",
    "        }\n",
    "        \n",
    "    def setup_optimizers(self, lr_e, lr_d):\n",
    "        \"\"\"Setup optimizers and schedulers\"\"\"\n",
    "        # Encoder optimizer\n",
    "        self.optimizer_E = torch.optim.AdamW(self.netE.parameters(), lr=0.002)               \n",
    "        self.scheduler_E = lr_scheduler.StepLR(self.optimizer_E, step_size=200, gamma=0.5) \n",
    "\n",
    "        # MMD Loss\n",
    "        self.mmd_fn = MMDLoss()   \n",
    "        \n",
    "        # Diffusion model optimizer\n",
    "        diff_params = []\n",
    "        diff_params.extend(self.time_embed.parameters())\n",
    "        diff_params.extend(self.coord_encoder.parameters())\n",
    "        diff_params.extend(self.feat_proj.parameters())\n",
    "        diff_params.extend(self.hierarchical_blocks.parameters())\n",
    "        # diff_params.extend(self.geometric_attention_blocks.parameters())\n",
    "        diff_params.extend(self.physics_layer.parameters())\n",
    "        diff_params.extend(self.uncertainty_head.parameters())\n",
    "        diff_params.extend(self.noise_predictor.parameters())\n",
    "        \n",
    "        if self.cell_type_embedding is not None:\n",
    "            diff_params.extend(self.cell_type_embedding.parameters())\n",
    "            \n",
    "        if self.transport_plan is not None:\n",
    "            diff_params.append(self.ot_guidance_strength)\n",
    "            \n",
    "        self.optimizer_diff = torch.optim.Adam(diff_params, lr=lr_d, betas=(0.9, 0.999))\n",
    "        self.scheduler_diff = lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer_diff, T_0=500)\n",
    "        \n",
    "    def add_noise(self, coords, t, noise_schedule):\n",
    "        \"\"\"Add noise to coordinates according to the diffusion schedule\"\"\"\n",
    "        noise = torch.randn_like(coords)\n",
    "        sqrt_alphas_cumprod_t = noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "        \n",
    "        noisy_coords = sqrt_alphas_cumprod_t * coords + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return noisy_coords, noise\n",
    "        \n",
    "        \n",
    "    def forward_diffusion(self, noisy_coords, t, features, cell_types=None):\n",
    "        \"\"\"Forward pass through the advanced diffusion model\"\"\"\n",
    "        batch_size = noisy_coords.shape[0]\n",
    "        \n",
    "        # Encode inputs\n",
    "        time_emb = self.time_embed(t)\n",
    "        coord_emb = self.coord_encoder(noisy_coords)\n",
    "        \n",
    "        # Process features with optional cell type\n",
    "        if cell_types is not None and self.cell_type_embedding is not None:\n",
    "            cell_type_emb = self.cell_type_embedding(cell_types)\n",
    "            combined_features = torch.cat([features, cell_type_emb], dim=-1)\n",
    "        else:\n",
    "            #when no cell types, pad with zeros to match expected input size\n",
    "            if self.cell_type_embedding is not None:\n",
    "                #create zero padding for cell type embedding\n",
    "                cell_type_dim = self.n_embedding[-1] // 2\n",
    "                zero_padding = torch.zeros(batch_size, cell_type_dim, device=features.device)\n",
    "                combined_features = torch.cat([features, zero_padding], dim=-1)\n",
    "            else:\n",
    "                combined_features = features\n",
    "            # combined_features = features\n",
    "            \n",
    "        feat_emb = self.feat_proj(combined_features)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        h = coord_emb + time_emb + feat_emb\n",
    "        \n",
    "        # Process through hierarchical blocks with geometric attention\n",
    "        for i, block in enumerate(self.hierarchical_blocks):\n",
    "            h = block(h, t)\n",
    "                \n",
    "        # Predict noise\n",
    "        noise_pred = self.noise_predictor(h)\n",
    "        \n",
    "        # Compute physics-informed correction\n",
    "        physics_correction, cell_radii = self.physics_layer(noisy_coords, h, cell_types)\n",
    "        \n",
    "        # Compute uncertainty\n",
    "        uncertainty = self.uncertainty_head(h)\n",
    "        \n",
    "        # Apply corrections based on timestep (less physics at high noise)\n",
    "        t_factor = (1 - t).unsqueeze(-1) #shape: (natch_size, 1)\n",
    "        noise_pred = noise_pred + t_factor * physics_correction * 0.1\n",
    "        \n",
    "        return noise_pred, uncertainty, cell_radii\n",
    "        \n",
    "    def train_encoder(self, n_epochs=1000, ratio_start=0, ratio_end=1.0):\n",
    "        \"\"\"Train the STEM encoder to align ST and SC data\"\"\"\n",
    "        print(\"Training STEM encoder...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting STEM encoder training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, ratio_start={ratio_start}, ratio_end={ratio_end}\\n\")\n",
    "        \n",
    "        # Calculate spatial adjacency matrix\n",
    "        if self.sigma == 0:\n",
    "            nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        else:\n",
    "            nettrue = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                self.st_coords.cpu().numpy(), \n",
    "                self.st_coords.cpu().numpy()\n",
    "            ), device=self.device).to(torch.float32)\n",
    "            \n",
    "            sigma = self.sigma\n",
    "            nettrue = torch.exp(-nettrue**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "            nettrue = F.normalize(nettrue, p=1, dim=1)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Schedule for circle loss weight\n",
    "            ratio = ratio_start + (ratio_end - ratio_start) * min(epoch / (n_epochs * 0.8), 1.0)\n",
    "            \n",
    "            # Forward pass ST data\n",
    "            e_seq_st = self.netE(self.st_gene_expr, True)\n",
    "            \n",
    "            # Sample from SC data due to large size\n",
    "            sc_idx = torch.randint(0, self.sc_gene_expr.shape[0], (min(self.batch_size, self.mmdbatch),), device=self.device)\n",
    "            sc_batch = self.sc_gene_expr[sc_idx]\n",
    "            e_seq_sc = self.netE(sc_batch, False)\n",
    "            \n",
    "            # Calculate losses\n",
    "            self.optimizer_E.zero_grad()\n",
    "            \n",
    "            # Prediction loss (equivalent to netpred in STEM)\n",
    "            netpred = e_seq_st.mm(e_seq_st.t())\n",
    "            loss_E_pred = F.cross_entropy(netpred, nettrue, reduction='mean')\n",
    "            \n",
    "            # Mapping matrices\n",
    "            st2sc = F.softmax(e_seq_st.mm(e_seq_sc.t()), dim=1)\n",
    "            sc2st = F.softmax(e_seq_sc.mm(e_seq_st.t()), dim=1)\n",
    "            \n",
    "            # Circle loss\n",
    "            st2st = torch.log(st2sc.mm(sc2st) + 1e-7)\n",
    "            loss_E_circle = F.kl_div(st2st, nettrue, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # MMD loss\n",
    "            ranidx = torch.randint(0, e_seq_sc.shape[0], (min(self.mmdbatch, e_seq_sc.shape[0]),), device=self.device)\n",
    "            loss_E_mmd = self.mmd_fn(e_seq_st, e_seq_sc[ranidx])\n",
    "            \n",
    "            # Total loss\n",
    "            loss_E = loss_E_pred + self.alpha * loss_E_mmd + ratio * loss_E_circle\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_E.backward()\n",
    "            self.optimizer_E.step()\n",
    "            self.scheduler_E.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 200 == 0:\n",
    "                log_msg = (f\"Encoder epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss_E: {loss_E.item():.6f}, \"\n",
    "                          f\"Loss_E_pred: {loss_E_pred.item():.6f}, \"\n",
    "                          f\"Loss_E_circle: {loss_E_circle.item():.6f}, \"\n",
    "                          f\"Loss_E_mmd: {loss_E_mmd.item():.6f}, \"\n",
    "                          f\"Ratio: {ratio:.4f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'netE_state_dict': self.netE.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_E.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_E.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'encoder_checkpoint_epoch_{epoch}.pt'))\n",
    "    \n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUATING SIGMA QUALITY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Evaluate current sigma\n",
    "        with torch.no_grad():\n",
    "            self.netE.eval()\n",
    "            st_embeddings = self.netE(self.st_gene_expr, True)  # Get final ST embeddings\n",
    "            current_overlap = self._evaluate_sigma_quality(st_embeddings, k=10)\n",
    "            print(f\"Current sigma ({self.sigma:.4f}) -> kNN overlap = {current_overlap:.3f}\")\n",
    "        \n",
    "        # Test different sigma values to find optimal\n",
    "        print(\"\\nTesting different sigma values...\")\n",
    "        sigma_candidates = [\n",
    "            self.sigma * 0.5,   # Half current\n",
    "            self.sigma * 0.75,  # 3/4 current  \n",
    "            self.sigma,         # Current (baseline)\n",
    "            self.sigma * 1.25,  # 5/4 current\n",
    "            self.sigma * 1.5,   # 1.5x current\n",
    "            self.sigma * 2.0,    # Double current\n",
    "            self.sigma * 2.5,   # Double current\n",
    "            self.sigma * 3.0,    # Double current\n",
    "            self.sigma * 4.0    # Double current\n",
    "\n",
    "        ]\n",
    "        \n",
    "        overlaps = []\n",
    "        for test_sigma in sigma_candidates:\n",
    "            # Recompute adjacency with test sigma\n",
    "            if test_sigma == 0:\n",
    "                test_nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "            else:\n",
    "                distances = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                    self.st_coords.cpu().numpy(), \n",
    "                    self.st_coords.cpu().numpy()\n",
    "                ), device=self.device).to(torch.float32)\n",
    "                \n",
    "                test_nettrue = torch.exp(-distances**2/(2*test_sigma**2))/(np.sqrt(2*np.pi)*test_sigma)\n",
    "                test_nettrue = F.normalize(test_nettrue, p=1, dim=1)\n",
    "            \n",
    "            # Quick test: how well does current encoder match this adjacency?\n",
    "            with torch.no_grad():\n",
    "                netpred = st_embeddings.mm(st_embeddings.t())\n",
    "                pred_knn = self._get_knn_from_similarity(netpred, k=15)\n",
    "                true_knn = self._get_knn_from_similarity(test_nettrue, k=15)\n",
    "                overlap = (pred_knn == true_knn).float().mean().item()\n",
    "                overlaps.append(overlap)\n",
    "                \n",
    "            print(f\"  sigma = {test_sigma:.4f} -> overlap = {overlap:.5f}\")\n",
    "        \n",
    "        # Find best sigma\n",
    "        best_idx = np.argmax(overlaps)\n",
    "        best_sigma = sigma_candidates[best_idx]\n",
    "        best_overlap = overlaps[best_idx]\n",
    "\n",
    "        # print(overlaps)\n",
    "        \n",
    "        print(f\"\\nBest sigma: {best_sigma:.4f} (overlap = {best_overlap:.5f})\")\n",
    "        if best_sigma != self.sigma:\n",
    "            print(f\"⚠️  Consider using sigma = {best_sigma:.4f} instead of {self.sigma:.4f}\")\n",
    "            print(f\"   Improvement: {best_overlap:.3f} vs {current_overlap:.3f} (+{(best_overlap-current_overlap)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"✅ Current sigma is optimal!\")\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        # ===================================\n",
    "        \n",
    "        # Save final encoder\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_encoder.pt'))\n",
    "        \n",
    "        print(\"Encoder training complete!\")\n",
    "\n",
    "    def train_graph_vae(self, epochs=800, lr=1e-3, warmup_epochs=320,  # 40% of epochs\n",
    "                    lambda_cov_max=0.3, angle_loss_weight=0.2, \n",
    "                    radius_loss_weight=1.0, angle_warmup_epochs=0, \n",
    "                    beta_final=1e-3):  # Small β_final to prevent blow-up\n",
    "        \"\"\"\n",
    "        Train the Graph-VAE with:\n",
    "        1) normalized KL (no free-bits, mean over batch and dims)\n",
    "        2) covariance warm-up (fix axes)  \n",
    "        3) geometry-anchored angle/radius loss (fix gauge deterministically)\n",
    "        4) NO reconstruction loss, NO gradient loss (as requested)\n",
    "        \"\"\"\n",
    "        print(\"Training Graph-VAE with normalized KL and polar losses...\")\n",
    "        \n",
    "        # Freeze encoder\n",
    "        self.netE.eval()\n",
    "        for p in self.netE.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # Build ST graph\n",
    "        adj_idx, adj_w = precompute_knn_edges(self.st_coords_norm, k=30, device=self.device)\n",
    "\n",
    "        # Precompute aligned features (ST only for training)\n",
    "        with torch.no_grad():\n",
    "            st_features_aligned = self.netE(self.st_gene_expr).float()\n",
    "\n",
    "        # Precompute canonical frame and angle/radius targets for ST\n",
    "        print('Computing canonical angular frame for geometry anchoring....')\n",
    "        c, a_theta, R, theta_true, r_true = self._compute_canonical_frame(self.st_coords_norm)\n",
    "\n",
    "        # Precompute true covariance of ST spots\n",
    "        with torch.no_grad():\n",
    "            centered = self.st_coords_norm - self.st_coords_norm.mean(0, keepdim=True)\n",
    "            self.cov_true = (centered.T @ centered) / (centered.shape[0] - 1)\n",
    "\n",
    "        # Optimizer + scheduler\n",
    "        vae_params = list(self.graph_vae_encoder.parameters()) + list(self.graph_vae_decoder.parameters())\n",
    "        optimizer = torch.optim.Adam(vae_params, lr=lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "        # Initialize loss logs\n",
    "        for key in ('total','reconstruction','kl','kl_raw','cov','lambda_cov','angle','radius','beta','epochs'):\n",
    "            self.vae_losses.setdefault(key, [])\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute loss weights with warmups\n",
    "            lambda_cov = lambda_cov_max * min(epoch+1, warmup_epochs) / warmup_epochs\n",
    "            w_theta = 0.0 if epoch < angle_warmup_epochs else angle_loss_weight\n",
    "            w_radius = 0.0 if epoch < angle_warmup_epochs else radius_loss_weight\n",
    "            \n",
    "            # KL warm-up: β from 0 to β_final (small value)\n",
    "            beta = beta_final * min(epoch+1, warmup_epochs) / warmup_epochs\n",
    "\n",
    "            # Forward pass for ST spots only\n",
    "            mu_st, logvar_st = self.graph_vae_encoder(st_features_aligned, adj_idx, adj_w)\n",
    "            z_st = self.graph_vae_encoder.reparameterize(mu_st, logvar_st)\n",
    "            \n",
    "            # Decoder takes ONLY z (no features)\n",
    "            coords_pred_st = self.graph_vae_decoder(z_st)\n",
    "            \n",
    "            # 1) NO Reconstruction loss (as requested)\n",
    "            L_recon = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "            # 2) Normalized KL divergence (Option A - no free-bits)\n",
    "            # KL per dim (positive values)\n",
    "            kl_per_dim = -0.5 * (1 + logvar_st - mu_st.pow(2) - logvar_st.exp())  # [N, latent_dim]\n",
    "            \n",
    "            # Mean over batch AND latent dims (this prevents blow-up)\n",
    "            KL_raw = kl_per_dim.mean()  # O(1) scale, not O(latent_dim)\n",
    "            L_KL = beta * KL_raw\n",
    "\n",
    "            # 3) Covariance alignment (ST spots only, warm-up)\n",
    "            coords_pred_centered = coords_pred_st - coords_pred_st.mean(0, keepdim=True)\n",
    "            cov_pred = (coords_pred_centered.T @ coords_pred_centered) / (coords_pred_centered.shape[0] - 1)\n",
    "            L_cov = F.mse_loss(cov_pred, self.cov_true)\n",
    "\n",
    "            # 4) NO Gradient anchoring (as requested)\n",
    "            # L_grad = 0.0\n",
    "\n",
    "            # 5) Geometry-anchored angle/radius losses\n",
    "            # Compute predicted angles/radii using the SAME canonical frame (c, a_theta, R)\n",
    "            v_hat = coords_pred_st - c \n",
    "            cross = a_theta[0] * v_hat[:, 1] - a_theta[1] * v_hat[:, 0]\n",
    "            dot = a_theta[0] * v_hat[:, 0] + a_theta[1] * v_hat[:, 1]\n",
    "            theta_hat = torch.atan2(cross, dot)\n",
    "            r_hat = v_hat.norm(dim=1) / (R + 1e-8)\n",
    "\n",
    "            # Circular angle loss: 1 - cos(delta)\n",
    "            delta = theta_hat - theta_true\n",
    "            L_angle_raw = (1.0 - torch.cos(delta)).mean()\n",
    "            L_angle = w_theta * L_angle_raw\n",
    "\n",
    "            # Radius loss\n",
    "            L_radius_raw = F.mse_loss(r_hat, r_true)\n",
    "            L_radius = w_radius * L_radius_raw\n",
    "\n",
    "            # 6) Total loss\n",
    "            total_loss = (\n",
    "                L_recon +           # 0.0 as requested\n",
    "                L_KL +              # Small, normalized\n",
    "                lambda_cov * L_cov +\n",
    "                L_angle + \n",
    "                L_radius\n",
    "            )\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Log losses\n",
    "            self.vae_losses['total'].append(total_loss.item())\n",
    "            self.vae_losses['reconstruction'].append(L_recon.item())\n",
    "            self.vae_losses['kl'].append(L_KL.item())\n",
    "            self.vae_losses['kl_raw'].append(KL_raw.item())\n",
    "            self.vae_losses['cov'].append(L_cov.item())\n",
    "            self.vae_losses['lambda_cov'].append(lambda_cov)\n",
    "            self.vae_losses['angle'].append(L_angle.item())\n",
    "            self.vae_losses['radius'].append(L_radius.item())\n",
    "            self.vae_losses['beta'].append(beta)\n",
    "            self.vae_losses['epochs'].append(epoch)\n",
    "\n",
    "            if epoch % 100 == 0 or epoch == epochs-1:\n",
    "                # Compute interpretable angle error in degrees\n",
    "                with torch.no_grad():\n",
    "                    mean_angle_err_deg = torch.mean(torch.abs(torch.rad2deg(torch.atan2(torch.sin(delta), torch.cos(delta))))).item()\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs}  \"\n",
    "                    f\"Loss={total_loss:.4f}  \"\n",
    "                    f\"L_recon={L_recon:.4f}  \"\n",
    "                    f\"L_KL={L_KL:.6f}(β={beta:.6f})  \"\n",
    "                    f\"KL_raw={KL_raw:.4f}  \"\n",
    "                    f\"L_cov={L_cov:.4f}  \"\n",
    "                    f\"L_angle={L_angle:.4f}  \"\n",
    "                    f\"L_radius={L_radius:.4f}  \"\n",
    "                    f\"AngleErr={mean_angle_err_deg:.2f}°\")\n",
    "\n",
    "        print(\"Graph-VAE training complete.\")\n",
    "        \n",
    "        # Diagnostic: Check if decoder uses z meaningfully\n",
    "        print(\"\\n=== DECODER DEPENDENCY DIAGNOSTIC ===\")\n",
    "        with torch.no_grad():\n",
    "            # Fix features, vary z\n",
    "            mu_fixed = mu_st[:5]\n",
    "            logvar_fixed = logvar_st[:5]\n",
    "            \n",
    "            coords_samples = []\n",
    "            for _ in range(5):\n",
    "                z_sample = self.graph_vae_encoder.reparameterize(mu_fixed, logvar_fixed)\n",
    "                coords_sample = self.graph_vae_decoder(z_sample)\n",
    "                coords_samples.append(coords_sample)\n",
    "            \n",
    "            coords_stack = torch.stack(coords_samples)  # (5, 5, 2)\n",
    "            coord_variance = coords_stack.var(dim=0).mean().item()  # Average variance across samples\n",
    "            \n",
    "            print(f\"Coordinate variance from z sampling: {coord_variance:.6f}\")\n",
    "            if coord_variance < 1e-4:\n",
    "                print(\"⚠️  WARNING: Low variance suggests potential posterior collapse!\")\n",
    "            else:\n",
    "                print(\"✅ Good: Decoder shows dependency on z\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "    def _compute_canonical_frame(self, X_st):\n",
    "        '''compute canonical angular frame from ST coordinates for geometry anchoring\n",
    "        \n",
    "        returns:\n",
    "            c: centroid (2, )\n",
    "            a_theta: reference direction vector (2, )\n",
    "            R: max radius (scalar)\n",
    "            theta_true: true_angles (N, )\n",
    "            r_true: true normalized radii (N,)\n",
    "        '''\n",
    "        #centroid\n",
    "        c = X_st.mean(dim=0)\n",
    "\n",
    "        #find farthest point to define reference direction\n",
    "        d = torch.linalg.norm(X_st - c, dim=1)\n",
    "        A = torch.argmax(d).item()\n",
    "        a_theta = (X_st[A] - c)\n",
    "        R = d.max().clamp_min(1e-8)\n",
    "\n",
    "        #compute true angles and radii for all points\n",
    "        v = X_st - c\n",
    "        cross = a_theta[0] * v[:, 1] - a_theta[1] * v[:, 0]\n",
    "        dot = a_theta[0] * v[:, 0] + a_theta[1] * v[:, 1]\n",
    "        theta_true = torch.atan2(cross, dot)\n",
    "        r_true = (v.norm(dim=1) / R)\n",
    "\n",
    "        print(f\"Canonical frame: center=({c[0]:.3f}, {c[1]:.3f}), \"\n",
    "            f\"ref_dir=({a_theta[0]:.3f}, {a_theta[1]:.3f}), max_radius={R:.3f}\")\n",
    "        \n",
    "        return c, a_theta, R, theta_true, r_true\n",
    "    \n",
    "    def _compute_spatial_pc1(self):\n",
    "        \"\"\"\n",
    "        Compute first spatial principal component from continuous SVGs for anchoring\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "        from sklearn.decomposition import TruncatedSVD\n",
    "        \n",
    "        # Step 1: Filter to continuous genes (5%-95% expression)\n",
    "        st_expr = self.st_gene_expr.cpu().numpy()\n",
    "        nonzero_frac = (st_expr > 0).mean(0)\n",
    "        mask = (nonzero_frac >= 0.05) & (nonzero_frac <= 0.95)\n",
    "        expr_cont = st_expr[:, mask]\n",
    "        \n",
    "        print(f\"Filtered to {expr_cont.shape[1]} continuous genes from {st_expr.shape[1]} total\")\n",
    "        \n",
    "        if expr_cont.shape[1] < 10:\n",
    "            print(\"Warning: Too few continuous genes, using all genes\")\n",
    "            expr_cont = st_expr\n",
    "        \n",
    "        # Step 2: Smooth expression and compute PCA\n",
    "        expr_smooth = gaussian_filter(expr_cont, sigma=(1, 0))  # smooth over spots only\n",
    "        \n",
    "        # Compute first PC\n",
    "        svd = TruncatedSVD(n_components=1, random_state=42)\n",
    "        pc1 = svd.fit_transform(expr_smooth).flatten()\n",
    "        \n",
    "        print(f\"PC-1 explains {svd.explained_variance_ratio_[0]:.3f} of spatial variance\")\n",
    "        \n",
    "        return torch.tensor(pc1, device=self.device, dtype=torch.float32)\n",
    "\n",
    "    def train_diffusion_latent(self, n_epochs=400, lambda_struct=10.0, p_drop=0.05, posterior_temp_floor=0.6):\n",
    "        \"\"\"\n",
    "        Train latent-space conditional DDPM as a proper conditional prior p(z|h).\n",
    "        \"\"\"\n",
    "        print(\"Training latent-space diffusion model...\")\n",
    "        \n",
    "        # Freeze encoder and Graph-VAE encoder\n",
    "        self.netE.eval()\n",
    "        self.graph_vae_encoder.eval()\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.graph_vae_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Precompute fixed ST latents as specified\n",
    "        print(\"Computing fixed ST latents...\")\n",
    "        st_adj_idx, st_adj_w = precompute_knn_edges(self.st_coords_norm, k=30, device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            st_features_aligned = self.netE(self.st_gene_expr).float()\n",
    "            st_mu, st_logvar = self.graph_vae_encoder(st_features_aligned, st_adj_idx, st_adj_w)\n",
    "            # z_st = self.graph_vae_encoder.reparameterize(st_mu, st_logvar)\n",
    "        \n",
    "        # Setup optimizer for latent denoiser\n",
    "        optimizer_latent = torch.optim.AdamW(\n",
    "            self.latent_denoiser.parameters(), \n",
    "            lr=self.optimizer_diff.param_groups[0]['lr'], \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        scheduler_latent = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer_latent, T_max=n_epochs, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Training loop - identical to old train_diffusion but in latent space\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch indices\n",
    "            idx = torch.randperm(len(st_mu))[:self.batch_size]\n",
    "            batch_mu = st_mu[idx]\n",
    "            batch_logvar = st_logvar[idx]\n",
    "            batch_h = st_features_aligned[idx]\n",
    "            \n",
    "            # Sample FRESH z0 from posterior (with temperature floor)\n",
    "            eps0 = torch.randn_like(batch_mu)\n",
    "            posterior_std = torch.sqrt(torch.exp(batch_logvar) + posterior_temp_floor**2)\n",
    "            z0 = batch_mu + posterior_std * eps0\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = torch.randint(0, self.n_timesteps, (len(z0),), device=self.device)\n",
    "            \n",
    "            # Forward diffusion: add noise to z0\n",
    "            eps = torch.randn_like(z0)\n",
    "            alpha_bar_t = self.noise_schedule['alphas_cumprod'][t].view(-1, 1)\n",
    "            z_t = torch.sqrt(alpha_bar_t) * z0 + torch.sqrt(1 - alpha_bar_t) * eps\n",
    "            \n",
    "            # Classifier-free guidance: randomly drop conditioning\n",
    "            cond = batch_h.clone()\n",
    "            drop_mask = (torch.rand(len(cond), 1, device=self.device) < p_drop).float()\n",
    "            cond = cond * (1 - drop_mask)  # Zero out dropped rows\n",
    "            \n",
    "            # Predict noise\n",
    "            t_norm = (t.float().unsqueeze(1) / max(self.n_timesteps - 1, 1)).clamp(0, 1)\n",
    "            # t_norm = torch.full((B, 1), t / max(self.n_timesteps - 1, 1), device=self.device)\n",
    "\n",
    "            eps_pred = self.latent_denoiser(z_t, t_norm, cond)\n",
    "\n",
    "            # v_tgt = torch.sqrt(alpha_bar_t) * eps - torch.sqrt(1 - alpha_bar_t) * z0\n",
    "            # v_pred = self.latent_denoiser(z_t, t_norm, cond)\n",
    "            \n",
    "            # Diffusion loss\n",
    "            loss_diffusion = F.mse_loss(eps_pred, eps)\n",
    "            # loss_diffusion = F.mse_loss(v_pred, v_tgt)\n",
    "\n",
    "            \n",
    "            # Structure loss on x0 (denoised latent)\n",
    "            loss_struct = torch.tensor(0.0, device=self.device)\n",
    "            if lambda_struct > 0:\n",
    "                # Predict x0 from eps_pred\n",
    "                x0_pred = (z_t - torch.sqrt(1 - alpha_bar_t) * eps_pred) / torch.sqrt(alpha_bar_t + 1e-8)\n",
    "                \n",
    "                # Preserve latent geometry\n",
    "                D_target = torch.cdist(z0, z0, p=2)\n",
    "                D_pred = torch.cdist(x0_pred, x0_pred, p=2)\n",
    "                loss_struct = F.mse_loss(D_pred, D_target)\n",
    "            \n",
    "            # Total loss\n",
    "            # total_loss = loss_diffusion + lambda_struct * loss_struct\n",
    "            total_loss = loss_diffusion\n",
    "\n",
    "            # Record losses for plotting\n",
    "            self.latent_diffusion_losses['total'].append(total_loss.item())\n",
    "            self.latent_diffusion_losses['diffusion'].append(loss_diffusion.item())\n",
    "            self.latent_diffusion_losses['struct'].append(loss_struct.item() if isinstance(loss_struct, torch.Tensor) else loss_struct)\n",
    "            self.latent_diffusion_losses['epochs'].append(epoch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer_latent.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.latent_denoiser.parameters(), 1.0)\n",
    "            optimizer_latent.step()\n",
    "            scheduler_latent.step()\n",
    "            \n",
    "            # Logging\n",
    "            if epoch % 500 == 0:\n",
    "                ls = float(loss_struct) if isinstance(loss_struct, torch.Tensor) else loss_struct\n",
    "                log_msg = (\n",
    "                    f\"Latent Diffusion epoch {epoch}/{n_epochs}, \"\n",
    "                    f\"Total: {total_loss.item():.6f}, \"\n",
    "                    f\"Diffusion: {loss_diffusion.item():.6f}, \"\n",
    "                    f\"Struct: {ls:.6f}\"\n",
    "                )\n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if epoch % 500 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'latent_denoiser_state_dict': self.latent_denoiser.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer_latent.state_dict(),\n",
    "                }, os.path.join(self.outf, f'latent_diffusion_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Save final model\n",
    "        torch.save({\n",
    "            'latent_denoiser_state_dict': self.latent_denoiser.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_latent_diffusion.pt'))\n",
    "        \n",
    "        print(\"Latent diffusion training complete!\")\n",
    "                        \n",
    "\n",
    "    def train(self, encoder_epochs=1000, vae_epochs=800, diffusion_epochs=400, **kwargs):\n",
    "        \"\"\"\n",
    "        Combined training pipeline: encoder → graph_vae → diffusion_latent\n",
    "        ⚠️ Do **not** touch `train_encoder`; its aligned embeddings are the sole conditioning signal throughout.\n",
    "        \"\"\"\n",
    "        print(\"Starting Graph-VAE + Latent Diffusion training pipeline...\")\n",
    "        \n",
    "        # Stage 1: Train encoder (DO NOT MODIFY - keep existing train_encoder)\n",
    "        print(\"Stage 1: Training domain alignment encoder...\")\n",
    "        self.train_encoder(n_epochs=encoder_epochs)\n",
    "        \n",
    "        # Stage 2: Train Graph-VAE\n",
    "        print(\"Stage 2: Training Graph-VAE...\")\n",
    "        self.train_graph_vae(epochs=vae_epochs)\n",
    "        \n",
    "        # Stage 3: Train latent diffusion\n",
    "        print(\"Stage 3: Training latent diffusion...\")\n",
    "        self.train_diffusion_latent(n_epochs=diffusion_epochs, **kwargs)\n",
    "        \n",
    "        print(\"Complete training pipeline finished!\")\n",
    "\n",
    "    def refine_coordinates_scale_free(self, coords, tau=0.9):\n",
    "        '''scale free coordinate refinement using relative min seperation\n",
    "        \n",
    "        tau: factor for minimum seperation (0.8-1.0) where 1.0 is tight'''\n",
    "\n",
    "        device = coords.device\n",
    "        n = coords.shape[0]\n",
    "\n",
    "        def compute_nearest_neighbor_distances(X):\n",
    "            '''compute distance to nearest enighbor for each point'''\n",
    "            distances = torch.cdist(X, X)\n",
    "            distances.fill_diagonal_(float('inf'))\n",
    "            nn_distances, _ = distances.min(dim=1)\n",
    "            return nn_distances\n",
    "        \n",
    "        def pairwise_distances_squared(X):\n",
    "            '''compute pairwise squared distances'''\n",
    "            s = (X * X).sum(1, keepdim=True)\n",
    "            S = s + s.T - 2 * X @ X.T\n",
    "            S = S.clamp_min_(0.0)\n",
    "            S.fill_diagonal_(0.0)\n",
    "            return S\n",
    "        \n",
    "        def gram_matrix(S):\n",
    "            '''convert gram matrix back to distance matrix'''\n",
    "            r = S.mean(1, keepdim=True)\n",
    "            c = S.mean(0, keepdim=True)\n",
    "            m = S.mean()\n",
    "            B = -0.5 * ( S - r - c + m)\n",
    "            return B\n",
    "        \n",
    "        def distance_from_gram(B):\n",
    "            \"\"\"Convert Gram matrix back to distance matrix\"\"\"\n",
    "            d = torch.diag(B)\n",
    "            S = d[:, None] + d[None, :] - 2 * B\n",
    "            S = S.clamp_min_(0.0)\n",
    "            S.fill_diagonal_(0.0)\n",
    "            return S\n",
    "        \n",
    "        def project_to_euclidean(S):\n",
    "            \"\"\"Project distance matrix to Euclidean space\"\"\"\n",
    "            B = gram_matrix(S)\n",
    "            vals, vecs = torch.linalg.eigh(B)\n",
    "            vals = vals.clamp_min(0.0)\n",
    "            \n",
    "            # Keep top 2 eigenvalues for 2D\n",
    "            idx = torch.argsort(vals, descending=True)[:2]\n",
    "            vals = vals[idx]\n",
    "            vecs = vecs[:, idx]\n",
    "            \n",
    "            B_proj = (vecs * vals.sqrt()) @ (vecs * vals.sqrt()).T\n",
    "            return distance_from_gram(B_proj)\n",
    "        \n",
    "        def enforce_min_separation(S, min_dist):\n",
    "            \"\"\"Enforce minimum distance constraints\"\"\"\n",
    "            S = S.clone()\n",
    "            min_dist_sq = min_dist ** 2\n",
    "            mask = ~torch.eye(n, dtype=torch.bool, device=device)\n",
    "            S[mask] = torch.maximum(S[mask], torch.full_like(S[mask], min_dist_sq))\n",
    "            S.fill_diagonal_(0.0)\n",
    "            return 0.5 * (S + S.T)\n",
    "        \n",
    "        def coords_from_distances(S):\n",
    "            \"\"\"Extract 2D coordinates from distance matrix\"\"\"\n",
    "            B = gram_matrix(S)\n",
    "            vals, vecs = torch.linalg.eigh(B)\n",
    "            idx = torch.argsort(vals, descending=True)[:2]\n",
    "            L = torch.diag(vals[idx].clamp_min(0.0).sqrt())\n",
    "            U = vecs[:, idx] @ L\n",
    "            return U\n",
    "        \n",
    "        # def align_to_original(U, X_orig):\n",
    "        #     \"\"\"Align refined coordinates to original using Procrustes\"\"\"\n",
    "        #     U_center = U.mean(0, keepdim=True)\n",
    "        #     X_center = X_orig.mean(0, keepdim=True)\n",
    "        #     U_centered = U - U_center\n",
    "        #     X_centered = X_orig - X_center\n",
    "            \n",
    "        #     M = U_centered.T @ X_centered\n",
    "        #     U_svd, _, Vt = torch.linalg.svd(M, full_matrices=False)\n",
    "        #     R = U_svd @ Vt\n",
    "            \n",
    "        #     return (U @ R.T) + X_center\n",
    "        \n",
    "        # def align_to_original(U, X_orig):\n",
    "        #     Um = U.mean(0)\n",
    "        #     Xm = X_orig.mean(0)\n",
    "        #     Uc = U - Um\n",
    "        #     Xc = X_orig - Xm\n",
    "        #     M = Uc.T @ Xc\n",
    "        #     U_svd, _, Vt = torch.linalg.svd(M, full_matrices=False)\n",
    "        #     R = U_svd @ Vt\n",
    "        #     # avoid reflection\n",
    "        #     if torch.linalg.det(R) < 0:\n",
    "        #         U_svd[:, -1] *= -1\n",
    "        #         R = U_svd @ Vt\n",
    "        #     t = Xm - (Um @ R)\n",
    "        #     return U @ R + t\n",
    "        \n",
    "        def align_to_original(U, X_orig):\n",
    "            Um = U.mean(0)\n",
    "            Xm = X_orig.mean(0)\n",
    "            Uc = U - Um\n",
    "            Xc = X_orig - Xm  # Fixed: was X_orig - Xc\n",
    "            M = Uc.T @ Xc\n",
    "            U_svd, _, Vt = torch.linalg.svd(M, full_matrices=False)\n",
    "            R = U_svd @ Vt\n",
    "            # avoid reflection\n",
    "            if torch.linalg.det(R) < 0:\n",
    "                U_svd[:, -1] *= -1\n",
    "                R = U_svd @ Vt\n",
    "            t = Xm - (Um @ R)\n",
    "            return U @ R + t\n",
    "\n",
    "\n",
    "        \n",
    "        # Step 1: Compute scale-free minimum separation\n",
    "        nn_distances = compute_nearest_neighbor_distances(coords)\n",
    "        median_nn_dist = torch.median(nn_distances)\n",
    "        min_separation = tau * median_nn_dist\n",
    "        \n",
    "        print(f\"Nearest neighbor distances: min={nn_distances.min():.4f}, \"\n",
    "            f\"median={median_nn_dist:.4f}, max={nn_distances.max():.4f}\")\n",
    "        print(f\"Using minimum separation: {min_separation:.4f} (tau={tau})\")\n",
    "        \n",
    "        # Step 2: Refinement loop\n",
    "        S = pairwise_distances_squared(coords)\n",
    "        \n",
    "        for iteration in range(100):\n",
    "            S_prev = S.clone()\n",
    "            \n",
    "            # Project to Euclidean distance matrix\n",
    "            S = project_to_euclidean(S)\n",
    "            \n",
    "            # Enforce minimum separation\n",
    "            S = enforce_min_separation(S, min_separation)\n",
    "            \n",
    "            # Check convergence\n",
    "            change = (S - S_prev).norm() / (S_prev.norm() + 1e-12)\n",
    "            if change < 1e-6:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        # Step 3: Extract refined coordinates\n",
    "        coords_refined = coords_from_distances(S)\n",
    "        coords_final = align_to_original(coords_refined, coords)\n",
    "        # coords_final = lock_orientation(coords_refined, coords)\n",
    "        \n",
    "        # Compute final statistics\n",
    "        final_nn_distances = compute_nearest_neighbor_distances(coords_final)\n",
    "        print(f\"After refinement - min distance: {final_nn_distances.min():.4f}, \"\n",
    "            f\"median: {torch.median(final_nn_distances):.4f}\")\n",
    "        \n",
    "        return coords_final\n",
    "\n",
    "\n",
    "\n",
    "    def sample_sc_coordinates_batched(self, batch_size=512, guidance_scale=1.0, refine_coords=True, refinement_tau=0.9):\n",
    "        \"\"\"\n",
    "        Sample SC coordinates using pure noise → guided denoising → decode.\n",
    "        No Graph-VAE latents used during sampling.\n",
    "        \"\"\"\n",
    "        n_total = len(self.sc_gene_expr)\n",
    "        print(f\"Sampling {n_total} SC coordinates using pure noise → guided diffusion → decode...\")\n",
    "        print(f\"Using guidance scale: {guidance_scale}\")\n",
    "        \n",
    "        # Set models to eval mode\n",
    "        self.netE.eval()\n",
    "        self.graph_vae_decoder.eval()\n",
    "        self.latent_denoiser.eval()\n",
    "        \n",
    "        # Update noise schedule for current n_timesteps\n",
    "        self.update_noise_schedule()\n",
    "        \n",
    "        all_coords = []\n",
    "        n_batches = (n_total + batch_size - 1) // batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx in range(n_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, n_total)\n",
    "                batch_sc_expr = self.sc_gene_expr[start_idx:end_idx]\n",
    "                \n",
    "                print(f\"Processing batch {batch_idx + 1}/{n_batches} ({len(batch_sc_expr)} cells)...\")\n",
    "                \n",
    "                # Get aligned SC embeddings\n",
    "                h_sc = self.netE(batch_sc_expr).float()\n",
    "                B = h_sc.size(0)\n",
    "                \n",
    "                # Start from PURE NOISE in latent space\n",
    "                z_t = torch.randn(B, self.latent_dim, device=self.device)\n",
    "                \n",
    "                # Reverse diffusion with classifier-free guidance\n",
    "                for t in reversed(range(self.n_timesteps)):\n",
    "                    # t_norm = torch.full((B, 1), t / (self.n_timesteps - 1), device=self.device)\n",
    "                    t_norm = torch.full((B, 1), t / max(self.n_timesteps - 1, 1), device=self.device)\n",
    "                    \n",
    "                    # Classifier-free guidance: conditional and unconditional predictions\n",
    "                    eps_c = self.latent_denoiser(z_t, t_norm, h_sc)\n",
    "                    eps_u = self.latent_denoiser(z_t, t_norm, torch.zeros_like(h_sc))\n",
    "                    eps_pred = (1 + guidance_scale) * eps_c - guidance_scale * eps_u\n",
    "\n",
    "                    # eps_pred = eps_pred.view(B, self.latent_dim)\n",
    "                    \n",
    "                    # DDPM reverse step\n",
    "                    alpha_t = self.noise_schedule['alphas'][t]\n",
    "                    alpha_bar_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                    beta_t = self.noise_schedule['betas'][t]\n",
    "                    \n",
    "                    # Compute mean\n",
    "                    mu = (1 / torch.sqrt(alpha_t)) * (\n",
    "                        z_t - ((1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)) * eps_pred\n",
    "                    )\n",
    "\n",
    "                    # mu = mu.view(B, self.latent_dim)\n",
    "                    \n",
    "                    if t > 0:\n",
    "                        # Add noise (use posterior variance for better sampling)\n",
    "                        sigma_t = torch.sqrt(self.noise_schedule['posterior_variance'][t])\n",
    "                        noise = torch.randn_like(z_t)\n",
    "                        z_t = mu + sigma_t * noise\n",
    "                    else:\n",
    "                        z_t = mu\n",
    "                \n",
    "                # Decode final latent to coordinates (z ONLY, no features)\n",
    "                batch_coords = self.graph_vae_decoder(z_t)\n",
    "\n",
    "                \n",
    "                # Move to CPU and store\n",
    "                all_coords.append(batch_coords.cpu()) \n",
    "                \n",
    "                # Clear GPU cache\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Combine all batches\n",
    "        final_coords = torch.cat(all_coords, dim=0)\n",
    "\n",
    "        # Scale-free coordinate refinement on ALL coordinates\n",
    "        if refine_coords:\n",
    "            print(\"Applying scale-free refinement to all coordinates...\")\n",
    "            # Convert back to tensor on device for refinement\n",
    "            coords_tensor = final_coords.to(self.device)\n",
    "            refined_coords = self.refine_coordinates_scale_free(coords_tensor, tau=refinement_tau)\n",
    "            final_coords = refined_coords.cpu()\n",
    "        \n",
    "        print(\"Pure noise → guided diffusion → decode sampling complete!\")\n",
    "        return final_coords.cpu().numpy()\n",
    "\n",
    "\n",
    "    def plot_training_losses(self):\n",
    "        \"\"\"Plot training losses for Graph-VAE + Latent Diffusion pipeline\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "        \n",
    "        # Determine how many subplots we need\n",
    "        n_plots = 0\n",
    "        if len(self.vae_losses['epochs']) > 0:\n",
    "            n_plots += 2  # VAE losses and VAE smoothed\n",
    "        if len(self.latent_diffusion_losses['epochs']) > 0:\n",
    "            n_plots += 2  # Latent diffusion losses and smoothed\n",
    "        \n",
    "        if n_plots == 0:\n",
    "            print(\"No training losses to plot.\")\n",
    "            return\n",
    "        \n",
    "        # Create figure with appropriate number of subplots\n",
    "        if n_plots == 2:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            axes = [axes] if n_plots == 2 else axes\n",
    "        elif n_plots == 4:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, n_plots, figsize=(7*n_plots, 5))\n",
    "            if n_plots == 1:\n",
    "                axes = [axes]\n",
    "        \n",
    "        plot_idx = 0\n",
    "        \n",
    "        # Plot 1: Graph-VAE losses\n",
    "        if len(self.vae_losses['epochs']) > 0:\n",
    "            epochs_vae = np.array(self.vae_losses['epochs'])\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            ax.plot(epochs_vae, self.vae_losses['total'], 'b-', label='Total VAE Loss', linewidth=2)\n",
    "            ax.plot(epochs_vae, self.vae_losses['reconstruction'], 'g-', label='Reconstruction Loss', linewidth=2)\n",
    "            ax.plot(epochs_vae, np.array(self.vae_losses['kl']) * 0.01, 'r--', label='KL Loss (×0.01)', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_title('Graph-VAE Training Losses')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_yscale('log')\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Plot 2: Graph-VAE smoothed\n",
    "            if len(self.vae_losses['total']) > 1:\n",
    "                ax = axes[plot_idx]\n",
    "                window = min(50, len(self.vae_losses['total']) // 10)\n",
    "                if window > 1:\n",
    "                    smoothed = np.convolve(self.vae_losses['total'], \n",
    "                                        np.ones(window)/window, mode='valid')\n",
    "                    smooth_epochs = epochs_vae[window-1:]\n",
    "                    ax.plot(epochs_vae, self.vae_losses['total'], 'lightblue', alpha=0.5, label='Raw')\n",
    "                    ax.plot(smooth_epochs, smoothed, 'blue', linewidth=2, label=f'Smoothed (window={window})')\n",
    "                else:\n",
    "                    ax.plot(epochs_vae, self.vae_losses['total'], 'blue', linewidth=2)\n",
    "                \n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Loss')\n",
    "                ax.set_title('Graph-VAE Total Loss (Smoothed)')\n",
    "                if window > 1:\n",
    "                    ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_yscale('log')\n",
    "                plot_idx += 1\n",
    "        \n",
    "        # Plot 3: Latent Diffusion losses\n",
    "        if len(self.latent_diffusion_losses['epochs']) > 0:\n",
    "            epochs_diff = np.array(self.latent_diffusion_losses['epochs'])\n",
    "            ax = axes[plot_idx]\n",
    "            \n",
    "            ax.plot(epochs_diff, self.latent_diffusion_losses['total'], 'b-', label='Total Loss', linewidth=2)\n",
    "            ax.plot(epochs_diff, self.latent_diffusion_losses['diffusion'], 'k-', label='Diffusion Loss', linewidth=2)\n",
    "            \n",
    "            # Only plot struct loss if it's non-zero\n",
    "            struct_losses = np.array(self.latent_diffusion_losses['struct'])\n",
    "            if np.any(struct_losses > 0):\n",
    "                ax.plot(epochs_diff, struct_losses, 'r--', label='Structure Loss', alpha=0.8)\n",
    "            \n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.set_title('Latent Diffusion Training Losses')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.set_yscale('log')\n",
    "            plot_idx += 1\n",
    "            \n",
    "            # Plot 4: Latent Diffusion smoothed\n",
    "            if len(self.latent_diffusion_losses['total']) > 1:\n",
    "                ax = axes[plot_idx]\n",
    "                window = min(50, len(self.latent_diffusion_losses['total']) // 10)\n",
    "                if window > 1:\n",
    "                    smoothed = np.convolve(self.latent_diffusion_losses['total'],\n",
    "                                        np.ones(window)/window, mode='valid')\n",
    "                    smooth_epochs = epochs_diff[window-1:]\n",
    "                    ax.plot(epochs_diff, self.latent_diffusion_losses['total'], 'lightcoral', alpha=0.5, label='Raw')\n",
    "                    ax.plot(smooth_epochs, smoothed, 'red', linewidth=2, label=f'Smoothed (window={window})')\n",
    "                else:\n",
    "                    ax.plot(epochs_diff, self.latent_diffusion_losses['total'], 'red', linewidth=2)\n",
    "                \n",
    "                ax.set_xlabel('Epoch')\n",
    "                ax.set_ylabel('Loss')\n",
    "                ax.set_title('Latent Diffusion Total Loss (Smoothed)')\n",
    "                if window > 1:\n",
    "                    ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                ax.set_yscale('log')\n",
    "                plot_idx += 1\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final loss values\n",
    "        print(\"\\n=== Training Loss Summary ===\")\n",
    "        \n",
    "        if len(self.vae_losses['total']) > 0:\n",
    "            print(f\"Graph-VAE - Initial Loss: {self.vae_losses['total'][0]:.6f}\")\n",
    "            print(f\"Graph-VAE - Final Loss: {self.vae_losses['total'][-1]:.6f}\")\n",
    "            print(f\"Graph-VAE - Loss Reduction: {(1 - self.vae_losses['total'][-1]/self.vae_losses['total'][0])*100:.2f}%\")\n",
    "            print(f\"Graph-VAE - Final Reconstruction Loss: {self.vae_losses['reconstruction'][-1]:.6f}\")\n",
    "            print(f\"Graph-VAE - Final KL Loss: {self.vae_losses['kl'][-1]:.6f}\")\n",
    "        \n",
    "        if len(self.latent_diffusion_losses['total']) > 0:\n",
    "            print(f\"Latent Diffusion - Initial Loss: {self.latent_diffusion_losses['total'][0]:.6f}\")\n",
    "            print(f\"Latent Diffusion - Final Loss: {self.latent_diffusion_losses['total'][-1]:.6f}\")\n",
    "            print(f\"Latent Diffusion - Loss Reduction: {(1 - self.latent_diffusion_losses['total'][-1]/self.latent_diffusion_losses['total'][0])*100:.2f}%\")\n",
    "            print(f\"Latent Diffusion - Final Diffusion Loss: {self.latent_diffusion_losses['diffusion'][-1]:.6f}\")\n",
    "            if np.any(np.array(self.latent_diffusion_losses['struct']) > 0):\n",
    "                print(f\"Latent Diffusion - Final Structure Loss: {self.latent_diffusion_losses['struct'][-1]:.6f}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# PART 4: MMD Loss Implementation\n",
    "# =====================================================\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        tmp = 0\n",
    "        for x in kernel_val:\n",
    "            tmp += x\n",
    "        return tmp\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "def analyze_sc_st_patterns(model):\n",
    "    \"\"\"Run the complete analysis\"\"\"\n",
    "    \n",
    "    print(\"Analyzing SC vs ST expression patterns...\")\n",
    "    \n",
    "    # Main comparison plot\n",
    "    common_genes = model.compare_sc_st_expression_patterns(n_genes=20)\n",
    "    \n",
    "    # Detailed gene-by-gene analysis\n",
    "    print(f\"\\nDetailed analysis for top {len(common_genes)} variable genes...\")\n",
    "    model.plot_detailed_gene_comparison(common_genes, n_genes=10)\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(\"\\nExpression Statistics:\")\n",
    "    print(f\"SC data shape: {model.sc_gene_expr.shape}\")\n",
    "    print(f\"ST data shape: {model.st_gene_expr.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sc_mean = model.sc_gene_expr.mean(0)\n",
    "        st_mean = model.st_gene_expr.mean(0)\n",
    "        \n",
    "        print(f\"SC mean expression: {sc_mean.mean():.3f} ± {sc_mean.std():.3f}\")\n",
    "        print(f\"ST mean expression: {st_mean.mean():.3f} ± {st_mean.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data_individual_norm():\n",
    "    \"\"\"\n",
    "    Load and process cSCC data with individual normalization per ST dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data with individual normalization...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize expression data (same for all)\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "\n",
    "\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def normalize_coordinates_individually(coords):\n",
    "    \"\"\"\n",
    "    Normalize coordinates to [-1, 1] range individually.\n",
    "    \"\"\"\n",
    "    coords_min = coords.min(axis=0)\n",
    "    coords_max = coords.max(axis=0)\n",
    "    coords_range = coords_max - coords_min\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    coords_range[coords_range == 0] = 1.0\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    coords_normalized = 2 * (coords - coords_min) / coords_range - 1\n",
    "    \n",
    "    return coords_normalized, coords_min, coords_max, coords_range\n",
    "\n",
    "def prepare_individually_normalized_st_data(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Normalize each ST dataset individually, then combine.\n",
    "    \"\"\"\n",
    "    print(\"Preparing individually normalized ST data...\")\n",
    "    \n",
    "    # Get common genes\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates and normalize individually\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "    \n",
    "    print(\"Normalizing coordinates individually...\")\n",
    "    st1_coords_norm, st1_min, st1_max, st1_range = normalize_coordinates_individually(st1_coords)\n",
    "    st2_coords_norm, st2_min, st2_max, st2_range = normalize_coordinates_individually(st2_coords)\n",
    "    st3_coords_norm, st3_min, st3_max, st3_range = normalize_coordinates_individually(st3_coords)\n",
    "    \n",
    "    print(f\"ST1 coord range: [{st1_coords_norm.min():.3f}, {st1_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST2 coord range: [{st2_coords_norm.min():.3f}, {st2_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST3 coord range: [{st3_coords_norm.min():.3f}, {st3_coords_norm.max():.3f}]\")\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "    st_coords_combined = np.vstack([st1_coords_norm, st2_coords_norm, st3_coords_norm])\n",
    "    \n",
    "    # Create dataset metadata\n",
    "    dataset_info = {\n",
    "        'labels': (['dataset1'] * len(st1_expr) + \n",
    "                  ['dataset2'] * len(st2_expr) + \n",
    "                  ['dataset3'] * len(st3_expr)),\n",
    "        'normalization_params': {\n",
    "            'dataset1': {'min': st1_min, 'max': st1_max, 'range': st1_range},\n",
    "            'dataset2': {'min': st2_min, 'max': st2_max, 'range': st2_range},\n",
    "            'dataset3': {'min': st3_min, 'max': st3_max, 'range': st3_range}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_info, common_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data_individual_norm()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cell_interactions_advanced(scadata, coords_key='advanced_diffusion_coords_avg'):\n",
    "    \"\"\"Analyze cell-cell interactions using the advanced diffusion coordinates\"\"\"\n",
    "    \n",
    "    # Get coordinates from scadata\n",
    "    coords_mean = scadata.obsm[coords_key]\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    distances = squareform(pdist(coords_mean))\n",
    "    \n",
    "    # Get cell types\n",
    "    cell_types = scadata.obs['rough_celltype'].values\n",
    "    unique_types = np.unique(cell_types)\n",
    "    \n",
    "    # Analyze minimum distances between cell types\n",
    "    min_distances = {}\n",
    "    for i, type1 in enumerate(unique_types):\n",
    "        for j, type2 in enumerate(unique_types):\n",
    "            if i <= j:  # Include self-interactions\n",
    "                mask1 = cell_types == type1\n",
    "                mask2 = cell_types == type2\n",
    "                \n",
    "                if i == j:\n",
    "                    # Same cell type - exclude self\n",
    "                    sub_dist = distances[np.ix_(mask1, mask2)]\n",
    "                    np.fill_diagonal(sub_dist, np.inf)\n",
    "                    if sub_dist.size > 0:\n",
    "                        min_dist = np.min(sub_dist[sub_dist < np.inf])\n",
    "                    else:\n",
    "                        min_dist = np.nan\n",
    "                else:\n",
    "                    # Different cell types\n",
    "                    sub_dist = distances[np.ix_(mask1, mask2)]\n",
    "                    min_dist = np.min(sub_dist) if sub_dist.size > 0 else np.nan\n",
    "                \n",
    "                min_distances[(type1, type2)] = min_dist\n",
    "    \n",
    "    # Create interaction matrix visualization\n",
    "    interaction_matrix = np.full((len(unique_types), len(unique_types)), np.nan)\n",
    "    for i, type1 in enumerate(unique_types):\n",
    "        for j, type2 in enumerate(unique_types):\n",
    "            key = (type1, type2) if i <= j else (type2, type1)\n",
    "            if key in min_distances:\n",
    "                interaction_matrix[i, j] = min_distances[key]\n",
    "                interaction_matrix[j, i] = min_distances[key]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask = ~np.isnan(interaction_matrix)\n",
    "    sns.heatmap(interaction_matrix, \n",
    "                annot=True, fmt='.3f', \n",
    "                xticklabels=unique_types,\n",
    "                yticklabels=unique_types,\n",
    "                cmap='coolwarm_r',\n",
    "                mask=~mask,\n",
    "                cbar_kws={'label': 'Minimum Distance'})\n",
    "    plt.title(f'Minimum Distances Between Cell Types ({coords_key})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return min_distances, interaction_matrix\n",
    "\n",
    "def visualize_advanced_results_multi_model(scadata):\n",
    "    \"\"\"Visualize results from multiple models with uncertainty analysis\"\"\"\n",
    "    \n",
    "    # Get coordinates from all models\n",
    "    coords_avg = scadata.obsm['advanced_diffusion_coords_avg']\n",
    "    coords_rep1 = scadata.obsm['advanced_diffusion_coords_rep1'] \n",
    "    coords_rep2 = scadata.obsm['advanced_diffusion_coords_rep2']\n",
    "    coords_rep3 = scadata.obsm['advanced_diffusion_coords_rep3']\n",
    "    \n",
    "    # Calculate uncertainty metrics across models\n",
    "    all_coords = np.stack([coords_rep1, coords_rep2, coords_rep3], axis=0)  # (3, n_cells, 2)\n",
    "    coords_std = np.std(all_coords, axis=0)  # Standard deviation across models\n",
    "    coords_var = np.var(all_coords, axis=0)  # Variance across models\n",
    "    \n",
    "    # Total variability (combining x and y dimensions)\n",
    "    total_std = np.sqrt(coords_std[:, 0]**2 + coords_std[:, 1]**2)\n",
    "    total_var = coords_var[:, 0] + coords_var[:, 1]\n",
    "    \n",
    "    # Create confidence scores (inverse of variability)\n",
    "    confidence = 1 / (1 + total_std)\n",
    "    scadata.obs['spatial_confidence'] = confidence\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Spatial coordinates colored by cell type\n",
    "    ax = axes[0, 0]\n",
    "    cell_types = scadata.obs['rough_celltype']\n",
    "    unique_types = cell_types.unique()\n",
    "    colors = sns.color_palette('tab20', n_colors=len(unique_types))\n",
    "    \n",
    "    for i, ct in enumerate(unique_types):\n",
    "        mask = cell_types == ct\n",
    "        ax.scatter(coords_avg[mask, 0], coords_avg[mask, 1], \n",
    "                  c=[colors[i]], label=ct, s=30, alpha=0.7)\n",
    "    ax.set_title('Averaged Spatial Coordinates by Cell Type', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    # 2. Model variability (standard deviation across 3 models)\n",
    "    ax = axes[0, 1]\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=total_std, cmap='viridis_r', \n",
    "                        s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Model Std Dev')\n",
    "    ax.set_title('Model Prediction Variability', fontsize=14)\n",
    "    \n",
    "    # 3. X vs Y coordinate uncertainty\n",
    "    ax = axes[0, 2]\n",
    "    scatter = ax.scatter(coords_std[:, 0], coords_std[:, 1], \n",
    "                        c=total_std, cmap='plasma', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Total Std')\n",
    "    ax.set_xlabel('X Coordinate Std')\n",
    "    ax.set_ylabel('Y Coordinate Std')\n",
    "    ax.set_title('Coordinate Uncertainty (X vs Y)', fontsize=14)\n",
    "    \n",
    "    # 4. Cell density heatmap\n",
    "    ax = axes[1, 0]\n",
    "    from scipy.stats import gaussian_kde\n",
    "    xy = coords_avg.T\n",
    "    z = gaussian_kde(xy)(xy)\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=z, cmap='hot', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Density')\n",
    "    ax.set_title('Cell Density (Averaged Coordinates)', fontsize=14)\n",
    "    \n",
    "    # 5. Confidence scores\n",
    "    ax = axes[1, 1]\n",
    "    scatter = ax.scatter(coords_avg[:, 0], coords_avg[:, 1], \n",
    "                        c=confidence, cmap='RdYlGn', s=30, alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax, label='Confidence')\n",
    "    ax.set_title('Prediction Confidence Across Models', fontsize=14)\n",
    "    \n",
    "    # 6. Model agreement visualization\n",
    "    ax = axes[1, 2]\n",
    "    # Show cells where models agree vs disagree\n",
    "    high_agreement = total_std < np.percentile(total_std, 25)  # Bottom 25%\n",
    "    low_agreement = total_std > np.percentile(total_std, 75)   # Top 25%\n",
    "    \n",
    "    ax.scatter(coords_avg[high_agreement, 0], coords_avg[high_agreement, 1], \n",
    "              c='green', s=20, alpha=0.7, label='High Agreement')\n",
    "    ax.scatter(coords_avg[low_agreement, 0], coords_avg[low_agreement, 1], \n",
    "              c='red', s=20, alpha=0.7, label='Low Agreement')\n",
    "    ax.scatter(coords_avg[~(high_agreement | low_agreement), 0], \n",
    "              coords_avg[~(high_agreement | low_agreement), 1], \n",
    "              c='gray', s=10, alpha=0.5, label='Medium Agreement')\n",
    "    ax.set_title('Model Agreement', fontsize=14)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig, total_std, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def procrustes_alignment_2d(coords_to_align, reference_coords):\n",
    "    \"\"\"\n",
    "    Align coords_to_align to reference_coords using 2D Procrustes analysis\n",
    "    (rotation + translation only, no scaling)\n",
    "    \n",
    "    Args:\n",
    "        coords_to_align: (n_points, 2) coordinates to be aligned\n",
    "        reference_coords: (n_points, 2) reference coordinates\n",
    "    \n",
    "    Returns:\n",
    "        aligned_coords: (n_points, 2) aligned coordinates\n",
    "        rotation_matrix: (2, 2) rotation matrix used\n",
    "        translation: (2,) translation vector used\n",
    "    \"\"\"\n",
    "    assert coords_to_align.shape == reference_coords.shape, \"Coordinate shapes must match\"\n",
    "    assert coords_to_align.shape[1] == 2, \"Only 2D coordinates supported\"\n",
    "    \n",
    "    # Center both coordinate sets\n",
    "    coords_centered = coords_to_align - np.mean(coords_to_align, axis=0)\n",
    "    ref_centered = reference_coords - np.mean(reference_coords, axis=0)\n",
    "    \n",
    "    # Compute cross-covariance matrix\n",
    "    H = coords_centered.T @ ref_centered\n",
    "    \n",
    "    # SVD to find optimal rotation\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    \n",
    "    # Compute rotation matrix\n",
    "    R = Vt.T @ U.T\n",
    "    \n",
    "    # Ensure proper rotation (det(R) = 1)\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[-1, :] *= -1\n",
    "        R = Vt.T @ U.T\n",
    "    \n",
    "    # Apply rotation to centered coordinates\n",
    "    coords_rotated = coords_centered @ R.T\n",
    "    \n",
    "    # Compute translation to align centroids\n",
    "    translation = np.mean(reference_coords, axis=0) - np.mean(coords_rotated, axis=0)\n",
    "    \n",
    "    # Apply translation\n",
    "    aligned_coords = coords_rotated + translation\n",
    "    \n",
    "    return aligned_coords, R, translation\n",
    "\n",
    "def plot_alignment_comparison(coords_list, cell_types, labels, title_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Plot coordinates before and after alignment for comparison\n",
    "    \n",
    "    Args:\n",
    "        coords_list: List of coordinate arrays [(n_points, 2), ...]\n",
    "        cell_types: Array of cell type labels\n",
    "        labels: List of labels for each coordinate set\n",
    "        title_prefix: Prefix for plot titles\n",
    "    \"\"\"\n",
    "    n_models = len(coords_list)\n",
    "    fig, axes = plt.subplots(2, n_models, figsize=(5*n_models, 10))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = axes.reshape(2, 1)\n",
    "    \n",
    "    # Get unique cell types and colors\n",
    "    unique_types = np.unique(cell_types)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_types)))\n",
    "    color_map = dict(zip(unique_types, colors))\n",
    "    \n",
    "    for i, (coords, label) in enumerate(zip(coords_list, labels)):\n",
    "        # Plot individual models\n",
    "        for j, cell_type in enumerate(unique_types):\n",
    "            mask = cell_types == cell_type\n",
    "            axes[0, i].scatter(coords[mask, 0], coords[mask, 1], \n",
    "                             c=[color_map[cell_type]], label=cell_type, \n",
    "                             alpha=0.6, s=20)\n",
    "        \n",
    "        axes[0, i].set_title(f'{title_prefix}{label}')\n",
    "        axes[0, i].set_xlabel('X coordinate')\n",
    "        axes[0, i].set_ylabel('Y coordinate')\n",
    "        axes[0, i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot overlay of all models\n",
    "    for i, (coords, label) in enumerate(zip(coords_list, labels)):\n",
    "        axes[1, 0].scatter(coords[:, 0], coords[:, 1], \n",
    "                          label=label, alpha=0.4, s=15)\n",
    "    \n",
    "    axes[1, 0].set_title(f'{title_prefix}All Models Overlay')\n",
    "    axes[1, 0].set_xlabel('X coordinate')\n",
    "    axes[1, 0].set_ylabel('Y coordinate')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(1, n_models):\n",
    "        axes[1, i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_individual_advanced_diffusion_models_with_alignment(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset, \n",
    "    apply Procrustes alignment, and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['advanced_diffusion_coords_avg']\n",
    "        models_all: All trained models for further analysis\n",
    "        alignment_info: Dictionary containing alignment details\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {dataset_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize AdvancedHierarchicalDiffusion model\n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,\n",
    "            transport_plan=None,\n",
    "            D_st=None,\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=3.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=600,\n",
    "            n_denoising_blocks=4,\n",
    "            hidden_dim=256,\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{dataset_name}'\n",
    "        )\n",
    "        \n",
    "        print(f\"Training model for {dataset_name}...\")\n",
    "        \n",
    "        # Train using new Graph-VAE + Latent Diffusion pipeline\n",
    "        model.train(\n",
    "            encoder_epochs=1000,\n",
    "            vae_epochs=1000,\n",
    "            diffusion_epochs=2500,\n",
    "            lambda_struct=2.0\n",
    "        )\n",
    "        \n",
    "        model.plot_training_losses()\n",
    "        \n",
    "        print(f\"Generating SC coordinates using model {i+1}...\")\n",
    "        sc_coords = model.sample_sc_coordinates_batched(\n",
    "            batch_size=512\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        print(f\"Model {i+1} complete! Generated coordinates shape: {sc_coords.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # ========================\n",
    "    # PROCRUSTES ALIGNMENT\n",
    "    # ========================\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"APPLYING PROCRUSTES ALIGNMENT\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Use second model (index 1) as reference\n",
    "    reference_idx = 1\n",
    "    reference_coords = sc_coords_results[reference_idx].copy()\n",
    "    \n",
    "    print(f\"Using model {reference_idx + 1} (dataset{reference_idx + 1}) as reference\")\n",
    "    \n",
    "    # Plot BEFORE alignment\n",
    "    print(\"Plotting coordinates BEFORE alignment...\")\n",
    "    plot_alignment_comparison(\n",
    "        coords_list=sc_coords_results.copy(),\n",
    "        cell_types=scadata.obs['rough_celltype'].values,\n",
    "        labels=[f\"Model {i+1}\" for i in range(len(sc_coords_results))],\n",
    "        title_prefix=\"BEFORE Alignment - \"\n",
    "    )\n",
    "    \n",
    "    # Apply Procrustes alignment\n",
    "    aligned_coords_results = []\n",
    "    alignment_info = {\n",
    "        'reference_model': reference_idx + 1,\n",
    "        'rotations': [],\n",
    "        'translations': [],\n",
    "        'rmse_before': [],\n",
    "        'rmse_after': []\n",
    "    }\n",
    "    \n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        if i == reference_idx:\n",
    "            # Reference model - no alignment needed\n",
    "            aligned_coords = coords.copy()\n",
    "            alignment_info['rotations'].append(np.eye(2))\n",
    "            alignment_info['translations'].append(np.zeros(2))\n",
    "            rmse_before = 0.0\n",
    "            rmse_after = 0.0\n",
    "        else:\n",
    "            # Calculate RMSE before alignment\n",
    "            rmse_before = np.sqrt(np.mean((coords - reference_coords)**2))\n",
    "            \n",
    "            # Apply Procrustes alignment\n",
    "            aligned_coords, rotation_matrix, translation = procrustes_alignment_2d(\n",
    "                coords, reference_coords\n",
    "            )\n",
    "            \n",
    "            # Calculate RMSE after alignment\n",
    "            rmse_after = np.sqrt(np.mean((aligned_coords - reference_coords)**2))\n",
    "            \n",
    "            # Store alignment info\n",
    "            alignment_info['rotations'].append(rotation_matrix)\n",
    "            alignment_info['translations'].append(translation)\n",
    "            \n",
    "            print(f\"Model {i+1} -> Reference alignment:\")\n",
    "            print(f\"  RMSE before: {rmse_before:.6f}\")\n",
    "            print(f\"  RMSE after:  {rmse_after:.6f}\")\n",
    "            print(f\"  Improvement: {rmse_before - rmse_after:.6f}\")\n",
    "        \n",
    "        alignment_info['rmse_before'].append(rmse_before)\n",
    "        alignment_info['rmse_after'].append(rmse_after)\n",
    "        aligned_coords_results.append(aligned_coords)\n",
    "    \n",
    "    # Plot AFTER alignment\n",
    "    print(\"Plotting coordinates AFTER alignment...\")\n",
    "    plot_alignment_comparison(\n",
    "        coords_list=aligned_coords_results.copy(),\n",
    "        cell_types=scadata.obs['rough_celltype'].values,\n",
    "        labels=[f\"Model {i+1}\" for i in range(len(aligned_coords_results))],\n",
    "        title_prefix=\"AFTER Alignment - \"\n",
    "    )\n",
    "    \n",
    "    # ========================\n",
    "    # AVERAGING\n",
    "    # ========================\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"AVERAGING ALIGNED RESULTS\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Average the aligned results\n",
    "    sc_coords_avg = np.mean(aligned_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in aligned_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    scadata.obsm['advanced_diffusion_coords_avg_aligned'] = sc_coords_avg  # For clarity\n",
    "    \n",
    "    # Save individual results (both original and aligned)\n",
    "    for i, (orig_coords, aligned_coords) in enumerate(zip(sc_coords_results, aligned_coords_results)):\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}_original'] = orig_coords\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}_aligned'] = aligned_coords\n",
    "    \n",
    "    # Plot final averaged result\n",
    "    print(\"Plotting final averaged coordinates...\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Get unique cell types and colors\n",
    "    unique_types = np.unique(scadata.obs['rough_celltype'].values)\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_types)))\n",
    "    color_map = dict(zip(unique_types, colors))\n",
    "    \n",
    "    cell_types = scadata.obs['rough_celltype'].values\n",
    "    \n",
    "    # Plot before averaging (overlay of aligned models)\n",
    "    for i, coords in enumerate(aligned_coords_results):\n",
    "        ax1.scatter(coords[:, 0], coords[:, 1], \n",
    "                   label=f\"Model {i+1}\", alpha=0.4, s=15)\n",
    "    ax1.set_title('Aligned Models (Before Averaging)')\n",
    "    ax1.set_xlabel('X coordinate')\n",
    "    ax1.set_ylabel('Y coordinate')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot final averaged result by cell type\n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        ax2.scatter(sc_coords_avg[mask, 0], sc_coords_avg[mask, 1], \n",
    "                   c=[color_map[cell_type]], label=cell_type, \n",
    "                   alpha=0.6, s=20)\n",
    "    ax2.set_title('Final Averaged Coordinates (by Cell Type)')\n",
    "    ax2.set_xlabel('X coordinate')\n",
    "    ax2.set_ylabel('Y coordinate')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nAdvanced diffusion training with alignment complete!\")\n",
    "    print(f\"Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "    print(f\"Individual results saved with '_original' and '_aligned' suffixes\")\n",
    "    \n",
    "    return scadata, models_all, alignment_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata, models_all, alignment_info = train_individual_advanced_diffusion_models_with_alignment(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_advanced_diffusion_models(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['advanced_diffusion_coords_avg']\n",
    "        models_all: All trained models for further analysis\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {dataset_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize AdvancedHierarchicalDiffusion model\n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,  # No cell type labels\n",
    "            transport_plan=None,  # No OT transport plan\n",
    "            D_st=None,           # No distance matrices\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],  # Same as STEMDiffusion\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=3.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=800,     # Same as STEMDiffusion\n",
    "            n_denoising_blocks=4,\n",
    "            hidden_dim=256,      # Same as STEMDiffusion\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{dataset_name}'\n",
    "        )\n",
    "        \n",
    "        print(f\"Training model for {dataset_name}...\")\n",
    "        \n",
    "        # Train using new Graph-VAE + Latent Diffusion pipeline\n",
    "        model.train(\n",
    "            encoder_epochs=800,  # Stage 1: Domain alignment encoder\n",
    "            vae_epochs=1500,       # Stage 2: Graph-VAE training\n",
    "            diffusion_epochs=2500, # Stage 3: Latent diffusion\n",
    "            lambda_struct=2.0     # Structure loss weight\n",
    "        )\n",
    "        \n",
    "        # model.plot_training_losses()\n",
    "\n",
    "        # # Add the analysis\n",
    "        # print(f\"\\nAnalyzing SC vs ST patterns for {dataset_name}...\")\n",
    "        # analyze_sc_st_patterns(model)\n",
    "        \n",
    "        print(f\"Generating SC coordinates using model {i+1}...\")\n",
    "        # Sample SC coordinates using new Graph-VAE + Latent Diffusion pipeline\n",
    "        # sc_coords = model.sample_sc_coordinates(\n",
    "        #     n_samples=None     # Use all SC cells\n",
    "        # )\n",
    "        sc_coords = model.sample_sc_coordinates_batched(\n",
    "            batch_size=512  # Even smaller batches\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        print(f\"Model {i+1} complete! Generated coordinates shape: {sc_coords.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average the results from all 3 models\n",
    "    print(f\"\\nAveraging results from {len(sc_coords_results)} models...\")\n",
    "    sc_coords_avg = np.mean(sc_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in sc_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    \n",
    "    # Optionally, save individual results too\n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}'] = coords\n",
    "    \n",
    "    print(f\"\\nAdvanced diffusion training complete!\")\n",
    "    print(f\"Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "    \n",
    "    return scadata, models_all\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Train individual AdvancedHierarchicalDiffusion models and get averaged results\n",
    "scadata, advanced_models = train_individual_advanced_diffusion_models(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")\n",
    "\n",
    "print(\"Advanced diffusion training complete! Results saved in scadata.obsm['advanced_diffusion_coords_avg']\")\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata.obsm['advanced_diffusion_coords_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata.obsm['advanced_diffusion_coords_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (6, 6)\n",
    "# import scanpy as sc\n",
    "# sc.settings.set_figure_params(figsize=(4,4), dpi=100)\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep1_original', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep1_aligned', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep2_original', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep2_aligned', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=12).as_hex()\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep3_original', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_rep3_aligned', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have: scadata.obsm['advanced_diffusion_coords_avg'], etc.\n",
    "\n",
    "print(\"\\n=== Advanced Analysis and Visualization ===\")\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "\n",
    "\n",
    "# 1. Visualize advanced results with uncertainty analysis\n",
    "print(\"Creating advanced visualization plots...\")\n",
    "fig, model_uncertainty, confidence_scores = visualize_advanced_results_multi_model(scadata)\n",
    "\n",
    "# 2. Analyze cell interactions for averaged coordinates\n",
    "print(\"Analyzing cell interactions (averaged coordinates)...\")\n",
    "min_distances_avg, interaction_matrix_avg = analyze_cell_interactions_advanced(\n",
    "    scadata, coords_key='advanced_diffusion_coords_avg'\n",
    ")\n",
    "\n",
    "# 3. Optional: Analyze interactions for individual models too\n",
    "print(\"Analyzing cell interactions (individual models)...\")\n",
    "for i in range(1, 4):\n",
    "    print(f\"\\nModel {i} interactions:\")\n",
    "    min_distances, interaction_matrix = analyze_cell_interactions_advanced(\n",
    "        scadata, coords_key=f'advanced_diffusion_coords_rep{i}'\n",
    "    )\n",
    "\n",
    "# 4. Print summary statistics\n",
    "print(\"\\n=== Advanced Model Statistics ===\")\n",
    "print(f\"Total cells mapped: {len(scadata.obsm['advanced_diffusion_coords_avg'])}\")\n",
    "print(f\"Average model uncertainty: {model_uncertainty.mean():.4f}\")\n",
    "print(f\"Model uncertainty range: [{model_uncertainty.min():.4f}, {model_uncertainty.max():.4f}]\")\n",
    "print(f\"Average confidence: {confidence_scores.mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Cell Type Confidence ===\")\n",
    "for ct in scadata.obs['rough_celltype'].unique():\n",
    "    mask = scadata.obs['rough_celltype'] == ct\n",
    "    print(f\"{ct}: {mask.sum()} cells, \"\n",
    "          f\"avg confidence: {confidence_scores[mask].mean():.3f}, \"\n",
    "          f\"avg uncertainty: {model_uncertainty[mask].mean():.4f}\")\n",
    "\n",
    "print(\"\\n=== Physics Constraints (Averaged Coordinates) ===\")\n",
    "all_distances = []\n",
    "for key, dist in min_distances_avg.items():\n",
    "    if not np.isnan(dist):\n",
    "        all_distances.append(dist)\n",
    "        print(f\"Min distance {key[0]} - {key[1]}: {dist:.4f}\")\n",
    "\n",
    "if all_distances:\n",
    "    print(f\"\\nOverall minimum cell-cell distance: {np.min(all_distances):.4f}\")\n",
    "    print(f\"Cells with potential overlaps (< 0.01): {np.sum(np.array(all_distances) < 0.01)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "mpl.rcParams['figure.figsize'] = (6, 6)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5)) \n",
    "    sc.pl.embedding(scadata, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_coordinates_isotropic(coords):\n",
    "    '''normalize coordinates to unit circle preserving aspect ratio'''\n",
    "    if torch.is_tensor(coords):\n",
    "        center = coords.mean(dim=0)\n",
    "        centered = coords - center\n",
    "        max_radius = torch.max(torch.norm(centered, dim=1))\n",
    "        coords_norm = centered / max_radius\n",
    "        return coords_norm, center, max_radius\n",
    "    else:\n",
    "        center = coords.mean(axis=0)\n",
    "        centered = coords - center\n",
    "        max_radius = np.max(np.linalg.norm(centered, axis=1))\n",
    "        coords_norm = centered / max_radius\n",
    "        return coords_norm, center, max_radius\n",
    "\n",
    "# Load and prepare data for validation\n",
    "scadata_val, stadata1_val, stadata2_val, stadata3_val = load_and_process_cscc_data_individual_norm()\n",
    "\n",
    "# Get normalized ground truth coordinates for ST3\n",
    "st3_coords_gt = stadata3_val.obsm['spatial']\n",
    "st3_coords_gt_norm, _, _ = normalize_coordinates_isotropic(st3_coords_gt)\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT ===\")\n",
    "print(\"Training diffusion models on ST1+ST2, testing on ST3...\")\n",
    "\n",
    "# Prepare datasets for training (only first 2)\n",
    "st_datasets_train = [\n",
    "    (stadata1_val, \"dataset1\"),\n",
    "    (stadata2_val, \"dataset2\")\n",
    "]\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Storage for results\n",
    "T_all = []\n",
    "D_induced_all = []\n",
    "D_st_all = []\n",
    "D_sc_all = []\n",
    "trained_models = []\n",
    "\n",
    "# Train on first two datasets\n",
    "for i, (stadata, dataset_name) in enumerate(st_datasets_train):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training Advanced Diffusion model {i+1}/2 for {dataset_name} using SpaOTsc\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Get common genes between SC and current ST dataset\n",
    "    sc_genes = set(scadata_val.var_names)\n",
    "    st_genes = set(stadata.var_names)\n",
    "    common_genes = sorted(list(sc_genes & st_genes))\n",
    "    \n",
    "    print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract expression data\n",
    "    sc_expr = scadata_val[:, common_genes].X\n",
    "    st_expr = stadata[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st_expr, 'toarray'):\n",
    "        st_expr = st_expr.toarray()\n",
    "        \n",
    "    # Get coordinates\n",
    "    st_coords = stadata.obsm['spatial']\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32).to(device)\n",
    "    X_st = torch.tensor(st_expr, dtype=torch.float32).to(device)\n",
    "    Y_st = torch.tensor(st_coords, dtype=torch.float32).to(device)\n",
    "    \n",
    "    print(f\"SC data shape: {X_sc.shape}\")\n",
    "    print(f\"ST data shape: {X_st.shape}\")\n",
    "    print(f\"ST coords shape: {Y_st.shape}\")\n",
    "    \n",
    "    # === REPLACE FUSED_GW_TORCH WITH SPAOTSC ===\n",
    "    print(f\"Running optimal transport for {dataset_name}...\")\n",
    "\n",
    "    # === PREPARE CELL TYPE INFORMATION ===\n",
    "    # Extract cell types from scadata\n",
    "    if 'rough_celltype' in scadata_val.obs.columns:\n",
    "        cell_types_sc = scadata_val.obs['rough_celltype'].values\n",
    "        unique_cell_types = np.unique(cell_types_sc)\n",
    "        print(f\"Found {len(unique_cell_types)} unique cell types: {unique_cell_types}\")\n",
    "    else:\n",
    "        cell_types_sc = None\n",
    "        print(\"No cell type information found\")\n",
    "\n",
    "    # === INITIALIZE ADVANCED HIERARCHICAL DIFFUSION MODEL ===\n",
    "    print(f\"Initializing AdvancedHierarchicalDiffusion for {dataset_name}...\")\n",
    "    \n",
    "    output_dir = f'./cscc_advanced_diffusion_{dataset_name}_validation'\n",
    "    \n",
    "    model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=st_expr,\n",
    "        st_coords=st_coords,\n",
    "        sc_gene_expr=sc_expr,\n",
    "        cell_types_sc=scadata.obs['rough_celltype'].values,  # No cell type labels\n",
    "        transport_plan=None,  # No OT transport plan\n",
    "        D_st=None,           # No distance matrices\n",
    "        D_induced=None,\n",
    "        n_genes=len(common_genes),\n",
    "        n_embedding=[512, 256, 128],  # Same as STEMDiffusion\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,     # Same as STEMDiffusion\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,      # Same as STEMDiffusion\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=f'advanced_diffusion_{dataset_name}'\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"Training model for {dataset_name}...\")\n",
    "    # model.train()\n",
    "\n",
    "    model.train(\n",
    "        encoder_epochs=1000,  # Stage 1: Domain alignment encoder\n",
    "        vae_epochs=1000,       # Stage 2: Graph-VAE training\n",
    "        diffusion_epochs=2500, # Stage 3: Latent diffusion\n",
    "        lambda_struct=5.0     # Structure loss weight\n",
    "    )\n",
    "    \n",
    "    # Store the trained model\n",
    "    trained_models.append(model)\n",
    "    print(f\"Model {i+1} training completed!\")\n",
    "\n",
    "print(f\"\\nTraining completed! {len(trained_models)} models trained.\")\n",
    "\n",
    "# Test on the third dataset by creating new model instances\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Testing on ST3 dataset...\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Get common genes for testing\n",
    "sc_genes = set(scadata_val.var_names)\n",
    "st1_genes = set(stadata1_val.var_names)\n",
    "st2_genes = set(stadata2_val.var_names)\n",
    "st3_genes = set(stadata3_val.var_names)\n",
    "common_genes_test = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "\n",
    "print(f\"Common genes for testing: {len(common_genes_test)}\")\n",
    "\n",
    "# Extract ST3 expression data\n",
    "st3_expr = stadata3_val[:, common_genes_test].X\n",
    "if hasattr(st3_expr, 'toarray'):\n",
    "    st3_expr = st3_expr.toarray()\n",
    "\n",
    "# We'll create dummy models just to get predictions\n",
    "# Use ST1 as reference for coordinates (since we need some spatial reference)\n",
    "st1_coords = stadata1_val.obsm['spatial']\n",
    "st1_expr_ref = stadata1_val[:, common_genes_test].X\n",
    "if hasattr(st1_expr_ref, 'toarray'):\n",
    "    st1_expr_ref = st1_expr_ref.toarray()\n",
    "\n",
    "# Convert to tensors\n",
    "X_st1_ref = torch.tensor(st1_expr_ref, dtype=torch.float32).to(device)\n",
    "X_st3_test = torch.tensor(st3_expr, dtype=torch.float32).to(device)\n",
    "Y_st1_ref = torch.tensor(st1_coords, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get cell types\n",
    "cell_types_sc = scadata_val.obs['rough_celltype'].values if 'rough_celltype' in scadata_val.obs.columns else None\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# Get the number of cell types from the original training\n",
    "original_cell_types = scadata_val.obs['rough_celltype'].values\n",
    "unique_cell_types = np.unique(original_cell_types)\n",
    "num_original_cell_types = len(unique_cell_types)\n",
    "\n",
    "print(f\"Original model has {num_original_cell_types} cell types\")\n",
    "\n",
    "# Create dummy cell types for ST3 data to match the original number\n",
    "dummy_cell_types = np.random.choice(unique_cell_types, size=X_st3_test.shape[0])\n",
    "\n",
    "# For each trained model, create a test version\n",
    "for i, trained_model in enumerate(trained_models):\n",
    "    print(f\"Creating test model based on trained model {i+1}...\")\n",
    "    \n",
    "    # Create a minimal model instance for testing (no training)\n",
    "    test_model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=X_st1_ref.cpu().numpy(),  # Use ST1 as reference\n",
    "        st_coords=Y_st1_ref.cpu().numpy(),     # Use ST1 coords as reference\n",
    "        sc_gene_expr=X_st3_test.cpu().numpy(), # ST3 data as \"SC\" data\n",
    "        cell_types_sc=dummy_cell_types,                    # No cell types for ST3\n",
    "        transport_plan=None,               # Use transport plan from training\n",
    "        D_st=None,                      # Use distance matrices from training\n",
    "        D_induced=None,\n",
    "        n_genes=len(common_genes_test),\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=f'./temp_test_model_{i}'\n",
    "    )\n",
    "    \n",
    "    # Copy trained parameters (this is a hack, but should work)\n",
    "    state_dict = trained_model.state_dict()\n",
    "    # if 'ot_guidance_strength' in state_dict:\n",
    "    #     del state_dict['ot_guidance_strength']\n",
    "    # test_model.load_state_dict(state_dict)\n",
    "    test_model.load_state_dict(trained_model.state_dict(), strict=False)\n",
    "    \n",
    "    # Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "    print(f\"Generating predictions from test model {i+1}...\")\n",
    "\n",
    "    predicted_coords = test_model.sample_sc_coordinates_batched(\n",
    "            batch_size=512  # Even smaller batches\n",
    "    )\n",
    "    all_predictions.append(predicted_coords)\n",
    "\n",
    "# Continue with the rest of your evaluation code...\n",
    "\n",
    "# Average predictions from both models\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# MSE and MAE\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "\n",
    "# Correlation for each dimension\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Correlation X-dimension: {corr_x:.4f} (p={p_x:.6f})\")\n",
    "print(f\"Correlation Y-dimension: {corr_y:.4f} (p={p_y:.6f})\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Predicted coordinates\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Predicted ST3 Coordinates\\n(Averaged from 2 Models)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional distance-based evaluation\n",
    "euclidean_distances = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "median_distance = np.median(euclidean_distances)\n",
    "mean_distance = np.mean(euclidean_distances)\n",
    "\n",
    "print(f\"\\nDistance-based metrics:\")\n",
    "print(f\"Mean Euclidean distance: {mean_distance:.6f}\")\n",
    "print(f\"Median Euclidean distance: {median_distance:.6f}\")\n",
    "print(f\"Max Euclidean distance: {np.max(euclidean_distances):.6f}\")\n",
    "print(f\"Min Euclidean distance: {np.min(euclidean_distances):.6f}\")\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "print(f\"Generating predictions from test model {i+1}...\")\n",
    "all_predictions = []\n",
    "for i, trained_model in enumerate(trained_models):\n",
    "    print(f\"Creating test model based on trained model {i+1}...\")\n",
    "    \n",
    "    # Create a minimal model instance for testing (no training)\n",
    "    test_model = AdvancedHierarchicalDiffusion(\n",
    "        st_gene_expr=X_st1_ref.cpu().numpy(),  # Use ST1 as reference\n",
    "        st_coords=Y_st1_ref.cpu().numpy(),     # Use ST1 coords as reference\n",
    "        sc_gene_expr=X_st3_test.cpu().numpy(), # ST3 data as \"SC\" data\n",
    "        cell_types_sc=dummy_cell_types,                    # No cell types for ST3\n",
    "        transport_plan=None,               # Use transport plan from training\n",
    "        D_st=None,                      # Use distance matrices from training\n",
    "        D_induced=None,\n",
    "        n_genes=len(common_genes_test),\n",
    "        n_embedding=[512, 256, 128],\n",
    "        coord_space_diameter=2.00,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        device=device,\n",
    "        lr_e=0.0001,\n",
    "        lr_d=0.0002,\n",
    "        n_timesteps=800,\n",
    "        n_denoising_blocks=6,\n",
    "        hidden_dim=256,\n",
    "        num_heads=8,\n",
    "        num_hierarchical_scales=3,\n",
    "        dp=0.2,\n",
    "        outf=f'./temp_test_model_{i}'\n",
    "    )\n",
    "    \n",
    "    # Copy trained parameters (this is a hack, but should work)\n",
    "    state_dict = trained_model.state_dict()\n",
    "    # if 'ot_guidance_strength' in state_dict:\n",
    "    #     del state_dict['ot_guidance_strength']\n",
    "    # test_model.load_state_dict(state_dict)\n",
    "    test_model.load_state_dict(trained_model.state_dict(), strict=False)\n",
    "    \n",
    "    # Now sample coordinates (this should work since ST3 data is the \"SC\" data)\n",
    "    print(f\"Generating predictions from test model {i+1}...\")\n",
    "    predicted_coords = test_model.sample_sc_coordinates_batched(\n",
    "            batch_size=512  # Even smaller batches\n",
    "    )\n",
    "    all_predictions.append(predicted_coords)\n",
    "\n",
    "# Continue with the rest of your evaluation code...\n",
    "\n",
    "# Average predictions from both models\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# MSE and MAE\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "\n",
    "# Correlation for each dimension\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(\"=== VALIDATION RESULTS ===\")\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"Correlation X-dimension: {corr_x:.4f} (p={p_x:.6f})\")\n",
    "print(f\"Correlation Y-dimension: {corr_y:.4f} (p={p_y:.6f})\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Predicted coordinates\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Predicted ST3 Coordinates\\n(Averaged from 2 Models)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Prediction vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional distance-based evaluation\n",
    "euclidean_distances = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "median_distance = np.median(euclidean_distances)\n",
    "mean_distance = np.mean(euclidean_distances)\n",
    "\n",
    "print(f\"\\nDistance-based metrics:\")\n",
    "print(f\"Mean Euclidean distance: {mean_distance:.6f}\")\n",
    "print(f\"Median Euclidean distance: {median_distance:.6f}\")\n",
    "print(f\"Max Euclidean distance: {np.max(euclidean_distances):.6f}\")\n",
    "print(f\"Min Euclidean distance: {np.min(euclidean_distances):.6f}\")\n",
    "\n",
    "print(\"=== VALIDATION EXPERIMENT COMPLETED ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model separately AND the average\n",
    "predicted_coords_avg = np.mean(all_predictions, axis=0)\n",
    "print(f\"Predicted coordinates shape: {predicted_coords_avg.shape}\")\n",
    "print(f\"Ground truth coordinates shape: {st3_coords_gt_norm.shape}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Evaluate individual models\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    mse_i = mean_squared_error(st3_coords_gt_norm, pred)\n",
    "    mae_i = mean_absolute_error(st3_coords_gt_norm, pred)\n",
    "    corr_x_i, p_x_i = pearsonr(st3_coords_gt_norm[:, 0], pred[:, 0])\n",
    "    corr_y_i, p_y_i = pearsonr(st3_coords_gt_norm[:, 1], pred[:, 1])\n",
    "    \n",
    "    print(f\"\\n=== MODEL {i+1} RESULTS ===\")\n",
    "    print(f\"MSE: {mse_i:.6f}, MAE: {mae_i:.6f}\")\n",
    "    print(f\"Corr X: {corr_x_i:.4f}, Corr Y: {corr_y_i:.4f}\")\n",
    "\n",
    "# Evaluate averaged results\n",
    "mse = mean_squared_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "mae = mean_absolute_error(st3_coords_gt_norm, predicted_coords_avg)\n",
    "corr_x, p_x = pearsonr(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0])\n",
    "corr_y, p_y = pearsonr(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1])\n",
    "\n",
    "print(f\"\\n=== AVERAGED RESULTS ===\")\n",
    "print(f\"MSE: {mse:.6f}, MAE: {mae:.6f}\")\n",
    "print(f\"Corr X: {corr_x:.4f}, Corr Y: {corr_y:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization - individual models + average + ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], st3_coords_gt_norm[:, 1], \n",
    "           c=range(len(st3_coords_gt_norm)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Ground Truth ST3 Coordinates\\n(Isotropic Normalized)')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 2: Model 1 predictions\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.scatter(all_predictions[0][:, 0], all_predictions[0][:, 1], \n",
    "           c=range(len(all_predictions[0])), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Model 1 Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 3: Model 2 predictions\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.scatter(all_predictions[1][:, 0], all_predictions[1][:, 1], \n",
    "           c=range(len(all_predictions[1])), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Model 2 Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "# Plot 4: Averaged predictions\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.scatter(predicted_coords_avg[:, 0], predicted_coords_avg[:, 1], \n",
    "           c=range(len(predicted_coords_avg)), cmap='viridis', alpha=0.6, s=20)\n",
    "plt.title('Averaged Predictions')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.colorbar(label='Spot index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation plots for each model\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    corr_x_i, _ = pearsonr(st3_coords_gt_norm[:, 0], pred[:, 0])\n",
    "    corr_y_i, _ = pearsonr(st3_coords_gt_norm[:, 1], pred[:, 1])\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(st3_coords_gt_norm[:, 0], pred[:, 0], \n",
    "               alpha=0.5, label=f'X-coord (r={corr_x_i:.3f})', s=15)\n",
    "    plt.scatter(st3_coords_gt_norm[:, 1], pred[:, 1], \n",
    "               alpha=0.5, label=f'Y-coord (r={corr_y_i:.3f})', s=15)\n",
    "    plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "             [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "             'r--', alpha=0.8, label='Perfect correlation')\n",
    "    plt.xlabel('Ground Truth Coordinates')\n",
    "    plt.ylabel('Predicted Coordinates')\n",
    "    plt.title(f'Model {i+1} vs Ground Truth')\n",
    "    plt.legend()\n",
    "\n",
    "# Averaged correlation plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(st3_coords_gt_norm[:, 0], predicted_coords_avg[:, 0], \n",
    "           alpha=0.5, label=f'X-coord (r={corr_x:.3f})', s=15)\n",
    "plt.scatter(st3_coords_gt_norm[:, 1], predicted_coords_avg[:, 1], \n",
    "           alpha=0.5, label=f'Y-coord (r={corr_y:.3f})', s=15)\n",
    "plt.plot([st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         [st3_coords_gt_norm.min(), st3_coords_gt_norm.max()], \n",
    "         'r--', alpha=0.8, label='Perfect correlation')\n",
    "plt.xlabel('Ground Truth Coordinates')\n",
    "plt.ylabel('Predicted Coordinates')\n",
    "plt.title('Averaged Predictions vs Ground Truth')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distance error plots for each model\n",
    "euclidean_distances_all = []\n",
    "for i, pred in enumerate(all_predictions):\n",
    "    distances = np.sqrt(np.sum((st3_coords_gt_norm - pred)**2, axis=1))\n",
    "    euclidean_distances_all.append(distances)\n",
    "\n",
    "euclidean_distances_avg = np.sqrt(np.sum((st3_coords_gt_norm - predicted_coords_avg)**2, axis=1))\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Distance histograms\n",
    "for i, distances in enumerate(euclidean_distances_all):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel('Euclidean Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Model {i+1} Error Distribution\\nMean: {np.mean(distances):.4f}')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(euclidean_distances_avg, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Euclidean Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Averaged Model Error Distribution\\nMean: {np.mean(euclidean_distances_avg):.4f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print distance metrics for all models\n",
    "for i, distances in enumerate(euclidean_distances_all):\n",
    "    print(f\"\\nModel {i+1} distance metrics:\")\n",
    "    print(f\"Mean: {np.mean(distances):.6f}, Median: {np.median(distances):.6f}\")\n",
    "    print(f\"Max: {np.max(distances):.6f}, Min: {np.min(distances):.6f}\")\n",
    "\n",
    "print(f\"\\nAveraged model distance metrics:\")\n",
    "print(f\"Mean: {np.mean(euclidean_distances_avg):.6f}, Median: {np.median(euclidean_distances_avg):.6f}\")\n",
    "print(f\"Max: {np.max(euclidean_distances_avg):.6f}, Min: {np.min(euclidean_distances_avg):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "\n",
    "sc.pl.spatial(scadata,color=\"rough_celltype\",spot_size=0.06, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"PDC\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rcParams['pdf.fonttype'] = 42\n",
    "# rcParams['ps.fonttype'] = 42\n",
    "# figsize(4,4)\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "sc.pl.spatial(scadata,color=\"level3_celltype\",groups=[\"TSK\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)\n",
    "#save='TSK',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize(4,4)\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"Tumor_KC_Cyc\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P2cyc')\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"Tumor_KC_Basal\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P2bas')\n",
    "sc.pl.spatial(scadata,color=\"level2_celltype\",groups=[\"Tumor_KC_Diff\"],spot_size=0.06, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P2diff')\n",
    "#save='nonTSK',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "sq.gr.spatial_neighbors(scadata,spatial_key='advanced_diffusion_coords_avg')\n",
    "sq.gr.nhood_enrichment(scadata,cluster_key='rough_celltype')\n",
    "sq.gr.interaction_matrix(scadata,cluster_key='rough_celltype')\n",
    "kscadata = scadata[ scadata.obs.level2_celltype.isin(['Tumor_KC_Cyc','Tumor_KC_Basal','Tumor_KC_Diff','TSK'])].copy()\n",
    "sq.gr.spatial_neighbors(kscadata,spatial_key='advanced_diffusion_coords_avg')\n",
    "sq.gr.nhood_enrichment(kscadata,cluster_key='level2_celltype')\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',save='TSKKC_new_good.png',figsize=(3,5))\n",
    "sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',figsize=(3,5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patient 10 stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "# Load all 3 ST datasets\n",
    "stadata1_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep1.h5ad')\n",
    "stadata2_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep2.h5ad')\n",
    "stadata3_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep3.h5ad')\n",
    "\n",
    "datasets = [stadata1_p10, stadata2_p10, stadata3_p10]\n",
    "names = ['ST_P10_Rep1', 'ST_P10_Rep2', 'ST_P10_Rep3']\n",
    "\n",
    "# Basic info\n",
    "print(\"Dataset Basic Info:\")\n",
    "for i, (data, name) in enumerate(zip(datasets, names)):\n",
    "    print(f\"{name}: {data.shape[0]} spots, {data.shape[1]} genes\")\n",
    "    print(f\"  Spatial coords range: X[{data.obsm['spatial'][:,0].min():.2f}, {data.obsm['spatial'][:,0].max():.2f}], Y[{data.obsm['spatial'][:,1].min():.2f}, {data.obsm['spatial'][:,1].max():.2f}]\")\n",
    "\n",
    "# Plot spatial coordinates\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Individual plots\n",
    "for i, (data, name) in enumerate(zip(datasets, names)):\n",
    "    coords = data.obsm['spatial']\n",
    "    row = 0 if i < 2 else 1\n",
    "    col = i if i < 2 else 0\n",
    "    axes[row, col].scatter(coords[:, 0], coords[:, 1], alpha=0.6, s=20)\n",
    "    axes[row, col].set_title(f'{name}\\n{data.shape[0]} spots')\n",
    "    axes[row, col].set_xlabel('X coordinate')\n",
    "    axes[row, col].set_ylabel('Y coordinate')\n",
    "\n",
    "# Overlay plot\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (data, name, color) in enumerate(zip(datasets, names, colors)):\n",
    "    coords = data.obsm['spatial']\n",
    "    axes[1, 1].scatter(coords[:, 0], coords[:, 1], alpha=0.5, s=15, c=color, label=name)\n",
    "axes[1, 1].set_title('All Datasets Overlay')\n",
    "axes[1, 1].set_xlabel('X coordinate')\n",
    "axes[1, 1].set_ylabel('Y coordinate')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find common genes\n",
    "all_genes = [set(data.var_names) for data in datasets]\n",
    "common_genes = sorted(list(all_genes[0] & all_genes[1] & all_genes[2]))\n",
    "print(f\"\\nCommon genes across all datasets: {len(common_genes)}\")\n",
    "\n",
    "# Coordinate overlap analysis\n",
    "print(\"\\nCoordinate Overlap Analysis:\")\n",
    "tolerance = 1.0  # Distance tolerance for \"overlap\"\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        coords_i = datasets[i].obsm['spatial']\n",
    "        coords_j = datasets[j].obsm['spatial']\n",
    "        \n",
    "        # Calculate pairwise distances\n",
    "        distances = cdist(coords_i, coords_j)\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        \n",
    "        # Count overlaps within tolerance\n",
    "        overlaps = np.sum(min_distances < tolerance)\n",
    "        \n",
    "        print(f\"{names[i]} vs {names[j]}:\")\n",
    "        print(f\"  Spots within {tolerance} units: {overlaps}/{len(coords_i)} ({overlaps/len(coords_i)*100:.1f}%)\")\n",
    "        print(f\"  Mean min distance: {np.mean(min_distances):.2f}\")\n",
    "\n",
    "# Gene expression similarity for closest spots\n",
    "print(\"\\nGene Expression Similarity (for closest coordinate pairs):\")\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        # Get common genes data\n",
    "        expr_i = datasets[i][:, common_genes].X\n",
    "        expr_j = datasets[j][:, common_genes].X\n",
    "        \n",
    "        if hasattr(expr_i, 'toarray'):\n",
    "            expr_i = expr_i.toarray()\n",
    "        if hasattr(expr_j, 'toarray'):\n",
    "            expr_j = expr_j.toarray()\n",
    "        \n",
    "        coords_i = datasets[i].obsm['spatial']\n",
    "        coords_j = datasets[j].obsm['spatial']\n",
    "        \n",
    "        # Find closest pairs\n",
    "        distances = cdist(coords_i, coords_j)\n",
    "        closest_j_indices = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Calculate correlations for closest pairs\n",
    "        correlations = []\n",
    "        for spot_i in range(len(expr_i)):\n",
    "            closest_j = closest_j_indices[spot_i]\n",
    "            corr = np.corrcoef(expr_i[spot_i], expr_j[closest_j])[0, 1]\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append(corr)\n",
    "        \n",
    "        print(f\"{names[i]} vs {names[j]}:\")\n",
    "        print(f\"  Mean gene expression correlation: {np.mean(correlations):.4f}\")\n",
    "        print(f\"  Median correlation: {np.median(correlations):.4f}\")\n",
    "        print(f\"  Correlations > 0.5: {np.sum(np.array(correlations) > 0.5)}/{len(correlations)} ({np.sum(np.array(correlations) > 0.5)/len(correlations)*100:.1f}%)\")\n",
    "\n",
    "# Distance distribution plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "pair_idx = 0\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        coords_i = datasets[i].obsm['spatial']\n",
    "        coords_j = datasets[j].obsm['spatial']\n",
    "        \n",
    "        distances = cdist(coords_i, coords_j)\n",
    "        min_distances = np.min(distances, axis=1)\n",
    "        \n",
    "        axes[pair_idx].hist(min_distances, bins=50, alpha=0.7)\n",
    "        axes[pair_idx].set_title(f'{names[i]} vs {names[j]}\\nMin Distance Distribution')\n",
    "        axes[pair_idx].set_xlabel('Distance to closest spot')\n",
    "        axes[pair_idx].set_ylabel('Frequency')\n",
    "        axes[pair_idx].axvline(tolerance, color='red', linestyle='--', label=f'Tolerance={tolerance}')\n",
    "        axes[pair_idx].legend()\n",
    "        \n",
    "        pair_idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.interpolate import griddata\n",
    "import pandas as pd\n",
    "\n",
    "# Get common genes and prepare data\n",
    "all_genes = [set(data.var_names) for data in datasets]\n",
    "common_genes = sorted(list(all_genes[0] & all_genes[1] & all_genes[2]))\n",
    "\n",
    "print(f\"Running comprehensive analysis on {len(common_genes)} common genes...\")\n",
    "\n",
    "# Prepare expression matrices\n",
    "expr_matrices = []\n",
    "coord_matrices = []\n",
    "for data in datasets:\n",
    "    expr = data[:, common_genes].X\n",
    "    if hasattr(expr, 'toarray'):\n",
    "        expr = expr.toarray()\n",
    "    expr_matrices.append(expr)\n",
    "    coord_matrices.append(data.obsm['spatial'])\n",
    "\n",
    "# 1. FIXED SPATIAL GENE EXPRESSION GRADIENTS\n",
    "print(\"\\n1. Calculating spatial gradients...\")\n",
    "\n",
    "def calculate_spatial_gradients_fixed(expr, coords, top_n_genes=20):\n",
    "    \"\"\"Calculate spatial gradients using local neighborhood differences\"\"\"\n",
    "    tree = cKDTree(coords)\n",
    "    gradients = {}\n",
    "    \n",
    "    for gene_idx in range(min(top_n_genes, expr.shape[1])):\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        \n",
    "        grad_magnitudes = []\n",
    "        \n",
    "        for i in range(len(coords)):\n",
    "            # Find 5 nearest neighbors\n",
    "            distances, indices = tree.query(coords[i], k=6)  # Include self\n",
    "            neighbors = indices[1:]  # Exclude self\n",
    "            \n",
    "            if len(neighbors) > 0:\n",
    "                # Calculate gradient as max difference with neighbors\n",
    "                expr_diffs = []\n",
    "                for neighbor in neighbors:\n",
    "                    if neighbor < len(gene_expr):\n",
    "                        coord_diff = coords[neighbor] - coords[i]\n",
    "                        expr_diff = gene_expr[neighbor] - gene_expr[i]\n",
    "                        if np.linalg.norm(coord_diff) > 0:\n",
    "                            # Directional derivative\n",
    "                            grad_component = expr_diff / np.linalg.norm(coord_diff)\n",
    "                            expr_diffs.append(abs(grad_component))\n",
    "                \n",
    "                if expr_diffs:\n",
    "                    grad_magnitudes.append(max(expr_diffs))\n",
    "                else:\n",
    "                    grad_magnitudes.append(0)\n",
    "            else:\n",
    "                grad_magnitudes.append(0)\n",
    "        \n",
    "        grad_magnitudes = np.array(grad_magnitudes)\n",
    "        gradients[common_genes[gene_idx]] = {\n",
    "            'magnitude': grad_magnitudes,\n",
    "            'mean_magnitude': np.mean(grad_magnitudes)\n",
    "        }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Calculate gradients for each dataset\n",
    "gradient_results = []\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    print(f\"  Processing {name}...\")\n",
    "    gradients = calculate_spatial_gradients_fixed(expr, coords, top_n_genes=20)\n",
    "    gradient_results.append(gradients)\n",
    "\n",
    "# Plot top gradient genes\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "for i, (gradients, name) in enumerate(zip(gradient_results, names)):\n",
    "    # Get top 3 genes with highest gradient magnitude\n",
    "    top_genes = sorted(gradients.items(), key=lambda x: x[1]['mean_magnitude'], reverse=True)[:3]\n",
    "    \n",
    "    for j, (gene, grad_data) in enumerate(top_genes):\n",
    "        coords = coord_matrices[i]\n",
    "        scatter = axes[i, j].scatter(coords[:, 0], coords[:, 1], c=grad_data['magnitude'], \n",
    "                          cmap='viridis', s=20, alpha=0.7)\n",
    "        axes[i, j].set_title(f'{name}\\n{gene} (grad: {grad_data[\"mean_magnitude\"]:.3f})')\n",
    "        axes[i, j].set_xlabel('X')\n",
    "        axes[i, j].set_ylabel('Y')\n",
    "        plt.colorbar(scatter, ax=axes[i, j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PCA/UMAP ANALYSIS\n",
    "print(\"\\n2. Performing PCA and UMAP analysis...\")\n",
    "\n",
    "# Combine all datasets for joint analysis\n",
    "all_expr = np.vstack(expr_matrices)\n",
    "all_coords = np.vstack(coord_matrices)\n",
    "dataset_labels = np.concatenate([np.full(len(expr), i) for i, expr in enumerate(expr_matrices)])\n",
    "\n",
    "# Standardize expression data\n",
    "scaler = StandardScaler()\n",
    "all_expr_scaled = scaler.fit_transform(all_expr)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca_result = pca.fit_transform(all_expr_scaled)\n",
    "\n",
    "# UMAP\n",
    "umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_result = umap_reducer.fit_transform(all_expr_scaled)\n",
    "\n",
    "# Plot PCA and UMAP\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# PCA by dataset\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (name, color) in enumerate(zip(names, colors)):\n",
    "    mask = dataset_labels == i\n",
    "    axes[0, 0].scatter(pca_result[mask, 0], pca_result[mask, 1], \n",
    "                      c=color, label=name, alpha=0.6, s=15)\n",
    "axes[0, 0].set_title('PCA by Dataset')\n",
    "axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# UMAP by dataset\n",
    "for i, (name, color) in enumerate(zip(names, colors)):\n",
    "    mask = dataset_labels == i\n",
    "    axes[0, 1].scatter(umap_result[mask, 0], umap_result[mask, 1], \n",
    "                      c=color, label=name, alpha=0.6, s=15)\n",
    "axes[0, 1].set_title('UMAP by Dataset')\n",
    "axes[0, 1].set_xlabel('UMAP1')\n",
    "axes[0, 1].set_ylabel('UMAP2')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# PCA variance explained\n",
    "axes[1, 0].plot(range(1, 21), pca.explained_variance_ratio_[:20], 'bo-')\n",
    "axes[1, 0].set_title('PCA Variance Explained')\n",
    "axes[1, 0].set_xlabel('Principal Component')\n",
    "axes[1, 0].set_ylabel('Variance Explained')\n",
    "\n",
    "# Spatial coordinates colored by first PC\n",
    "scatter = axes[1, 1].scatter(all_coords[:, 0], all_coords[:, 1], c=pca_result[:, 0], \n",
    "                  cmap='viridis', s=15, alpha=0.7)\n",
    "axes[1, 1].set_title('Spatial Distribution (colored by PC1)')\n",
    "axes[1, 1].set_xlabel('X coordinate')\n",
    "axes[1, 1].set_ylabel('Y coordinate')\n",
    "plt.colorbar(scatter, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. HIGHLY VARIABLE GENES\n",
    "print(\"\\n3. Finding highly variable genes...\")\n",
    "\n",
    "def find_highly_variable_genes(expr, gene_names, top_n=20):\n",
    "    \"\"\"Find genes with highest coefficient of variation\"\"\"\n",
    "    cv_scores = []\n",
    "    for i in range(expr.shape[1]):\n",
    "        mean_expr = np.mean(expr[:, i])\n",
    "        std_expr = np.std(expr[:, i])\n",
    "        cv = std_expr / (mean_expr + 1e-8)\n",
    "        cv_scores.append(cv)\n",
    "    \n",
    "    top_indices = np.argsort(cv_scores)[-top_n:][::-1]\n",
    "    return [(gene_names[i], cv_scores[i]) for i in top_indices]\n",
    "\n",
    "hvg_results = []\n",
    "for i, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    hvgs = find_highly_variable_genes(expr, common_genes)\n",
    "    hvg_results.append(hvgs)\n",
    "    print(f\"\\nTop 10 HVGs in {name}:\")\n",
    "    for gene, cv in hvgs[:10]:\n",
    "        print(f\"  {gene}: CV = {cv:.3f}\")\n",
    "\n",
    "# 4. SPATIAL AUTOCORRELATION\n",
    "print(\"\\n4. Calculating spatial autocorrelation...\")\n",
    "\n",
    "def moran_i(expr, coords, k=8):\n",
    "    \"\"\"Calculate Moran's I for spatial autocorrelation\"\"\"\n",
    "    tree = cKDTree(coords)\n",
    "    n = len(expr)\n",
    "    \n",
    "    distances, indices = tree.query(coords, k=min(k+1, n))\n",
    "    \n",
    "    mean_expr = np.mean(expr)\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    w_sum = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        neighbors = indices[i, 1:min(k+1, len(indices[i]))]\n",
    "        for j in neighbors:\n",
    "            if j < n:\n",
    "                numerator += (expr[i] - mean_expr) * (expr[j] - mean_expr)\n",
    "                w_sum += 1\n",
    "        denominator += (expr[i] - mean_expr) ** 2\n",
    "    \n",
    "    if w_sum > 0 and denominator > 0:\n",
    "        moran_i = (n / w_sum) * (numerator / denominator)\n",
    "    else:\n",
    "        moran_i = 0\n",
    "    \n",
    "    return moran_i\n",
    "\n",
    "autocorr_results = []\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    print(f\"  Processing {name}...\")\n",
    "    autocorr_genes = []\n",
    "    \n",
    "    # Test top 50 most variable genes\n",
    "    hvgs = find_highly_variable_genes(expr, common_genes, top_n=50)\n",
    "    \n",
    "    for gene, _ in hvgs:\n",
    "        gene_idx = common_genes.index(gene)\n",
    "        moran = moran_i(expr[:, gene_idx], coords)\n",
    "        autocorr_genes.append((gene, moran))\n",
    "    \n",
    "    autocorr_genes.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    autocorr_results.append(autocorr_genes)\n",
    "    \n",
    "    print(f\"Top 5 spatially autocorrelated genes in {name}:\")\n",
    "    for gene, moran in autocorr_genes[:5]:\n",
    "        print(f\"  {gene}: Moran's I = {moran:.3f}\")\n",
    "\n",
    "# 5. FIXED CROSS-DATASET GENE CORRELATION\n",
    "print(\"\\n5. Cross-dataset gene correlation analysis...\")\n",
    "\n",
    "def calculate_cross_dataset_correlation_fixed(expr1, expr2, coords1, coords2, gene_names):\n",
    "    \"\"\"Calculate gene-wise correlation using spatially matched spots\"\"\"\n",
    "    correlations = []\n",
    "    \n",
    "    # Find spatially closest spots between datasets\n",
    "    tree = cKDTree(coords2)\n",
    "    distances, indices = tree.query(coords1)\n",
    "    \n",
    "    # Only use matches within reasonable distance\n",
    "    good_matches = distances < 5.0  # Adjust threshold as needed\n",
    "    \n",
    "    if np.sum(good_matches) < 10:\n",
    "        print(f\"    Warning: Only {np.sum(good_matches)} good spatial matches found\")\n",
    "        return correlations\n",
    "    \n",
    "    matched_expr1 = expr1[good_matches]\n",
    "    matched_expr2 = expr2[indices[good_matches]]\n",
    "    \n",
    "    for i in range(len(gene_names)):\n",
    "        if len(matched_expr1) > 1:  # Need at least 2 points for correlation\n",
    "            corr, _ = pearsonr(matched_expr1[:, i], matched_expr2[:, i])\n",
    "            if not np.isnan(corr):\n",
    "                correlations.append((gene_names[i], corr))\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Calculate pairwise correlations\n",
    "corr_results = {}\n",
    "for i in range(len(datasets)):\n",
    "    for j in range(i+1, len(datasets)):\n",
    "        pair_name = f\"{names[i]}_vs_{names[j]}\"\n",
    "        correlations = calculate_cross_dataset_correlation_fixed(\n",
    "            expr_matrices[i], expr_matrices[j], \n",
    "            coord_matrices[i], coord_matrices[j], \n",
    "            common_genes\n",
    "        )\n",
    "        correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        corr_results[pair_name] = correlations\n",
    "        \n",
    "        print(f\"\\nTop correlated genes between {names[i]} and {names[j]}:\")\n",
    "        for gene, corr in correlations[:10]:\n",
    "            print(f\"  {gene}: r = {corr:.3f}\")\n",
    "\n",
    "# 6. EXPRESSION CLUSTERING\n",
    "print(\"\\n6. Performing expression clustering...\")\n",
    "\n",
    "n_clusters = 5\n",
    "clustering_results = []\n",
    "\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    expr_scaled = StandardScaler().fit_transform(expr)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(expr_scaled)\n",
    "    \n",
    "    clustering_results.append((cluster_labels, kmeans))\n",
    "    \n",
    "    print(f\"\\nCluster sizes in {name}:\")\n",
    "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    for cluster, count in zip(unique, counts):\n",
    "        print(f\"  Cluster {cluster}: {count} spots\")\n",
    "\n",
    "# Plot clustering results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, ((cluster_labels, _), coords, name) in enumerate(zip(clustering_results, coord_matrices, names)):\n",
    "    scatter = axes[i].scatter(coords[:, 0], coords[:, 1], c=cluster_labels, \n",
    "                             cmap='tab10', s=20, alpha=0.7)\n",
    "    axes[i].set_title(f'{name}\\nExpression Clusters')\n",
    "    axes[i].set_xlabel('X coordinate')\n",
    "    axes[i].set_ylabel('Y coordinate')\n",
    "    plt.colorbar(scatter, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. DIFFERENTIAL EXPRESSION BY SPATIAL REGIONS\n",
    "print(\"\\n7. Differential expression by spatial regions...\")\n",
    "\n",
    "def spatial_differential_expression(expr, coords, gene_names, n_regions=4):\n",
    "    \"\"\"Find differentially expressed genes across spatial regions\"\"\"\n",
    "    x_median = np.median(coords[:, 0])\n",
    "    y_median = np.median(coords[:, 1])\n",
    "    \n",
    "    regions = []\n",
    "    regions.append((coords[:, 0] <= x_median) & (coords[:, 1] <= y_median))\n",
    "    regions.append((coords[:, 0] > x_median) & (coords[:, 1] <= y_median))\n",
    "    regions.append((coords[:, 0] <= x_median) & (coords[:, 1] > y_median))\n",
    "    regions.append((coords[:, 0] > x_median) & (coords[:, 1] > y_median))\n",
    "    \n",
    "    de_results = []\n",
    "    \n",
    "    for gene_idx in range(min(100, expr.shape[1])):\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        region_means = [np.mean(gene_expr[region]) for region in regions]\n",
    "        \n",
    "        cv_regions = np.std(region_means) / (np.mean(region_means) + 1e-8)\n",
    "        de_results.append((gene_names[gene_idx], cv_regions, region_means))\n",
    "    \n",
    "    de_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return de_results\n",
    "\n",
    "de_results = []\n",
    "for i, (expr, coords, name) in enumerate(zip(expr_matrices, coord_matrices, names)):\n",
    "    de_genes = spatial_differential_expression(expr, coords, common_genes)\n",
    "    de_results.append(de_genes)\n",
    "    \n",
    "    print(f\"\\nTop spatially DE genes in {name}:\")\n",
    "    for gene, cv, means in de_genes[:10]:\n",
    "        print(f\"  {gene}: CV = {cv:.3f}\")\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE ANALYSIS COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the expression patterns of these suspicious HVGs\n",
    "print(\"INVESTIGATING SUSPICIOUS HVG PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get the top HVG genes for each dataset\n",
    "top_hvg_genes = []\n",
    "for i, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    hvgs = find_highly_variable_genes(expr, common_genes, top_n=10)\n",
    "    top_hvg_genes.append([gene for gene, cv in hvgs])\n",
    "\n",
    "# Check expression patterns for the first few genes in each dataset\n",
    "for dataset_idx, (expr, name, top_genes) in enumerate(zip(expr_matrices, names, top_hvg_genes)):\n",
    "    print(f\"\\n{name} - Examining top 3 HVG genes:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for gene_idx in range(3):  # Check first 3 genes\n",
    "        gene_name = top_genes[gene_idx]\n",
    "        gene_position = common_genes.index(gene_name)\n",
    "        gene_expr = expr[:, gene_position]\n",
    "        \n",
    "        print(f\"\\nGene: {gene_name}\")\n",
    "        print(f\"Expression values (first 20 spots): {gene_expr[:20]}\")\n",
    "        print(f\"Unique values: {np.unique(gene_expr)}\")\n",
    "        print(f\"Number of unique values: {len(np.unique(gene_expr))}\")\n",
    "        print(f\"Min: {np.min(gene_expr):.4f}, Max: {np.max(gene_expr):.4f}\")\n",
    "        print(f\"Mean: {np.mean(gene_expr):.4f}, Std: {np.std(gene_expr):.4f}\")\n",
    "        print(f\"CV: {np.std(gene_expr) / (np.mean(gene_expr) + 1e-8):.4f}\")\n",
    "        \n",
    "        # Count how many zeros vs non-zeros\n",
    "        zeros = np.sum(gene_expr == 0)\n",
    "        non_zeros = np.sum(gene_expr != 0)\n",
    "        print(f\"Zeros: {zeros}, Non-zeros: {non_zeros}\")\n",
    "        \n",
    "        # Show value distribution\n",
    "        unique_vals, counts = np.unique(gene_expr, return_counts=True)\n",
    "        print(f\"Value distribution:\")\n",
    "        for val, count in zip(unique_vals[:10], counts[:10]):  # Show first 10 most common\n",
    "            print(f\"  {val:.4f}: {count} spots\")\n",
    "        if len(unique_vals) > 10:\n",
    "            print(f\"  ... and {len(unique_vals)-10} more unique values\")\n",
    "\n",
    "# Let's also check if this is a pattern across ALL genes, not just HVGs\n",
    "print(f\"\\n\\nCHECKING OVERALL GENE EXPRESSION PATTERNS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for dataset_idx, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    print(f\"\\n{name} - Overall statistics:\")\n",
    "    \n",
    "    # Check how many genes have very few unique values\n",
    "    genes_with_few_values = 0\n",
    "    genes_with_binary = 0\n",
    "    genes_with_identical_cv = 0\n",
    "    \n",
    "    cv_values = []\n",
    "    \n",
    "    for gene_idx in range(min(100, expr.shape[1])):  # Check first 100 genes\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        unique_vals = np.unique(gene_expr)\n",
    "        cv = np.std(gene_expr) / (np.mean(gene_expr) + 1e-8)\n",
    "        cv_values.append(cv)\n",
    "        \n",
    "        if len(unique_vals) <= 3:\n",
    "            genes_with_few_values += 1\n",
    "        if len(unique_vals) == 2:\n",
    "            genes_with_binary += 1\n",
    "    \n",
    "    # Check for identical CV values\n",
    "    unique_cvs = np.unique(cv_values)\n",
    "    print(f\"  Genes with ≤3 unique values: {genes_with_few_values}/100\")\n",
    "    print(f\"  Genes with binary expression: {genes_with_binary}/100\") \n",
    "    print(f\"  Number of unique CV values: {len(unique_cvs)}\")\n",
    "    print(f\"  Most common CV values: {unique_cvs[:10]}\")\n",
    "    \n",
    "    # Show distribution of CV values\n",
    "    cv_values = np.array(cv_values)\n",
    "    print(f\"  CV range: {np.min(cv_values):.4f} to {np.max(cv_values):.4f}\")\n",
    "    print(f\"  CV mean: {np.mean(cv_values):.4f}\")\n",
    "\n",
    "# Create a histogram of CV values to visualize the pattern\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for dataset_idx, (expr, name) in enumerate(zip(expr_matrices, names)):\n",
    "    cv_values = []\n",
    "    for gene_idx in range(min(200, expr.shape[1])):  # Check first 200 genes\n",
    "        gene_expr = expr[:, gene_idx]\n",
    "        cv = np.std(gene_expr) / (np.mean(gene_expr) + 1e-8)\n",
    "        cv_values.append(cv)\n",
    "    \n",
    "    axes[dataset_idx].hist(cv_values, bins=50, alpha=0.7)\n",
    "    axes[dataset_idx].set_title(f'{name}\\nCV Distribution')\n",
    "    axes[dataset_idx].set_xlabel('Coefficient of Variation')\n",
    "    axes[dataset_idx].set_ylabel('Number of Genes')\n",
    "    axes[dataset_idx].set_yscale('log')  # Log scale to see patterns better\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data_p10():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP10.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep1.h5ad')\n",
    "    stadata2_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep2.h5ad')\n",
    "    stadata3_p10 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP10rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata_p10.obs['rough_celltype'] = scadata_p10.obs['level1_celltype'].astype(str)\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata_p10.obs.loc[scadata_p10.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10\n",
    "\n",
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Key innovation: Use ALL ST data points for better training.\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "\n",
    "    # Store separate coordinate lists for block-diagonal graph\n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    st_expr_combined = scaler.fit_transform(st_expr_combined)\n",
    "\n",
    "    st_coords_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "\n",
    "    sc_expr = scaler.fit_transform(sc_expr)\n",
    "\n",
    "\n",
    "    \n",
    "    # Create dataset labels for tracking\n",
    "    dataset_labels = (['dataset1'] * len(st1_expr) + \n",
    "                     ['dataset2'] * len(st2_expr) + \n",
    "                     ['dataset3'] * len(st3_expr))\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list\n",
    "\n",
    "# Load and process data\n",
    "scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10 = load_and_process_cscc_data_p10()\n",
    "\n",
    "# Prepare combined data for diffusion\n",
    "X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = prepare_combined_st_for_diffusion(\n",
    "    stadata1_p10, stadata2_p10, stadata3_p10, scadata_p10\n",
    ")\n",
    "\n",
    "print(f\"Data preparation complete!\")\n",
    "print(f\"SC cells: {X_sc.shape[0]}\")\n",
    "print(f\"Combined ST spots: {X_st_combined.shape[0]}\")\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code block to your notebook BEFORE the training loop\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "def analyze_sc_st_patterns(model, n_genes=20, figsize=(20, 15)):\n",
    "    \"\"\"\n",
    "    Comprehensive comparison of gene expression patterns between SC and ST data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get data\n",
    "    sc_expr = model.sc_gene_expr\n",
    "    st_expr = model.st_gene_expr\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if torch.is_tensor(sc_expr):\n",
    "        sc_expr = sc_expr.cpu().numpy()\n",
    "    if torch.is_tensor(st_expr):\n",
    "        st_expr = st_expr.cpu().numpy()\n",
    "    \n",
    "    # Find common highly variable genes for comparison\n",
    "    common_genes = find_common_variable_genes(sc_expr, st_expr, n_genes)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 1. Expression distribution comparison (violin plots)\n",
    "    plot_expression_distributions(sc_expr, st_expr, common_genes, fig, 1)\n",
    "    \n",
    "    # 2. Correlation heatmap between SC and ST\n",
    "    plot_sc_st_correlation(sc_expr, st_expr, common_genes, fig, 2)\n",
    "    \n",
    "    # 3. Joint embedding (t-SNE) of SC and ST\n",
    "    plot_joint_embedding(sc_expr, st_expr, fig, 3)\n",
    "    \n",
    "    # 4. Spatial expression patterns for ST\n",
    "    plot_spatial_expression_patterns(model, common_genes, fig, 4)\n",
    "    \n",
    "    # 5. Domain alignment quality\n",
    "    plot_domain_alignment(model, fig, 5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return common_genes\n",
    "\n",
    "def find_common_variable_genes(sc_expr, st_expr, n_genes=20):\n",
    "    \"\"\"Find genes that are variable in both SC and ST\"\"\"\n",
    "    \n",
    "    # Calculate CV for SC data\n",
    "    sc_mean = np.mean(sc_expr, axis=0) + 1e-8\n",
    "    sc_std = np.std(sc_expr, axis=0)\n",
    "    sc_cv = sc_std / sc_mean\n",
    "    \n",
    "    # Calculate CV for ST data  \n",
    "    st_mean = np.mean(st_expr, axis=0) + 1e-8\n",
    "    st_std = np.std(st_expr, axis=0)\n",
    "    st_cv = st_std / st_mean\n",
    "    \n",
    "    # Find genes variable in both (avoid extreme single-spot genes)\n",
    "    sc_nonzero_frac = (sc_expr > 0).mean(0)\n",
    "    st_nonzero_frac = (st_expr > 0).mean(0)\n",
    "    \n",
    "    # Filter for genes expressed in reasonable fraction of cells/spots\n",
    "    valid_mask = (sc_nonzero_frac >= 0.1) & (sc_nonzero_frac <= 0.9) & \\\n",
    "                 (st_nonzero_frac >= 0.1) & (st_nonzero_frac <= 0.9)\n",
    "    \n",
    "    # Rank by combined variability\n",
    "    combined_cv = sc_cv + st_cv\n",
    "    combined_cv[~valid_mask] = 0\n",
    "    \n",
    "    top_gene_indices = np.argsort(combined_cv)[-n_genes:]\n",
    "    \n",
    "    print(f\"Selected {len(top_gene_indices)} variable genes for analysis\")\n",
    "    \n",
    "    return top_gene_indices\n",
    "\n",
    "def plot_expression_distributions(sc_expr, st_expr, gene_indices, fig, subplot_num):\n",
    "    \"\"\"Plot violin plots comparing expression distributions\"\"\"\n",
    "    \n",
    "    n_genes = min(len(gene_indices), 10)  # Limit to 10 genes for clarity\n",
    "    n_cols = 5\n",
    "    n_rows = 2\n",
    "    \n",
    "    for i in range(n_genes):\n",
    "        gene_idx = gene_indices[i]\n",
    "        ax = fig.add_subplot(5, n_cols, i + 1)\n",
    "        \n",
    "        # Prepare data\n",
    "        sc_values = sc_expr[:, gene_idx]\n",
    "        st_values = st_expr[:, gene_idx]\n",
    "        \n",
    "        data_df = pd.DataFrame({\n",
    "            'Expression': np.concatenate([sc_values, st_values]),\n",
    "            'Data_Type': ['SC'] * len(sc_values) + ['ST'] * len(st_values)\n",
    "        })\n",
    "        \n",
    "        # Plot violin plot\n",
    "        sns.violinplot(data=data_df, x='Data_Type', y='Expression', ax=ax)\n",
    "        ax.set_title(f'Gene {gene_idx}', fontsize=8)\n",
    "        ax.set_xlabel('')\n",
    "        \n",
    "        if i % n_cols != 0:\n",
    "            ax.set_ylabel('')\n",
    "\n",
    "def plot_sc_st_correlation(sc_expr, st_expr, gene_indices, fig, subplot_num):\n",
    "    \"\"\"Plot correlation between SC and ST expression levels\"\"\"\n",
    "    \n",
    "    ax = fig.add_subplot(5, 2, 3)\n",
    "    \n",
    "    # Calculate mean expression for each gene\n",
    "    sc_means = np.mean(sc_expr[:, gene_indices], axis=0)\n",
    "    st_means = np.mean(st_expr[:, gene_indices], axis=0)\n",
    "    \n",
    "    # Scatter plot\n",
    "    ax.scatter(sc_means, st_means, alpha=0.7)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr, p_val = stats.pearsonr(sc_means, st_means)\n",
    "    ax.set_title(f'SC vs ST Mean Expression\\nCorr = {corr:.3f}, p = {p_val:.3e}')\n",
    "    ax.set_xlabel('SC Mean Expression')\n",
    "    ax.set_ylabel('ST Mean Expression')\n",
    "    \n",
    "    # Add diagonal line\n",
    "    min_val, max_val = 0, max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n",
    "\n",
    "def plot_joint_embedding(sc_expr, st_expr, fig, subplot_num):\n",
    "    \"\"\"Create joint t-SNE embedding of SC and ST data\"\"\"\n",
    "    \n",
    "    ax = fig.add_subplot(5, 2, 4)\n",
    "    \n",
    "    # Sample data for faster computation\n",
    "    n_sample = min(2000, sc_expr.shape[0])\n",
    "    sc_sample_idx = np.random.choice(sc_expr.shape[0], n_sample, replace=False)\n",
    "    \n",
    "    n_st_sample = min(500, st_expr.shape[0])\n",
    "    st_sample_idx = np.random.choice(st_expr.shape[0], n_st_sample, replace=False)\n",
    "    \n",
    "    # Combine data\n",
    "    combined_data = np.vstack([\n",
    "        sc_expr[sc_sample_idx],\n",
    "        st_expr[st_sample_idx]\n",
    "    ])\n",
    "    \n",
    "    # t-SNE embedding\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embedding = tsne.fit_transform(combined_data)\n",
    "    \n",
    "    # Plot\n",
    "    colors = ['blue'] * n_sample + ['red'] * n_st_sample\n",
    "    labels = ['SC'] * n_sample + ['ST'] * n_st_sample\n",
    "    \n",
    "    for label, color in [('SC', 'blue'), ('ST', 'red')]:\n",
    "        mask = np.array(labels) == label\n",
    "        ax.scatter(embedding[mask, 0], embedding[mask, 1], \n",
    "                  c=color, alpha=0.6, s=20, label=label)\n",
    "    \n",
    "    ax.set_title('Joint t-SNE: SC vs ST')\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('t-SNE 1')\n",
    "    ax.set_ylabel('t-SNE 2')\n",
    "\n",
    "def plot_spatial_expression_patterns(model, gene_indices, fig, subplot_num):\n",
    "    \"\"\"Plot spatial expression patterns for ST data\"\"\"\n",
    "    \n",
    "    n_genes_show = min(4, len(gene_indices))\n",
    "    \n",
    "    for i in range(n_genes_show):\n",
    "        ax = fig.add_subplot(5, 4, 8 + i + 1)\n",
    "        gene_idx = gene_indices[-(i+1)]  # Take top variable genes\n",
    "        \n",
    "        # Get expression values\n",
    "        if torch.is_tensor(model.st_gene_expr):\n",
    "            expr_values = model.st_gene_expr[:, gene_idx].cpu().numpy()\n",
    "        else:\n",
    "            expr_values = model.st_gene_expr[:, gene_idx]\n",
    "            \n",
    "        if torch.is_tensor(model.st_coords):\n",
    "            coords = model.st_coords.cpu().numpy()\n",
    "        else:\n",
    "            coords = model.st_coords\n",
    "        \n",
    "        # Spatial scatter plot\n",
    "        scatter = ax.scatter(coords[:, 0], coords[:, 1], \n",
    "                           c=expr_values, cmap='viridis', \n",
    "                           s=30, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'ST Spatial: Gene {gene_idx}', fontsize=8)\n",
    "        ax.set_aspect('equal')\n",
    "        plt.colorbar(scatter, ax=ax, shrink=0.8)\n",
    "\n",
    "def plot_domain_alignment(model, fig, subplot_num):\n",
    "    \"\"\"Plot domain alignment quality using encoder embeddings\"\"\"\n",
    "    \n",
    "    ax = fig.add_subplot(5, 1, 5)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get aligned embeddings\n",
    "        if torch.is_tensor(model.sc_gene_expr):\n",
    "            sc_data = model.sc_gene_expr\n",
    "        else:\n",
    "            sc_data = torch.tensor(model.sc_gene_expr, device=model.device)\n",
    "            \n",
    "        if torch.is_tensor(model.st_gene_expr):\n",
    "            st_data = model.st_gene_expr\n",
    "        else:\n",
    "            st_data = torch.tensor(model.st_gene_expr, device=model.device)\n",
    "        \n",
    "        sc_embedding = model.netE(sc_data).cpu().numpy()\n",
    "        st_embedding = model.netE(st_data).cpu().numpy()\n",
    "        \n",
    "        # Sample for visualization\n",
    "        n_sample = min(1000, sc_embedding.shape[0])\n",
    "        sc_idx = np.random.choice(sc_embedding.shape[0], n_sample, replace=False)\n",
    "        st_idx = np.random.choice(st_embedding.shape[0], min(300, st_embedding.shape[0]), replace=False)\n",
    "        \n",
    "        # PCA for visualization\n",
    "        combined_embedding = np.vstack([sc_embedding[sc_idx], st_embedding[st_idx]])\n",
    "        pca = PCA(n_components=2)\n",
    "        embedding_2d = pca.fit_transform(combined_embedding)\n",
    "        \n",
    "        # Plot\n",
    "        n_sc = len(sc_idx)\n",
    "        ax.scatter(embedding_2d[:n_sc, 0], embedding_2d[:n_sc, 1], \n",
    "                  c='blue', alpha=0.6, s=20, label='SC')\n",
    "        ax.scatter(embedding_2d[n_sc:, 0], embedding_2d[n_sc:, 1], \n",
    "                  c='red', alpha=0.6, s=20, label='ST')\n",
    "        \n",
    "        ax.set_title('Domain Alignment (Encoder Embeddings)')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2f})')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2f})')\n",
    "\n",
    "print(\"Analysis functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_advanced_diffusion_models(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate AdvancedHierarchicalDiffusion models for each ST dataset and average the results.\n",
    "    MODIFIED: Run stadata1 three times to test for SC cluster rotation/sliding\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    models_all = []\n",
    "    \n",
    "    # STEP 1: Build canonical angular frame from ST slide (ONCE)\n",
    "    # st_coords_raw = stadata1.obsm['spatial']  # Use raw ST coordinates\n",
    "    # angular_frame = _build_canonical_angular_frame(st_coords_raw)\n",
    "    \n",
    "    # List of ST datasets for iteration - Use stadata1 three times\n",
    "    st_datasets = [\n",
    "        (stadata1, \"run1\"),\n",
    "        (stadata2, \"run2\"), \n",
    "        (stadata3, \"run3\")\n",
    "    ]\n",
    "    \n",
    "    for i, (stadata, run_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training AdvancedHierarchicalDiffusion model {i+1}/3 for {run_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {run_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        \n",
    "        print(f\"SC data shape: {sc_expr.shape}\")\n",
    "        print(f\"ST data shape: {st_expr.shape}\")\n",
    "        print(f\"ST coords shape: {st_coords.shape}\")\n",
    "        \n",
    "        # Initialize model with different random seed for each run\n",
    "        torch.manual_seed(42 + i)\n",
    "        np.random.seed(42 + i)\n",
    "        \n",
    "        model = AdvancedHierarchicalDiffusion(\n",
    "            st_gene_expr=st_expr,\n",
    "            st_coords=st_coords,\n",
    "            sc_gene_expr=sc_expr,\n",
    "            cell_types_sc=scadata.obs['rough_celltype'].values,\n",
    "            transport_plan=None,\n",
    "            D_st=None,\n",
    "            D_induced=None,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            coord_space_diameter=2.00,\n",
    "            sigma=2.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=256,\n",
    "            device=device,\n",
    "            lr_e=0.0001,\n",
    "            lr_d=0.0002,\n",
    "            n_timesteps=800,\n",
    "            n_denoising_blocks=6,\n",
    "            hidden_dim=256,\n",
    "            num_heads=8,\n",
    "            num_hierarchical_scales=3,\n",
    "            dp=0.2,\n",
    "            outf=f'advanced_diffusion_{run_name}'\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        print(f\"Training model for {run_name}...\")\n",
    "        model.train(\n",
    "            encoder_epochs=1000,\n",
    "            vae_epochs=1500,\n",
    "            diffusion_epochs=3000,\n",
    "            lambda_struct=10.0\n",
    "        )\n",
    "\n",
    "        st_coords_raw = model.st_coords_norm.cpu().numpy()  # Use normalized coords from model\n",
    "        angular_frame = _build_canonical_angular_frame(st_coords_raw)\n",
    "        \n",
    "        # Generate SC coordinates\n",
    "        print(f\"Generating SC coordinates using {run_name} model...\")\n",
    "        # sc_coords = model.generate_sc_coordinates()\n",
    "        sc_coords = model.sample_sc_coordinates_batched(\n",
    "            batch_size=512,  # Even smaller batches\n",
    "            refine_coords=False\n",
    "        )\n",
    "        sc_coords_results.append(sc_coords)\n",
    "        models_all.append(model)\n",
    "        \n",
    "        # STEP 2: Plot SC cells colored by angle (using ST-derived frame)\n",
    "        _plot_sc_angle_analysis(sc_coords, scadata.obs['rough_celltype'].values, \n",
    "                               angular_frame, st_coords_raw, run_name, i+1)\n",
    "    \n",
    "    # STEP 3: Comparative analysis across runs\n",
    "    _plot_comparative_sc_angle_analysis(sc_coords_results, scadata.obs['rough_celltype'].values,\n",
    "                                       angular_frame, st_coords_raw)\n",
    "    \n",
    "    # Compute averaged SC coordinates\n",
    "    sc_coords_avg = np.mean(sc_coords_results, axis=0)\n",
    "    sc_coords_std = np.std(sc_coords_results, axis=0)\n",
    "    \n",
    "    # Store results in scadata\n",
    "    scadata.obsm['advanced_diffusion_coords_avg'] = sc_coords_avg\n",
    "    scadata.obsm['advanced_diffusion_coords_std'] = sc_coords_std\n",
    "    \n",
    "    # Store individual results\n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        scadata.obsm[f'advanced_diffusion_coords_rep{i+1}'] = coords\n",
    "    \n",
    "    print(f\"\\nTraining complete. Results stored in scadata.obsm\")\n",
    "    return scadata, models_all\n",
    "\n",
    "def _build_canonical_angular_frame(st_coords):\n",
    "    \"\"\"Build canonical angular frame from ST coordinates (dataset-specific, run-independent)\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute centroid\n",
    "    centroid = st_coords.mean(axis=0)\n",
    "    \n",
    "    # Find farthest spot from centroid (deterministic 0° direction)\n",
    "    distances = np.linalg.norm(st_coords - centroid, axis=1)\n",
    "    farthest_idx = np.argmax(distances)\n",
    "    a0 = st_coords[farthest_idx] - centroid  # 0° direction vector\n",
    "    \n",
    "    def angle_fn(x):\n",
    "        \"\"\"Compute angle from canonical frame\"\"\"\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        \n",
    "        v = x - centroid\n",
    "        cross = a0[0] * v[:, 1] - a0[1] * v[:, 0]  # z-component of 2D cross\n",
    "        dot = a0[0] * v[:, 0] + a0[1] * v[:, 1]\n",
    "        angles = np.arctan2(cross, dot)\n",
    "        angles = np.where(angles < 0, angles + 2*np.pi, angles)  # Map to [0, 2π)\n",
    "        return angles\n",
    "    \n",
    "    return {\n",
    "        'centroid': centroid,\n",
    "        'zero_direction': a0,\n",
    "        'farthest_idx': farthest_idx,\n",
    "        'angle_fn': angle_fn\n",
    "    }\n",
    "\n",
    "def _plot_sc_angle_analysis(sc_coords, cell_types, angular_frame, st_coords_bg, run_name, run_num):\n",
    "    \"\"\"Plot SC cells colored by angle from ST-derived frame\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Compute angles for SC cells using ST-derived frame\n",
    "    sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "    sc_angles_degrees = np.degrees(sc_angles)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: SC cells colored by angle (with ST outline in background)\n",
    "    ax1.scatter(st_coords_bg[:, 0], st_coords_bg[:, 1], \n",
    "               c='lightgray', s=10, alpha=0.3, label='ST outline')\n",
    "    \n",
    "    scatter = ax1.scatter(sc_coords[:, 0], sc_coords[:, 1], \n",
    "                         c=sc_angles_degrees, cmap='hsv', s=30, alpha=0.8)\n",
    "    \n",
    "    # Mark centroid and 0° direction\n",
    "    centroid = angular_frame['centroid']\n",
    "    zero_dir = angular_frame['zero_direction']\n",
    "    ax1.scatter(centroid[0], centroid[1], c='black', s=100, marker='x', linewidth=3)\n",
    "    ax1.arrow(centroid[0], centroid[1], zero_dir[0]*0.3, zero_dir[1]*0.3, \n",
    "              head_width=0.05, head_length=0.05, fc='red', ec='red', linewidth=2)\n",
    "    \n",
    "    ax1.set_title(f'{run_name}: SC Cells Colored by Angle θ')\n",
    "    ax1.set_xlabel('X coordinate')\n",
    "    ax1.set_ylabel('Y coordinate')\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax1)\n",
    "    cbar.set_label('Angle (degrees)')\n",
    "    \n",
    "    # Plot 2: Per-cell-type angle distribution\n",
    "    unique_types = np.unique(cell_types)\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n",
    "    \n",
    "    for i, cell_type in enumerate(unique_types):\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 0:\n",
    "            angles_subset = sc_angles_degrees[mask]\n",
    "            ax2.hist(angles_subset, bins=36, alpha=0.6, label=cell_type, \n",
    "                    color=colors[i], density=True)\n",
    "    \n",
    "    ax2.set_title(f'{run_name}: Angle Distribution by Cell Type')\n",
    "    ax2.set_xlabel('Angle (degrees)')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_xlim(0, 360)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'sc_angle_analysis_{run_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print circular statistics per cell type\n",
    "    print(f\"\\n{run_name} - Circular statistics per cell type:\")\n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 5:  # Only if enough cells\n",
    "            angles_rad = sc_angles[mask]\n",
    "            # Circular mean\n",
    "            mean_cos = np.mean(np.cos(angles_rad))\n",
    "            mean_sin = np.mean(np.sin(angles_rad))\n",
    "            circular_mean = np.arctan2(mean_sin, mean_cos)\n",
    "            if circular_mean < 0:\n",
    "                circular_mean += 2*np.pi\n",
    "            \n",
    "            print(f\"  {cell_type}: mean={np.degrees(circular_mean):.1f}°, n={np.sum(mask)}\")\n",
    "\n",
    "def _plot_comparative_sc_angle_analysis(sc_coords_list, cell_types, angular_frame, st_coords_bg):\n",
    "    \"\"\"Plot comparative SC angle analysis across all runs\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    n_runs = len(sc_coords_list)\n",
    "    unique_types = np.unique(cell_types)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_runs, figsize=(5*n_runs, 10))\n",
    "    if n_runs == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    # Top row: SC scatter plots per run\n",
    "    for i, sc_coords in enumerate(sc_coords_list):\n",
    "        ax = axes[0, i]\n",
    "        \n",
    "        # ST background\n",
    "        ax.scatter(st_coords_bg[:, 0], st_coords_bg[:, 1], \n",
    "                  c='lightgray', s=5, alpha=0.3)\n",
    "        \n",
    "        # SC cells colored by angle\n",
    "        sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "        sc_angles_degrees = np.degrees(sc_angles)\n",
    "        \n",
    "        scatter = ax.scatter(sc_coords[:, 0], sc_coords[:, 1], \n",
    "                           c=sc_angles_degrees, cmap='hsv', s=20, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f'Run {i+1}: SC Cells by Angle')\n",
    "        ax.set_aspect('equal')\n",
    "        \n",
    "        if i == n_runs-1:  # Add colorbar to last plot\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('Angle (degrees)')\n",
    "    \n",
    "    # Bottom row: Cell type angle distributions per run  \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_types)))\n",
    "    \n",
    "    for i, sc_coords in enumerate(sc_coords_list):\n",
    "        ax = axes[1, i]\n",
    "        \n",
    "        sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "        sc_angles_degrees = np.degrees(sc_angles)\n",
    "        \n",
    "        for j, cell_type in enumerate(unique_types):\n",
    "            mask = cell_types == cell_type\n",
    "            if np.sum(mask) > 5:\n",
    "                angles_subset = sc_angles_degrees[mask]\n",
    "                ax.hist(angles_subset, bins=36, alpha=0.6, \n",
    "                       label=cell_type if i == 0 else \"\", \n",
    "                       color=colors[j], density=True)\n",
    "        \n",
    "        ax.set_title(f'Run {i+1}: Cell Type Angles')\n",
    "        ax.set_xlabel('Angle (degrees)')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.set_xlim(0, 360)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparative_sc_angle_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for sector sliding\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"SECTOR SLIDING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for cell_type in unique_types:\n",
    "        mask = cell_types == cell_type\n",
    "        if np.sum(mask) > 10:  # Only analyze cell types with enough cells\n",
    "            circular_means = []\n",
    "            \n",
    "            for i, sc_coords in enumerate(sc_coords_list):\n",
    "                sc_angles = angular_frame['angle_fn'](sc_coords)\n",
    "                angles_subset = sc_angles[mask]\n",
    "                \n",
    "                # Circular mean\n",
    "                mean_cos = np.mean(np.cos(angles_subset))\n",
    "                mean_sin = np.mean(np.sin(angles_subset))\n",
    "                circular_mean = np.arctan2(mean_sin, mean_cos)\n",
    "                if circular_mean < 0:\n",
    "                    circular_mean += 2*np.pi\n",
    "                \n",
    "                circular_means.append(np.degrees(circular_mean))\n",
    "            \n",
    "            # Check for large differences between runs\n",
    "            max_diff = max(circular_means) - min(circular_means)\n",
    "            if max_diff > 180:  # Handle wraparound\n",
    "                max_diff = 360 - max_diff\n",
    "            \n",
    "            print(f\"{cell_type}:\")\n",
    "            print(f\"  Run means: {[f'{m:.1f}°' for m in circular_means]}\")\n",
    "            print(f\"  Max difference: {max_diff:.1f}°\")\n",
    "            \n",
    "            if max_diff > 30:  # Significant sliding\n",
    "                print(f\"  ⚠️  SECTOR SLIDING DETECTED!\")\n",
    "            else:\n",
    "                print(f\"  ✅ Consistent placement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process data\n",
    "scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10 = load_and_process_cscc_data_p10()\n",
    "\n",
    "# Train individual AdvancedHierarchicalDiffusion models and get averaged results\n",
    "scadata_p10, advanced_models_p10 = train_individual_advanced_diffusion_models(\n",
    "    scadata_p10, stadata1_p10, stadata2_p10, stadata3_p10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata_p10, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(8, 6))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Advanced Diffusion Coords (Averaged)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Individual model results\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc.pl.embedding(scadata_p10, basis=f'advanced_diffusion_coords_rep{i+1}', color='rough_celltype',\n",
    "                   size=85, title=f'SC Coordinates (Advanced Model {i+1})',\n",
    "                   palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata_p10.obsm['advanced_diffusion_coords_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (4, 4)\n",
    "# import scanpy as sc\n",
    "# sc.settings.set_figure_params(figsize=(4,4), dpi=100)\n",
    "\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_rep1', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_rep2', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=12).as_hex()\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata_p10, basis='advanced_diffusion_coords_rep3', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scadata_p10.obs['selection'] = (scadata_p10.obs['level2_celltype']=='TSK').astype(int)\n",
    "scadata_p10.obs['selection2'] = (scadata_p10.obs['level1_celltype']=='Fibroblast').astype(int)\n",
    "scadata_p10.obs['selection3'] = (scadata_p10.obs['rough_celltype']=='Epithelial').astype(int)\n",
    "\n",
    "# figsize(6,5)\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "sc.pl.spatial(scadata_p10, color=['selection','selection2','selection3','level3_celltype'], spot_size=0.025,cmap='bwr',basis='advanced_diffusion_coords_avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(scadata_p10,color=\"level3_celltype\",groups=[\"TSK\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squidpy as sq\n",
    "sq.gr.spatial_neighbors(scadata_p10,spatial_key='advanced_diffusion_coords_avg')\n",
    "sq.gr.nhood_enrichment(scadata_p10,cluster_key='rough_celltype')\n",
    "sq.gr.interaction_matrix(scadata_p10,cluster_key='rough_celltype')\n",
    "kscadata_p10 = scadata_p10[ scadata_p10.obs.level2_celltype.isin(['Tumor_KC_Cyc','Tumor_KC_Basal','Tumor_KC_Diff','TSK'])].copy()\n",
    "sq.gr.spatial_neighbors(kscadata_p10,spatial_key='advanced_diffusion_coords_avg')\n",
    "sq.gr.nhood_enrichment(kscadata_p10,cluster_key='level2_celltype')\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',save='TSKKC_new_best_p10.svg',figsize=(3,5), title=None)\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\", cmap='coolwarm', save='TSKKC_new_best_p10.svg', figsize=(3,5), ylabel='')\n",
    "# sq.pl.nhood_enrichment(kscadata, cluster_key=\"level2_celltype\",cmap='coolwarm',figsize=(3,5))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3,5))\n",
    "sq.pl.nhood_enrichment(kscadata_p10, cluster_key=\"level2_celltype\", cmap='coolwarm', ax=ax)\n",
    "ax.set_ylabel('')\n",
    "# plt.savefig('TSKKC_new_best_p10.svg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figsize(4,4)\n",
    "# sc.settings.file_format_figs = 'svg'\n",
    "\n",
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Cyc\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P10_cyc')\n",
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Basal\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P10_bas')\n",
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"Tumor_KC_Diff\"],spot_size=0.03, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False,save='P10_diff')\n",
    "#save='nonTSK',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=[\"PDC\"],spot_size=0.04, show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.spatial(scadata_p10,color=\"level2_celltype\",groups=['Tumor_KC_Cyc','Tumor_KC_Basal','Tumor_KC_Diff'],spot_size=0.025, \n",
    "              show=True,basis='advanced_diffusion_coords_avg',title='reconstructed',na_in_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
