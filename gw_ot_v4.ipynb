{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_torch(X, k, mode='connectivity', metric = 'minkowski', p=2, device='cuda'):\n",
    "    '''construct knn graph with torch and gpu\n",
    "    args:\n",
    "        X: input data containing features (torch tensor)\n",
    "        k: number of neighbors for each data point\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric (now euclidean supported for gpu knn)\n",
    "        p: param for minkowski (not used if metric is euclidean)\n",
    "    \n",
    "    Returns:\n",
    "        knn graph as a pytorch sparse tensor (coo format) or dense tensor depending on mode     \n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'.\"\n",
    "    assert metric == 'euclidean', \"for gpu knn, only 'euclidean' metric is currently supported in this implementation\"\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "        mode_knn = 'connectivity'\n",
    "    else:\n",
    "        include_self = False\n",
    "        mode_knn = 'distance'\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm='auto')\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_cpu = X.cpu().numpy()\n",
    "    else:\n",
    "        X_cpu = X.numpy()\n",
    "\n",
    "    knn.fit(X_cpu)\n",
    "    knn_graph_cpu = kneighbors_graph(knn, k, mode=mode_knn, include_self=include_self, metric=metric) #scipy sparse matrix on cpu\n",
    "    knn_graph_coo = knn_graph_cpu.tocoo()\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        knn_graph = torch.sparse_coo_tensor(torch.LongTensor([knn_graph_coo.row, knn_graph_coo.col]),\n",
    "                                            torch.FloatTensor(knn_graph_coo.data),\n",
    "                                            size = knn_graph_coo.shape).to(device)\n",
    "    elif mode == 'distance':\n",
    "        knn_graph_dense = torch.tensor(knn_graph_cpu.toarray(), dtype=torch.float32, device=device) #move to gpu as dense tensor\n",
    "        knn_graph = knn_graph_dense\n",
    "    \n",
    "    return knn_graph\n",
    "    \n",
    "def distances_cal_torch(graph, type_aware=None, aware_power =2, device='cuda'):\n",
    "    '''\n",
    "    calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (pytorch sparse or dense tensor)\n",
    "        type_aware: not implemented in this torch version for simplicity\n",
    "        aware_power: same ^^\n",
    "        device (str): 'cpu' or 'cuda' device to use\n",
    "    Returns:\n",
    "        distance matrix as a torch tensor\n",
    "    '''\n",
    "\n",
    "    if isinstance(graph, torch.Tensor) and graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().to_dense().numpy())\n",
    "    elif isinstance(graph, torch.Tensor) and not graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().numpy())\n",
    "    else:\n",
    "        graph_cpu_csr = csr_matrix(graph) #assume scipy sparse matrix if not torch tensor\n",
    "\n",
    "    shortestPath_cpu = dijkstra(csgraph = graph_cpu_csr, directed=False, return_predecessors=False) #dijkstra on cpu\n",
    "    shortestPath = torch.tensor(shortestPath_cpu, dtype=torch.float32, device=device)\n",
    "\n",
    "    # the_max = torch.nanmax(shortestPath[shortestPath != float('inf')])\n",
    "    # shortestPath[shortestPath > the_max] = the_max\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = torch.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    C_dis = shortestPath / the_max\n",
    "    C_dis -= torch.mean(C_dis)\n",
    "    return C_dis\n",
    "\n",
    "def calculate_D_sc_torch(X_sc, k_neighbors=10, graph_mode='connectivity', device='cpu'):\n",
    "    '''calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (torch sparse or dense tensor)\n",
    "        type_aware: not implemented\n",
    "        aware_power: same ^^\n",
    "        \n",
    "    returns:\n",
    "        distanced matrix as torch tensor'''\n",
    "    \n",
    "    if not isinstance(X_sc, torch.Tensor):\n",
    "        raise TypeError('Input X_sc must be a pytorch tensor')\n",
    "    \n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_sc = X_sc.cuda(device=device)\n",
    "    else:\n",
    "        X_sc = X_sc.cpu()\n",
    "        device= 'cpu'\n",
    "\n",
    "    print(f'using device: {device}')\n",
    "    print(f'constructing knn graph...')\n",
    "    # X_normalized = normalize(X_sc.cpu().numpy(), norm='l2') #normalize on cpu for sklearn knn\n",
    "    X_normalized = X_sc\n",
    "    X_normalized_torch = torch.tensor(X_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "    Xgraph = construct_graph_torch(X_normalized_torch, k=k_neighbors, mode=graph_mode, metric='euclidean', device=device)\n",
    "\n",
    "    print('calculating distances from graph....')\n",
    "    D_sc = distances_cal_torch(Xgraph, device=device)\n",
    "\n",
    "    print('D_sc calculation complete')\n",
    "    \n",
    "    return D_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot\n",
    "\n",
    "def construct_graph_spatial(location_array, k, mode='distance', metric='euclidean', p=2):\n",
    "    '''construct KNN graph based on spatial coordinates\n",
    "    args:\n",
    "        location_array: spatial coordinates of spots (n-spots * 2)\n",
    "        k: number of neighbors for each spot\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric for knn (p=2 is euclidean)\n",
    "        p: param for minkowski if connectivity\n",
    "        \n",
    "    returns:\n",
    "        scipy.sparse.csr_matrix: knn graph in csr format\n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'\"\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "    else:\n",
    "        include_self = False\n",
    "    \n",
    "    c_graph = kneighbors_graph(location_array, k, mode=mode, metric=metric, include_self=include_self, p=p)\n",
    "    return c_graph\n",
    "\n",
    "def distances_cal_spatial(graph, spot_ids=None, spot_types=None, aware_power=2):\n",
    "    '''calculate spatial distance matrix from knn graph\n",
    "    args:\n",
    "        graph (scipy.sparse.csr_matrix): knn graph\n",
    "        spot_ids (list, optional): list of spot ids corresponding to the rows/cols of the graph. required if type_aware is used\n",
    "        spot_types (pd.Series, optinal): pandas series of spot types for type aware distance adjustment. required if type_aware is used\n",
    "        aware_power (int): power for type-aware distance adjustment\n",
    "        \n",
    "    returns:\n",
    "        sptial distance matrix'''\n",
    "    shortestPath = dijkstra(csgraph = csr_matrix(graph), directed=False, return_predecessors=False)\n",
    "    shortestPath = np.nan_to_num(shortestPath, nan=np.inf) #handle potential inf valyes after dijkstra\n",
    "\n",
    "    if spot_types is not None and spot_ids is not None:\n",
    "        shortestPath_df = pd.DataFrame(shortestPath, index=spot_ids, columns=spot_ids)\n",
    "        shortestPath_df['id1'] = shortestPath_df.index\n",
    "        shortestPath_melted = shortestPath_df.melt(id_vars=['id1'], var_name='id2', value_name='value')\n",
    "\n",
    "        type_aware_df = pd.DataFrame({'spot': spot_ids, 'spot_type': spot_types}, index=spot_ids)\n",
    "        meta1 = type_aware_df.copy()\n",
    "        meta1.columns = ['id1', 'type1']\n",
    "        meta2 = type_aware_df.copy()\n",
    "        meta2.columns = ['id2', 'type2']\n",
    "\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta1, on='id1', how='left')\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta2, on='id2', how='left')\n",
    "\n",
    "        shortestPath_melted['same_type'] = shortestPath_melted['type1'] == shortestPath_melted['type2']\n",
    "        shortestPath_melted.loc[(~shortestPath_melted.smae_type), 'value'] = shortestPath_melted.loc[(~shortestPath_melted.same_type),\n",
    "                                                                                                     'value'] * aware_power\n",
    "        shortestPath_melted.drop(['type1', 'type2', 'same_type'], axis=1, inplace=True)\n",
    "        shortestPath_pivot = shortestPath_melted.pivot(index='id1', columns='id2', values='value')\n",
    "\n",
    "        order = spot_ids\n",
    "        shortestPath = shortestPath_pivot[order].loc[order].values\n",
    "    else:\n",
    "        shortestPath = np.asarray(shortestPath) #ensure it's a numpy array\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = np.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    C_dis = shortestPath / the_max\n",
    "    C_dis -= np.mean(C_dis)\n",
    "\n",
    "    return C_dis\n",
    "\n",
    "def calculate_D_st_from_coords(spatial_coords, X_st=None, k_neighbors=10, graph_mode='distance', aware_st=False, \n",
    "                               spot_types=None, aware_power_st=2, spot_ids=None):\n",
    "    '''calculates the spatial distance matrix D_st for spatial transcriptomics data directly from coordinates and optional spot types\n",
    "    args:\n",
    "        spatial_coords: spatial coordinates of spots (n_spots * 2)\n",
    "        X_st: St gene expression data (not used for D_st calculation itself)\n",
    "        k_neighbors: number of neighbors for knn graph\n",
    "        graph_mode: 'connectivity or 'distance' for knn graph\n",
    "        aware_st: whether to use type-aware distance adjustment\n",
    "        spot_types: pandas series of spot types for type-aware adjustment\n",
    "        aware_power_st: power for type-aware distance adjustment\n",
    "        spot_ids: list or index of spot ids, required if spot_ids is provided\n",
    "        \n",
    "    returns:\n",
    "        np.ndarray: spatial disance matrix D_st'''\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        location_array = spatial_coords.values\n",
    "        if spot_ids is None:\n",
    "            spot_ids = spatial_coords.index.tolist() #use index of dataframe if available\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        location_array = spatial_coords\n",
    "        if spot_ids is None:\n",
    "            spot_ids = list(range(location_array.shape[0])) #generate default ids if not provided\n",
    "\n",
    "    else:\n",
    "        raise TypeError('spatial_coords must be a pandas dataframe or a numpy array')\n",
    "    \n",
    "    print(f'constructing {graph_mode} graph for ST data with k={k_neighbors}.....')\n",
    "    Xgraph_st = construct_graph_spatial(location_array, k=k_neighbors, mode=graph_mode)\n",
    "    \n",
    "    if aware_st:\n",
    "        if spot_types is None or spot_ids is None:\n",
    "            raise ValueError('spot_types and spot_ids must be provided when aware_st=True')\n",
    "        if not isinstance(spot_types, pd.Series):\n",
    "            spot_types = pd.Series(spot_types, idnex=spot_ids) \n",
    "        print('applying type aware distance adjustment for ST data')\n",
    "        print(f'aware power for ST: {aware_power_st}')\n",
    "    else:\n",
    "        spot_types = None \n",
    "\n",
    "    print(f'calculating spatial distances.....')\n",
    "    D_st = distances_cal_spatial(Xgraph_st, spot_ids=spot_ids, spot_types=spot_types, aware_power=aware_power_st)\n",
    "\n",
    "    print('D_st calculation complete')\n",
    "    return D_st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_gw_torch(X_sc, X_st, Y_st, alpha, k=100, G0=None, max_iter = 100, tol=1e-9, device='cuda', n_iter = 1):\n",
    "    n = X_sc.shape[0]\n",
    "    m = X_st.shape[0]\n",
    "\n",
    "    X_sc = X_sc.to(device)\n",
    "    X_st = X_st.to(device)\n",
    "\n",
    "    if not torch.is_tensor(Y_st):\n",
    "        Y_st_tensor = torch.tensor(Y_st, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        Y_st_tensor = Y_st.to(device, dtype=torch.float32)\n",
    "\n",
    "    #calculate distance matrices\n",
    "    print('calculating SC distances with knn-dijkstra.....')\n",
    "    D_sc = calculate_D_sc_torch(X_sc, k_neighbors=k, device=device)\n",
    "\n",
    "    print('Calculating ST distances.....')\n",
    "    D_st = calculate_D_st_from_coords(spatial_coords=Y_st, k_neighbors=15, graph_mode=\"distance\") # Using calculate_D_st_from_coords\n",
    "    D_st = torch.tensor(D_st, dtype=torch.float32, device=device) # Convert D_st to tensor and move to device\n",
    "\n",
    "    #get expression distance matrix\n",
    "    C_exp = torch.cdist(X_sc, X_st, p=2) #euclidean distance\n",
    "    C_exp = C_exp / (torch.max(C_exp) + 1e-16) #normalize\n",
    "\n",
    "    #ensure distance matries are C-contiguouse numpy arrays for POT\n",
    "    D_sc_np = D_sc.cpu().numpy()\n",
    "    D_st_np = D_st.cpu().numpy()\n",
    "    C_exp_np = C_exp.cpu().numpy()\n",
    "    D_sc_np = np.ascontiguousarray(D_sc_np)\n",
    "    D_st_np = np.ascontiguousarray(D_st_np)\n",
    "    C_exp_np = np.ascontiguousarray(C_exp_np)\n",
    "\n",
    "    #uniform distributions\n",
    "    p = ot.unif(n)\n",
    "    q = ot.unif(m)\n",
    "\n",
    "    #anneal the reg param over several steps\n",
    "    T_np = None\n",
    "    for i in range(n_iter):\n",
    "        #run fused gw with POT\n",
    "        T_np, log = ot.gromov.fused_gromov_wasserstein(\n",
    "            M=C_exp_np, C1=D_sc_np, C2=D_st_np,\n",
    "            p=p, q=q, loss_fun='square_loss',\n",
    "            alpha=alpha,\n",
    "            G0=T_np if T_np is not None else (G0.cpu().numpy() if G0 is not None else None),\n",
    "            log=True,\n",
    "            verbose=True,\n",
    "            max_iter = max_iter,\n",
    "            tol_abs=tol\n",
    "        )\n",
    "\n",
    "    fgw_dist = log['fgw_dist']\n",
    "\n",
    "    print(f'fgw distance: {fgw_dist}')\n",
    "\n",
    "    T = torch.tensor(T_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    return T, D_sc, D_st, fgw_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata = pd.read_csv('./data/mousedata_2020/E1z2/simu_sc_counts.csv',index_col=0)\n",
    "scdata = scdata.T\n",
    "stdata = pd.read_csv('data/mousedata_2020/E1z2/simu_st_counts.csv',index_col=0)\n",
    "stdata = stdata.T\n",
    "stgtcelltype = pd.read_csv('./data/mousedata_2020/E1z2/simu_st_celltype.csv',index_col=0)\n",
    "spcoor = pd.read_csv('./data/mousedata_2020/E1z2/simu_st_metadata.csv',index_col=0)\n",
    "scmetadata = pd.read_csv('./data/mousedata_2020/E1z2/metadata.csv',index_col=0)\n",
    "\n",
    "adata = sc.AnnData(scdata,obs=scmetadata)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "scdata = pd.DataFrame(adata.X,index=adata.obs_names,columns=adata.var_names)\n",
    "stadata = sc.AnnData(stdata)\n",
    "sc.pp.normalize_total(stadata)\n",
    "sc.pp.log1p(stadata)\n",
    "stdata = pd.DataFrame(stadata.X,index=stadata.obs_names,columns=stadata.var_names)\n",
    "\n",
    "adata.obsm['spatial'] = scmetadata[['x_global','y_global']].values\n",
    "stadata.obsm['spatial'] = spcoor\n",
    "\n",
    "# Preprocess data (normalize, log transform)\n",
    "adata = sc.AnnData(scdata, obs=scmetadata)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "scdata_processed = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n",
    "X_sc = torch.tensor(scdata_processed.values, dtype=torch.float32)\n",
    "\n",
    "stadata = sc.AnnData(stdata)\n",
    "sc.pp.normalize_total(stadata)\n",
    "sc.pp.log1p(stadata)\n",
    "stdata_processed = pd.DataFrame(stadata.X, index=stadata.obs_names, columns=stadata.var_names)\n",
    "X_st = torch.tensor(stdata_processed.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "Y_st = spcoor.values\n",
    "# --- Run FGW using POT ---\n",
    "T, D_sc, D_st, fgw_dist = fused_gw_torch(\n",
    "    X_sc=X_sc, X_st=X_st, Y_st=Y_st,\n",
    "    alpha=0.3, # Example: balance expression and structure equally\n",
    "    k=300,      # k for SC graph\n",
    "    max_iter=200,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_st = D_st.to(device)\n",
    "D_induced = T @ D_st @ T.t()\n",
    "D_induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sc_coords = np.column_stack([\n",
    "    adata.obs['x_global'].values,\n",
    "    adata.obs['y_global'].values\n",
    "])\n",
    "\n",
    "gt_sc_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, n_genes, n_embedding=[512, 256, 128], dp=0):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_genes, n_embedding[0])\n",
    "        self.bn1 = nn.LayerNorm(n_embedding[0])\n",
    "        self.fc2 = nn.Linear(n_embedding[0], n_embedding[1])\n",
    "        self.bn2 = nn.LayerNorm(n_embedding[1])\n",
    "        self.fc3 = nn.Linear(n_embedding[1], n_embedding[2])\n",
    "        \n",
    "        self.dp = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self, x, isdp=False):\n",
    "        if isdp:\n",
    "            x = self.dp(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        tmp = 0\n",
    "        for x in kernel_val:\n",
    "            tmp += x\n",
    "        return tmp\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for diffusion timesteps\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1, 0, 0))\n",
    "        return emb\n",
    "\n",
    "class STEMDiffusion:\n",
    "    def __init__(\n",
    "        self, \n",
    "        st_gene_expr,\n",
    "        st_coords,\n",
    "        D_st,\n",
    "        sc_gene_expr,\n",
    "        D_induced=None,\n",
    "        outf='./diffusion_output',\n",
    "        device='cuda',\n",
    "        n_genes=None,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        hidden_dim=256,\n",
    "        dp=0.1,\n",
    "        n_timesteps=800,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256\n",
    "    ):\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # Create output directory\n",
    "        self.outf = outf\n",
    "        if not os.path.exists(outf):\n",
    "            os.makedirs(outf)\n",
    "        \n",
    "        self.train_log = os.path.join(outf, 'train.log')\n",
    "        \n",
    "        # Store data\n",
    "        self.st_gene_expr = torch.tensor(st_gene_expr, dtype=torch.float32).to(self.device)\n",
    "        self.st_coords = torch.tensor(st_coords, dtype=torch.float32).to(self.device)\n",
    "        self.sc_gene_expr = torch.tensor(sc_gene_expr, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Store distance matrices\n",
    "        self.D_st = torch.tensor(D_st, dtype=torch.float32).to(self.device)\n",
    "        if D_induced is not None:\n",
    "            self.D_induced = torch.tensor(D_induced, dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            self.D_induced = None\n",
    "        \n",
    "        # Normalize coordinates for diffusion model\n",
    "        coords_min = self.st_coords.min(dim=0)[0]\n",
    "        coords_max = self.st_coords.max(dim=0)[0]\n",
    "        coords_range = coords_max - coords_min\n",
    "        self.st_coords_norm = 2 * (self.st_coords - coords_min) / coords_range - 1\n",
    "        self.coords_min, self.coords_max = coords_min, coords_max\n",
    "        self.coords_range = coords_range\n",
    "        \n",
    "        # STEM parameters\n",
    "        self.n_genes = n_genes or st_gene_expr.shape[1]\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.mmdbatch = mmdbatch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize feature encoder (shared between ST and SC data)\n",
    "        self.netE = FeatureNet(self.n_genes, n_embedding=n_embedding, dp=dp).to(self.device)\n",
    "        \n",
    "        # Initialize diffusion model components\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Feature to hidden projection\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(n_embedding[-1], hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Main network blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ).to(self.device) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim//2, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Setup optimizers and losses\n",
    "        self.optimizer_E = torch.optim.AdamW(self.netE.parameters(), lr=0.002)\n",
    "        self.scheduler_E = lr_scheduler.StepLR(optimizer=self.optimizer_E, step_size=200, gamma=0.5)\n",
    "        \n",
    "        diffusion_params = list(self.time_embed.parameters()) + \\\n",
    "                           list(self.coord_encoder.parameters()) + \\\n",
    "                           list(self.feat_proj.parameters()) + \\\n",
    "                           list(self.blocks.parameters()) + \\\n",
    "                           list(self.final.parameters())\n",
    "        \n",
    "        self.optimizer_diff = torch.optim.AdamW(diffusion_params, lr=1e-4, weight_decay=1e-6)\n",
    "        self.scheduler_diff = lr_scheduler.CosineAnnealingLR(self.optimizer_diff, T_max=3000, eta_min=1e-6)\n",
    "        \n",
    "        self.mmd_fn = MMDLoss()\n",
    "        \n",
    "        # Setup noise schedule for diffusion\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.noise_schedule = self.get_noise_schedule(n_timesteps, beta_start, beta_end)\n",
    "        \n",
    "        # Tracking losses\n",
    "        self.loss_names = ['E', 'E_pred', 'E_circle', 'E_mmd', 'diffusion']\n",
    "    \n",
    "    def get_noise_schedule(self, timesteps=1000, beta1=1e-4, beta2=0.02):\n",
    "        \"\"\"Returns diffusion noise schedule parameters\"\"\"\n",
    "        # Linear schedule\n",
    "        betas = torch.linspace(beta1, beta2, timesteps, device=self.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n",
    "            'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod\n",
    "        }\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise_schedule):\n",
    "        \"\"\"Add noise to coordinates according to timestep t\"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "        \n",
    "        # Add noise according to schedule\n",
    "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "    \n",
    "    def forward_diffusion(self, coords, t, features):\n",
    "        \"\"\"Forward pass of diffusion model, predicting noise from noisy coordinates and conditioning\"\"\"\n",
    "        # Get feature embeddings\n",
    "        feat_emb = self.netE(features, isdp=False)\n",
    "        feat_proj = self.feat_proj(feat_emb)\n",
    "        \n",
    "        # Get time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Get coordinate embedding\n",
    "        coord_emb = self.coord_encoder(coords)\n",
    "        \n",
    "        # Combine all inputs\n",
    "        h = coord_emb + t_emb + feat_proj\n",
    "        \n",
    "        # Process through residual blocks\n",
    "        for block in self.blocks:\n",
    "            h = h + block(h)  # Residual connection\n",
    "        \n",
    "        # Predict noise\n",
    "        return self.final(h)\n",
    "    \n",
    "    def train_encoder(self, n_epochs=1000, ratio_start=0, ratio_end=1.0):\n",
    "        \"\"\"Train the STEM encoder to align ST and SC data\"\"\"\n",
    "        print(\"Training STEM encoder...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting STEM encoder training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, ratio_start={ratio_start}, ratio_end={ratio_end}\\n\")\n",
    "        \n",
    "        # Calculate spatial adjacency matrix\n",
    "        if self.sigma == 0:\n",
    "            nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        else:\n",
    "            nettrue = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                self.st_coords.cpu().numpy(), \n",
    "                self.st_coords.cpu().numpy()\n",
    "            ), device=self.device).to(torch.float32)\n",
    "            \n",
    "            sigma = self.sigma\n",
    "            nettrue = torch.exp(-nettrue**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "            nettrue = F.normalize(nettrue, p=1, dim=1)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Schedule for circle loss weight\n",
    "            ratio = ratio_start + (ratio_end - ratio_start) * min(epoch / (n_epochs * 0.8), 1.0)\n",
    "            \n",
    "            # Forward pass ST data\n",
    "            e_seq_st = self.netE(self.st_gene_expr, True)\n",
    "            \n",
    "            # Sample from SC data due to large size\n",
    "            sc_idx = torch.randint(0, self.sc_gene_expr.shape[0], (min(self.batch_size, self.mmdbatch),), device=self.device)\n",
    "            sc_batch = self.sc_gene_expr[sc_idx]\n",
    "            e_seq_sc = self.netE(sc_batch, False)\n",
    "            \n",
    "            # Calculate losses\n",
    "            self.optimizer_E.zero_grad()\n",
    "            \n",
    "            # Prediction loss (equivalent to netpred in STEM)\n",
    "            netpred = e_seq_st.mm(e_seq_st.t())\n",
    "            loss_E_pred = F.cross_entropy(netpred, nettrue, reduction='mean')\n",
    "            \n",
    "            # Mapping matrices\n",
    "            st2sc = F.softmax(e_seq_st.mm(e_seq_sc.t()), dim=1)\n",
    "            sc2st = F.softmax(e_seq_sc.mm(e_seq_st.t()), dim=1)\n",
    "            \n",
    "            # Circle loss\n",
    "            st2st = torch.log(st2sc.mm(sc2st) + 1e-7)\n",
    "            loss_E_circle = F.kl_div(st2st, nettrue, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # MMD loss\n",
    "            ranidx = torch.randint(0, e_seq_sc.shape[0], (min(self.mmdbatch, e_seq_sc.shape[0]),), device=self.device)\n",
    "            loss_E_mmd = self.mmd_fn(e_seq_st, e_seq_sc[ranidx])\n",
    "            \n",
    "            # Total loss\n",
    "            loss_E = loss_E_pred + self.alpha * loss_E_mmd + ratio * loss_E_circle\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_E.backward()\n",
    "            self.optimizer_E.step()\n",
    "            self.scheduler_E.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Encoder epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss_E: {loss_E.item():.6f}, \"\n",
    "                          f\"Loss_E_pred: {loss_E_pred.item():.6f}, \"\n",
    "                          f\"Loss_E_circle: {loss_E_circle.item():.6f}, \"\n",
    "                          f\"Loss_E_mmd: {loss_E_mmd.item():.6f}, \"\n",
    "                          f\"Ratio: {ratio:.4f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'netE_state_dict': self.netE.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_E.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_E.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'encoder_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Save final encoder\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_encoder.pt'))\n",
    "        \n",
    "        print(\"Encoder training complete!\")\n",
    "    \n",
    "    def train_diffusion(self, n_epochs=2000, lambda_struct=10.0):\n",
    "        \"\"\"Train diffusion model using the trained encoder\"\"\"\n",
    "        print(\"Training diffusion model...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting diffusion model training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, lambda_struct={lambda_struct}\\n\")\n",
    "        \n",
    "        # Freeze encoder during diffusion training\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Precompute adjacency matrix for structure loss\n",
    "        def compute_adjacency_matrix(distances, sigma=3.0):\n",
    "            weights = torch.exp(-(distances ** 2) / (2 * sigma * sigma))\n",
    "            # Zero out self-connections\n",
    "            weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "            # Normalize rows to sum to 1\n",
    "            row_sums = weights.sum(dim=1, keepdim=True)\n",
    "            adjacency = weights / (row_sums + 1e-8)\n",
    "            # Explicit second normalization to guarantee rows sum to 1\n",
    "            row_sums = adjacency.sum(dim=1, keepdim=True)\n",
    "            adjacency = adjacency / row_sums\n",
    "\n",
    "            return adjacency\n",
    "        \n",
    "        st_adj = compute_adjacency_matrix(self.D_st, sigma=self.sigma)\n",
    "        \n",
    "        # Keep track of best model\n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch of ST data\n",
    "            idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "            coords = self.st_coords_norm[idx]\n",
    "            features = self.st_gene_expr[idx]\n",
    "            sub_adj = st_adj[idx][:, idx]\n",
    "\n",
    "            print(sub_adj)\n",
    "            \n",
    "            # Sample timesteps with emphasis on early and late stages\n",
    "            if np.random.random() < 0.3:\n",
    "                # Focus on early timesteps (high noise)\n",
    "                t = torch.randint(int(0.7 * self.n_timesteps), self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            elif np.random.random() < 0.6:\n",
    "                # Focus on late timesteps (low noise, more structure)\n",
    "                t = torch.randint(0, int(0.3 * self.n_timesteps), (self.batch_size,), device=self.device)\n",
    "            else:\n",
    "                # Random timesteps across the range\n",
    "                t = torch.randint(0, self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            \n",
    "            # Add noise to coordinates\n",
    "            noisy_coords, target_noise = self.add_noise(coords, t, self.noise_schedule)\n",
    "            \n",
    "            # Forward pass to predict noise\n",
    "            pred_noise = self.forward_diffusion(noisy_coords, t.unsqueeze(1).float() / self.n_timesteps, features)\n",
    "            \n",
    "            # Compute diffusion loss (noise prediction MSE)\n",
    "            diffusion_loss = F.mse_loss(pred_noise, target_noise)\n",
    "            \n",
    "            # Compute denoised coordinates for structure loss\n",
    "            sqrt_alphas_cumprod_t = self.noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = self.noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "            pred_coords = (noisy_coords - sqrt_one_minus_alphas_cumprod_t * pred_noise) / sqrt_alphas_cumprod_t\n",
    "            \n",
    "            # Compute pairwise distances and adjacency for predicted coordinates\n",
    "            pred_distances = torch.cdist(pred_coords, pred_coords, p=2)\n",
    "            pred_adj = compute_adjacency_matrix(pred_distances, sigma=self.sigma)\n",
    "\n",
    "            print(pred_adj)\n",
    "\n",
    "            # assert torch.allclose(pred_adj.sum(dim=1), torch.ones_like(pred_adj.sum(dim=1)), rtol=1e-5)\n",
    "            # assert torch.allclose(sub_adj.sum(dim=1), torch.ones_like(sub_adj.sum(dim=1)), rtol=1e-5)\n",
    "            \n",
    "            # Structure loss (KL divergence between adjacency matrices)\n",
    "            # Using KL divergence as you preferred\n",
    "            struct_loss = F.kl_div(\n",
    "                torch.log(pred_adj + 1e-10),\n",
    "                sub_adj,\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "\n",
    "            struct_loss = struct_loss.sum(dim=1).mean()\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = diffusion_loss + lambda_struct * struct_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer_diff.zero_grad()\n",
    "            total_loss.backward()\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.time_embed.parameters()) + \n",
    "                list(self.coord_encoder.parameters()) + \n",
    "                list(self.feat_proj.parameters()) + \n",
    "                list(self.blocks.parameters()) + \n",
    "                list(self.final.parameters()),\n",
    "                1.0\n",
    "            )\n",
    "            self.optimizer_diff.step()\n",
    "            self.scheduler_diff.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                best_state = {\n",
    "                    'epoch': epoch,\n",
    "                    'time_embed': self.time_embed.state_dict(),\n",
    "                    'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                    'feat_proj': self.feat_proj.state_dict(),\n",
    "                    'blocks': [block.state_dict() for block in self.blocks],\n",
    "                    'final': self.final.state_dict(),\n",
    "                    'loss': best_loss\n",
    "                }\n",
    "                # Save best model\n",
    "                torch.save(best_state, os.path.join(self.outf, 'best_diffusion_model.pt'))\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Diffusion epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss: {total_loss.item():.6f}, \"\n",
    "                          f\"Diffusion Loss: {diffusion_loss.item():.6f}, \"\n",
    "                          f\"Structure Loss: {struct_loss.item():.6f}, \"\n",
    "                          f\"LR: {self.scheduler_diff.get_last_lr()[0]:.6f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'time_embed': self.time_embed.state_dict(),\n",
    "                        'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                        'feat_proj': self.feat_proj.state_dict(),\n",
    "                        'blocks': [block.state_dict() for block in self.blocks],\n",
    "                        'final': self.final.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_diff.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_diff.state_dict(),\n",
    "                        'loss': total_loss.item()\n",
    "                    }, os.path.join(self.outf, f'diffusion_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_state:\n",
    "            self.time_embed.load_state_dict(best_state['time_embed'])\n",
    "            self.coord_encoder.load_state_dict(best_state['coord_encoder'])\n",
    "            self.feat_proj.load_state_dict(best_state['feat_proj'])\n",
    "            for i, block_state in enumerate(best_state['blocks']):\n",
    "                self.blocks[i].load_state_dict(block_state)\n",
    "            self.final.load_state_dict(best_state['final'])\n",
    "            print(f\"Restored best model from epoch {best_state['epoch']} with loss {best_state['loss']:.6f}\")\n",
    "        \n",
    "        print(\"Diffusion training complete!\")\n",
    "    \n",
    "    def train(self, encoder_epochs=1000, diffusion_epochs=2000, ratio_start=0, ratio_end=1.0, lambda_struct=10.0):\n",
    "        \"\"\"Combined training of encoder and diffusion model\"\"\"\n",
    "        # First train the encoder to align ST and SC\n",
    "        self.train_encoder(n_epochs=encoder_epochs, ratio_start=ratio_start, ratio_end=ratio_end)\n",
    "        \n",
    "        # Then train the diffusion model\n",
    "        self.train_diffusion(n_epochs=diffusion_epochs, lambda_struct=lambda_struct)\n",
    "    \n",
    "    def generate_st_coordinates_batched(self, batch_size=64, timesteps=None):\n",
    "        \"\"\"Generate ST coordinates in batches to avoid memory issues\"\"\"\n",
    "        print(\"Generating ST coordinates for evaluation in batches...\")\n",
    "        self.netE.eval()\n",
    "        \n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_spots = len(self.st_gene_expr)\n",
    "        n_batches = (n_spots + batch_size - 1) // batch_size\n",
    "        \n",
    "        all_coords = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = min((b + 1) * batch_size, n_spots)\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            # Get batch features\n",
    "            features = self.st_gene_expr[start_idx:end_idx]\n",
    "            \n",
    "            # Start from random noise\n",
    "            x = torch.randn(batch_size_actual, 2, device=self.device)\n",
    "            \n",
    "            # Gradually denoise\n",
    "            for t in tqdm(range(timesteps-1, -1, -1), \n",
    "                         desc=f\"Generating batch {b+1}/{n_batches}\",\n",
    "                         leave=(b == n_batches-1)):  # Only keep last progress bar\n",
    "                \n",
    "                # Create timestep tensor\n",
    "                time_tensor = torch.ones(batch_size_actual, 1, device=self.device) * t / timesteps\n",
    "                \n",
    "                # Predict noise\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                \n",
    "                # Get parameters for this timestep\n",
    "                alpha_t = self.noise_schedule['alphas'][t]\n",
    "                alpha_cumprod_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                beta_t = self.noise_schedule['betas'][t]\n",
    "                \n",
    "                # Apply noise (except for last step)\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = 0\n",
    "                \n",
    "                # Update sample with reverse diffusion step\n",
    "                x = (1 / torch.sqrt(alpha_t)) * (\n",
    "                    x - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * pred_noise\n",
    "                ) + torch.sqrt(beta_t) * noise\n",
    "            \n",
    "            # Store batch results\n",
    "            all_coords.append(x.detach().cpu())\n",
    "        \n",
    "        # Combine all batches\n",
    "        st_gen_coords_norm = torch.cat(all_coords, dim=0)\n",
    "        \n",
    "        # Denormalize coordinates\n",
    "        st_gen_coords = self.denormalize_coordinates(st_gen_coords_norm)\n",
    "        \n",
    "        print(\"Generation complete!\")\n",
    "        return st_gen_coords\n",
    "    \n",
    "    def sample_sc_coordinates_batched(self, batch_size=64, timesteps=None, use_structure_guidance=True):\n",
    "        \"\"\"Sample SC coordinates in batches to avoid memory issues\"\"\"\n",
    "        print(\"Sampling SC coordinates in batches...\")\n",
    "        self.netE.eval()\n",
    "        \n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_cells = len(self.sc_gene_expr)\n",
    "        n_batches = (n_cells + batch_size - 1) // batch_size\n",
    "        \n",
    "        all_coords = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = min((b + 1) * batch_size, n_cells)\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            # Get batch features\n",
    "            features = self.sc_gene_expr[start_idx:end_idx]\n",
    "            \n",
    "            # Start from random noise\n",
    "            x = torch.randn(batch_size_actual, 2, device=self.device)\n",
    "            \n",
    "            # Get relevant subset of D_induced for structure guidance if available\n",
    "            if use_structure_guidance and self.D_induced is not None:\n",
    "                sub_D_induced = self.D_induced[start_idx:end_idx, start_idx:end_idx]\n",
    "                \n",
    "                # Compute adjacency matrix\n",
    "                weights = torch.exp(-(sub_D_induced ** 2) / (2 * self.sigma * self.sigma))\n",
    "                weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "                row_sums = weights.sum(dim=1, keepdim=True)\n",
    "                target_adj = weights / (row_sums + 1e-8)\n",
    "            else:\n",
    "                target_adj = None\n",
    "            \n",
    "            # Gradually denoise\n",
    "            for t in tqdm(range(timesteps-1, -1, -1), \n",
    "                         desc=f\"Sampling batch {b+1}/{n_batches}\",\n",
    "                         leave=(b == n_batches-1)):  # Only keep last progress bar\n",
    "                \n",
    "                # Create timestep tensor\n",
    "                time_tensor = torch.ones(batch_size_actual, 1, device=self.device) * t / timesteps\n",
    "                \n",
    "                # Predict noise\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                \n",
    "                # Get parameters for this timestep\n",
    "                alpha_t = self.noise_schedule['alphas'][t]\n",
    "                alpha_cumprod_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                beta_t = self.noise_schedule['betas'][t]\n",
    "                \n",
    "                # Apply noise (except for last step)\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = 0\n",
    "                \n",
    "                # Update sample with reverse diffusion step\n",
    "                x = (1 / torch.sqrt(alpha_t)) * (\n",
    "                    x - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * pred_noise\n",
    "                ) + torch.sqrt(beta_t) * noise\n",
    "                \n",
    "                # Apply structure guidance in later steps if available\n",
    "                if use_structure_guidance and target_adj is not None and t < timesteps * 0.7 and t % 10 == 0:\n",
    "                    x = self.adjust_coordinates_to_match_structure(x, target_adj, t, timesteps)\n",
    "            \n",
    "            # Store batch results\n",
    "            all_coords.append(x.detach().cpu())\n",
    "        \n",
    "        # Combine all batches\n",
    "        sc_coords_norm = torch.cat(all_coords, dim=0)\n",
    "        \n",
    "        # Denormalize coordinates\n",
    "        sc_coords = self.denormalize_coordinates(sc_coords_norm)\n",
    "        \n",
    "        print(\"Sampling complete!\")\n",
    "        return sc_coords\n",
    "    \n",
    "    def adjust_coordinates_to_match_structure(self, coords, target_adj, t, timesteps, lr=0.05):\n",
    "        \"\"\"Adjust coordinates to better match target adjacency structure\"\"\"\n",
    "        # Compute current adjacency matrix\n",
    "        distances = torch.cdist(coords, coords, p=2)\n",
    "        weights = torch.exp(-(distances ** 2) / (2 * self.sigma * self.sigma))\n",
    "        weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "        row_sums = weights.sum(dim=1, keepdim=True)\n",
    "        cur_adj = weights / (row_sums + 1e-8)\n",
    "        \n",
    "        # Adjust learning rate based on timestep (smaller adjustments near the end)\n",
    "        lr_scale = 0.1 * (t / timesteps) + 0.01\n",
    "        \n",
    "        # Compute adjustment direction\n",
    "        diff = cur_adj - target_adj\n",
    "        \n",
    "        # Direction vectors between all pairs\n",
    "        coord_i = coords.unsqueeze(1)  # [n, 1, 2]\n",
    "        coord_j = coords.unsqueeze(0)  # [1, n, 2]\n",
    "        directions = coord_i - coord_j  # [n, n, 2]\n",
    "        \n",
    "        # Normalize directions\n",
    "        distances = torch.norm(directions, dim=2, keepdim=True)\n",
    "        norm_directions = directions / (distances + 1e-8)\n",
    "        \n",
    "        # Scale directions by adjacency difference\n",
    "        delta = diff.unsqueeze(2) * norm_directions  # [n, n, 2]\n",
    "        \n",
    "        # Sum influences from all other points\n",
    "        adjustments = -delta.sum(dim=1)  # [n, 2]\n",
    "        \n",
    "        # Apply adjustments with learning rate\n",
    "        adjusted_coords = coords - lr_scale * lr * adjustments\n",
    "        \n",
    "        return adjusted_coords\n",
    "    \n",
    "    def denormalize_coordinates(self, normalized_coords):\n",
    "        \"\"\"Convert normalized coordinates back to original scale\"\"\"\n",
    "        if isinstance(normalized_coords, torch.Tensor):\n",
    "            # Make sure coords_range and coords_min are on the same device\n",
    "            coords_range = self.coords_range.to(normalized_coords.device)\n",
    "            coords_min = self.coords_min.to(normalized_coords.device)\n",
    "            \n",
    "            # Convert from [-1,1] to original scale\n",
    "            original_coords = (normalized_coords + 1) / 2 * coords_range + coords_min\n",
    "            return original_coords\n",
    "        else:\n",
    "            # Handle numpy arrays\n",
    "            coords_range = self.coords_range.cpu().numpy()\n",
    "            coords_min = self.coords_min.cpu().numpy()\n",
    "            original_coords = (normalized_coords + 1) / 2 * coords_range + coords_min\n",
    "            return original_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "stem_diffusion = STEMDiffusion(\n",
    "    st_gene_expr=X_st,\n",
    "    st_coords=Y_st,\n",
    "    D_st=D_st,  # Distance matrix for ST data\n",
    "    sc_gene_expr=X_sc,\n",
    "    D_induced=D_induced,  # Induced distance matrix from GW-OT\n",
    "    outf='./stem_diffusion_output',\n",
    "    device='cuda',\n",
    "    n_genes=X_st.shape[1],\n",
    "    n_embedding=[512, 256, 128],\n",
    "    hidden_dim=256,\n",
    "    dp=0.1,\n",
    "    n_timesteps=800,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    sigma=3.0,\n",
    "    alpha=0.8,\n",
    "    mmdbatch=1000,\n",
    "    batch_size=256  # Adjust based on memory\n",
    ")\n",
    "\n",
    "# Train first encoder component (STEM-inspired) to align ST and SC\n",
    "stem_diffusion.train_encoder(\n",
    "    n_epochs=1000,\n",
    "    ratio_start=0,\n",
    "    ratio_end=1.0  # Gradually increase circle loss weight\n",
    ")\n",
    "\n",
    "# Train diffusion model using trained encoder\n",
    "stem_diffusion.train_diffusion(\n",
    "    n_epochs=2000,\n",
    "    lambda_struct=10.0  # Weight for KL divergence structure loss\n",
    ")\n",
    "\n",
    "# Or use the combined training method\n",
    "# stem_diffusion.train(encoder_epochs=1000, diffusion_epochs=2000)\n",
    "\n",
    "# Generate ST coordinates to evaluate model (using batched approach to save memory)\n",
    "st_gen_coords = stem_diffusion.generate_st_coordinates_batched(batch_size=64)\n",
    "\n",
    "# Visualize and compare with original\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(original_coords, generated_coords, title=\"Comparison of Original vs Generated ST Coordinates\"):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Plot original coordinates\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(original_coords[:, 0], original_coords[:, 1], alpha=0.7, s=10)\n",
    "    plt.title(\"Original ST Coordinates\")\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Plot generated coordinates\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(generated_coords[:, 0], generated_coords[:, 1], alpha=0.7, s=10)\n",
    "    plt.title(\"Generated ST Coordinates\")\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to numpy if needed\n",
    "if isinstance(Y_st, torch.Tensor):\n",
    "    Y_st_np = Y_st.cpu().numpy()\n",
    "else:\n",
    "    Y_st_np = Y_st\n",
    "\n",
    "if isinstance(st_gen_coords, torch.Tensor):\n",
    "    st_gen_np = st_gen_coords.cpu().numpy()\n",
    "else:\n",
    "    st_gen_np = st_gen_coords\n",
    "\n",
    "# Plot ST comparison\n",
    "plot_comparison(Y_st_np, st_gen_np)\n",
    "\n",
    "# Once ST results look good, generate SC coordinates (also batched)\n",
    "sc_coords = stem_diffusion.sample_sc_coordinates_batched(\n",
    "    batch_size=64,\n",
    "    timesteps=800,\n",
    "    use_structure_guidance=True  # Use D_induced to guide generation\n",
    ")\n",
    "\n",
    "# Convert PyTorch tensor to NumPy array before assigning\n",
    "if isinstance(sc_coords, torch.Tensor):\n",
    "    sc_coords_np = sc_coords.cpu().numpy()\n",
    "else:\n",
    "    sc_coords_np = sc_coords\n",
    "\n",
    "# Now assign the NumPy array to the obsm attribute\n",
    "adata.obsm['stem_diffusion_coords'] = sc_coords_np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))  # your preferred size\n",
    "\n",
    "\n",
    "# Visualization should now work\n",
    "import scanpy as sc\n",
    "sc.pl.embedding(adata, basis='stem_diffusion_coords', color='celltype_mapped_refined',\n",
    "                size=75, title='SC spatial coordinates (STEM-Diffusion Model)',\n",
    "                palette='tab20', legend_loc='right margin', legend_fontsize=10, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PyTorch tensor to NumPy array before assigning\n",
    "if isinstance(sc_coords, torch.Tensor):\n",
    "    sc_coords_np = sc_coords.cpu().numpy()\n",
    "else:\n",
    "    sc_coords_np = sc_coords\n",
    "\n",
    "# Now assign the NumPy array to the obsm attribute\n",
    "adata.obsm['stem_diffusion_coords'] = sc_coords_np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))  # your preferred size\n",
    "\n",
    "\n",
    "# Visualization should now work\n",
    "import scanpy as sc\n",
    "sc.pl.embedding(adata, basis='stem_diffusion_coords', color='celltype_mapped_refined',\n",
    "                size=75, title='SC spatial coordinates (STEM-Diffusion Model)',\n",
    "                palette='tab20', legend_loc='right margin', legend_fontsize=10, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
