{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Cell 2: force single‐threaded BLAS\n",
    "os.environ[\"OMP_NUM_THREADS\"]       = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: actually cap BLAS to 1 thread\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# 'blas' covers OpenBLAS, MKL, etc.\n",
    "threadpool_limits(limits=1, user_api='blas')\n",
    "\n",
    "# now import as usual, no more warning\n",
    "import numpy as np\n",
    "import scipy\n",
    "# … any other packages that use OpenBLAS …\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_torch(X, k, mode='connectivity', metric = 'minkowski', p=2, device='cuda'):\n",
    "    '''construct knn graph with torch and gpu\n",
    "    args:\n",
    "        X: input data containing features (torch tensor)\n",
    "        k: number of neighbors for each data point\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric (now euclidean supported for gpu knn)\n",
    "        p: param for minkowski (not used if metric is euclidean)\n",
    "    \n",
    "    Returns:\n",
    "        knn graph as a pytorch sparse tensor (coo format) or dense tensor depending on mode     \n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'.\"\n",
    "    assert metric == 'euclidean', \"for gpu knn, only 'euclidean' metric is currently supported in this implementation\"\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "        mode_knn = 'connectivity'\n",
    "    else:\n",
    "        include_self = False\n",
    "        mode_knn = 'distance'\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm='auto')\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_cpu = X.cpu().numpy()\n",
    "    else:\n",
    "        X_cpu = X.numpy()\n",
    "\n",
    "    knn.fit(X_cpu)\n",
    "    knn_graph_cpu = kneighbors_graph(knn, k, mode=mode_knn, include_self=include_self, metric=metric) #scipy sparse matrix on cpu\n",
    "    knn_graph_coo = knn_graph_cpu.tocoo()\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        knn_graph = torch.sparse_coo_tensor(torch.LongTensor([knn_graph_coo.row, knn_graph_coo.col]),\n",
    "                                            torch.FloatTensor(knn_graph_coo.data),\n",
    "                                            size = knn_graph_coo.shape).to(device)\n",
    "    elif mode == 'distance':\n",
    "        knn_graph_dense = torch.tensor(knn_graph_cpu.toarray(), dtype=torch.float32, device=device) #move to gpu as dense tensor\n",
    "        knn_graph = knn_graph_dense\n",
    "    \n",
    "    return knn_graph\n",
    "    \n",
    "def distances_cal_torch(graph, type_aware=None, aware_power =2, device='cuda'):\n",
    "    '''\n",
    "    calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (pytorch sparse or dense tensor)\n",
    "        type_aware: not implemented in this torch version for simplicity\n",
    "        aware_power: same ^^\n",
    "        device (str): 'cpu' or 'cuda' device to use\n",
    "    Returns:\n",
    "        distance matrix as a torch tensor\n",
    "    '''\n",
    "\n",
    "    if isinstance(graph, torch.Tensor) and graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().to_dense().numpy())\n",
    "    elif isinstance(graph, torch.Tensor) and not graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().numpy())\n",
    "    else:\n",
    "        graph_cpu_csr = csr_matrix(graph) #assume scipy sparse matrix if not torch tensor\n",
    "\n",
    "    shortestPath_cpu = dijkstra(csgraph = graph_cpu_csr, directed=False, return_predecessors=False) #dijkstra on cpu\n",
    "    shortestPath = torch.tensor(shortestPath_cpu, dtype=torch.float32, device=device)\n",
    "\n",
    "    # the_max = torch.nanmax(shortestPath[shortestPath != float('inf')])\n",
    "    # shortestPath[shortestPath > the_max] = the_max\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = torch.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    original_max_distance = the_max.item()\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= torch.mean(C_dis)\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_sc_torch(X_sc, k_neighbors=10, graph_mode='connectivity', device='cpu'):\n",
    "    '''calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (torch sparse or dense tensor)\n",
    "        type_aware: not implemented\n",
    "        aware_power: same ^^\n",
    "        \n",
    "    returns:\n",
    "        distanced matrix as torch tensor'''\n",
    "    \n",
    "    if not isinstance(X_sc, torch.Tensor):\n",
    "        raise TypeError('Input X_sc must be a pytorch tensor')\n",
    "    \n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_sc = X_sc.cuda(device=device)\n",
    "    else:\n",
    "        X_sc = X_sc.cpu()\n",
    "        device= 'cpu'\n",
    "\n",
    "    print(f'using device: {device}')\n",
    "    print(f'constructing knn graph...')\n",
    "    # X_normalized = normalize(X_sc.cpu().numpy(), norm='l2') #normalize on cpu for sklearn knn\n",
    "    X_normalized = X_sc\n",
    "    X_normalized_torch = torch.tensor(X_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "    Xgraph = construct_graph_torch(X_normalized_torch, k=k_neighbors, mode=graph_mode, metric='euclidean', device=device)\n",
    "\n",
    "    print('calculating distances from graph....')\n",
    "    D_sc, sc_max_distance = distances_cal_torch(Xgraph, device=device)\n",
    "\n",
    "    print('D_sc calculation complete')\n",
    "    \n",
    "    return D_sc, sc_max_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot\n",
    "\n",
    "def construct_graph_spatial(location_array, k, mode='distance', metric='euclidean', p=2):\n",
    "    '''construct KNN graph based on spatial coordinates\n",
    "    args:\n",
    "        location_array: spatial coordinates of spots (n-spots * 2)\n",
    "        k: number of neighbors for each spot\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric for knn (p=2 is euclidean)\n",
    "        p: param for minkowski if connectivity\n",
    "        \n",
    "    returns:\n",
    "        scipy.sparse.csr_matrix: knn graph in csr format\n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'\"\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "    else:\n",
    "        include_self = False\n",
    "    \n",
    "    c_graph = kneighbors_graph(location_array, k, mode=mode, metric=metric, include_self=include_self, p=p)\n",
    "    return c_graph\n",
    "\n",
    "def distances_cal_spatial(graph, spot_ids=None, spot_types=None, aware_power=2):\n",
    "    '''calculate spatial distance matrix from knn graph\n",
    "    args:\n",
    "        graph (scipy.sparse.csr_matrix): knn graph\n",
    "        spot_ids (list, optional): list of spot ids corresponding to the rows/cols of the graph. required if type_aware is used\n",
    "        spot_types (pd.Series, optinal): pandas series of spot types for type aware distance adjustment. required if type_aware is used\n",
    "        aware_power (int): power for type-aware distance adjustment\n",
    "        \n",
    "    returns:\n",
    "        sptial distance matrix'''\n",
    "    shortestPath = dijkstra(csgraph = csr_matrix(graph), directed=False, return_predecessors=False)\n",
    "    shortestPath = np.nan_to_num(shortestPath, nan=np.inf) #handle potential inf valyes after dijkstra\n",
    "\n",
    "    if spot_types is not None and spot_ids is not None:\n",
    "        shortestPath_df = pd.DataFrame(shortestPath, index=spot_ids, columns=spot_ids)\n",
    "        shortestPath_df['id1'] = shortestPath_df.index\n",
    "        shortestPath_melted = shortestPath_df.melt(id_vars=['id1'], var_name='id2', value_name='value')\n",
    "\n",
    "        type_aware_df = pd.DataFrame({'spot': spot_ids, 'spot_type': spot_types}, index=spot_ids)\n",
    "        meta1 = type_aware_df.copy()\n",
    "        meta1.columns = ['id1', 'type1']\n",
    "        meta2 = type_aware_df.copy()\n",
    "        meta2.columns = ['id2', 'type2']\n",
    "\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta1, on='id1', how='left')\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta2, on='id2', how='left')\n",
    "\n",
    "        shortestPath_melted['same_type'] = shortestPath_melted['type1'] == shortestPath_melted['type2']\n",
    "        shortestPath_melted.loc[(~shortestPath_melted.smae_type), 'value'] = shortestPath_melted.loc[(~shortestPath_melted.same_type),\n",
    "                                                                                                     'value'] * aware_power\n",
    "        shortestPath_melted.drop(['type1', 'type2', 'same_type'], axis=1, inplace=True)\n",
    "        shortestPath_pivot = shortestPath_melted.pivot(index='id1', columns='id2', values='value')\n",
    "\n",
    "        order = spot_ids\n",
    "        shortestPath = shortestPath_pivot[order].loc[order].values\n",
    "    else:\n",
    "        shortestPath = np.asarray(shortestPath) #ensure it's a numpy array\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = np.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    #store original max distance for scale reference\n",
    "    original_max_distance = the_max\n",
    "    C_dis = shortestPath / the_max\n",
    "    # C_dis = shortestPath\n",
    "    # C_dis -= np.mean(C_dis)\n",
    "\n",
    "    return C_dis, original_max_distance\n",
    "\n",
    "def calculate_D_st_from_coords(spatial_coords, X_st=None, k_neighbors=10, graph_mode='distance', aware_st=False, \n",
    "                               spot_types=None, aware_power_st=2, spot_ids=None):\n",
    "    '''calculates the spatial distance matrix D_st for spatial transcriptomics data directly from coordinates and optional spot types\n",
    "    args:\n",
    "        spatial_coords: spatial coordinates of spots (n_spots * 2)\n",
    "        X_st: St gene expression data (not used for D_st calculation itself)\n",
    "        k_neighbors: number of neighbors for knn graph\n",
    "        graph_mode: 'connectivity or 'distance' for knn graph\n",
    "        aware_st: whether to use type-aware distance adjustment\n",
    "        spot_types: pandas series of spot types for type-aware adjustment\n",
    "        aware_power_st: power for type-aware distance adjustment\n",
    "        spot_ids: list or index of spot ids, required if spot_ids is provided\n",
    "        \n",
    "    returns:\n",
    "        np.ndarray: spatial disance matrix D_st'''\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        location_array = spatial_coords.values\n",
    "        if spot_ids is None:\n",
    "            spot_ids = spatial_coords.index.tolist() #use index of dataframe if available\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        location_array = spatial_coords\n",
    "        if spot_ids is None:\n",
    "            spot_ids = list(range(location_array.shape[0])) #generate default ids if not provided\n",
    "\n",
    "    else:\n",
    "        raise TypeError('spatial_coords must be a pandas dataframe or a numpy array')\n",
    "    \n",
    "    print(f'constructing {graph_mode} graph for ST data with k={k_neighbors}.....')\n",
    "    Xgraph_st = construct_graph_spatial(location_array, k=k_neighbors, mode=graph_mode)\n",
    "    \n",
    "    if aware_st:\n",
    "        if spot_types is None or spot_ids is None:\n",
    "            raise ValueError('spot_types and spot_ids must be provided when aware_st=True')\n",
    "        if not isinstance(spot_types, pd.Series):\n",
    "            spot_types = pd.Series(spot_types, idnex=spot_ids) \n",
    "        print('applying type aware distance adjustment for ST data')\n",
    "        print(f'aware power for ST: {aware_power_st}')\n",
    "    else:\n",
    "        spot_types = None \n",
    "\n",
    "    print(f'calculating spatial distances.....')\n",
    "    D_st, st_max_distance = distances_cal_spatial(Xgraph_st, spot_ids=spot_ids, spot_types=spot_types, aware_power=aware_power_st)\n",
    "\n",
    "    print('D_st calculation complete')\n",
    "    return D_st, st_max_distance\n",
    "\n",
    "\n",
    "def calculate_D_st_euclidean(spatial_coords):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance matrix for ST spots.\n",
    "    \n",
    "    Args:\n",
    "        spatial_coords: (m_spots, 2) spatial coordinates\n",
    "        \n",
    "    Returns:\n",
    "        D_st_euclid: (m_spots, m_spots) normalized Euclidean distance matrix\n",
    "    \"\"\"\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        coords_array = spatial_coords.values\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        coords_array = spatial_coords\n",
    "    else:\n",
    "        coords_array = np.array(spatial_coords)\n",
    "    \n",
    "    # Compute pairwise Euclidean distances\n",
    "    D_euclid = squareform(pdist(coords_array, metric='euclidean'))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    max_dist = D_euclid.max()\n",
    "    if max_dist > 0:\n",
    "        D_euclid = D_euclid / max_dist\n",
    "    \n",
    "    return D_euclid.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import ot\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "def compute_feature_cost_matrix(X_sc, X_st, metric='euclidean'):\n",
    "    \"\"\"\n",
    "    Compute feature dissimilarity matrix M between SC cells and ST spots.\n",
    "    \n",
    "    Args:\n",
    "        X_sc: SC gene expression (n_cells, n_genes)\n",
    "        X_st: ST gene expression (m_spots, n_genes) \n",
    "        metric: Distance metric ('euclidean', 'cosine', 'correlation')\n",
    "    \n",
    "    Returns:\n",
    "        M: Feature cost matrix (n_cells, m_spots)\n",
    "    \"\"\"\n",
    "    if metric == 'euclidean':\n",
    "        M = euclidean_distances(X_sc, X_st)\n",
    "    elif metric == 'cosine':\n",
    "        # Convert cosine similarity to distance\n",
    "        cos_sim = cosine_similarity(X_sc, X_st)\n",
    "        M = 1.0 - cos_sim\n",
    "    elif metric == 'correlation':\n",
    "        # Pearson correlation distance\n",
    "        M = np.zeros((X_sc.shape[0], X_st.shape[0]))\n",
    "        for i in range(X_sc.shape[0]):\n",
    "            for j in range(X_st.shape[0]):\n",
    "                corr = np.corrcoef(X_sc[i], X_st[j])[0,1]\n",
    "                M[i,j] = 1.0 - corr if not np.isnan(corr) else 1.0\n",
    "    \n",
    "    return M.astype(np.float32)\n",
    "\n",
    "def compute_marginal_weights(M, method='exponential'):\n",
    "    \"\"\"\n",
    "    Compute marginal weights for unbalanced OT from feature cost matrix.\n",
    "    \n",
    "    Args:\n",
    "        M: Feature cost matrix (n_cells, m_spots)\n",
    "        method: How to compute weights ('uniform', 'exponential', 'softmax')\n",
    "    \n",
    "    Returns:\n",
    "        w_a: Cell weights (n_cells,)\n",
    "        w_b: Spot weights (m_spots,)\n",
    "    \"\"\"\n",
    "    if method == 'uniform':\n",
    "        w_a = np.ones(M.shape[0]) / M.shape[0]\n",
    "        w_b = np.ones(M.shape[1]) / M.shape[1]\n",
    "    elif method == 'exponential':\n",
    "        # Use exp(-M) as similarity, then normalize\n",
    "        weight_matrix = np.exp(-M)\n",
    "        w_a = np.sum(weight_matrix, axis=1)\n",
    "        w_b = np.sum(weight_matrix, axis=0)\n",
    "        w_a = w_a / np.sum(w_a)\n",
    "        w_b = w_b / np.sum(w_b)\n",
    "    elif method == 'softmax':\n",
    "        # Softmax over each row/column\n",
    "        w_a = np.sum(np.exp(-M), axis=1)\n",
    "        w_b = np.sum(np.exp(-M), axis=0)\n",
    "        w_a = w_a / np.sum(w_a)\n",
    "        w_b = w_b / np.sum(w_b)\n",
    "    \n",
    "    return w_a.astype(np.float32), w_b.astype(np.float32)\n",
    "\n",
    "def unbalanced_optimal_transport(w_a, w_b, cost_matrix, epsilon=0.1, rho=100.0, max_iter=1000, tol=1e-7):\n",
    "    \"\"\"\n",
    "    Solve unbalanced optimal transport using Sinkhorn-like iterations.\n",
    "    \n",
    "    Args:\n",
    "        w_a: Source marginals (n,)\n",
    "        w_b: Target marginals (m,)\n",
    "        cost_matrix: Transport cost (n, m)\n",
    "        epsilon: Entropic regularization\n",
    "        rho: KL penalty weight for unbalanced transport\n",
    "        max_iter: Maximum iterations\n",
    "        tol: Convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "        gamma: Transport plan (n, m)\n",
    "    \"\"\"\n",
    "    lmbda = rho / (rho + epsilon) if not np.isinf(rho) else 1.0\n",
    "    \n",
    "    w_a = w_a.reshape(-1, 1)\n",
    "    w_b = w_b.reshape(-1, 1)\n",
    "    \n",
    "    n, m = cost_matrix.shape\n",
    "    u = np.zeros((n, 1))\n",
    "    v = np.zeros((m, 1))\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        u_old = u.copy()\n",
    "        \n",
    "        # Update u\n",
    "        K = np.exp((-cost_matrix + u @ np.ones((1, m)) + np.ones((n, 1)) @ v.T) / epsilon)\n",
    "        u = lmbda * epsilon * np.log(w_a) - lmbda * epsilon * np.log(np.sum(K, axis=1, keepdims=True)) + lmbda * u\n",
    "        \n",
    "        # Update v  \n",
    "        K = np.exp((-cost_matrix + u @ np.ones((1, m)) + np.ones((n, 1)) @ v.T) / epsilon)\n",
    "        v = lmbda * epsilon * np.log(w_b) - lmbda * epsilon * np.log(np.sum(K, axis=0, keepdims=True).T) + lmbda * v\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(u - u_old) < tol:\n",
    "            break\n",
    "    \n",
    "    # Final transport plan\n",
    "    gamma = np.exp((-cost_matrix + u @ np.ones((1, m)) + np.ones((n, 1)) @ v.T) / epsilon)\n",
    "    \n",
    "    return gamma\n",
    "\n",
    "def structured_optimal_transport(w_a, w_b, M, D_sc, D_st, alpha=0.1, epsilon=0.1, rho=100.0, max_iter=50):\n",
    "    \"\"\"\n",
    "    Solve structured optimal transport that aligns both features and internal geometries.\n",
    "    This is the core of SpaOTsc Stage 1.\n",
    "    \n",
    "    Args:\n",
    "        w_a: Cell marginals (n_cells,)\n",
    "        w_b: Spot marginals (m_spots,) \n",
    "        M: Feature cost matrix (n_cells, m_spots)\n",
    "        D_sc: SC distance matrix (n_cells, n_cells)\n",
    "        D_st: ST distance matrix (m_spots, m_spots)  \n",
    "        alpha: Weight for structured term (0=pure feature, 1=pure structure)\n",
    "        epsilon: Entropic regularization\n",
    "        rho: Unbalanced transport penalty\n",
    "        max_iter: Maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        gamma: Transport plan (n_cells, m_spots)\n",
    "    \"\"\"\n",
    "    # Normalize distance matrices\n",
    "    # D_sc_norm = D_sc / np.max(D_sc) if np.max(D_sc) > 0 else D_sc\n",
    "    # D_st_norm = D_st / np.max(D_st) if np.max(D_st) > 0 else D_st\n",
    "    D_sc_norm = D_sc\n",
    "    D_st_norm = D_st\n",
    "    \n",
    "    # Initialize with uniform coupling\n",
    "    w_a = w_a.reshape(-1, 1) \n",
    "    w_b = w_b.reshape(-1, 1)\n",
    "    gamma = w_a @ w_b.T\n",
    "    \n",
    "    n, m = M.shape\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        gamma_old = gamma.copy()\n",
    "        \n",
    "        # === STRUCTURED COST COMPUTATION ===\n",
    "        # This is the key innovation: align internal geometries\n",
    "        \n",
    "        # Feature cost component (1-α) * M  \n",
    "        cost_feature = (1.0 - alpha) * M\n",
    "        \n",
    "        # Structured cost component α * GW_cost\n",
    "        if alpha > 0:\n",
    "            # Gromov-Wasserstein structured term\n",
    "            # For squared loss: L(a,b) = 0.5 * (a-b)^2\n",
    "            \n",
    "            # Precompute constant terms\n",
    "            constC1 = 0.5 * (D_sc_norm**2) @ w_a @ np.ones((1, m))\n",
    "            constC2 = np.ones((n, 1)) @ w_b.T @ (0.5 * (D_st_norm**2)).T\n",
    "            constC = constC1 + constC2\n",
    "            \n",
    "            # Variable term: -D_sc @ gamma @ D_st  \n",
    "            variable_term = D_sc_norm @ gamma @ D_st_norm.T\n",
    "            \n",
    "            # Full structured cost\n",
    "            cost_structured = alpha * 2.0 * (constC - variable_term)\n",
    "        else:\n",
    "            cost_structured = 0\n",
    "        \n",
    "        # Total cost matrix\n",
    "        total_cost = cost_feature + cost_structured\n",
    "        \n",
    "        # === OPTIMAL TRANSPORT STEP ===\n",
    "        # Solve unbalanced OT with combined cost\n",
    "        if np.isinf(rho):\n",
    "            # Balanced case: use standard Sinkhorn\n",
    "            gamma_new = ot.sinkhorn(w_a.flatten(), w_b.flatten(), total_cost, epsilon)\n",
    "        else:\n",
    "            # Unbalanced case\n",
    "            gamma_new = unbalanced_optimal_transport(w_a.flatten(), w_b.flatten(), total_cost, epsilon, rho)\n",
    "        \n",
    "        # === LINE SEARCH UPDATE ===\n",
    "        # Optimal step size for convergence\n",
    "        if alpha > 0:\n",
    "            CxC_diff = D_sc_norm @ (gamma_new - gamma) @ D_st_norm.T\n",
    "            a = -alpha * np.sum(CxC_diff * (gamma_new - gamma))\n",
    "            b = np.sum((cost_feature + alpha * constC - 2.0 * alpha * (D_sc_norm @ gamma @ D_st_norm.T)) * (gamma_new - gamma))\n",
    "            \n",
    "            if a > 0:\n",
    "                tau = min(1.0, max(0.0, -0.5 * b / a))\n",
    "            elif a + b < 0:\n",
    "                tau = 1.0\n",
    "            else:\n",
    "                tau = 0.0\n",
    "        else:\n",
    "            tau = 1.0\n",
    "        \n",
    "        # Update with line search\n",
    "        gamma = (1.0 - tau) * gamma + tau * gamma_new\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.linalg.norm(gamma - gamma_old) < 1e-6:\n",
    "            print(f\"Structured OT converged at iteration {iteration}\")\n",
    "            break\n",
    "    \n",
    "    return gamma\n",
    "\n",
    "import faiss\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "import ot\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def transport_aware_geodesic_distance(gamma, Y_st_coords, sigma=3.0, k_neighbors=30):\n",
    "    '''\n",
    "    compute sc-sc distances using transport-weighted approach\n",
    "    '''\n",
    "    print(f'computing transport-aware geodasic distances (sigma={sigma}, k={k_neighbors})')\n",
    "\n",
    "    n_cells, m_spots = gamma.shape\n",
    "\n",
    "    #1. compute euclidean distance matrix for ST spots\n",
    "    print(\"Computing Euclidean D_st for transport weighting...\")\n",
    "    D_st_euclid = calculate_D_st_euclidean(Y_st_coords)\n",
    "\n",
    "    # 2) Compute spatial affinity kernel using Euclidean distances\n",
    "    print(\"Computing spatial affinity kernel...\")\n",
    "    K = np.exp(-(D_st_euclid**2) / (2 * sigma**2))  # (m_spots, m_spots)\n",
    "\n",
    "    # 3) Compute transport-weighted similarity matrix W\n",
    "    print(\"Computing transport-weighted similarity matrix...\")\n",
    "    W = gamma.dot(K).dot(gamma.T)  # (n_cells, n_cells)\n",
    "\n",
    "    # Make symmetric for numerical stability\n",
    "    W = (W + W.T) / 2\n",
    "    \n",
    "    print(f\"Similarity matrix W: shape={W.shape}, range=[{W.min():.6f}, {W.max():.6f}]\")\n",
    "    \n",
    "    # 4) Build sparse k-NN graph from similarity matrix W\n",
    "    print(f\"Building k-NN graph with k={k_neighbors}...\")\n",
    "\n",
    "    # For each cell, find k_neighbors with highest similarity\n",
    "    neighbors = np.argpartition(-W, k_neighbors, axis=1)[:, :k_neighbors]  # (n_cells, k)\n",
    "    \n",
    "    # Create sparse graph edges\n",
    "    rows = np.repeat(np.arange(n_cells), k_neighbors)\n",
    "    cols = neighbors.flatten()\n",
    "    similarities = W[rows, cols]\n",
    "    \n",
    "    # Convert similarities to costs using log distance (more stable than 1/similarity)\n",
    "    epsilon = 1e-10\n",
    "    costs = -np.log(similarities + epsilon)\n",
    "    \n",
    "    # Make graph symmetric (undirected)\n",
    "    rows_sym = np.concatenate([rows, cols])\n",
    "    cols_sym = np.concatenate([cols, rows]) \n",
    "    costs_sym = np.concatenate([costs, costs])\n",
    "    \n",
    "    # Create sparse CSR matrix\n",
    "    graph_csr = coo_matrix((costs_sym, (rows_sym, cols_sym)), \n",
    "                          shape=(n_cells, n_cells)).tocsr()\n",
    "    \n",
    "    print(f\"Sparse graph: {len(costs_sym)} edges, density={len(costs_sym)/(n_cells**2):.4f}\")\n",
    "    \n",
    "    # 5) Compute all-pairs shortest path distances\n",
    "    print(\"Computing shortest path distances...\")\n",
    "    \n",
    "    D_geodesic = shortest_path(csgraph=graph_csr, directed=False, method='D')\n",
    "    \n",
    "    # 6) Handle infinite distances (disconnected components)\n",
    "    inf_mask = ~np.isfinite(D_geodesic)\n",
    "    if inf_mask.any():\n",
    "        max_finite = D_geodesic[~inf_mask].max()\n",
    "        D_geodesic[inf_mask] = max_finite * 2.0\n",
    "        print(f\"Warning: {inf_mask.sum()} infinite distances replaced with {max_finite*2.0:.4f}\")\n",
    "    \n",
    "    # 7) Normalize to [0,1] range\n",
    "    d_min, d_max = D_geodesic.min(), D_geodesic.max()\n",
    "    if d_max > d_min:\n",
    "        D_geodesic = (D_geodesic - d_min) / (d_max - d_min)\n",
    "    \n",
    "    print(f\"Final geodesic distances: range=[{D_geodesic.min():.6f}, {D_geodesic.max():.6f}]\")\n",
    "    \n",
    "    return D_geodesic.astype(np.float32)\n",
    "\n",
    "def knn_sparse_ot_FAISS(gamma, D_st, Y_st_coords, k_neighbors=300, batch_size=1024, epsilon=0.01, device='cuda'):\n",
    "    \"\"\"\n",
    "    Option 1: k-NN Graph + Sparse OT + Shortest-Path\n",
    "    Complete implementation following ChatGPT's specifications exactly\n",
    "    \"\"\"    \n",
    "    n_cells, m_spots = gamma.shape\n",
    "    print(f\"Processing {n_cells} cells, {m_spots} spots with k={k_neighbors}\")\n",
    "    \n",
    "    # =================== STEP 1: FAISS k-NN SEARCH ===================\n",
    "    print(\"Building FAISS-GPU index...\")\n",
    "    \n",
    "    # Convert to float32 for FAISS\n",
    "    gamma_faiss = gamma.astype(np.float32)\n",
    "    \n",
    "    # Build FAISS-GPU index\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index_flat = faiss.IndexFlatL2(m_spots)  # L2 distance in gamma space\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "    \n",
    "    # Add vectors to index and search for neighbors\n",
    "    gpu_index.add(gamma_faiss)\n",
    "    _, indices_knn = gpu_index.search(gamma_faiss, k_neighbors + 1)  # +1 for self\n",
    "    \n",
    "    # =============== STEP 2: VECTORIZED NEIGHBOR EXTRACTION ===============\n",
    "    # No Python loops - vectorized\n",
    "    neighbors = indices_knn[:, 1:]  # Drop self-match (column 0)\n",
    "    rows = np.repeat(np.arange(n_cells), k_neighbors)\n",
    "    cols = neighbors.flatten()\n",
    "    total_pairs = len(rows)\n",
    "    \n",
    "    print(f\"Found {total_pairs} k-NN pairs instead of {n_cells*(n_cells-1)//2} total pairs\")\n",
    "    \n",
    "    # ================= STEP 3: MOVE DATA TO GPU ONCE =================\n",
    "    gamma_cuda = torch.from_numpy(gamma_faiss).to(device)\n",
    "    spot_coords = torch.from_numpy(Y_st_coords.astype(np.float32)).to(device)  # [m, d]\n",
    "    \n",
    "    print(f\"Spot coordinates shape: {spot_coords.shape}\")\n",
    "    \n",
    "    # ============= STEP 4: INITIALIZE GEOMLOSS ONCE OUTSIDE LOOP =============\n",
    "    try:\n",
    "        from geomloss import SamplesLoss\n",
    "        sinkhorn = SamplesLoss(\"sinkhorn\", p=2, blur=epsilon)\n",
    "        print(\"Using GeomLoss batch mode - no Python loops!\")\n",
    "    except ImportError:\n",
    "        raise ImportError(\"GeomLoss required. Install with: pip install geomloss\")\n",
    "    \n",
    "    # ========== STEP 5: BATCH PROCESSING - SINGLE GEOMLOSS CALLS ==========\n",
    "    distances_list = []\n",
    "    \n",
    "    for batch_start in tqdm(range(0, total_pairs, batch_size), desc=\"Batch OT\"):\n",
    "        batch_end = min(batch_start + batch_size, total_pairs)\n",
    "        B = batch_end - batch_start  # Actual batch size\n",
    "        \n",
    "        # Get batch probability distributions\n",
    "        batch_rows = rows[batch_start:batch_end] \n",
    "        batch_cols = cols[batch_start:batch_end]\n",
    "\n",
    "        P = gamma_cuda[batch_rows]  # [B, m] - source distributions\n",
    "        Q = gamma_cuda[batch_cols]  # [B, m] - target distributions\n",
    "\n",
    "\n",
    "        # Force consistent shapes\n",
    "        P = P.reshape(B, -1)  # Force [B, m] shape\n",
    "        Q = Q.reshape(B, -1)  # Force [B, m] shape\n",
    "\n",
    "        # Ensure they have exactly the same shape\n",
    "        assert P.shape == Q.shape, f\"Shape mismatch: P {P.shape} vs Q {Q.shape}\"\n",
    "\n",
    "        # Normalization\n",
    "        P = P / (P.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        Q = Q / (Q.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        # Clamp to avoid zeros\n",
    "        P = torch.clamp(P, min=1e-8)\n",
    "        Q = torch.clamp(Q, min=1e-8)\n",
    "\n",
    "        # Re-normalize\n",
    "        P = P / P.sum(dim=1, keepdim=True)\n",
    "        Q = Q / Q.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Final shape check\n",
    "        # print(f\"Final shapes - P: {P.shape}, Q: {Q.shape}, dims: P={P.dim()}, Q={Q.dim()}\")\n",
    "        \n",
    "        # ONE SINGLE GEOMLOSS CALL - processes entire batch in parallel\n",
    "        # GeomLoss expects: (x_i, x_j, a_i, a_j) where x are points, a are weights\n",
    "        spot_coords_batch_i = spot_coords.unsqueeze(0).repeat(B, 1, 1)  # [B, m, d]\n",
    "        spot_coords_batch_j = spot_coords.unsqueeze(0).repeat(B, 1, 1)  # [B, m, d]\n",
    "\n",
    "        distances_cuda = sinkhorn(\n",
    "            P,                      # [B, m]  weights for source\n",
    "            spot_coords_batch_i,    # [B, m, d] points for source\n",
    "            Q,                      # [B, m]  weights for target\n",
    "            spot_coords_batch_j     # [B, m, d] points for target\n",
    "        )\n",
    "        \n",
    "        # Convert to Python list and extend\n",
    "        batch_distances = distances_cuda.cpu().tolist()\n",
    "        distances_list.extend(batch_distances)\n",
    "    \n",
    "    # ============== STEP 6: BUILD SYMMETRIC SPARSE MATRIX ==============\n",
    "    print(\"Building sparse matrix...\")\n",
    "    \n",
    "    # Make symmetric by adding (i,j) and (j,i) entries\n",
    "    all_rows = np.concatenate([rows, cols])\n",
    "    all_cols = np.concatenate([cols, rows]) \n",
    "    all_distances = np.array(distances_list + distances_list)\n",
    "    \n",
    "    # Create sparse COO matrix\n",
    "    sparse_matrix = coo_matrix((all_distances, (all_rows, all_cols)), \n",
    "                              shape=(n_cells, n_cells))\n",
    "    \n",
    "    # =============== STEP 7: SHORTEST PATH COMPLETION ===============\n",
    "    print(\"Computing shortest paths to fill distance matrix...\")\n",
    "    \n",
    "    # Convert to CSR for efficient shortest path computation\n",
    "    D_spatial = shortest_path(sparse_matrix.tocsr(), directed=False, method='D')\n",
    "    \n",
    "    # Handle infinite distances (disconnected components)\n",
    "    finite_mask = D_spatial != np.inf\n",
    "    if finite_mask.any():\n",
    "        max_finite_dist = np.max(D_spatial[finite_mask])\n",
    "        D_spatial[~finite_mask] = max_finite_dist * 2  # Set to 2x max for disconnected pairs\n",
    "    else:\n",
    "        print(\"Warning: All distances are infinite - check k_neighbors value\")\n",
    "        D_spatial[D_spatial == np.inf] = 1.0  # Fallback\n",
    "    \n",
    "    print(f\"Final distance matrix: {D_spatial.shape}, range [{D_spatial.min():.6f}, {D_spatial.max():.6f}]\")\n",
    "    D_min = D_spatial.min()\n",
    "    D_max = D_spatial.max()\n",
    "\n",
    "    if D_max > D_min:\n",
    "        D_spatial_normalized = (D_spatial - D_min) / (D_max - D_min)\n",
    "    else:\n",
    "        D_spatial_normalized = D_spatial  # All same value, keep as is\n",
    "    \n",
    "    return D_spatial_normalized.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "def proper_spaotsc_landmarks(gamma_transport, D_st, Y_st_coords, n_landmarks=300):\n",
    "    '''\n",
    "    simplified proper spaotsc stage 2 with transport-aware landmarks\n",
    "    '''\n",
    "\n",
    "    print(f'proper spaotsc with {n_landmarks} landmarks')\n",
    "\n",
    "    #step 1: select transport aware landmarks\n",
    "    spot_importance = np.sum(gamma_transport, axis=0)\n",
    "    spot_importance = spot_importance / np.sum(spot_importance)\n",
    "\n",
    "    #weight coordinates by transport importance\n",
    "    weighted_coords = []\n",
    "    weighted_indices = []\n",
    "    for i, importance in enumerate(spot_importance):\n",
    "        n_copies = max(1, int(importance * n_landmarks * 3))\n",
    "        weighted_coords.extend([Y_st_coords[i]] * n_copies)\n",
    "        weighted_indices.extend([i] * n_copies)\n",
    "\n",
    "    #cluster and select landmarks\n",
    "    kmeans = KMeans(n_clusters = n_landmarks, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(weighted_coords)\n",
    "\n",
    "    landmark_indices = []\n",
    "    for cluster_id in range(n_landmarks):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        if not cluster_mask.any():\n",
    "            continue\n",
    "\n",
    "        cluster_indices = np.array(weighted_indices)[cluster_mask]\n",
    "        unique_indices = np.unique(cluster_indices)\n",
    "\n",
    "        if len(unique_indices) == 0:\n",
    "            continue\n",
    "\n",
    "        centroid = kmeans.cluster_centers_[cluster_id]\n",
    "        distances = [np.linalg.norm(Y_st_coords[idx] - centroid) for idx in unique_indices]\n",
    "        best_spot = unique_indices[np.argmin(distances)]\n",
    "        landmark_indices.append(best_spot)\n",
    "\n",
    "    landmark_indices = np.array(landmark_indices)\n",
    "    print(f'selected {len(landmark_indices)} landmarks')\n",
    "\n",
    "    #step 2: reduce to landmark space\n",
    "    gamma_landmarks = gamma_transport[:, landmark_indices]\n",
    "    D_st_landmarks = D_st[np.ix_(landmark_indices, landmark_indices)]\n",
    "\n",
    "    #renormalize transport plan\n",
    "    for i in range(gamma_landmarks.shape[0]):\n",
    "        row_sum = np.sum(gamma_landmarks[i, :])\n",
    "        if row_sum > 0:\n",
    "            gamma_landmarks[i, :] = gamma_landmarks[i, :] / row_sum\n",
    "        else:\n",
    "            gamma_landmarks[i, :] = 1.0 / gamma_landmarks.shape[1]\n",
    "\n",
    "    #step 3: compute pairwise wasserstein distances\n",
    "    n_cells = gamma_landmarks.shape[0]\n",
    "    D_spatial = np.zeros((n_cells, n_cells), dtype=np.float32)\n",
    "\n",
    "    #normalize for numerical stability\n",
    "    D_st_max = np.max(D_st_landmarks)\n",
    "    if D_st_max > 0:\n",
    "        D_st_norm = D_st_landmarks / D_st_max\n",
    "    else:\n",
    "        D_st_norm = D_st_landmarks\n",
    "\n",
    "    # Ensure arrays are C-contiguous for POT library\n",
    "    D_st_norm = np.ascontiguousarray(D_st_norm)\n",
    "    gamma_landmarks = np.ascontiguousarray(gamma_landmarks)\n",
    "\n",
    "\n",
    "    # FAST BATCH PROCESSING - NO INNER PYTHON LOOPS\n",
    "    print(\"Computing pairwise Wasserstein distances with TRUE GPU batching...\")\n",
    "\n",
    "    import torch\n",
    "\n",
    "    # Move to GPU once\n",
    "    gamma_gpu = torch.tensor(gamma_landmarks, device=\"cuda\", dtype=torch.float32)  \n",
    "    D_st_gpu = torch.tensor(D_st_norm, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    # Generate all pairs at once\n",
    "    n_cells = gamma_landmarks.shape[0]\n",
    "    idx_i, idx_j = torch.triu_indices(n_cells, n_cells, offset=1, device='cuda')\n",
    "    total_pairs = idx_i.numel()\n",
    "\n",
    "    print(f\"Processing {total_pairs} pairs in batches...\")\n",
    "\n",
    "    # Use larger batches since we're eliminating the inner loop\n",
    "    batch_size = 50000  # Much larger batches now possible\n",
    "    D_spatial = torch.zeros((n_cells, n_cells), device='cuda', dtype=torch.float32)\n",
    "\n",
    "    for start in tqdm(range(0, total_pairs, batch_size), desc=\"True batch processing\"):\n",
    "        end = min(start + batch_size, total_pairs)\n",
    "        \n",
    "        # Get batch indices  \n",
    "        batch_i = idx_i[start:end]\n",
    "        batch_j = idx_j[start:end]\n",
    "        \n",
    "        # Get batch data - this is the key: (batch_size, n_landmarks)\n",
    "        a_batch = gamma_gpu[batch_i]  # Shape: (batch_size, n_landmarks)\n",
    "        b_batch = gamma_gpu[batch_j]  # Shape: (batch_size, n_landmarks)\n",
    "        \n",
    "        # SINGLE GPU KERNEL CALL FOR ENTIRE BATCH - NO PYTHON LOOP!\n",
    "        try:\n",
    "            dists = ot.sinkhorn2(\n",
    "                a_batch,      # (batch_size, n_landmarks) \n",
    "                b_batch,      # (batch_size, n_landmarks)\n",
    "                D_st_gpu,     # (n_landmarks, n_landmarks)\n",
    "                reg=0.01, \n",
    "                numItermax=100\n",
    "            )\n",
    "            \n",
    "            # Handle POT return format\n",
    "            if isinstance(dists, (list, tuple)):\n",
    "                dists = dists[0]\n",
    "            \n",
    "            # Restore scale\n",
    "            dists = dists * D_st_max\n",
    "            \n",
    "            # Fill symmetric matrix directly - no loops!\n",
    "            D_spatial[batch_i, batch_j] = dists\n",
    "            D_spatial[batch_j, batch_i] = dists\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Batch failed, falling back to smaller chunks: {e}\")\n",
    "            # Fallback: split this batch into smaller pieces\n",
    "            mini_batch_size = 1000\n",
    "            for mini_start in range(start, end, mini_batch_size):\n",
    "                mini_end = min(mini_start + mini_batch_size, end)\n",
    "                mini_i = idx_i[mini_start:mini_end] \n",
    "                mini_j = idx_j[mini_start:mini_end]\n",
    "                \n",
    "                a_mini = gamma_gpu[mini_i]\n",
    "                b_mini = gamma_gpu[mini_j]\n",
    "                \n",
    "                mini_dists = ot.sinkhorn2(a_mini, b_mini, D_st_gpu, reg=0.01, numItermax=100)\n",
    "                if isinstance(mini_dists, (list, tuple)):\n",
    "                    mini_dists = mini_dists[0]\n",
    "                \n",
    "                mini_dists = mini_dists * D_st_max\n",
    "                D_spatial[mini_i, mini_j] = mini_dists\n",
    "                D_spatial[mini_j, mini_i] = mini_dists\n",
    "\n",
    "    # Convert back to numpy\n",
    "    D_spatial = D_spatial.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    print(f\"GPU batch processing complete, Range: [{D_spatial.min():.6f}, {D_spatial.max():.6f}]\")\n",
    "\n",
    "\n",
    "def spaotsc_spatial_distance_matrix(X_sc, X_st, D_sc, D_st, Y_st,\n",
    "                                  alpha=0.1, epsilon_stage1=0.1, epsilon_stage2=0.01,\n",
    "                                  rho=100.0, feature_metric='euclidean',\n",
    "                                  marginal_method='exponential', max_iter_stage1=50, k_neighbors=50,\n",
    "                                  use_landmarks=False, n_landmarks=500,\n",
    "                                  verbose=True):\n",
    "    \"\"\"\n",
    "    Main function implementing SpaOTsc's two-stage approach for spatial distance matrix.\n",
    "    \n",
    "    Stage 1: Structured & Unbalanced Optimal Transport\n",
    "    Stage 2: Pairwise Wasserstein Distance Computation\n",
    "    \n",
    "    Args:\n",
    "        X_sc: SC gene expression (n_cells, n_genes) \n",
    "        X_st: ST gene expression (m_spots, n_genes)\n",
    "        D_sc: SC distance matrix from k-NN graph (n_cells, n_cells)\n",
    "        D_st: ST spatial distance matrix (m_spots, m_spots)\n",
    "        alpha: Structure vs feature weight (0=pure feature, 1=pure structure)\n",
    "        epsilon_stage1: Entropic regularization for Stage 1\n",
    "        epsilon_stage2: Entropic regularization for Stage 2\n",
    "        rho: Unbalanced transport penalty (np.inf for balanced)\n",
    "        feature_metric: Distance metric for gene expression ('euclidean', 'cosine', 'correlation')\n",
    "        marginal_method: How to compute marginal weights ('uniform', 'exponential', 'softmax')\n",
    "        max_iter_stage1: Maximum iterations for structured OT\n",
    "        use_landmarks: Use landmark approximation in Stage 2\n",
    "        n_landmarks: Number of landmarks for approximation\n",
    "        verbose: Print progress information\n",
    "    \n",
    "    Returns:\n",
    "        D_induced_spaotsc: Spatial distance matrix for SC cells (n_cells, n_cells)\n",
    "        gamma_transport: Transport plan from Stage 1 (n_cells, m_spots)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=== SpaOTsc Spatial Distance Matrix Computation ===\")\n",
    "        print(f\"SC data: {X_sc.shape}, ST data: {X_st.shape}\")\n",
    "        print(f\"Alpha (structure weight): {alpha}\")\n",
    "        print(f\"Feature metric: {feature_metric}\")\n",
    "        print(f\"Marginal method: {marginal_method}\")\n",
    "    \n",
    "    # === STAGE 1: STRUCTURED & UNBALANCED OPTIMAL TRANSPORT ===\n",
    "    if verbose:\n",
    "        print(\"\\n--- Stage 1: Structured Optimal Transport ---\")\n",
    "    \n",
    "    # Step 1.1: Compute feature cost matrix M\n",
    "    if verbose:\n",
    "        print(\"Computing feature cost matrix...\")\n",
    "    M = compute_feature_cost_matrix(X_sc, X_st, metric=feature_metric)\n",
    "    \n",
    "    # Step 1.2: Compute marginal weights\n",
    "    if verbose:\n",
    "        print(\"Computing marginal weights...\")\n",
    "    w_a, w_b = compute_marginal_weights(M, method=marginal_method)\n",
    "    \n",
    "    # Step 1.3: Solve structured optimal transport\n",
    "    if verbose:\n",
    "        print(\"Solving structured optimal transport...\")\n",
    "    gamma_transport = structured_optimal_transport(\n",
    "        w_a, w_b, M, D_sc, D_st, \n",
    "        alpha=alpha, epsilon=epsilon_stage1, rho=rho, max_iter=max_iter_stage1\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Transport plan shape: {gamma_transport.shape}\")\n",
    "        print(f\"Transport plan mass: {np.sum(gamma_transport):.6f}\")\n",
    "\n",
    "    cell_idx = 0  # Pick any cell\n",
    "    spot_weights = gamma_transport[cell_idx, :]\n",
    "    top_spots = np.argsort(spot_weights)[-10:]  # Top 10 spots\n",
    "\n",
    "    # 2. Are these spots spatially close?\n",
    "    # Get their coordinates\n",
    "    top_spot_coords = Y_st[top_spots]\n",
    "    # Compute pairwise distances\n",
    "    spatial_spread = np.std(pdist(top_spot_coords))\n",
    "    print(f\"Spatial spread of top spots: {spatial_spread}\")\n",
    "\n",
    "    # 3. Compare to random\n",
    "    random_spots = np.random.choice(len(Y_st), 10)\n",
    "    random_spread = np.std(pdist(Y_st[random_spots]))\n",
    "    print(f\"Random spread: {random_spread}\")\n",
    "        \n",
    "    # === STAGE 2: PAIRWISE WASSERSTEIN DISTANCES ===\n",
    "    if verbose:\n",
    "        print(\"\\n--- Stage 2: Pairwise Wasserstein Distances ---\")\n",
    "    \n",
    "    # Step 2.1: FAST k-NN + Sparse OT using FAISS\n",
    "    # D_induced_spaotsc = knn_sparse_ot_faiss(\n",
    "    #     gamma_transport, D_st,\n",
    "    #     k_neighbors=50,\n",
    "    #     batch_size=1024,\n",
    "    #     epsilon=epsilon_stage2,\n",
    "    #     device='cuda'\n",
    "    # )\n",
    "\n",
    "    if isinstance(Y_st, torch.Tensor):\n",
    "        Y_st_np = Y_st.cpu().numpy()\n",
    "    else:\n",
    "        Y_st_np = Y_st\n",
    "\n",
    "    # Replace the slow compute_pairwise_wasserstein_distances with:\n",
    "    # D_induced_spaotsc = knn_sparse_ot_FAISS(\n",
    "    #     gamma_transport, D_st, Y_st_np,\n",
    "    #     k_neighbors=k_neighbors,\n",
    "    #     batch_size=512,  # Adjust based on GPU memory\n",
    "    #     epsilon=epsilon_stage2,\n",
    "    #     device='cuda'\n",
    "    # )\n",
    "\n",
    "    # D_induced_spaotsc = transport_aware_geodesic_distance(\n",
    "    #     gamma_transport, \n",
    "    #     Y_st_np,        # ST coordinates for Euclidean distances\n",
    "    #     sigma=3.0,  # Repurpose epsilon_stage2 as sigma\n",
    "    #     k_neighbors=k_neighbors\n",
    "    # )\n",
    "\n",
    "    D_induced_spaotsc = proper_spaotsc_landmarks(\n",
    "        gamma_transport, \n",
    "        D_st if isinstance(D_st, np.ndarray) else D_st.cpu().numpy(),\n",
    "        Y_st_np, \n",
    "        n_landmarks=min(300, X_st.shape[0] // 3) \n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final spatial distance matrix shape: {D_induced_spaotsc.shape}\")\n",
    "        print(f\"Distance range: [{np.min(D_induced_spaotsc):.6f}, {np.max(D_induced_spaotsc):.6f}]\")\n",
    "        print(f\"Mean distance: {np.mean(D_induced_spaotsc):.6f}\")\n",
    "        print(\"=== SpaOTsc computation complete! ===\")\n",
    "    \n",
    "    return D_induced_spaotsc, gamma_transport\n",
    "\n",
    "# Example usage function to replace your current approach\n",
    "def replace_fused_gw_with_spaotsc(X_sc, X_st, Y_st, k_neighbors=10, alpha=0.9, device='cuda'):\n",
    "    \"\"\"\n",
    "    Drop-in replacement for your current fused_gw_torch function using SpaOTsc approach.\n",
    "    \n",
    "    Args:\n",
    "        X_sc: SC gene expression tensor\n",
    "        X_st: ST gene expression tensor  \n",
    "        Y_st: ST spatial coordinates\n",
    "        k_neighbors: Number of neighbors for k-NN graph\n",
    "        alpha: Structure vs feature weight for SpaOTsc\n",
    "        device: Device for computation\n",
    "    \n",
    "    Returns:\n",
    "        gamma_transport: Transport plan (n_cells, m_spots)\n",
    "        D_sc: SC distance matrix\n",
    "        D_st: ST spatial distance matrix\n",
    "        D_induced_spaotsc: SpaOTsc spatial distance matrix\n",
    "        spaotsc_quality: Quality metric (transport plan mass)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert tensors to numpy\n",
    "    if isinstance(X_sc, torch.Tensor):\n",
    "        X_sc_np = X_sc.cpu().numpy()\n",
    "    else:\n",
    "        X_sc_np = X_sc\n",
    "        \n",
    "    if isinstance(X_st, torch.Tensor):\n",
    "        X_st_np = X_st.cpu().numpy()  \n",
    "    else:\n",
    "        X_st_np = X_st\n",
    "        \n",
    "    if isinstance(Y_st, torch.Tensor):\n",
    "        Y_st_np = Y_st.cpu().numpy()\n",
    "    else:\n",
    "        Y_st_np = Y_st\n",
    "    \n",
    "    # Compute distance matrices (keeping your existing functions)\n",
    "    print('Calculating SC distances with k-NN Dijkstra...')\n",
    "    \n",
    "    # D_sc_tensor = calculate_D_sc_torch(torch.tensor(X_sc_np), k_neighbors=k_neighbors, device=device)\n",
    "    # D_sc = D_sc_tensor.cpu().numpy()\n",
    "\n",
    "    D_sc_tensor, sc_max_distance = calculate_D_sc_torch(torch.tensor(X_sc_np), k_neighbors=k_neighbors, device=device)\n",
    "    D_sc = D_sc_tensor.cpu().numpy()\n",
    "    \n",
    "    print('Calculating ST distances...')\n",
    "    D_st, st_max_distance = calculate_D_st_from_coords(\n",
    "        spatial_coords=Y_st_np, k_neighbors=50, graph_mode=\"distance\"\n",
    "    )\n",
    "    \n",
    "    # Apply SpaOTsc method\n",
    "    print('Applying SpaOTsc spatial distance computation...')\n",
    "    D_induced_spaotsc, gamma_transport = spaotsc_spatial_distance_matrix(\n",
    "        X_sc_np, X_st_np, D_sc, D_st, Y_st=Y_st, k_neighbors=k_neighbors,\n",
    "        alpha=alpha,  # Higher alpha = more emphasis on spatial structure\n",
    "        epsilon_stage1=0.1,\n",
    "        epsilon_stage2=0.1, \n",
    "        rho=50.0,\n",
    "        feature_metric='euclidean',\n",
    "        use_landmarks=True,  # Speed up computation\n",
    "        n_landmarks=min(500, X_st_np.shape[0]),\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Convert back to tensors if needed\n",
    "    if device == 'cuda':\n",
    "        D_sc = torch.tensor(D_sc, dtype=torch.float32, device=device)\n",
    "        D_st = torch.tensor(D_st, dtype=torch.float32, device=device)\n",
    "        D_induced_spaotsc = torch.tensor(D_induced_spaotsc, dtype=torch.float32, device=device)\n",
    "        gamma_transport = torch.tensor(gamma_transport, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Quality metric (transport plan mass conservation)\n",
    "    spaotsc_quality = np.sum(gamma_transport) if isinstance(gamma_transport, np.ndarray) else torch.sum(gamma_transport).item()\n",
    "    \n",
    "    print(f'SpaOTsc quality (transport mass): {spaotsc_quality:.6f}')\n",
    "    \n",
    "    return gamma_transport, D_sc, D_st, D_induced_spaotsc, spaotsc_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_validation(D_spatial):\n",
    "    \"\"\"Quick validation of distance matrix properties\"\"\"\n",
    "    print(\"🧪 Quick Validation:\")\n",
    "    \n",
    "    # Symmetry\n",
    "    symmetry_error = np.max(np.abs(D_spatial - D_spatial.T))\n",
    "    print(f\"   Symmetry error: {symmetry_error:.2e} (should be ~0)\")\n",
    "    \n",
    "    # Triangle inequality (sample)\n",
    "    violations = 0\n",
    "    n_check = min(1000, D_spatial.shape[0])\n",
    "    for _ in range(n_check):\n",
    "        i, j, k = np.random.choice(D_spatial.shape[0], 3, replace=False)\n",
    "        if D_spatial[i,k] > D_spatial[i,j] + D_spatial[j,k] + 1e-6:\n",
    "            violations += 1\n",
    "    \n",
    "    print(f\"   Triangle violations: {violations}/{n_check} (should be 0)\")\n",
    "    print(f\"   Range: [{D_spatial.min():.6f}, {D_spatial.max():.6f}]\")\n",
    "    print(f\"   Mean: {D_spatial.mean():.6f}\")\n",
    "    \n",
    "    is_valid = symmetry_error < 1e-6 and violations == 0\n",
    "    print(f\"   Overall: {'✅ VALID' if is_valid else '❌ ISSUES DETECTED'}\")\n",
    "    \n",
    "    return is_valid\n",
    "\n",
    "def compare_methods(gamma_transport, D_st, Y_st_coords, subset_size=1000):\n",
    "    \"\"\"Compare your geodesic vs proper SpaOTsc on subset\"\"\"\n",
    "    \n",
    "    # Test on subset\n",
    "    indices = np.random.choice(gamma_transport.shape[0], subset_size, replace=False)\n",
    "    gamma_subset = gamma_transport[indices]\n",
    "    \n",
    "    print(f\"🔬 Comparing methods on {subset_size} cells...\")\n",
    "    \n",
    "    # Your current method\n",
    "    print(\"\\n1️⃣ Your geodesic method:\")\n",
    "    start = time.time()\n",
    "    D_geodesic = transport_aware_geodesic_distance(gamma_subset, Y_st_coords, sigma=3.0, k_neighbors=30)\n",
    "    geodesic_time = time.time() - start\n",
    "    print(f\"   Time: {geodesic_time:.2f}s\")\n",
    "    \n",
    "    # Proper SpaOTsc  \n",
    "    print(\"\\n2️⃣ Proper SpaOTsc:\")\n",
    "    start = time.time()\n",
    "    D_proper = proper_spaotsc_landmarks(gamma_subset, D_st, Y_st_coords, n_landmarks=200)\n",
    "    proper_time = time.time() - start\n",
    "    print(f\"   Time: {proper_time:.2f}s\")\n",
    "    \n",
    "    # Correlation\n",
    "    correlation = np.corrcoef(D_geodesic.flatten(), D_proper.flatten())[0,1]\n",
    "    print(f\"\\n📊 Correlation: {correlation:.4f}\")\n",
    "    print(f\"   Speed ratio: {proper_time/geodesic_time:.2f}x\")\n",
    "    \n",
    "    # Validation\n",
    "    print(f\"\\n🧪 Validation:\")\n",
    "    geodesic_symmetry = np.max(np.abs(D_geodesic - D_geodesic.T))\n",
    "    proper_symmetry = np.max(np.abs(D_proper - D_proper.T))\n",
    "    print(f\"   Symmetry - Geodesic: {geodesic_symmetry:.2e}, SpaOTsc: {proper_symmetry:.2e}\")\n",
    "    \n",
    "    return correlation, proper_time/geodesic_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_D_induced_proper_scaling(T, D_st, n_sc, n_st):\n",
    "    '''compute D_induced with proper scaling'''\n",
    "    #reweight transport matrix\n",
    "    T_reweight = T * n_sc\n",
    "    D_induced_raw = T_reweight @ D_st @ T_reweight.t()\n",
    "\n",
    "    #normalize to [0,1] range\n",
    "    D_induced_max = torch.max(D_induced_raw[D_induced_raw > 0])\n",
    "    if D_induced_max > 1e-10: #avoid dvision by very small numbers\n",
    "        D_induced = D_induced_raw / D_induced_max\n",
    "    else:\n",
    "        D_induced = D_induced_raw\n",
    "\n",
    "    return D_induced\n",
    "\n",
    "def fused_gw_torch(X_sc, X_st, Y_st, alpha, k=100, G0=None, max_iter = 100, tol=1e-9, device='cuda', n_iter = 1, D_st_precomputed=None):\n",
    "    n = X_sc.shape[0]\n",
    "    m = X_st.shape[0]\n",
    "\n",
    "    X_sc = X_sc.to(device)\n",
    "    X_st = X_st.to(device)\n",
    "\n",
    "    if not torch.is_tensor(Y_st):\n",
    "        Y_st_tensor = torch.tensor(Y_st, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        Y_st_tensor = Y_st.to(device, dtype=torch.float32)\n",
    "\n",
    "    #calculate distance matrices\n",
    "    print('calculating SC distances with knn-dijkstra.....')\n",
    "    D_sc, sc_max_distance = calculate_D_sc_torch(X_sc, graph_mode='distance', k_neighbors=k, device=device)\n",
    "\n",
    "    if D_st_precomputed is not None:\n",
    "        print(\"Using precomputed block-diagonal D_st...\")\n",
    "        D_st = D_st_precomputed.to(device)\n",
    "        st_max_distance = 1.0 #assume already normalized\n",
    "    else:\n",
    "        print('Calculating ST distances.....')\n",
    "        D_st, st_max_distance = calculate_D_st_from_coords(spatial_coords=Y_st, k_neighbors=50, graph_mode=\"distance\")\n",
    "        D_st = torch.tensor(D_st, dtype=torch.float32, device=device)\n",
    "\n",
    "    #get expression distance matrix\n",
    "    C_exp = torch.cdist(X_sc, X_st, p=2) #euclidean distance\n",
    "    C_exp = C_exp / (torch.max(C_exp) + 1e-16) #normalize\n",
    "\n",
    "    #ensure distance matries are C-contiguouse numpy arrays for POT\n",
    "    D_sc_np = D_sc.cpu().numpy()\n",
    "    D_st_np = D_st.cpu().numpy()\n",
    "    C_exp_np = C_exp.cpu().numpy()\n",
    "    D_sc_np = np.ascontiguousarray(D_sc_np)\n",
    "    D_st_np = np.ascontiguousarray(D_st_np)\n",
    "    C_exp_np = np.ascontiguousarray(C_exp_np)\n",
    "\n",
    "    #uniform distributions\n",
    "    p = ot.unif(n)\n",
    "    q = ot.unif(m)\n",
    "\n",
    "    #anneal the reg param over several steps\n",
    "    T_np = None\n",
    "    for i in range(n_iter):\n",
    "        #run fused gw with POT\n",
    "        T_np, log = ot.gromov.fused_gromov_wasserstein(\n",
    "            M=C_exp_np, C1=D_sc_np, C2=D_st_np,\n",
    "            p=p, q=q, loss_fun='square_loss',\n",
    "            alpha=alpha,\n",
    "            G0=T_np if T_np is not None else (G0.cpu().numpy() if G0 is not None else None),\n",
    "            log=True,\n",
    "            verbose=True,\n",
    "            max_iter = max_iter,\n",
    "            tol_abs=tol\n",
    "        )\n",
    "\n",
    "        # T_np, log = ot.gromov.entropic_fused_gromov_wasserstein(\n",
    "        #     M=C_exp_np, C1=D_sc_np, C2=D_st_np,\n",
    "        #     p=p, q=q, loss_fun='square_loss',\n",
    "        #     alpha=alpha,\n",
    "        #     epsilon=0.001,  # START HERE - try 0.01, 0.001, 0.0001\n",
    "        #     G0=T_np if T_np is not None else (G0.cpu().numpy() if G0 is not None else None),\n",
    "        #     max_iter=max_iter,\n",
    "        #     tol=tol,  # Note: tol instead of tol_abs\n",
    "        #     solver='PGD',  # Projected Gradient Descent\n",
    "        #     verbose=True,\n",
    "        #     log=True\n",
    "        # )\n",
    "\n",
    "    fgw_dist = log['fgw_dist']\n",
    "\n",
    "    print(f'fgw distance: {fgw_dist}')\n",
    "\n",
    "    T = torch.tensor(T_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    n_sc = X_sc.shape[0]\n",
    "    n_st = X_st.shape[0]\n",
    "\n",
    "    D_induced = compute_D_induced_proper_scaling(T, D_st, n_sc, n_st)\n",
    "\n",
    "    return T, D_sc, D_st, D_induced, fgw_dist, sc_max_distance, st_max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patient 2 data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Key innovation: Use ALL ST data points for better training.\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "\n",
    "    # Store separate coordinate lists for block-diagonal graph\n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    st_expr_combined = scaler.fit_transform(st_expr_combined)\n",
    "\n",
    "    st_coords_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "\n",
    "    sc_expr = scaler.fit_transform(sc_expr)\n",
    "\n",
    "    \n",
    "    # Create dataset labels for tracking\n",
    "    dataset_labels = (['dataset1'] * len(st1_expr) + \n",
    "                     ['dataset2'] * len(st2_expr) + \n",
    "                     ['dataset3'] * len(st3_expr))\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list\n",
    "\n",
    "# Load and process data\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Prepare combined data for diffusion\n",
    "X_sc, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = prepare_combined_st_for_diffusion(\n",
    "    stadata1, stadata2, stadata3, scadata\n",
    ")\n",
    "\n",
    "print(f\"Data preparation complete!\")\n",
    "print(f\"SC cells: {X_sc.shape[0]}\")\n",
    "print(f\"Combined ST spots: {X_st_combined.shape[0]}\")\n",
    "print(f\"Common genes: {len(common_genes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# diffusion model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, n_genes, n_embedding=[512, 256, 128], dp=0):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_genes, n_embedding[0])\n",
    "        self.bn1 = nn.LayerNorm(n_embedding[0])\n",
    "        self.fc2 = nn.Linear(n_embedding[0], n_embedding[1])\n",
    "        self.bn2 = nn.LayerNorm(n_embedding[1])\n",
    "        self.fc3 = nn.Linear(n_embedding[1], n_embedding[2])\n",
    "        \n",
    "        self.dp = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self, x, isdp=False):\n",
    "        if isdp:\n",
    "            x = self.dp(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        tmp = 0\n",
    "        for x in kernel_val:\n",
    "            tmp += x\n",
    "        return tmp\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for diffusion timesteps\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1, 0, 0))\n",
    "        return emb\n",
    "\n",
    "def normalize_coordinates_isotropic(coords):\n",
    "    '''normalize coordinates to unit circle preserving aspect ratio'''\n",
    "    if torch.is_tensor(coords):\n",
    "        center = coords.mean(dim=0)\n",
    "        centered = coords - center\n",
    "        max_radius = torch.max(torch.norm(centered, dim=1))\n",
    "        coords_norm = centered / max_radius\n",
    "        return coords_norm, center, max_radius\n",
    "    else:\n",
    "        center = coords.mean(axis=0)\n",
    "        centered = coords - center\n",
    "        max_radius = np.max(np.linalg.norm(centered, axis=1))\n",
    "        coords_norm = centered / max_radius\n",
    "        return coords_norm, center, max_radius\n",
    "\n",
    "class STEMDiffusion:\n",
    "    def __init__(\n",
    "        self, \n",
    "        st_gene_expr,\n",
    "        st_coords,\n",
    "        D_st,\n",
    "        sc_gene_expr,\n",
    "        D_induced=None,\n",
    "        outf='./diffusion_output',\n",
    "        device='cuda',\n",
    "        n_genes=None,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        hidden_dim=256,\n",
    "        dp=0.1,\n",
    "        n_timesteps=800,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256,\n",
    "        coord_space_diameter = 2.0,\n",
    "        st_max_distance = None,\n",
    "        sc_max_distance = None\n",
    "    ):\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.sc_coord_predictor = nn.Sequential(\n",
    "            nn.Linear(n_embedding[-1], hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.LayerNorm(hidden_dim//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim//2, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Separate optimizer for SC head (will be used in Stage 2)\n",
    "        self.optimizer_sc = torch.optim.AdamW(\n",
    "            self.sc_coord_predictor.parameters(), \n",
    "            lr=1e-3, \n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.scheduler_sc = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer_sc, T_max=1000, eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Create output directory\n",
    "        self.outf = outf\n",
    "        if not os.path.exists(outf):\n",
    "            os.makedirs(outf)\n",
    "        \n",
    "        self.train_log = os.path.join(outf, 'train.log')\n",
    "        \n",
    "        # Store data\n",
    "        self.st_gene_expr = torch.tensor(st_gene_expr, dtype=torch.float32).to(self.device)\n",
    "        self.st_coords = torch.tensor(st_coords, dtype=torch.float32).to(self.device)\n",
    "        self.sc_gene_expr = torch.tensor(sc_gene_expr, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Store distance matrices\n",
    "        self.D_st = torch.tensor(D_st, dtype=torch.float32).to(self.device)\n",
    "        if D_induced is not None:\n",
    "            self.D_induced = torch.tensor(D_induced, dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            self.D_induced = None\n",
    "        \n",
    "        # Normalize coordinates for diffusion model\n",
    "        coords_min = self.st_coords.min(dim=0)[0]\n",
    "        coords_max = self.st_coords.max(dim=0)[0]\n",
    "        coords_range = coords_max - coords_min\n",
    "        self.st_coords_norm = 2 * (self.st_coords - coords_min) / coords_range - 1\n",
    "        self.coords_min, self.coords_max = coords_min, coords_max\n",
    "        self.coords_range = coords_range\n",
    "\n",
    "        self.coord_space_diameter = coord_space_diameter\n",
    "        self.st_max_distance = st_max_distance\n",
    "        self.sc_max_distance = sc_max_distance\n",
    "\n",
    "        #use isotropic normalization\n",
    "        self.st_coords_norm, self.coords_center, self.coords_radius = normalize_coordinates_isotropic(self.st_coords)\n",
    "        \n",
    "\n",
    "        # self.st_coords_norm = self.st_coords\n",
    "\n",
    "        # STEM parameters\n",
    "        self.n_genes = n_genes or st_gene_expr.shape[1]\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.mmdbatch = mmdbatch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize feature encoder (shared between ST and SC data)\n",
    "        self.netE = FeatureNet(self.n_genes, n_embedding=n_embedding, dp=dp).to(self.device)\n",
    "        \n",
    "        # Initialize diffusion model components\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Feature to hidden projection\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(n_embedding[-1], hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        ).to(self.device)\n",
    "\n",
    "        #coordinate head to get coords direct from gene expression\n",
    "        self.coord_head = nn.Sequential(\n",
    "            nn.Linear(n_embedding[-1], hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Main network blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ).to(self.device) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim//2, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Setup optimizers and losses\n",
    "        self.optimizer_E = torch.optim.AdamW(self.netE.parameters(), lr=0.002)\n",
    "        self.scheduler_E = lr_scheduler.StepLR(optimizer=self.optimizer_E, step_size=200, gamma=0.5)\n",
    "        \n",
    "        diffusion_params = list(self.time_embed.parameters()) + \\\n",
    "                           list(self.coord_encoder.parameters()) + \\\n",
    "                           list(self.feat_proj.parameters()) + \\\n",
    "                           list(self.blocks.parameters()) + \\\n",
    "                           list(self.final.parameters())\n",
    "        \n",
    "        self.optimizer_diff = torch.optim.AdamW(diffusion_params, lr=1e-4, weight_decay=1e-6)\n",
    "        self.scheduler_diff = lr_scheduler.CosineAnnealingLR(self.optimizer_diff, T_max=3000, eta_min=1e-6)\n",
    "        \n",
    "        self.mmd_fn = MMDLoss()\n",
    "        \n",
    "        # Setup noise schedule for diffusion\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.noise_schedule = self.get_noise_schedule(n_timesteps, beta_start, beta_end)\n",
    "        \n",
    "        # Tracking losses\n",
    "        self.loss_names = ['E', 'E_pred', 'E_circle', 'E_mmd', 'diffusion']\n",
    "    \n",
    "    def get_noise_schedule(self, timesteps=1000, beta1=1e-4, beta2=0.02):\n",
    "        \"\"\"Returns diffusion noise schedule parameters\"\"\"\n",
    "        # Linear schedule\n",
    "        betas = torch.linspace(beta1, beta2, timesteps, device=self.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n",
    "            'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod\n",
    "        }\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise_schedule):\n",
    "        \"\"\"Add noise to coordinates according to timestep t\"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "        \n",
    "        # Add noise according to schedule\n",
    "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "    \n",
    "    def forward_diffusion(self, coords, t, features):\n",
    "        \"\"\"Forward pass of diffusion model, predicting noise from noisy coordinates and conditioning\"\"\"\n",
    "        # Get feature embeddings\n",
    "        feat_emb = self.netE(features, isdp=False)\n",
    "        feat_proj = self.feat_proj(feat_emb)\n",
    "        \n",
    "        # Get time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Get coordinate embedding\n",
    "        coord_emb = self.coord_encoder(coords)\n",
    "        \n",
    "        # Combine all inputs\n",
    "        h = coord_emb + t_emb + feat_proj\n",
    "        \n",
    "        # Process through residual blocks\n",
    "        for block in self.blocks:\n",
    "            h = h + block(h)  # Residual connection\n",
    "        \n",
    "        # Predict noise\n",
    "        return self.final(h)\n",
    "    \n",
    "    def train_encoder(self, n_epochs=1000, ratio_start=0, ratio_end=1.0):\n",
    "        \"\"\"Train the STEM encoder to align ST and SC data\"\"\"\n",
    "        print(\"Training STEM encoder...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting STEM encoder training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, ratio_start={ratio_start}, ratio_end={ratio_end}\\n\")\n",
    "        \n",
    "        # Calculate spatial adjacency matrix\n",
    "        if self.sigma == 0:\n",
    "            nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        else:\n",
    "            nettrue = torch.tensor(scipy.spatial.distance.cdist(\n",
    "                self.st_coords.cpu().numpy(), \n",
    "                self.st_coords.cpu().numpy()\n",
    "            ), device=self.device).to(torch.float32)\n",
    "            \n",
    "            sigma = self.sigma\n",
    "            nettrue = torch.exp(-nettrue**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "            nettrue = F.normalize(nettrue, p=1, dim=1)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Schedule for circle loss weight\n",
    "            ratio = ratio_start + (ratio_end - ratio_start) * min(epoch / (n_epochs * 0.8), 1.0)\n",
    "            \n",
    "            # Forward pass ST data\n",
    "            e_seq_st = self.netE(self.st_gene_expr, True)\n",
    "            \n",
    "            # Sample from SC data due to large size\n",
    "            sc_idx = torch.randint(0, self.sc_gene_expr.shape[0], (min(self.batch_size, self.mmdbatch),), device=self.device)\n",
    "            sc_batch = self.sc_gene_expr[sc_idx]\n",
    "            e_seq_sc = self.netE(sc_batch, False)\n",
    "            \n",
    "            # Calculate losses\n",
    "            self.optimizer_E.zero_grad()\n",
    "            \n",
    "            # Prediction loss (equivalent to netpred in STEM)\n",
    "            netpred = e_seq_st.mm(e_seq_st.t())\n",
    "            loss_E_pred = F.cross_entropy(netpred, nettrue, reduction='mean')\n",
    "            \n",
    "            # Mapping matrices\n",
    "            st2sc = F.softmax(e_seq_st.mm(e_seq_sc.t()), dim=1)\n",
    "            sc2st = F.softmax(e_seq_sc.mm(e_seq_st.t()), dim=1)\n",
    "            \n",
    "            # Circle loss\n",
    "            st2st = torch.log(st2sc.mm(sc2st) + 1e-7)\n",
    "            loss_E_circle = F.kl_div(st2st, nettrue, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # MMD loss\n",
    "            ranidx = torch.randint(0, e_seq_sc.shape[0], (min(self.mmdbatch, e_seq_sc.shape[0]),), device=self.device)\n",
    "            loss_E_mmd = self.mmd_fn(e_seq_st, e_seq_sc[ranidx])\n",
    "            \n",
    "            # Total loss\n",
    "            loss_E = loss_E_pred + self.alpha * loss_E_mmd + ratio * loss_E_circle\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_E.backward()\n",
    "            self.optimizer_E.step()\n",
    "            self.scheduler_E.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Encoder epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss_E: {loss_E.item():.6f}, \"\n",
    "                          f\"Loss_E_pred: {loss_E_pred.item():.6f}, \"\n",
    "                          f\"Loss_E_circle: {loss_E_circle.item():.6f}, \"\n",
    "                          f\"Loss_E_mmd: {loss_E_mmd.item():.6f}, \"\n",
    "                          f\"Ratio: {ratio:.4f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'netE_state_dict': self.netE.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_E.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_E.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'encoder_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Save final encoder\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_encoder.pt'))\n",
    "        \n",
    "        print(\"Encoder training complete!\")\n",
    "    \n",
    "    def train_diffusion(self, n_epochs=2000, lambda_struct=5.0):\n",
    "        \"\"\"Train diffusion model using the trained encoder\"\"\"\n",
    "        print(\"Training diffusion model...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting diffusion model training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, lambda_struct={lambda_struct}\\n\")\n",
    "        \n",
    "        # Freeze encoder during diffusion training\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Precompute adjacency matrix for structure loss\n",
    "        def compute_adjacency_matrix(distances, sigma=3.0):\n",
    "            weights = torch.exp(-(distances ** 2) / (2 * sigma * sigma))\n",
    "            # Zero out self-connections\n",
    "            weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "            # Normalize rows to sum to 1\n",
    "            row_sums = weights.sum(dim=1, keepdim=True)\n",
    "            row_sums = torch.clamp(row_sums, min=1e-10)\n",
    "            adjacency = weights / (row_sums + 1e-8)\n",
    "            return adjacency\n",
    "        \n",
    "        st_adj = compute_adjacency_matrix(self.D_st, sigma=self.sigma)\n",
    "        \n",
    "        # Keep track of best model\n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch of ST data\n",
    "            idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "            coords = self.st_coords_norm[idx]\n",
    "            features = self.st_gene_expr[idx]\n",
    "            sub_adj = st_adj[idx][:, idx]\n",
    "            sub_adj = sub_adj / (sub_adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "            \n",
    "            # Sample timesteps with emphasis on early and late stages\n",
    "            if np.random.random() < 0.3:\n",
    "                # Focus on early timesteps (high noise)\n",
    "                t = torch.randint(int(0.7 * self.n_timesteps), self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            elif np.random.random() < 0.6:\n",
    "                # Focus on late timesteps (low noise, more structure)\n",
    "                t = torch.randint(0, int(0.3 * self.n_timesteps), (self.batch_size,), device=self.device)\n",
    "            else:\n",
    "                # Random timesteps across the range\n",
    "                t = torch.randint(0, self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            \n",
    "            # Add noise to coordinates\n",
    "            noisy_coords, target_noise = self.add_noise(coords, t, self.noise_schedule)\n",
    "            \n",
    "            # Forward pass to predict noise\n",
    "            pred_noise = self.forward_diffusion(noisy_coords, t.unsqueeze(1).float() / self.n_timesteps, features)\n",
    "            \n",
    "            # Compute diffusion loss (noise prediction MSE)\n",
    "            diffusion_loss = F.mse_loss(pred_noise, target_noise)\n",
    "            \n",
    "            # Compute denoised coordinates for structure loss\n",
    "            sqrt_alphas_cumprod_t = self.noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = self.noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "            pred_coords = (noisy_coords - sqrt_one_minus_alphas_cumprod_t * pred_noise) / sqrt_alphas_cumprod_t\n",
    "            \n",
    "            # Compute pairwise distances and adjacency for predicted coordinates\n",
    "            pred_distances = torch.cdist(pred_coords, pred_coords, p=2)\n",
    "            pred_adj = compute_adjacency_matrix(pred_distances, sigma=self.sigma)\n",
    "\n",
    "            # print(pred_adj.sum(dim=1))\n",
    "            # print(sub_adj.sum(dim=1))\n",
    "            \n",
    "            # Structure loss (KL divergence between adjacency matrices)\n",
    "            # Using KL divergence as you preferred\n",
    "            struct_loss = F.kl_div(\n",
    "                torch.log(pred_adj + 1e-10),\n",
    "                sub_adj,\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "            # Total loss\n",
    "            total_loss = diffusion_loss + lambda_struct * struct_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer_diff.zero_grad()\n",
    "            total_loss.backward()\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.time_embed.parameters()) + \n",
    "                list(self.coord_encoder.parameters()) + \n",
    "                list(self.feat_proj.parameters()) + \n",
    "                list(self.blocks.parameters()) + \n",
    "                list(self.final.parameters()) +\n",
    "                list(self.coord_head.parameters()),\n",
    "                1.0\n",
    "            )\n",
    "            self.optimizer_diff.step()\n",
    "            self.scheduler_diff.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                best_state = {\n",
    "                    'epoch': epoch,\n",
    "                    'time_embed': self.time_embed.state_dict(),\n",
    "                    'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                    'feat_proj': self.feat_proj.state_dict(),\n",
    "                    'blocks': [block.state_dict() for block in self.blocks],\n",
    "                    'final': self.final.state_dict(),\n",
    "                    'loss': best_loss\n",
    "                }\n",
    "                # Save best model\n",
    "                torch.save(best_state, os.path.join(self.outf, 'best_diffusion_model.pt'))\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Diffusion epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss: {total_loss.item():.6f}, \"\n",
    "                          f\"Diffusion Loss: {diffusion_loss.item():.6f}, \"\n",
    "                          f\"Structure Loss: {struct_loss.item():.6f}, \"\n",
    "                          f\"LR: {self.scheduler_diff.get_last_lr()[0]:.6f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'time_embed': self.time_embed.state_dict(),\n",
    "                        'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                        'feat_proj': self.feat_proj.state_dict(),\n",
    "                        'blocks': [block.state_dict() for block in self.blocks],\n",
    "                        'final': self.final.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_diff.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_diff.state_dict(),\n",
    "                        'loss': total_loss.item()\n",
    "                    }, os.path.join(self.outf, f'diffusion_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_state:\n",
    "            self.time_embed.load_state_dict(best_state['time_embed'])\n",
    "            self.coord_encoder.load_state_dict(best_state['coord_encoder'])\n",
    "            self.feat_proj.load_state_dict(best_state['feat_proj'])\n",
    "            for i, block_state in enumerate(best_state['blocks']):\n",
    "                self.blocks[i].load_state_dict(block_state)\n",
    "            self.final.load_state_dict(best_state['final'])\n",
    "            print(f\"Restored best model from epoch {best_state['epoch']} with loss {best_state['loss']:.6f}\")\n",
    "        \n",
    "        print(\"Diffusion training complete!\")\n",
    "    \n",
    "    def train_diffusion_stage1_st_only(self, n_epochs=2000, lambda_struct=5.0):\n",
    "        \"\"\"\n",
    "        STAGE 1: Train diffusion model ONLY on ST data for clean spatial learning.\n",
    "        This is your original working approach - keep it clean and focused.\n",
    "        \"\"\"\n",
    "        print(\"STAGE 1: Training diffusion model on ST data only...\")\n",
    "        \n",
    "        # Freeze encoder during diffusion training\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Precompute adjacency matrix for structure loss\n",
    "        def compute_adjacency_matrix(distances, sigma=3.0):\n",
    "            weights = torch.exp(-(distances ** 2) / (2 * sigma * sigma))\n",
    "            weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "            row_sums = weights.sum(dim=1, keepdim=True)\n",
    "            row_sums = torch.clamp(row_sums, min=1e-10)\n",
    "            adjacency = weights / (row_sums + 1e-8)\n",
    "            return adjacency\n",
    "        \n",
    "        st_adj = compute_adjacency_matrix(self.D_st, sigma=self.sigma)\n",
    "        \n",
    "        # Keep track of best model\n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        \n",
    "        # Training loop - ONLY ST data\n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch of ST data\n",
    "            idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "            coords = self.st_coords_norm[idx]\n",
    "            features = self.st_gene_expr[idx]\n",
    "            sub_adj = st_adj[idx][:, idx]\n",
    "            sub_adj = sub_adj / (sub_adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            # Sample timesteps with emphasis on early and late stages\n",
    "            if np.random.random() < 0.3:\n",
    "                t = torch.randint(int(0.7 * self.n_timesteps), self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            elif np.random.random() < 0.6:\n",
    "                t = torch.randint(0, int(0.3 * self.n_timesteps), (self.batch_size,), device=self.device)\n",
    "            else:\n",
    "                t = torch.randint(0, self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            \n",
    "            # Add noise to coordinates\n",
    "            noisy_coords, target_noise = self.add_noise(coords, t, self.noise_schedule)\n",
    "            \n",
    "            # Forward pass to predict noise\n",
    "            pred_noise = self.forward_diffusion(noisy_coords, t.unsqueeze(1).float() / self.n_timesteps, features)\n",
    "            \n",
    "            # Compute diffusion loss (noise prediction MSE)\n",
    "            diffusion_loss = F.mse_loss(pred_noise, target_noise)\n",
    "            \n",
    "            # Compute denoised coordinates for structure loss\n",
    "            sqrt_alphas_cumprod_t = self.noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = self.noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "            pred_coords = (noisy_coords - sqrt_one_minus_alphas_cumprod_t * pred_noise) / sqrt_alphas_cumprod_t\n",
    "            \n",
    "            # Compute pairwise distances and adjacency for predicted coordinates\n",
    "            pred_distances = torch.cdist(pred_coords, pred_coords, p=2)\n",
    "            pred_adj = compute_adjacency_matrix(pred_distances, sigma=self.sigma)\n",
    "            \n",
    "            # Structure loss (KL divergence between adjacency matrices)\n",
    "            struct_loss = F.kl_div(\n",
    "                torch.log(pred_adj + 1e-10),\n",
    "                sub_adj,\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = diffusion_loss + lambda_struct * struct_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer_diff.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.time_embed.parameters()) +\n",
    "                list(self.coord_encoder.parameters()) +\n",
    "                list(self.feat_proj.parameters()) +\n",
    "                list(self.blocks.parameters()) +\n",
    "                list(self.final.parameters()),\n",
    "                1.0\n",
    "            )\n",
    "            self.optimizer_diff.step()\n",
    "            self.scheduler_diff.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                best_state = {\n",
    "                    'epoch': epoch,\n",
    "                    'time_embed': self.time_embed.state_dict(),\n",
    "                    'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                    'feat_proj': self.feat_proj.state_dict(),\n",
    "                    'blocks': [block.state_dict() for block in self.blocks],\n",
    "                    'final': self.final.state_dict(),\n",
    "                    'loss': best_loss\n",
    "                }\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"STAGE 1 - Epoch {epoch}/{n_epochs}, \"\n",
    "                        f\"Loss: {total_loss.item():.6f}, \"\n",
    "                        f\"Diffusion: {diffusion_loss.item():.6f}, \"\n",
    "                        f\"Structure: {struct_loss.item():.6f}\")\n",
    "                print(log_msg)\n",
    "                \n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'time_embed': self.time_embed.state_dict(),\n",
    "                        'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                        'feat_proj': self.feat_proj.state_dict(),\n",
    "                        'blocks': [block.state_dict() for block in self.blocks],\n",
    "                        'final': self.final.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_diff.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_diff.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'stage1_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_state:\n",
    "            self.time_embed.load_state_dict(best_state['time_embed'])\n",
    "            self.coord_encoder.load_state_dict(best_state['coord_encoder'])\n",
    "            self.feat_proj.load_state_dict(best_state['feat_proj'])\n",
    "            for i, block_state in enumerate(best_state['blocks']):\n",
    "                self.blocks[i].load_state_dict(block_state)\n",
    "            self.final.load_state_dict(best_state['final'])\n",
    "            print(f\"Restored best ST model from epoch {best_state['epoch']} with loss {best_state['loss']:.6f}\")\n",
    "        \n",
    "        # Save final Stage 1 model\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "            'time_embed': self.time_embed.state_dict(),\n",
    "            'coord_encoder': self.coord_encoder.state_dict(),\n",
    "            'feat_proj': self.feat_proj.state_dict(),\n",
    "            'blocks': [block.state_dict() for block in self.blocks],\n",
    "            'final': self.final.state_dict(),\n",
    "        }, os.path.join(self.outf, 'stage1_final_model.pt'))\n",
    "        \n",
    "        print(\"STAGE 1 complete! ST-based spatial diffusion model trained.\")\n",
    "\n",
    "\n",
    "    def train_stage2_sc_transfer(self, n_epochs=1000, lambda_struct=2.0, lambda_consist=0.5):\n",
    "        \"\"\"\n",
    "        STAGE 2: Train SC coordinate predictor using frozen ST diffusion model.\n",
    "        \"\"\"\n",
    "        print(\"STAGE 2: Training SC coordinate predictor...\")\n",
    "        \n",
    "        # FREEZE everything except SC predictor\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.time_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.coord_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.feat_proj.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.blocks.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.final.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Only SC predictor is trainable\n",
    "        for param in self.sc_coord_predictor.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Manually collect all parameters from individual modules\n",
    "        all_params = (list(self.netE.parameters()) + \n",
    "                    list(self.time_embed.parameters()) + \n",
    "                    list(self.coord_encoder.parameters()) + \n",
    "                    list(self.feat_proj.parameters()) + \n",
    "                    list(self.blocks.parameters()) + \n",
    "                    list(self.final.parameters()) + \n",
    "                    list(self.sc_coord_predictor.parameters()))\n",
    "\n",
    "        print(\"Frozen parameters:\", sum(p.numel() for p in all_params if not p.requires_grad))\n",
    "        print(\"Trainable parameters:\", sum(p.numel() for p in all_params if p.requires_grad))\n",
    "        \n",
    "        # Precompute SC adjacency from D_induced\n",
    "        def compute_adjacency_matrix(distances, sigma=3.0):\n",
    "            weights = torch.exp(-(distances ** 2) / (2 * sigma * sigma))\n",
    "            weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "            row_sums = weights.sum(dim=1, keepdim=True)\n",
    "            row_sums = torch.clamp(row_sums, min=1e-10)\n",
    "            return weights / (row_sums + 1e-8)\n",
    "        \n",
    "        # Scale D_induced to coordinate space\n",
    "        D_induced_scaled = self.D_induced * self.coord_space_diameter\n",
    "        sc_adj_target = compute_adjacency_matrix(D_induced_scaled, sigma=self.sigma)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample SC batch\n",
    "            sc_idx = torch.randperm(len(self.sc_gene_expr))[:self.batch_size]\n",
    "            sc_features = self.sc_gene_expr[sc_idx]\n",
    "            sc_sub_adj = sc_adj_target[sc_idx][:, sc_idx]\n",
    "            sc_sub_adj = sc_sub_adj / (sc_sub_adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "            \n",
    "            # Get SC feature embeddings (frozen encoder)\n",
    "            with torch.no_grad():\n",
    "                sc_embeddings = self.netE(sc_features, isdp=False)\n",
    "            \n",
    "            # Predict SC coordinates using new head\n",
    "            sc_coords_pred = self.sc_coord_predictor(sc_embeddings)\n",
    "            \n",
    "            # Compute structure loss based on D_induced\n",
    "            pred_distances = torch.cdist(sc_coords_pred, sc_coords_pred, p=2)\n",
    "            pred_adj = compute_adjacency_matrix(pred_distances, sigma=self.sigma)\n",
    "            \n",
    "            structure_loss = F.kl_div(\n",
    "                torch.log(pred_adj + 1e-10),\n",
    "                sc_sub_adj,\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "            \n",
    "            # Consistency loss: SC coordinates should have similar statistics to ST\n",
    "            with torch.no_grad():\n",
    "                # Sample some ST coordinates for comparison\n",
    "                st_sample_idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "                st_coords_sample = self.st_coords_norm[st_sample_idx]\n",
    "            \n",
    "            # Compare coordinate distributions\n",
    "            sc_mean = sc_coords_pred.mean(dim=0)\n",
    "            sc_std = sc_coords_pred.std(dim=0)\n",
    "            st_mean = st_coords_sample.mean(dim=0)\n",
    "            st_std = st_coords_sample.std(dim=0)\n",
    "            \n",
    "            consistency_loss = F.mse_loss(sc_mean, st_mean) + F.mse_loss(sc_std, st_std)\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = lambda_struct * structure_loss + lambda_consist * consistency_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer_sc.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.sc_coord_predictor.parameters(), 1.0)\n",
    "            self.optimizer_sc.step()\n",
    "            self.scheduler_sc.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                torch.save({\n",
    "                    'sc_coord_predictor': self.sc_coord_predictor.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'loss': best_loss\n",
    "                }, os.path.join(self.outf, 'best_sc_predictor.pt'))\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"STAGE 2 - Epoch {epoch}/{n_epochs}, \"\n",
    "                        f\"Total: {total_loss.item():.6f}, \"\n",
    "                        f\"Structure: {structure_loss.item():.6f}, \"\n",
    "                        f\"Consistency: {consistency_loss.item():.6f}\")\n",
    "                print(log_msg)\n",
    "        \n",
    "        print(\"STAGE 2 complete! SC coordinate predictor trained.\")\n",
    "\n",
    "\n",
    "    def train_two_stage(self, \n",
    "                    stage1_epochs=2000, \n",
    "                    stage2_epochs=1000,\n",
    "                    lambda_struct_st=5.0,\n",
    "                    lambda_struct_sc=2.0,\n",
    "                    lambda_consist=0.5):\n",
    "        \"\"\"Combined two-stage training\"\"\"\n",
    "        print(\"Starting two-stage training...\")\n",
    "        \n",
    "        # Stage 1: Clean ST training\n",
    "        self.train_diffusion_stage1_st_only(\n",
    "            n_epochs=stage1_epochs, \n",
    "            lambda_struct=lambda_struct_st\n",
    "        )\n",
    "        \n",
    "        # Stage 2: SC transfer learning\n",
    "        self.train_stage2_sc_transfer(\n",
    "            n_epochs=stage2_epochs,\n",
    "            lambda_struct=lambda_struct_sc,\n",
    "            lambda_consist=lambda_consist\n",
    "        )\n",
    "        \n",
    "        print(\"Two-stage training complete!\")\n",
    "    \n",
    "    def train(self, encoder_epochs=1000, diffusion_epochs=2000, ratio_start=0, ratio_end=1.0, lambda_struct=10.0):\n",
    "        \"\"\"Combined training of encoder and diffusion model\"\"\"\n",
    "        # First train the encoder to align ST and SC\n",
    "        self.train_encoder(n_epochs=encoder_epochs, ratio_start=ratio_start, ratio_end=ratio_end)\n",
    "        \n",
    "        # Then train the diffusion model\n",
    "        self.train_diffusion(n_epochs=diffusion_epochs, lambda_struct=lambda_struct)\n",
    "\n",
    "        # self.train_two_stage(stage1_epochs=2500,    # ST diffusion training\n",
    "        #     stage2_epochs=1000,    # SC transfer learning\n",
    "        #     lambda_struct_st=5.0,  # ST structure weight\n",
    "        #     lambda_struct_sc=2.0,  # SC structure weight (from D_induced)\n",
    "        #     lambda_consist=0.5     # Cross-modality consistency\n",
    "        # )\n",
    "\n",
    "    def generate_sc_coordinates_two_stage(self, return_normalized=True):\n",
    "        \"\"\"Generate SC coordinates using the trained SC predictor\"\"\"\n",
    "        print(\"Generating SC coordinates using Stage 2 predictor...\")\n",
    "        \n",
    "        self.netE.eval()\n",
    "        self.sc_coord_predictor.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get SC feature embeddings\n",
    "            sc_embeddings = self.netE(self.sc_gene_expr, isdp=False)\n",
    "            \n",
    "            # Predict coordinates using SC head\n",
    "            sc_coords_pred = self.sc_coord_predictor(sc_embeddings)\n",
    "        \n",
    "        if return_normalized:\n",
    "            return sc_coords_pred.cpu()\n",
    "        else:\n",
    "            return self.denormalize_coordinates(sc_coords_pred).cpu()\n",
    "    \n",
    "    def generate_st_coordinates_batched(self, batch_size=64, timesteps=None):\n",
    "        \"\"\"Generate ST coordinates in batches to avoid memory issues\"\"\"\n",
    "        print(\"Generating ST coordinates for evaluation in batches...\")\n",
    "        self.netE.eval()\n",
    "        \n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_spots = len(self.st_gene_expr)\n",
    "        n_batches = (n_spots + batch_size - 1) // batch_size\n",
    "        \n",
    "        all_coords = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = min((b + 1) * batch_size, n_spots)\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            # Get batch features\n",
    "            features = self.st_gene_expr[start_idx:end_idx]\n",
    "            \n",
    "            # Start from random noise\n",
    "            x = torch.randn(batch_size_actual, 2, device=self.device)\n",
    "            \n",
    "            # Gradually denoise\n",
    "            for t in tqdm(range(timesteps-1, -1, -1), \n",
    "                         desc=f\"Generating batch {b+1}/{n_batches}\",\n",
    "                         leave=(b == n_batches-1)):  # Only keep last progress bar\n",
    "                \n",
    "                # Create timestep tensor\n",
    "                time_tensor = torch.ones(batch_size_actual, 1, device=self.device) * t / timesteps\n",
    "                \n",
    "                # Predict noise\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                \n",
    "                # Get parameters for this timestep\n",
    "                alpha_t = self.noise_schedule['alphas'][t]\n",
    "                alpha_cumprod_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                beta_t = self.noise_schedule['betas'][t]\n",
    "                \n",
    "                # Apply noise (except for last step)\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = 0\n",
    "                \n",
    "                # Update sample with reverse diffusion step\n",
    "                x = (1 / torch.sqrt(alpha_t)) * (\n",
    "                    x - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * pred_noise\n",
    "                ) + torch.sqrt(beta_t) * noise\n",
    "            \n",
    "            # Store batch results\n",
    "            all_coords.append(x.detach().cpu())\n",
    "        \n",
    "        # Combine all batches\n",
    "        st_gen_coords_norm = torch.cat(all_coords, dim=0)\n",
    "        \n",
    "        # Denormalize coordinates\n",
    "        st_gen_coords = self.denormalize_coordinates(st_gen_coords_norm)\n",
    "        \n",
    "        print(\"Generation complete!\")\n",
    "        return st_gen_coords    \n",
    "\n",
    "    def sample_sc_with_soft_springs(\n",
    "        self,\n",
    "        batch_size=64,\n",
    "        timesteps=None,\n",
    "        use_springs=True,\n",
    "        spring_lr=0.01,\n",
    "        spring_iters=100,\n",
    "        k_neighbors=20,\n",
    "        lambda_s=0.1,\n",
    "        min_dist=None,\n",
    "        overlap_jitter=1e-3,\n",
    "        return_normalized=True  # ADD this parameter\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Hybrid sampling: pure diffusion followed by a global spring refinement.\n",
    "        \"\"\"\n",
    "        # 1) Diffusion generation (unchanged)\n",
    "        coords_list = []\n",
    "        self.netE.eval()\n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_cells = len(self.sc_gene_expr)\n",
    "        n_batches = (n_cells + batch_size - 1) // batch_size\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(n_cells, (b + 1) * batch_size)\n",
    "            bs_actual = end - start\n",
    "            features = self.sc_gene_expr[start:end]\n",
    "            x = torch.randn(bs_actual, 2, device=self.device)\n",
    "            \n",
    "            for t in range(timesteps - 1, -1, -1):\n",
    "                time_tensor = torch.full((bs_actual, 1), t / timesteps, device=self.device)\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                a_t = self.noise_schedule['alphas'][t]\n",
    "                acp = self.noise_schedule['alphas_cumprod'][t]\n",
    "                b_t = self.noise_schedule['betas'][t]\n",
    "                noise = torch.randn_like(x) if t > 0 else 0\n",
    "                x = (1/torch.sqrt(a_t)) * (\n",
    "                    x - ((1 - a_t)/torch.sqrt(1 - acp)) * pred_noise\n",
    "                ) + torch.sqrt(b_t) * noise\n",
    "                \n",
    "            coords_list.append(x.detach().cpu())\n",
    "        \n",
    "        coords_norm = torch.cat(coords_list, dim=0)\n",
    "\n",
    "        if not use_springs:\n",
    "            if return_normalized:\n",
    "                return coords_norm\n",
    "            else:\n",
    "                return self.denormalize_coordinates(coords_norm)\n",
    "\n",
    "        # 2) Spring refinement (FIXED!)\n",
    "        neighbor_idx, target_dists = self._precompute_spring_graph(k_neighbors)\n",
    "        \n",
    "        # CRITICAL: Scale D_induced to match coordinate space AND use the scaled version\n",
    "        target_dists_scaled = target_dists * self.coord_space_diameter\n",
    "        \n",
    "        coords_np = coords_norm.numpy().astype(np.float64)\n",
    "        for _ in range(spring_iters):\n",
    "            coords_np = self._spring_step(coords_np, neighbor_idx, target_dists_scaled, spring_lr)\n",
    "            #                                                      ^^^^^^^^^^^^^^^^^^^ FIXED!\n",
    "\n",
    "        # 3) Enforce minimum distance (if specified, should be in normalized space)\n",
    "        if min_dist is not None:\n",
    "            # Scale min_dist to normalized space if it was given in original space\n",
    "            min_dist_normalized = min_dist  # Assume it's already in normalized space\n",
    "            coords_np = self._enforce_minimum_distance(coords_np, min_dist_normalized, overlap_jitter)   \n",
    "\n",
    "        # 4) Blend\n",
    "        coords_refined = torch.from_numpy(coords_np).to(coords_list[0].dtype)\n",
    "        coords_orig = torch.cat(coords_list, dim=0)\n",
    "\n",
    "        # After spring refinement, before blending\n",
    "        coords_diff = coords_refined - coords_orig\n",
    "        print(f\"Coordinate difference stats:\")\n",
    "        print(f\"  Mean shift: {np.abs(coords_diff).mean():.4f}\")\n",
    "        print(f\"  Max shift: {np.abs(coords_diff).max():.4f}\")\n",
    "        print(f\"  Relative shift: {np.linalg.norm(coords_diff, axis=1).mean() / 2.0:.4f}\")  # relative to diameter\n",
    "\n",
    "        # 1. Distribution of movements (not just average)\n",
    "        shifts = np.linalg.norm(coords_diff, axis=1)\n",
    "        print(f\"\\nMovement distribution:\")\n",
    "        print(f\"  10th percentile: {np.percentile(shifts, 10):.4f}\")\n",
    "        print(f\"  50th percentile: {np.percentile(shifts, 50):.4f}\")  \n",
    "        print(f\"  90th percentile: {np.percentile(shifts, 90):.4f}\")\n",
    "        print(f\"  99th percentile: {np.percentile(shifts, 99):.4f}\")\n",
    "\n",
    "        # 2. Check spatial density changes\n",
    "        from scipy.stats import gaussian_kde\n",
    "        kde_orig = gaussian_kde(coords_orig.T)\n",
    "        kde_refined = gaussian_kde(coords_refined.T)\n",
    "\n",
    "        # Sample points to compare densities\n",
    "        grid = np.mgrid[-1:1:50j, -1:1:50j]\n",
    "        density_orig = kde_orig(grid.reshape(2, -1))\n",
    "        density_refined = kde_refined(grid.reshape(2, -1))\n",
    "        density_change = np.abs(density_orig - density_refined).mean()\n",
    "        print(f\"\\nDensity change: {density_change:.4f}\")\n",
    "\n",
    "        # 3. Check if movements are systematic (all in one direction) or random\n",
    "        mean_movement = coords_diff.mean(axis=0)\n",
    "        print(f\"\\nSystematic bias: {np.linalg.norm(mean_movement):.4f}\")\n",
    "        print(f\"  X-bias: {mean_movement[0]:.4f}, Y-bias: {mean_movement[1]:.4f}\")\n",
    "\n",
    "        # 4. Most importantly - check local neighborhood preservation\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nbrs_orig = NearestNeighbors(n_neighbors=10).fit(coords_orig)\n",
    "        _, indices_orig = nbrs_orig.kneighbors(coords_orig)\n",
    "\n",
    "        nbrs_refined = NearestNeighbors(n_neighbors=10).fit(coords_refined)  \n",
    "        _, indices_refined = nbrs_refined.kneighbors(coords_refined)\n",
    "\n",
    "        # How many cells kept their local neighbors?\n",
    "        neighbor_preservation = []\n",
    "        for i in range(len(coords_orig)):\n",
    "            overlap = len(set(indices_orig[i]) & set(indices_refined[i])) / 10\n",
    "            neighbor_preservation.append(overlap)\n",
    "            \n",
    "        print(f\"\\nLocal structure preservation: {np.mean(neighbor_preservation):.4f}\")\n",
    "        print(f\"  Cells with >50% neighbors preserved: {np.mean(np.array(neighbor_preservation) > 0.5):.2%}\")\n",
    "        \n",
    "        coords_final = (1-lambda_s) * coords_orig + lambda_s * coords_refined\n",
    "\n",
    "        # 5) Return in requested space\n",
    "        if return_normalized:\n",
    "            return coords_final\n",
    "        else:\n",
    "            return self.denormalize_coordinates(coords_final)\n",
    "    \n",
    "    def _enforce_minimum_distance(self, X, min_dist, jitter):\n",
    "        \"\"\"\n",
    "        For any pair (i,j) with ||X[i]-X[j]|| < min_dist, push them apart along their line-of-centers.\n",
    "        X: np.ndarray (n,2)\n",
    "        \"\"\"\n",
    "        tree = cKDTree(X)\n",
    "        pairs = tree.query_pairs(r=min_dist)\n",
    "        for i,j in pairs:\n",
    "            diff = X[i] - X[j]\n",
    "            if np.allclose(diff, 0):\n",
    "                # exact overlap: nudge by random direction\n",
    "                diff = np.random.randn(2) * jitter\n",
    "            dist = np.linalg.norm(diff)\n",
    "            # how much to push each point\n",
    "            delta = (min_dist - dist) / 2\n",
    "            direction = diff / (dist + 1e-16)\n",
    "            X[i] +=  direction * delta\n",
    "            X[j] -=  direction * delta\n",
    "        return X\n",
    "\n",
    "    def _spring_step(self, X, neighbor_idx, target_dists, lr, eps=1e-6, clip=1.0):\n",
    "        \"\"\"\n",
    "        Single spring update (Hooke's law) on coords X (n,2).\n",
    "        \"\"\"\n",
    "        diffs = X[:, None, :] - X[neighbor_idx]             # (n,k,2)\n",
    "        d_cur = np.linalg.norm(diffs, axis=2) + eps         # (n,k)\n",
    "        # compute normalized force magnitude\n",
    "        f_mag = np.clip((d_cur - target_dists) / d_cur, -clip, clip)\n",
    "        # forces on each spring\n",
    "        forces = diffs * f_mag[..., None]                  # (n,k,2)\n",
    "        F_mean = forces.mean(axis=1)                       # (n,2)\n",
    "        # update\n",
    "        X_new = X - lr * F_mean\n",
    "        return X_new\n",
    "    \n",
    "    def _precompute_spring_graph(self, k):\n",
    "        if isinstance(self.D_induced, torch.Tensor):\n",
    "            D_cpu = self.D_induced.cpu().numpy()\n",
    "        else:\n",
    "            D_cpu = self.D_induced\n",
    "        \n",
    "        n = D_cpu.shape[0]\n",
    "        k = min(k, n - 1)\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1, metric='precomputed').fit(D_cpu)\n",
    "        distances, indices = nbrs.kneighbors(D_cpu)\n",
    "        \n",
    "        neighbor_idx = indices[:, 1:].astype(np.int64)\n",
    "        target_dists = distances[:, 1:].astype(np.float32)\n",
    "        \n",
    "        print(f\"Built spring graph: {n} cells, {k} neighbors each\")\n",
    "        print(\"Target distance stats (before scaling):\",\n",
    "            target_dists.min(), target_dists.mean(), target_dists.max(), np.median(target_dists))\n",
    "        \n",
    "        return neighbor_idx, target_dists\n",
    "\n",
    "    # def denormalize_coordinates(self, normalized_coords):\n",
    "    #     \"\"\"Convert normalized coordinates back to original scale\"\"\"\n",
    "    #     if isinstance(normalized_coords, torch.Tensor):\n",
    "    #         # Make sure coords_range and coords_min are on the same device\n",
    "    #         coords_range = self.coords_range.to(normalized_coords.device)\n",
    "    #         coords_min = self.coords_min.to(normalized_coords.device)\n",
    "            \n",
    "    #         # Convert from [-1,1] to original scale\n",
    "    #         original_coords = (normalized_coords + 1) / 2 * coords_range + coords_min\n",
    "    #         return original_coords\n",
    "    #     else:\n",
    "    #         # Handle numpy arrays\n",
    "    #         coords_range = self.coords_range.cpu().numpy()\n",
    "    #         coords_min = self.coords_min.cpu().numpy()\n",
    "    #         original_coords = (normalized_coords + 1) / 2 * coords_range + coords_min\n",
    "    #         return original_coords\n",
    "\n",
    "    def sample_sc_with_smacof(\n",
    "        self,\n",
    "        batch_size=256,\n",
    "        timesteps=None,\n",
    "        smacof_iters=500,\n",
    "        k_neighbors=500,  # Keep for compatibility \n",
    "        lambda_s=0.5,\n",
    "        return_normalized=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Hybrid sampling: pure diffusion followed by SMACOF refinement.\n",
    "        Based exactly on sample_sc_with_soft_springs structure.\n",
    "        \"\"\"\n",
    "        from sklearn.manifold import MDS\n",
    "        \n",
    "        # 1) Diffusion generation (EXACT COPY from your code)\n",
    "        coords_list = []\n",
    "        self.netE.eval()\n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_cells = len(self.sc_gene_expr)\n",
    "        n_batches = (n_cells + batch_size - 1) // batch_size\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start = b * batch_size\n",
    "            end = min(n_cells, (b + 1) * batch_size)\n",
    "            bs_actual = end - start\n",
    "            features = self.sc_gene_expr[start:end]\n",
    "            x = torch.randn(bs_actual, 2, device=self.device)\n",
    "            \n",
    "            for t in range(timesteps - 1, -1, -1):\n",
    "                time_tensor = torch.full((bs_actual, 1), t / timesteps, device=self.device)\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                a_t = self.noise_schedule['alphas'][t]\n",
    "                acp = self.noise_schedule['alphas_cumprod'][t]\n",
    "                b_t = self.noise_schedule['betas'][t]\n",
    "                noise = torch.randn_like(x) if t > 0 else 0\n",
    "                x = (1/torch.sqrt(a_t)) * (\n",
    "                    x - ((1 - a_t)/torch.sqrt(1 - acp)) * pred_noise\n",
    "                ) + torch.sqrt(b_t) * noise\n",
    "            \n",
    "            coords_list.append(x.detach().cpu())\n",
    "        \n",
    "        # Combine all batches\n",
    "        coords_diffused = torch.cat(coords_list, dim=0)\n",
    "        coords_diffused = coords_diffused.detach().cpu().numpy()\n",
    "        print(f\"Diffusion sampling complete: {coords_diffused.shape}\")\n",
    "\n",
    "        \n",
    "        # 2) If no refinement desired, return diffusion result\n",
    "        if smacof_iters <= 0 or lambda_s == 0.0:\n",
    "            print(\"Skipping SMACOF refinement (lambda_s=0 or smacof_iters=0)\")\n",
    "            return coords_diffused if return_normalized else self.denormalize_coordinates(coords_diffused)\n",
    "        \n",
    "        # 3) SMACOF post-processing to match D_induced distances\n",
    "        print(f\"Running SMACOF with {smacof_iters} iterations, lambda_s={lambda_s}\")\n",
    "        \n",
    "\n",
    "        # Step 1: grab whatever self.D_induced is...\n",
    "        raw = self.D_induced\n",
    "        if torch.is_tensor(raw):\n",
    "            # detach in case it has gradients, move to CPU, then to numpy\n",
    "            raw = raw.detach().cpu().numpy()\n",
    "\n",
    "        D_target = raw.astype(np.float64, copy=True)\n",
    "        # Step 2: force into a bona-fide numpy float64 array\n",
    "        # Step 3: enforce symmetry & zero diagonal\n",
    "        D_target = 0.5 * (D_target + D_target.T)\n",
    "        np.fill_diagonal(D_target, 0.0)\n",
    "        \n",
    "        try:\n",
    "            # Use low-level SMACOF to avoid PyTorch dispatch issues\n",
    "            from sklearn.manifold import smacof\n",
    "\n",
    "            \n",
    "            # Call SMACOF core directly  \n",
    "            mds_coords, stress = smacof(\n",
    "                D_target,\n",
    "                n_components=2,\n",
    "                metric=True,\n",
    "                n_init=1,\n",
    "                max_iter=smacof_iters,\n",
    "                verbose=False,\n",
    "                random_state=42, normalized_stress=True\n",
    "            )\n",
    "            \n",
    "            print(f\"SMACOF stress: {stress:.6f}\")\n",
    "            \n",
    "            # Align MDS result with diffusion coordinates (rest stays same)\n",
    "            diffused_center = np.mean(coords_diffused, axis=0)\n",
    "            diffused_scale = np.std(coords_diffused)\n",
    "            \n",
    "            mds_center = np.mean(mds_coords, axis=0) \n",
    "            mds_scale = np.std(mds_coords)\n",
    "            \n",
    "            if mds_scale > 1e-8:\n",
    "                mds_coords_aligned = (mds_coords - mds_center) * (diffused_scale / mds_scale) + diffused_center\n",
    "            else:\n",
    "                mds_coords_aligned = mds_coords\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"SMACOF failed: {e}. Using diffusion coordinates only.\")\n",
    "            mds_coords_aligned = coords_diffused\n",
    "        \n",
    "        # 4) Blend diffusion and SMACOF coordinates\n",
    "        coords_final = (1.0 - lambda_s) * coords_diffused + lambda_s * mds_coords_aligned\n",
    "        \n",
    "        print(f\"Final coordinates: diffusion weight={1.0-lambda_s:.2f}, SMACOF weight={lambda_s:.2f}\")\n",
    "        \n",
    "        return coords_final if return_normalized else self.denormalize_coordinates(coords_final)\n",
    "        \n",
    "    def denormalize_coordinates(self, normalized_coords):\n",
    "        '''convert normalized coords back to original scale'''\n",
    "        if isinstance(normalized_coords, torch.Tensor):\n",
    "            coords_radius = self.coords_radius.to(normalized_coords.device)\n",
    "            coords_center = self.coords_center.to(normalized_coords.device)\n",
    "            original_coords = normalized_coords * coords_radius + coords_center\n",
    "            return original_coords\n",
    "        else:\n",
    "            coords_radius = self.coords_radius.cpu().numpy()\n",
    "            coords_center = self.coords_center.cpu().numpy()\n",
    "            original_coords = normalized_coords * coords_radius + coords_center\n",
    "            return original_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_cscc_data_individual_norm():\n",
    "    \"\"\"\n",
    "    Load and process cSCC data with individual normalization per ST dataset.\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data with individual normalization...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize expression data (same for all)\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC','rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC','rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac','rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell','rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK','rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff','Tumor_KC_Cyc']),'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3\n",
    "\n",
    "def normalize_coordinates_individually(coords):\n",
    "    \"\"\"\n",
    "    Normalize coordinates to [-1, 1] range individually.\n",
    "    \"\"\"\n",
    "    coords_min = coords.min(axis=0)\n",
    "    coords_max = coords.max(axis=0)\n",
    "    coords_range = coords_max - coords_min\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    coords_range[coords_range == 0] = 1.0\n",
    "    \n",
    "    # Normalize to [-1, 1]\n",
    "    coords_normalized = 2 * (coords - coords_min) / coords_range - 1\n",
    "    \n",
    "    return coords_normalized, coords_min, coords_max, coords_range\n",
    "\n",
    "def prepare_individually_normalized_st_data(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Normalize each ST dataset individually, then combine.\n",
    "    \"\"\"\n",
    "    print(\"Preparing individually normalized ST data...\")\n",
    "    \n",
    "    # Get common genes\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates and normalize individually\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "    \n",
    "    print(\"Normalizing coordinates individually...\")\n",
    "    st1_coords_norm, st1_min, st1_max, st1_range = normalize_coordinates_individually(st1_coords)\n",
    "    st2_coords_norm, st2_min, st2_max, st2_range = normalize_coordinates_individually(st2_coords)\n",
    "    st3_coords_norm, st3_min, st3_max, st3_range = normalize_coordinates_individually(st3_coords)\n",
    "    \n",
    "    print(f\"ST1 coord range: [{st1_coords_norm.min():.3f}, {st1_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST2 coord range: [{st2_coords_norm.min():.3f}, {st2_coords_norm.max():.3f}]\")\n",
    "    print(f\"ST3 coord range: [{st3_coords_norm.min():.3f}, {st3_coords_norm.max():.3f}]\")\n",
    "    \n",
    "    # Combine all ST data\n",
    "    st_expr_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "    st_coords_combined = np.vstack([st1_coords_norm, st2_coords_norm, st3_coords_norm])\n",
    "    \n",
    "    # Create dataset metadata\n",
    "    dataset_info = {\n",
    "        'labels': (['dataset1'] * len(st1_expr) + \n",
    "                  ['dataset2'] * len(st2_expr) + \n",
    "                  ['dataset3'] * len(st3_expr)),\n",
    "        'normalization_params': {\n",
    "            'dataset1': {'min': st1_min, 'max': st1_max, 'range': st1_range},\n",
    "            'dataset2': {'min': st2_min, 'max': st2_max, 'range': st2_range},\n",
    "            'dataset3': {'min': st3_min, 'max': st3_max, 'range': st3_range}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Combined ST data shape: {st_expr_combined.shape}\")\n",
    "    print(f\"Combined ST coords shape: {st_coords_combined.shape}\")\n",
    "    print(f\"SC data shape: {sc_expr.shape}\")\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    X_st_combined = torch.tensor(st_expr_combined, dtype=torch.float32)\n",
    "    Y_st_combined = st_coords_combined.astype(np.float32)\n",
    "    \n",
    "    return X_sc, X_st_combined, Y_st_combined, dataset_info, common_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_diffusion_models_spaotsc(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate diffusion models for each ST dataset using SpaOTsc and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['diffusion_coords_avg']\n",
    "        T_all: All transport plans from SpaOTsc\n",
    "        D_induced_all: All induced distance matrices from SpaOTsc  \n",
    "        D_st_all: All ST distance matrices\n",
    "        D_sc_all: All SC distance matrices\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "\n",
    "    T_all = []\n",
    "    D_induced_all = []\n",
    "    D_st_all = []\n",
    "    D_sc_all = []\n",
    "    spaotsc_quality_all = []\n",
    "\n",
    "    \n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training diffusion model {i+1}/3 for {dataset_name} using SpaOTsc\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract aligned expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial'].astype(np.float32)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "        X_st = torch.tensor(st_expr, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"SC shape: {X_sc.shape}, ST shape: {X_st.shape}\")\n",
    "        \n",
    "        # === REPLACE FUSED_GW_TORCH WITH SPAOTSC ===\n",
    "        print(f\"Running SpaOTsc for {dataset_name}...\")\n",
    "\n",
    "        # gamma_transport, D_sc, D_st, D_induced, spaotsc_quality = replace_fused_gw_with_spaotsc(\n",
    "        #     X_sc=X_sc,\n",
    "        #     X_st=X_st,\n",
    "        #     Y_st=st_coords,\n",
    "        #     k_neighbors=30,  # Keep same as your original k\n",
    "        #     alpha=0.25,       # Keep same as your original alpha\n",
    "        #     device=device\n",
    "        # )\n",
    "        \n",
    "        # print(f\"SpaOTsc completed for {dataset_name}! Quality: {spaotsc_quality:.4f}\")\n",
    "\n",
    "        # # Store all matrices for testing\n",
    "        # T_all.append(gamma_transport.cpu().numpy())\n",
    "        # D_induced_all.append(D_induced.cpu().numpy())\n",
    "        # D_st_all.append(D_st.cpu().numpy()) \n",
    "        # D_sc_all.append(D_sc.cpu().numpy())\n",
    "        # spaotsc_quality_all.append(spaotsc_quality)\n",
    "\n",
    "\n",
    "        T, D_sc, D_st, D_induced, fgw_dist, sc_max_dist, st_max_dist = fused_gw_torch(\n",
    "            X_sc=X_sc,\n",
    "            X_st=X_st,\n",
    "            Y_st=st_coords,\n",
    "            alpha=0.4,\n",
    "            k=35,\n",
    "            max_iter=1000,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"FGW completed for {dataset_name}! Distance: {fgw_dist:.4f}\")\n",
    "\n",
    "        T_all.append(T.cpu().numpy())\n",
    "        D_induced_all.append(D_induced.cpu().numpy())\n",
    "        D_st_all.append(D_st.cpu().numpy())\n",
    "        D_sc_all.append(D_sc.cpu().numpy())\n",
    "\n",
    "        # Initialize diffusion model (EXACTLY THE SAME as before)\n",
    "        output_dir = f'./cscc_stem_diffusion_spaotsc_{dataset_name}'\n",
    "        stem_diffusion = STEMDiffusion(\n",
    "            st_gene_expr=X_st.cpu().numpy(),\n",
    "            st_coords=st_coords,\n",
    "            D_st=D_st.cpu().numpy(),\n",
    "            sc_gene_expr=X_sc.cpu().numpy(),\n",
    "            D_induced=D_induced.cpu().numpy(),  # ← This is the key difference!\n",
    "            outf=output_dir,\n",
    "            device=device,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            hidden_dim=256,\n",
    "            dp=0.2,\n",
    "            n_timesteps=800,\n",
    "            beta_start=1e-4,\n",
    "            beta_end=0.02,\n",
    "            sigma=3.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=300,\n",
    "            coord_space_diameter=2.0,\n",
    "            # Note: SpaOTsc doesn't return max distances, so we'll compute them\n",
    "            st_max_distance=np.max(D_st.cpu().numpy()),\n",
    "            sc_max_distance=np.max(D_sc.cpu().numpy())\n",
    "        )\n",
    "        \n",
    "        # Train the model (EXACTLY THE SAME as before)\n",
    "        print(f\"Training diffusion model for {dataset_name}...\")\n",
    "        stem_diffusion.train(encoder_epochs=800)\n",
    "        \n",
    "        # Generate SC coordinates (EXACTLY THE SAME as before)\n",
    "        print(f\"Generating SC coordinates for {dataset_name}...\")\n",
    "\n",
    "        sc_coords = stem_diffusion.sample_sc_with_soft_springs(\n",
    "            batch_size   = 256,\n",
    "            timesteps    = 800,\n",
    "            use_springs  = True,\n",
    "            spring_lr    = 0.01,\n",
    "            spring_iters = 500,\n",
    "            k_neighbors  = 500,\n",
    "            lambda_s     = 0.0,\n",
    "            min_dist     = None,   # ensure no two cells end up closer than 0.05 (in normalized space)\n",
    "            return_normalized=True\n",
    "        )\n",
    "        # sc_coords = stem_diffusion.generate_sc_coordinates_two_stage(return_normalized=True)\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(sc_coords, torch.Tensor):\n",
    "            sc_coords_np = sc_coords.cpu().numpy()\n",
    "        else:\n",
    "            sc_coords_np = sc_coords\n",
    "            \n",
    "        sc_coords_results.append(sc_coords_np)\n",
    "        print(f\"Completed {dataset_name}: {sc_coords_np.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del T, D_sc, D_st, D_induced, stem_diffusion, X_st\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average the results (EXACTLY THE SAME as before)\n",
    "    print(f\"\\nAveraging results from {len(sc_coords_results)} models...\")\n",
    "    sc_coords_avg = np.mean(sc_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in sc_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['diffusion_coords_avg'] = sc_coords_avg\n",
    "    \n",
    "    # Optionally, save individual results too\n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        scadata.obsm[f'diffusion_coords_rep{i+1}'] = coords\n",
    "    \n",
    "    # Print SpaOTsc quality metrics\n",
    "    print(f\"\\nSpaOTsc Quality Metrics:\")\n",
    "    for i, quality in enumerate(spaotsc_quality_all):\n",
    "        print(f\"  Dataset {i+1}: {quality:.6f}\")\n",
    "    print(f\"  Average: {np.mean(spaotsc_quality_all):.6f}\")\n",
    "    \n",
    "    return scadata, T_all, D_induced_all, D_st_all, D_sc_all\n",
    "\n",
    "# Load and process data (keeping your existing function)\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Train individual models using SpaOTsc and get averaged results\n",
    "scadata, T_all, D_induced_all, D_st_all, D_sc_all = train_individual_diffusion_models_spaotsc(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")\n",
    "\n",
    "print(\"SpaOTsc training complete! Results saved in scadata.obsm['diffusion_coords_avg']\")\n",
    "\n",
    "# Now you have all the matrices for testing:\n",
    "print(f\"\\nMatrices available for testing:\")\n",
    "print(f\"T_spaotsc_all: {len(T_all)} transport plans\")\n",
    "print(f\"D_induced_spaotsc_all: {len(D_induced_all)} induced distance matrices\")\n",
    "print(f\"D_st_all: {len(D_st_all)} ST distance matrices\") \n",
    "print(f\"D_sc_all: {len(D_sc_all)} SC distance matrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep1', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep2', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep3', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_individual_diffusion_models_spaotsc(scadata, stadata1, stadata2, stadata3):\n",
    "    \"\"\"\n",
    "    Train separate diffusion models for each ST dataset using SpaOTsc and average the results.\n",
    "    \n",
    "    Returns:\n",
    "        scadata: Updated with averaged coordinates in obsm['diffusion_coords_avg']\n",
    "        T_all: All transport plans from SpaOTsc\n",
    "        D_induced_all: All induced distance matrices from SpaOTsc  \n",
    "        D_st_all: All ST distance matrices\n",
    "        D_sc_all: All SC distance matrices\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Store results from each model\n",
    "    sc_coords_results = []\n",
    "    \n",
    "    # List of ST datasets for iteration\n",
    "    st_datasets = [\n",
    "        (stadata1, \"dataset1\"),\n",
    "        (stadata2, \"dataset2\"), \n",
    "        (stadata3, \"dataset3\")\n",
    "    ]\n",
    "\n",
    "    T_all = []\n",
    "    D_induced_all = []\n",
    "    D_st_all = []\n",
    "    D_sc_all = []\n",
    "    spaotsc_quality_all = []\n",
    "\n",
    "    \n",
    "    for i, (stadata, dataset_name) in enumerate(st_datasets):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training diffusion model {i+1}/3 for {dataset_name} using SpaOTsc\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Get common genes between SC and current ST dataset\n",
    "        sc_genes = set(scadata.var_names)\n",
    "        st_genes = set(stadata.var_names)\n",
    "        common_genes = sorted(list(sc_genes & st_genes))\n",
    "        \n",
    "        print(f\"Common genes for {dataset_name}: {len(common_genes)}\")\n",
    "        \n",
    "        # Extract aligned expression data\n",
    "        sc_expr = scadata[:, common_genes].X\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if hasattr(sc_expr, 'toarray'):\n",
    "            sc_expr = sc_expr.toarray()\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "            \n",
    "        # Get spatial coordinates\n",
    "        st_coords = stadata.obsm['spatial'].astype(np.float32)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_sc = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "        X_st = torch.tensor(st_expr, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"SC shape: {X_sc.shape}, ST shape: {X_st.shape}\")\n",
    "        \n",
    "        # === REPLACE FUSED_GW_TORCH WITH SPAOTSC ===\n",
    "        print(f\"Running SpaOTsc for {dataset_name}...\")\n",
    "\n",
    "        gamma_transport, D_sc, D_st, D_induced, spaotsc_quality = replace_fused_gw_with_spaotsc(\n",
    "            X_sc=X_sc,\n",
    "            X_st=X_st,\n",
    "            Y_st=st_coords,\n",
    "            k_neighbors=30,  # Keep same as your original k\n",
    "            alpha=0.9,       # Keep same as your original alpha\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(f\"SpaOTsc completed for {dataset_name}! Quality: {spaotsc_quality:.4f}\")\n",
    "\n",
    "        # Store all matrices for testing\n",
    "        T_all.append(gamma_transport.cpu().numpy())\n",
    "        D_induced_all.append(D_induced.cpu().numpy())\n",
    "        D_st_all.append(D_st.cpu().numpy()) \n",
    "        D_sc_all.append(D_sc.cpu().numpy())\n",
    "        spaotsc_quality_all.append(spaotsc_quality)\n",
    "\n",
    "\n",
    "        # Initialize diffusion model (EXACTLY THE SAME as before)\n",
    "        output_dir = f'./cscc_stem_diffusion_spaotsc_{dataset_name}'\n",
    "        stem_diffusion = STEMDiffusion(\n",
    "            st_gene_expr=X_st.cpu().numpy(),\n",
    "            st_coords=st_coords,\n",
    "            D_st=D_st.cpu().numpy(),\n",
    "            sc_gene_expr=X_sc.cpu().numpy(),\n",
    "            D_induced=D_induced.cpu().numpy(),  # ← This is the key difference!\n",
    "            outf=output_dir,\n",
    "            device=device,\n",
    "            n_genes=len(common_genes),\n",
    "            n_embedding=[512, 256, 128],\n",
    "            hidden_dim=256,\n",
    "            dp=0.2,\n",
    "            n_timesteps=800,\n",
    "            beta_start=1e-4,\n",
    "            beta_end=0.02,\n",
    "            sigma=3.0,\n",
    "            alpha=0.8,\n",
    "            mmdbatch=1000,\n",
    "            batch_size=300,\n",
    "            coord_space_diameter=2.0,\n",
    "            # Note: SpaOTsc doesn't return max distances, so we'll compute them\n",
    "            st_max_distance=np.max(D_st.cpu().numpy()),\n",
    "            sc_max_distance=np.max(D_sc.cpu().numpy())\n",
    "        )\n",
    "        \n",
    "        # Train the model (EXACTLY THE SAME as before)\n",
    "        print(f\"Training diffusion model for {dataset_name}...\")\n",
    "        stem_diffusion.train(encoder_epochs=800)\n",
    "        \n",
    "        # Generate SC coordinates (EXACTLY THE SAME as before)\n",
    "        print(f\"Generating SC coordinates for {dataset_name}...\")\n",
    "\n",
    "        # sc_coords = stem_diffusion.sample_sc_with_soft_springs(\n",
    "        #     batch_size   = 256,\n",
    "        #     timesteps    = 800,\n",
    "        #     use_springs  = True,\n",
    "        #     spring_lr    = 0.01,\n",
    "        #     spring_iters = 500,\n",
    "        #     k_neighbors  = 500,\n",
    "        #     lambda_s     = 0.5,\n",
    "        #     min_dist     = None,   # ensure no two cells end up closer than 0.05 (in normalized space)\n",
    "        #     return_normalized=True\n",
    "        # )\n",
    "\n",
    "        sc_coords = stem_diffusion.sample_sc_with_smacof(\n",
    "            batch_size=256,\n",
    "            timesteps=800,\n",
    "            k_neighbors=500,  # Not used but kept for compatibility\n",
    "            smacof_iters=500,  # Replaces spring_iters\n",
    "            lambda_s=0.0,\n",
    "            return_normalized=True\n",
    "        )\n",
    "        # sc_coords = stem_diffusion.generate_sc_coordinates_two_stage(return_normalized=True)\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if isinstance(sc_coords, torch.Tensor):\n",
    "            sc_coords_np = sc_coords.cpu().numpy()\n",
    "        else:\n",
    "            sc_coords_np = sc_coords\n",
    "            \n",
    "        sc_coords_results.append(sc_coords_np)\n",
    "        print(f\"Completed {dataset_name}: {sc_coords_np.shape}\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        # del gamma_transport, D_sc, D_st, D_induced, stem_diffusion, X_st\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Average the results (EXACTLY THE SAME as before)\n",
    "    print(f\"\\nAveraging results from {len(sc_coords_results)} models...\")\n",
    "    sc_coords_avg = np.mean(sc_coords_results, axis=0)\n",
    "    \n",
    "    # Verify shapes match\n",
    "    shapes = [coords.shape for coords in sc_coords_results]\n",
    "    assert all(shape == shapes[0] for shape in shapes), f\"Shape mismatch: {shapes}\"\n",
    "    \n",
    "    print(f\"Final averaged coordinates shape: {sc_coords_avg.shape}\")\n",
    "    \n",
    "    # Add to AnnData\n",
    "    scadata.obsm['diffusion_coords_avg'] = sc_coords_avg\n",
    "    \n",
    "    # Optionally, save individual results too\n",
    "    for i, coords in enumerate(sc_coords_results):\n",
    "        scadata.obsm[f'diffusion_coords_rep{i+1}'] = coords\n",
    "    \n",
    "    # Print SpaOTsc quality metrics\n",
    "    print(f\"\\nSpaOTsc Quality Metrics:\")\n",
    "    for i, quality in enumerate(spaotsc_quality_all):\n",
    "        print(f\"  Dataset {i+1}: {quality:.6f}\")\n",
    "    print(f\"  Average: {np.mean(spaotsc_quality_all):.6f}\")\n",
    "    \n",
    "    return scadata, T_all, D_induced_all, D_st_all, D_sc_all\n",
    "\n",
    "# Load and process data (keeping your existing function)\n",
    "scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "\n",
    "# Train individual models using SpaOTsc and get averaged results\n",
    "scadata, T_spaotsc_all, D_induced_spaotsc_all, D_st_all, D_sc_all = train_individual_diffusion_models_spaotsc(\n",
    "    scadata, stadata1, stadata2, stadata3\n",
    ")\n",
    "\n",
    "print(\"SpaOTsc training complete! Results saved in scadata.obsm['diffusion_coords_avg']\")\n",
    "\n",
    "# Now you have all the matrices for testing:\n",
    "print(f\"\\nMatrices available for testing:\")\n",
    "print(f\"T_spaotsc_all: {len(T_spaotsc_all)} transport plans\")\n",
    "print(f\"D_induced_spaotsc_all: {len(D_induced_spaotsc_all)} induced distance matrices\")\n",
    "print(f\"D_st_all: {len(D_st_all)} ST distance matrices\") \n",
    "print(f\"D_sc_all: {len(D_sc_all)} SC distance matrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results with separate plots\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "\n",
    "# Plot 1: Averaged coordinates\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_avg', color='rough_celltype',\n",
    "               size=85, title='SC Spatial Coordinates (Averaged from 3 Models)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Model 1 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep1', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 1)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Model 2 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep2', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 2)',\n",
    "               palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Plot 4: Model 3 results\n",
    "plt.figure(figsize=(4, 4))\n",
    "sc.pl.embedding(scadata, basis='diffusion_coords_rep3', color='rough_celltype',\n",
    "               size=85, title='SC Coordinates (Model 3)',\n",
    "             palette=my_tab20, legend_loc='right margin', legend_fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_sc_all[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_induced_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_induced_spaotsc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import smacof\n",
    "\n",
    "def plot_smacof_from_distance_matrices(D_induced_spaotsc_all, scadata, st_datasets=None):\n",
    "    \"\"\"\n",
    "    Apply SMACOF to each D_induced matrix and plot coordinates colored by cell type.\n",
    "    \n",
    "    Args:\n",
    "        D_induced_spaotsc_all: List of 3 D_induced distance matrices\n",
    "        scadata: AnnData with cell type info in obs['rough_celltype']\n",
    "        st_datasets: Optional list of (stadata1, stadata2, stadata3) for overlay\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up colors for cell types\n",
    "    my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "    cell_types = scadata.obs['rough_celltype'].unique()\n",
    "    color_map = {ct: my_tab20[i % len(my_tab20)] for i, ct in enumerate(cell_types)}\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, D_induced in enumerate(D_induced_spaotsc_all):\n",
    "        print(f\"\\n🔬 Processing Dataset {i+1}\")\n",
    "        print(f\"   Distance matrix shape: {D_induced.shape}\")\n",
    "        \n",
    "        # Step 1: Prepare distance matrix\n",
    "        if torch.is_tensor(D_induced):\n",
    "            D_target = D_induced.detach().cpu().numpy()\n",
    "        else:\n",
    "            D_target = D_induced.copy()\n",
    "            \n",
    "        D_target = D_target.astype(np.float64)\n",
    "        \n",
    "        # Step 2: Enforce symmetry & zero diagonal\n",
    "        D_target = 0.5 * (D_target + D_target.T)\n",
    "        np.fill_diagonal(D_target, 0.0)\n",
    "        \n",
    "        print(f\"   Distance range: [{D_target.min():.4f}, {D_target.max():.4f}]\")\n",
    "        \n",
    "        # Step 3: Apply SMACOF\n",
    "        try:\n",
    "            mds_coords, stress = smacof(\n",
    "                D_target,\n",
    "                n_components=2,\n",
    "                metric=True,\n",
    "                n_init=1,\n",
    "                max_iter=300,\n",
    "                verbose=False,\n",
    "                random_state=42,\n",
    "                normalized_stress=True\n",
    "            )\n",
    "            \n",
    "            print(f\"   SMACOF stress: {stress:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ SMACOF failed: {e}\")\n",
    "            # Fallback to random coordinates\n",
    "            mds_coords = np.random.randn(D_target.shape[0], 2)\n",
    "            stress = float('inf')\n",
    "        \n",
    "        # Step 4: Plot coordinates colored by cell type\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get cell types for coloring\n",
    "        cell_types_list = scadata.obs['rough_celltype'].values\n",
    "        \n",
    "        # Plot each cell type separately for legend\n",
    "        # for ct in cell_types:\n",
    "        #     mask = cell_types_list == ct\n",
    "        #     if np.any(mask):\n",
    "        #         ax.scatter(mds_coords[mask, 0], mds_coords[mask, 1], \n",
    "        #                   c=color_map[ct], label=ct, s=20, alpha=0.7)\n",
    "        \n",
    "        # Optional: Overlay ST spots if provided\n",
    "        if st_datasets is not None and i < len(st_datasets):\n",
    "            stadata = st_datasets[i]\n",
    "            st_coords = stadata.obsm['spatial']\n",
    "            \n",
    "            # Normalize ST coordinates to roughly match SC coordinate range\n",
    "            st_coords_norm = (st_coords - st_coords.mean(axis=0)) / st_coords.std(axis=0)\n",
    "            sc_scale = mds_coords.std()\n",
    "            st_coords_scaled = st_coords_norm * sc_scale\n",
    "            \n",
    "            ax.scatter(st_coords_scaled[:, 0], st_coords_scaled[:, 1], \n",
    "                      c='black', marker='x', s=10, alpha=0.5, label='ST spots')\n",
    "        \n",
    "        ax.set_title(f'Dataset {i+1}: SMACOF from D_induced\\n(stress: {stress:.4f})', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel('Coordinate 1')\n",
    "        ax.set_ylabel('Coordinate 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend (only for first plot to avoid clutter)\n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('smacof_coordinates_from_d_induced.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📊 SUMMARY:\")\n",
    "    print(f\"   Generated coordinates for {len(D_induced_spaotsc_all)} datasets\")\n",
    "    print(f\"   Cell types: {list(cell_types)}\")\n",
    "    print(f\"   Plot saved as 'smacof_coordinates_from_d_induced.png'\")\n",
    "\n",
    "# Usage:\n",
    "print(\"🎯 Generating SMACOF coordinates from D_induced matrices...\")\n",
    "\n",
    "# With ST overlay (optional)\n",
    "st_datasets = [stadata1, stadata2, stadata3]\n",
    "plot_smacof_from_distance_matrices(D_induced_spaotsc_all, scadata, st_datasets)\n",
    "\n",
    "# Or without ST overlay\n",
    "# plot_smacof_from_distance_matrices(D_induced_spaotsc_all, scadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "def plot_mds_from_distance_matrices(D_induced_spaotsc_all, scadata):\n",
    "    \"\"\"\n",
    "    Apply standard MDS to each D_induced matrix and plot coordinates colored by cell type.\n",
    "    \n",
    "    Args:\n",
    "        D_induced_spaotsc_all: List of 3 D_induced distance matrices\n",
    "        scadata: AnnData with cell type info in obs['rough_celltype']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up colors for cell types\n",
    "    my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "    cell_types = scadata.obs['rough_celltype'].unique()\n",
    "    color_map = {ct: my_tab20[i % len(my_tab20)] for i, ct in enumerate(cell_types)}\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, D_induced in enumerate(D_induced_spaotsc_all):\n",
    "        print(f\"\\n📊 Processing Dataset {i+1}\")\n",
    "        print(f\"   Distance matrix shape: {D_induced.shape}\")\n",
    "        \n",
    "        # Step 1: Prepare distance matrix\n",
    "        if torch.is_tensor(D_induced):\n",
    "            D_target = D_induced.detach().cpu().numpy()\n",
    "        else:\n",
    "            D_target = D_induced.copy()\n",
    "            \n",
    "        D_target = D_target.astype(np.float64)\n",
    "        \n",
    "        # Step 2: Enforce symmetry & zero diagonal\n",
    "        D_target = 0.5 * (D_target + D_target.T)\n",
    "        np.fill_diagonal(D_target, 0.0)\n",
    "        \n",
    "        print(f\"   Distance range: [{D_target.min():.4f}, {D_target.max():.4f}]\")\n",
    "        \n",
    "        # Step 3: Apply standard MDS\n",
    "        try:\n",
    "            mds = MDS(\n",
    "                n_components=2,\n",
    "                dissimilarity=\"precomputed\",\n",
    "                metric=True,  # Use metric MDS\n",
    "                n_init=4,     # Multiple random initializations\n",
    "                max_iter=300,\n",
    "                eps=1e-6,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            mds_coords = mds.fit_transform(D_target)\n",
    "            stress = mds.stress_\n",
    "            \n",
    "            print(f\"   MDS stress: {stress:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ MDS failed: {e}\")\n",
    "            # Fallback to random coordinates\n",
    "            mds_coords = np.random.randn(D_target.shape[0], 2)\n",
    "            stress = float('inf')\n",
    "        \n",
    "        # Step 4: Plot coordinates colored by cell type\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get cell types for coloring\n",
    "        cell_types_list = scadata.obs['rough_celltype'].values\n",
    "        \n",
    "        # Plot each cell type separately for legend\n",
    "        for ct in cell_types:\n",
    "            mask = cell_types_list == ct\n",
    "            if np.any(mask):\n",
    "                ax.scatter(mds_coords[mask, 0], mds_coords[mask, 1], \n",
    "                          c=color_map[ct], label=ct, s=20, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'Dataset {i+1}: MDS from D_induced\\n(stress: {stress:.4f})', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel('MDS Dimension 1')\n",
    "        ax.set_ylabel('MDS Dimension 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend (only for first plot to avoid clutter)\n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mds_coordinates_from_d_induced.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📋 SUMMARY:\")\n",
    "    print(f\"   Generated MDS embeddings for {len(D_induced_spaotsc_all)} datasets\")\n",
    "    print(f\"   Cell types: {list(cell_types)}\")\n",
    "    print(f\"   Plot saved as 'mds_coordinates_from_d_induced.png'\")\n",
    "\n",
    "# Usage:\n",
    "print(\"🎯 Generating MDS embeddings from D_induced matrices...\")\n",
    "plot_mds_from_distance_matrices(D_induced_all, scadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "def plot_mds_from_distance_matrices(D_induced_spaotsc_all, scadata):\n",
    "    \"\"\"\n",
    "    Apply standard MDS to each D_induced matrix and plot coordinates colored by cell type.\n",
    "    \n",
    "    Args:\n",
    "        D_induced_spaotsc_all: List of 3 D_induced distance matrices\n",
    "        scadata: AnnData with cell type info in obs['rough_celltype']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up colors for cell types\n",
    "    my_tab20 = sns.color_palette(\"tab20\", n_colors=20).as_hex()\n",
    "    cell_types = scadata.obs['rough_celltype'].unique()\n",
    "    color_map = {ct: my_tab20[i % len(my_tab20)] for i, ct in enumerate(cell_types)}\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    for i, D_induced in enumerate(D_induced_spaotsc_all):\n",
    "        print(f\"\\n📊 Processing Dataset {i+1}\")\n",
    "        print(f\"   Distance matrix shape: {D_induced.shape}\")\n",
    "        \n",
    "        # Step 1: Prepare distance matrix\n",
    "        if torch.is_tensor(D_induced):\n",
    "            D_target = D_induced.detach().cpu().numpy()\n",
    "        else:\n",
    "            D_target = D_induced.copy()\n",
    "            \n",
    "        D_target = D_target.astype(np.float64)\n",
    "        \n",
    "        # Step 2: Enforce symmetry & zero diagonal\n",
    "        D_target = 0.5 * (D_target + D_target.T)\n",
    "        np.fill_diagonal(D_target, 0.0)\n",
    "        \n",
    "        print(f\"   Distance range: [{D_target.min():.4f}, {D_target.max():.4f}]\")\n",
    "        \n",
    "        # Step 3: Apply standard MDS\n",
    "        try:\n",
    "            mds = MDS(\n",
    "                n_components=2,\n",
    "                dissimilarity=\"precomputed\",\n",
    "                metric=True,  # Use metric MDS\n",
    "                n_init=4,     # Multiple random initializations\n",
    "                max_iter=300,\n",
    "                eps=1e-6,\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            mds_coords = mds.fit_transform(D_target)\n",
    "            stress = mds.stress_\n",
    "            \n",
    "            print(f\"   MDS stress: {stress:.6f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ MDS failed: {e}\")\n",
    "            # Fallback to random coordinates\n",
    "            mds_coords = np.random.randn(D_target.shape[0], 2)\n",
    "            stress = float('inf')\n",
    "        \n",
    "        # Step 4: Plot coordinates colored by cell type\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get cell types for coloring\n",
    "        cell_types_list = scadata.obs['rough_celltype'].values\n",
    "        \n",
    "        # Plot each cell type separately for legend\n",
    "        for ct in cell_types:\n",
    "            mask = cell_types_list == ct\n",
    "            if np.any(mask):\n",
    "                ax.scatter(mds_coords[mask, 0], mds_coords[mask, 1], \n",
    "                          c=color_map[ct], label=ct, s=20, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(f'Dataset {i+1}: MDS from D_induced\\n(stress: {stress:.4f})', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "        ax.set_xlabel('MDS Dimension 1')\n",
    "        ax.set_ylabel('MDS Dimension 2')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend (only for first plot to avoid clutter)\n",
    "        if i == 0:\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('mds_coordinates_from_d_induced.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n📋 SUMMARY:\")\n",
    "    print(f\"   Generated MDS embeddings for {len(D_induced_spaotsc_all)} datasets\")\n",
    "    print(f\"   Cell types: {list(cell_types)}\")\n",
    "    print(f\"   Plot saved as 'mds_coordinates_from_d_induced.png'\")\n",
    "\n",
    "# Usage:\n",
    "print(\"🎯 Generating MDS embeddings from D_induced matrices...\")\n",
    "plot_mds_from_distance_matrices(D_induced_spaotsc_all, scadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def compare_d_induced_matrices(D_induced_spaotsc_all, D_induced_all, save_plots=True):\n",
    "    \"\"\"\n",
    "    Comprehensive comparison of D_induced matrices from SpaOTsc vs FGW methods.\n",
    "    \n",
    "    Args:\n",
    "        D_induced_spaotsc_all: List of 3 D_induced matrices from SpaOTsc\n",
    "        D_induced_all: List of 3 D_induced matrices from FGW  \n",
    "        save_plots: Whether to save plots to files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Storage for statistics\n",
    "    stats_summary = []\n",
    "    \n",
    "    for dataset_idx in range(3):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"DATASET {dataset_idx + 1} COMPARISON\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get matrices for current dataset\n",
    "        D_spaotsc = D_induced_spaotsc_all[dataset_idx]\n",
    "        D_fgw = D_induced_all[dataset_idx]\n",
    "        \n",
    "        print(f\"SpaOTsc matrix shape: {D_spaotsc.shape}\")\n",
    "        print(f\"FGW matrix shape: {D_fgw.shape}\")\n",
    "        \n",
    "        # Ensure matrices are the same size\n",
    "        min_size = min(D_spaotsc.shape[0], D_fgw.shape[0])\n",
    "        D_spaotsc_trimmed = D_spaotsc[:min_size, :min_size]\n",
    "        D_fgw_trimmed = D_fgw[:min_size, :min_size]\n",
    "        \n",
    "        # Get upper triangular indices (excluding diagonal) for comparison\n",
    "        triu_indices = np.triu_indices(min_size, k=1)\n",
    "        spaotsc_values = D_spaotsc_trimmed[triu_indices]\n",
    "        fgw_values = D_fgw_trimmed[triu_indices]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        pearson_corr, pearson_p = pearsonr(spaotsc_values, fgw_values)\n",
    "        spearman_corr, spearman_p = spearmanr(spaotsc_values, fgw_values)\n",
    "        mse = mean_squared_error(spaotsc_values, fgw_values)\n",
    "        mae = mean_absolute_error(spaotsc_values, fgw_values)\n",
    "        \n",
    "        # Store statistics\n",
    "        stats_summary.append({\n",
    "            'Dataset': f'Dataset {dataset_idx + 1}',\n",
    "            'Pearson_r': pearson_corr,\n",
    "            'Pearson_p': pearson_p,\n",
    "            'Spearman_r': spearman_corr,\n",
    "            'Spearman_p': spearman_p,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'SpaOTsc_mean': np.mean(spaotsc_values),\n",
    "            'FGW_mean': np.mean(fgw_values),\n",
    "            'SpaOTsc_std': np.std(spaotsc_values),\n",
    "            'FGW_std': np.std(fgw_values)\n",
    "        })\n",
    "        \n",
    "        print(f\"Pearson correlation: {pearson_corr:.4f} (p={pearson_p:.2e})\")\n",
    "        print(f\"Spearman correlation: {spearman_corr:.4f} (p={spearman_p:.2e})\")\n",
    "        print(f\"MSE: {mse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        \n",
    "        # Row 1: Heatmaps of distance matrices\n",
    "        # SpaOTsc heatmap\n",
    "        ax1 = plt.subplot(4, 9, dataset_idx * 3 + 1)\n",
    "        im1 = plt.imshow(D_spaotsc_trimmed, cmap='viridis', aspect='auto')\n",
    "        plt.title(f'Dataset {dataset_idx + 1}\\nSpaOTsc D_induced', fontsize=10)\n",
    "        plt.colorbar(im1, shrink=0.8)\n",
    "        \n",
    "        # FGW heatmap  \n",
    "        ax2 = plt.subplot(4, 9, dataset_idx * 3 + 2)\n",
    "        im2 = plt.imshow(D_fgw_trimmed, cmap='viridis', aspect='auto')\n",
    "        plt.title(f'Dataset {dataset_idx + 1}\\nFGW D_induced', fontsize=10)\n",
    "        plt.colorbar(im2, shrink=0.8)\n",
    "        \n",
    "        # Difference heatmap\n",
    "        ax3 = plt.subplot(4, 9, dataset_idx * 3 + 3)\n",
    "        diff_matrix = D_spaotsc_trimmed - D_fgw_trimmed\n",
    "        im3 = plt.imshow(diff_matrix, cmap='RdBu_r', aspect='auto')\n",
    "        plt.title(f'Dataset {dataset_idx + 1}\\nDifference\\n(SpaOTsc - FGW)', fontsize=10)\n",
    "        plt.colorbar(im3, shrink=0.8)\n",
    "        \n",
    "        # Row 2: Scatter plots\n",
    "        ax4 = plt.subplot(4, 3, dataset_idx + 4)\n",
    "        \n",
    "        # Subsample for clearer visualization if too many points\n",
    "        if len(spaotsc_values) > 10000:\n",
    "            indices = np.random.choice(len(spaotsc_values), 10000, replace=False)\n",
    "            x_plot = spaotsc_values[indices]\n",
    "            y_plot = fgw_values[indices]\n",
    "        else:\n",
    "            x_plot = spaotsc_values\n",
    "            y_plot = fgw_values\n",
    "            \n",
    "        plt.scatter(x_plot, y_plot, alpha=0.5, s=1)\n",
    "        plt.plot([min(x_plot.min(), y_plot.min()), max(x_plot.max(), y_plot.max())], \n",
    "                [min(x_plot.min(), y_plot.min()), max(x_plot.max(), y_plot.max())], \n",
    "                'r--', alpha=0.8)\n",
    "        plt.xlabel('SpaOTsc D_induced values')\n",
    "        plt.ylabel('FGW D_induced values')\n",
    "        plt.title(f'Dataset {dataset_idx + 1} Scatter Plot\\nr={pearson_corr:.3f}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Row 3: Distribution comparisons\n",
    "        ax5 = plt.subplot(4, 3, dataset_idx + 7)\n",
    "        \n",
    "        # Histogram comparison\n",
    "        plt.hist(spaotsc_values, bins=50, alpha=0.7, label='SpaOTsc', density=True)\n",
    "        plt.hist(fgw_values, bins=50, alpha=0.7, label='FGW', density=True)\n",
    "        plt.xlabel('Distance values')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'Dataset {dataset_idx + 1} Distributions')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Row 4: Difference distribution\n",
    "        ax6 = plt.subplot(4, 3, dataset_idx + 10)\n",
    "        differences = spaotsc_values - fgw_values\n",
    "        plt.hist(differences, bins=50, alpha=0.7, color='purple')\n",
    "        plt.axvline(np.mean(differences), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(differences):.4f}')\n",
    "        plt.xlabel('Difference (SpaOTsc - FGW)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Dataset {dataset_idx + 1} Difference Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig('d_induced_comparison_comprehensive.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nPlot saved as 'd_induced_comparison_comprehensive.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary statistics table\n",
    "    stats_df = pd.DataFrame(stats_summary)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(stats_df.round(4))\n",
    "    \n",
    "    # Create a focused correlation plot\n",
    "    create_correlation_summary_plot(stats_df, save_plots)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def create_correlation_summary_plot(stats_df, save_plots=True):\n",
    "    \"\"\"Create a summary plot focusing on correlations across datasets.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Correlation comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    x = np.arange(len(stats_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, stats_df['Pearson_r'], width, label='Pearson', alpha=0.8)\n",
    "    ax1.bar(x + width/2, stats_df['Spearman_r'], width, label='Spearman', alpha=0.8)\n",
    "    ax1.set_xlabel('Dataset')\n",
    "    ax1.set_ylabel('Correlation Coefficient')\n",
    "    ax1.set_title('Correlation between SpaOTsc and FGW')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(stats_df['Dataset'])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Error metrics\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2_twin = ax2.twinx()\n",
    "    \n",
    "    line1 = ax2.plot(stats_df['Dataset'], stats_df['MSE'], 'ro-', label='MSE')\n",
    "    line2 = ax2_twin.plot(stats_df['Dataset'], stats_df['MAE'], 'bo-', label='MAE')\n",
    "    \n",
    "    ax2.set_xlabel('Dataset')\n",
    "    ax2.set_ylabel('MSE', color='r')\n",
    "    ax2_twin.set_ylabel('MAE', color='b')\n",
    "    ax2.set_title('Error Metrics')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "    ax2_twin.tick_params(axis='y', labelcolor='b')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Mean values comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(stats_df['Dataset'], stats_df['SpaOTsc_mean'], 'o-', label='SpaOTsc', linewidth=2)\n",
    "    ax3.plot(stats_df['Dataset'], stats_df['FGW_mean'], 's-', label='FGW', linewidth=2)\n",
    "    ax3.set_xlabel('Dataset')\n",
    "    ax3.set_ylabel('Mean Distance Value')\n",
    "    ax3.set_title('Mean Distance Values Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Standard deviation comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(stats_df['Dataset'], stats_df['SpaOTsc_std'], 'o-', label='SpaOTsc', linewidth=2)\n",
    "    ax4.plot(stats_df['Dataset'], stats_df['FGW_std'], 's-', label='FGW', linewidth=2)\n",
    "    ax4.set_xlabel('Dataset')\n",
    "    ax4.set_ylabel('Standard Deviation')\n",
    "    ax4.set_title('Standard Deviation Comparison')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.savefig('d_induced_correlation_summary.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Summary plot saved as 'd_induced_correlation_summary.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def analyze_matrix_properties(D_induced_spaotsc_all, D_induced_all):\n",
    "    \"\"\"Analyze mathematical properties of the distance matrices.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"MATRIX PROPERTIES ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for dataset_idx in range(3):\n",
    "        print(f\"\\nDataset {dataset_idx + 1}:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        D_spaotsc = D_induced_spaotsc_all[dataset_idx]\n",
    "        D_fgw = D_induced_all[dataset_idx]\n",
    "        \n",
    "        # Check symmetry\n",
    "        spaotsc_symmetric = np.allclose(D_spaotsc, D_spaotsc.T, rtol=1e-5)\n",
    "        fgw_symmetric = np.allclose(D_fgw, D_fgw.T, rtol=1e-5)\n",
    "        \n",
    "        # Check if diagonal is zero (distance from cell to itself)\n",
    "        spaotsc_diag_zero = np.allclose(np.diag(D_spaotsc), 0, atol=1e-5)\n",
    "        fgw_diag_zero = np.allclose(np.diag(D_fgw), 0, atol=1e-5)\n",
    "        \n",
    "        # Matrix statistics\n",
    "        spaotsc_stats = {\n",
    "            'min': np.min(D_spaotsc),\n",
    "            'max': np.max(D_spaotsc),\n",
    "            'mean': np.mean(D_spaotsc),\n",
    "            'std': np.std(D_spaotsc)\n",
    "        }\n",
    "        \n",
    "        fgw_stats = {\n",
    "            'min': np.min(D_fgw),\n",
    "            'max': np.max(D_fgw),\n",
    "            'mean': np.mean(D_fgw),\n",
    "            'std': np.std(D_fgw)\n",
    "        }\n",
    "        \n",
    "        print(f\"SpaOTsc - Symmetric: {spaotsc_symmetric}, Diagonal=0: {spaotsc_diag_zero}\")\n",
    "        print(f\"FGW     - Symmetric: {fgw_symmetric}, Diagonal=0: {fgw_diag_zero}\")\n",
    "        print(f\"SpaOTsc stats: {spaotsc_stats}\")\n",
    "        print(f\"FGW stats:     {fgw_stats}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # This assumes you have the variables D_induced_spaotsc_all and D_induced_all \n",
    "    # available in your notebook environment\n",
    "    \n",
    "    print(\"Starting comprehensive D_induced matrix comparison...\")\n",
    "    print(\"Make sure you have run the cells that generate:\")\n",
    "    print(\"- D_induced_spaotsc_all (from SpaOTsc method)\")\n",
    "    print(\"- D_induced_all (from FGW method)\")\n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"stats_df = compare_d_induced_matrices(D_induced_spaotsc_all, D_induced_all)\")\n",
    "    print(\"analyze_matrix_properties(D_induced_spaotsc_all, D_induced_all)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell after you have both D_induced_spaotsc_all and D_induced_all available\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def compare_methods():\n",
    "    \"\"\"\n",
    "    Compare SpaOTsc and FGW D_induced matrices across all 3 datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔍 COMPARING SpaOTsc vs FGW D_induced MATRICES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Verify we have the right variables\n",
    "    try:\n",
    "        print(f\"✓ D_induced_spaotsc_all: {len(D_induced_spaotsc_all)} matrices\")\n",
    "        print(f\"✓ D_induced_all: {len(D_induced_all)} matrices\")\n",
    "    except NameError:\n",
    "        print(\"❌ Missing variables! Make sure you have run the cells that create:\")\n",
    "        print(\"   - D_induced_spaotsc_all\")  \n",
    "        print(\"   - D_induced_all\")\n",
    "        return\n",
    "    \n",
    "    # Create comprehensive comparison plot\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    stats_summary = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"\\n📊 DATASET {i+1} ANALYSIS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Get matrices\n",
    "        D_spaotsc = D_induced_spaotsc_all[i]\n",
    "        D_fgw = D_induced_all[i]\n",
    "        \n",
    "        print(f\"SpaOTsc shape: {D_spaotsc.shape}\")\n",
    "        print(f\"FGW shape:     {D_fgw.shape}\")\n",
    "        \n",
    "        # Ensure same size for comparison\n",
    "        min_size = min(D_spaotsc.shape[0], D_fgw.shape[0])\n",
    "        D_spaotsc = D_spaotsc[:min_size, :min_size]\n",
    "        D_fgw = D_fgw[:min_size, :min_size]\n",
    "        \n",
    "        # Get upper triangular values (excluding diagonal)\n",
    "        triu_idx = np.triu_indices(min_size, k=1)\n",
    "        vals_spaotsc = D_spaotsc[triu_idx]\n",
    "        vals_fgw = D_fgw[triu_idx]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        r_pearson, p_pearson = pearsonr(vals_spaotsc, vals_fgw)\n",
    "        r_spearman, p_spearman = spearmanr(vals_spaotsc, vals_fgw)\n",
    "        mse = mean_squared_error(vals_spaotsc, vals_fgw)\n",
    "        mae = mean_absolute_error(vals_spaotsc, vals_fgw)\n",
    "        \n",
    "        print(f\"Pearson r:  {r_pearson:.4f} (p={p_pearson:.2e})\")\n",
    "        print(f\"Spearman r: {r_spearman:.4f} (p={p_spearman:.2e})\")\n",
    "        print(f\"MSE:        {mse:.6f}\")\n",
    "        print(f\"MAE:        {mae:.6f}\")\n",
    "        \n",
    "        # Store stats\n",
    "        stats_summary.append({\n",
    "            'Dataset': f'Dataset {i+1}',\n",
    "            'Pearson_r': r_pearson,\n",
    "            'Spearman_r': r_spearman,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae,\n",
    "            'SpaOTsc_mean': np.mean(vals_spaotsc),\n",
    "            'FGW_mean': np.mean(vals_fgw),\n",
    "            'SpaOTsc_std': np.std(vals_spaotsc),\n",
    "            'FGW_std': np.std(vals_fgw)\n",
    "        })\n",
    "        \n",
    "        # Create visualizations for this dataset\n",
    "        \n",
    "        # Row 1: Heatmaps\n",
    "        # SpaOTsc heatmap\n",
    "        ax1 = plt.subplot(3, 4, i*4 + 1)\n",
    "        im1 = plt.imshow(D_spaotsc, cmap='viridis', aspect='auto')\n",
    "        plt.title(f'Dataset {i+1}: SpaOTsc', fontsize=11, fontweight='bold')\n",
    "        plt.colorbar(im1, shrink=0.7)\n",
    "        if i == 2:\n",
    "            plt.xlabel('Cell index')\n",
    "        plt.ylabel('Cell index')\n",
    "        \n",
    "        # FGW heatmap  \n",
    "        ax2 = plt.subplot(3, 4, i*4 + 2)\n",
    "        im2 = plt.imshow(D_fgw, cmap='viridis', aspect='auto')\n",
    "        plt.title(f'Dataset {i+1}: FGW', fontsize=11, fontweight='bold')\n",
    "        plt.colorbar(im2, shrink=0.7)\n",
    "        if i == 2:\n",
    "            plt.xlabel('Cell index')\n",
    "        \n",
    "        # Difference heatmap\n",
    "        ax3 = plt.subplot(3, 4, i*4 + 3)\n",
    "        diff = D_spaotsc - D_fgw\n",
    "        im3 = plt.imshow(diff, cmap='RdBu_r', aspect='auto')\n",
    "        plt.title(f'Dataset {i+1}: Difference', fontsize=11, fontweight='bold')\n",
    "        plt.colorbar(im3, shrink=0.7)\n",
    "        if i == 2:\n",
    "            plt.xlabel('Cell index')\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax4 = plt.subplot(3, 4, i*4 + 4)\n",
    "        \n",
    "        # Subsample if too many points for clearer visualization\n",
    "        if len(vals_spaotsc) > 5000:\n",
    "            idx = np.random.choice(len(vals_spaotsc), 5000, replace=False)\n",
    "            x_plot, y_plot = vals_spaotsc[idx], vals_fgw[idx]\n",
    "        else:\n",
    "            x_plot, y_plot = vals_spaotsc, vals_fgw\n",
    "            \n",
    "        plt.scatter(x_plot, y_plot, alpha=0.6, s=2, c='steelblue')\n",
    "        \n",
    "        # Add diagonal line\n",
    "        min_val, max_val = min(x_plot.min(), y_plot.min()), max(x_plot.max(), y_plot.max())\n",
    "        plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, linewidth=2)\n",
    "        \n",
    "        plt.xlabel('SpaOTsc values')\n",
    "        plt.ylabel('FGW values')\n",
    "        plt.title(f'Dataset {i+1}: Correlation\\nr = {r_pearson:.3f}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('d_induced_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create summary statistics table and plots\n",
    "    stats_df = pd.DataFrame(stats_summary)\n",
    "    \n",
    "    print(f\"\\n📈 SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(stats_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Summary plots\n",
    "    create_summary_plots(stats_df)\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "def create_summary_plots(stats_df):\n",
    "    \"\"\"Create summary visualization plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Correlation comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    x_pos = np.arange(len(stats_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, stats_df['Pearson_r'], width, \n",
    "                   label='Pearson r', alpha=0.8, color='skyblue')\n",
    "    bars2 = ax1.bar(x_pos + width/2, stats_df['Spearman_r'], width, \n",
    "                   label='Spearman r', alpha=0.8, color='lightcoral')\n",
    "    \n",
    "    ax1.set_xlabel('Dataset', fontweight='bold')\n",
    "    ax1.set_ylabel('Correlation Coefficient', fontweight='bold')\n",
    "    ax1.set_title('📊 Correlation: SpaOTsc vs FGW', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(stats_df['Dataset'])\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Error metrics\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2_twin = ax2.twinx()\n",
    "    \n",
    "    line1 = ax2.plot(stats_df['Dataset'], stats_df['MSE'], 'ro-', \n",
    "                    linewidth=3, markersize=8, label='MSE')\n",
    "    line2 = ax2_twin.plot(stats_df['Dataset'], stats_df['MAE'], 'bo-', \n",
    "                         linewidth=3, markersize=8, label='MAE')\n",
    "    \n",
    "    ax2.set_xlabel('Dataset', fontweight='bold')\n",
    "    ax2.set_ylabel('MSE', color='red', fontweight='bold')\n",
    "    ax2_twin.set_ylabel('MAE', color='blue', fontweight='bold')\n",
    "    ax2.set_title('📉 Error Metrics', fontweight='bold', fontsize=12)\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2_twin.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Mean values comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(stats_df['Dataset'], stats_df['SpaOTsc_mean'], 'o-', \n",
    "            linewidth=3, markersize=8, label='SpaOTsc', color='green')\n",
    "    ax3.plot(stats_df['Dataset'], stats_df['FGW_mean'], 's-', \n",
    "            linewidth=3, markersize=8, label='FGW', color='orange')\n",
    "    \n",
    "    ax3.set_xlabel('Dataset', fontweight='bold')\n",
    "    ax3.set_ylabel('Mean Distance Value', fontweight='bold')\n",
    "    ax3.set_title('📊 Mean Distance Values', fontweight='bold', fontsize=12)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Standard deviation comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(stats_df['Dataset'], stats_df['SpaOTsc_std'], 'o-', \n",
    "            linewidth=3, markersize=8, label='SpaOTsc', color='purple')\n",
    "    ax4.plot(stats_df['Dataset'], stats_df['FGW_std'], 's-', \n",
    "            linewidth=3, markersize=8, label='FGW', color='brown')\n",
    "    \n",
    "    ax4.set_xlabel('Dataset', fontweight='bold')\n",
    "    ax4.set_ylabel('Standard Deviation', fontweight='bold')\n",
    "    ax4.set_title('📊 Standard Deviation', fontweight='bold', fontsize=12)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('d_induced_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def quick_matrix_check():\n",
    "    \"\"\"Quick check of matrix properties\"\"\"\n",
    "    \n",
    "    print(\"\\n🔧 MATRIX PROPERTIES CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i in range(3):\n",
    "        print(f\"\\nDataset {i+1}:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        D_spa = D_induced_spaotsc_all[i]\n",
    "        D_fgw = D_induced_all[i]\n",
    "        \n",
    "        # Check basic properties\n",
    "        spa_symmetric = np.allclose(D_spa, D_spa.T, rtol=1e-5)\n",
    "        fgw_symmetric = np.allclose(D_fgw, D_fgw.T, rtol=1e-5)\n",
    "        \n",
    "        spa_diag_zero = np.allclose(np.diag(D_spa), 0, atol=1e-5)\n",
    "        fgw_diag_zero = np.allclose(np.diag(D_fgw), 0, atol=1e-5)\n",
    "        \n",
    "        print(f\"SpaOTsc: Symmetric={spa_symmetric}, Diag=0={spa_diag_zero}\")\n",
    "        print(f\"FGW:     Symmetric={fgw_symmetric}, Diag=0={fgw_diag_zero}\")\n",
    "        print(f\"SpaOTsc range: [{D_spa.min():.4f}, {D_spa.max():.4f}]\")\n",
    "        print(f\"FGW range:     [{D_fgw.min():.4f}, {D_fgw.max():.4f}]\")\n",
    "\n",
    "# 🚀 MAIN EXECUTION\n",
    "print(\"🎯 D_INDUCED COMPARISON TOOL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"This will compare SpaOTsc vs FGW D_induced matrices\")\n",
    "print(\"across your 3 datasets with comprehensive visualizations.\")\n",
    "print(\"\\n📋 Usage:\")\n",
    "print(\"1. stats_df = compare_methods()\")\n",
    "print(\"2. quick_matrix_check()\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Uncomment these lines to run automatically:\n",
    "stats_df = compare_methods()\n",
    "quick_matrix_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def test_which_d_induced_is_better():\n",
    "    \"\"\"\n",
    "    Simple test to see which D_induced method gives better spatial relationships\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🧪 TESTING: Which D_induced is better?\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # We'll test each dataset\n",
    "    for i in range(3):\n",
    "        print(f\"\\n📊 DATASET {i+1}\")\n",
    "        print(\"-\"*30)\n",
    "        \n",
    "        # Get the matrices\n",
    "        D_spaotsc = D_induced_spaotsc_all[i]\n",
    "        D_fgw = D_induced_all[i]\n",
    "        \n",
    "        print(f\"SpaOTsc shape: {D_spaotsc.shape}\")\n",
    "        print(f\"FGW shape: {D_fgw.shape}\")\n",
    "        \n",
    "        # Simple tests\n",
    "        test_1_range_and_scale(D_spaotsc, D_fgw, i+1)\n",
    "        test_2_local_structure(D_spaotsc, D_fgw, i+1)\n",
    "        test_3_visual_comparison(D_spaotsc, D_fgw, i+1)\n",
    "\n",
    "def test_1_range_and_scale(D_spa, D_fgw, dataset_num):\n",
    "    \"\"\"Test 1: Check if the distance ranges make sense\"\"\"\n",
    "    \n",
    "    print(f\"\\n✅ Test 1: Distance Range & Scale\")\n",
    "    \n",
    "    # Basic stats\n",
    "    spa_min, spa_max = D_spa.min(), D_spa.max()\n",
    "    spa_mean, spa_std = D_spa.mean(), D_spa.std()\n",
    "    \n",
    "    fgw_min, fgw_max = D_fgw.min(), D_fgw.max()\n",
    "    fgw_mean, fgw_std = D_fgw.mean(), D_fgw.std()\n",
    "    \n",
    "    print(f\"SpaOTsc: [{spa_min:.3f}, {spa_max:.3f}], mean={spa_mean:.3f}, std={spa_std:.3f}\")\n",
    "    print(f\"FGW:     [{fgw_min:.3f}, {fgw_max:.3f}], mean={fgw_mean:.3f}, std={fgw_std:.3f}\")\n",
    "    \n",
    "    # Check if diagonal is zero (distance from cell to itself should be 0)\n",
    "    spa_diag_ok = np.allclose(np.diag(D_spa), 0, atol=1e-5)\n",
    "    fgw_diag_ok = np.allclose(np.diag(D_fgw), 0, atol=1e-5)\n",
    "    \n",
    "    print(f\"Diagonal = 0? SpaOTsc: {spa_diag_ok}, FGW: {fgw_diag_ok}\")\n",
    "    \n",
    "    # Check if symmetric (distance i->j should equal j->i)\n",
    "    spa_sym_ok = np.allclose(D_spa, D_spa.T, rtol=1e-5)\n",
    "    fgw_sym_ok = np.allclose(D_fgw, D_fgw.T, rtol=1e-5)\n",
    "    \n",
    "    print(f\"Symmetric? SpaOTsc: {spa_sym_ok}, FGW: {fgw_sym_ok}\")\n",
    "\n",
    "def test_2_local_structure(D_spa, D_fgw, dataset_num):\n",
    "    \"\"\"Test 2: Check if local neighborhoods are preserved\"\"\"\n",
    "    \n",
    "    print(f\"\\n✅ Test 2: Local Neighborhood Structure\")\n",
    "    \n",
    "    # For each cell, find its 5 nearest neighbors in both matrices\n",
    "    n_neighbors = 5\n",
    "    n_cells = min(D_spa.shape[0], 100)  # Test on first 100 cells for speed\n",
    "    \n",
    "    spa_consistency = []\n",
    "    fgw_consistency = []\n",
    "    \n",
    "    for cell_i in range(n_cells):\n",
    "        # Get nearest neighbors in both matrices\n",
    "        spa_neighbors = np.argsort(D_spa[cell_i, :])[:n_neighbors+1][1:]  # +1 to exclude self\n",
    "        fgw_neighbors = np.argsort(D_fgw[cell_i, :])[:n_neighbors+1][1:]\n",
    "        \n",
    "        # Check how many neighbors overlap\n",
    "        overlap = len(set(spa_neighbors) & set(fgw_neighbors))\n",
    "        spa_consistency.append(overlap / n_neighbors)\n",
    "        \n",
    "        # Check if distances to neighbors are well-ordered\n",
    "        spa_dists = D_spa[cell_i, spa_neighbors]\n",
    "        fgw_dists = D_fgw[cell_i, fgw_neighbors]\n",
    "        \n",
    "        # Good if distances increase monotonically\n",
    "        spa_ordered = np.all(np.diff(spa_dists) >= 0)\n",
    "        fgw_ordered = np.all(np.diff(fgw_dists) >= 0)\n",
    "        \n",
    "    avg_consistency = np.mean(spa_consistency)\n",
    "    print(f\"Neighbor overlap: {avg_consistency:.3f} (higher = more consistent)\")\n",
    "\n",
    "def test_3_visual_comparison(D_spa, D_fgw, dataset_num):\n",
    "    \"\"\"Test 3: Visual comparison\"\"\"\n",
    "    \n",
    "    print(f\"\\n✅ Test 3: Visual Comparison\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Subsample for visualization if matrix is large\n",
    "    max_size = 200\n",
    "    if D_spa.shape[0] > max_size:\n",
    "        idx = np.random.choice(D_spa.shape[0], max_size, replace=False)\n",
    "        D_spa_vis = D_spa[np.ix_(idx, idx)]\n",
    "        D_fgw_vis = D_fgw[np.ix_(idx, idx)]\n",
    "    else:\n",
    "        D_spa_vis = D_spa\n",
    "        D_fgw_vis = D_fgw\n",
    "    \n",
    "    # Row 1: SpaOTsc\n",
    "    # Heatmap\n",
    "    im1 = axes[0, 0].imshow(D_spa_vis, cmap='viridis')\n",
    "    axes[0, 0].set_title(f'Dataset {dataset_num}: SpaOTsc Heatmap')\n",
    "    plt.colorbar(im1, ax=axes[0, 0], shrink=0.7)\n",
    "    \n",
    "    # Distance histogram\n",
    "    axes[0, 1].hist(D_spa_vis[np.triu_indices(D_spa_vis.shape[0], k=1)], \n",
    "                   bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 1].set_title(f'SpaOTsc Distance Distribution')\n",
    "    axes[0, 1].set_xlabel('Distance')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Distance vs rank\n",
    "    spa_flat = D_spa_vis[np.triu_indices(D_spa_vis.shape[0], k=1)]\n",
    "    spa_sorted = np.sort(spa_flat)\n",
    "    axes[0, 2].plot(spa_sorted, 'b-', alpha=0.7)\n",
    "    axes[0, 2].set_title(f'SpaOTsc: Sorted Distances')\n",
    "    axes[0, 2].set_xlabel('Rank')\n",
    "    axes[0, 2].set_ylabel('Distance')\n",
    "    \n",
    "    # Row 2: FGW\n",
    "    # Heatmap\n",
    "    im2 = axes[1, 0].imshow(D_fgw_vis, cmap='viridis')\n",
    "    axes[1, 0].set_title(f'Dataset {dataset_num}: FGW Heatmap')\n",
    "    plt.colorbar(im2, ax=axes[1, 0], shrink=0.7)\n",
    "    \n",
    "    # Distance histogram\n",
    "    axes[1, 1].hist(D_fgw_vis[np.triu_indices(D_fgw_vis.shape[0], k=1)], \n",
    "                   bins=50, alpha=0.7, color='red')\n",
    "    axes[1, 1].set_title(f'FGW Distance Distribution')\n",
    "    axes[1, 1].set_xlabel('Distance')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Distance vs rank\n",
    "    fgw_flat = D_fgw_vis[np.triu_indices(D_fgw_vis.shape[0], k=1)]\n",
    "    fgw_sorted = np.sort(fgw_flat)\n",
    "    axes[1, 2].plot(fgw_sorted, 'r-', alpha=0.7)\n",
    "    axes[1, 2].set_title(f'FGW: Sorted Distances')\n",
    "    axes[1, 2].set_xlabel('Rank')\n",
    "    axes[1, 2].set_ylabel('Distance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quick correlation check\n",
    "    spa_vals = spa_flat\n",
    "    fgw_vals = fgw_flat\n",
    "    \n",
    "    if len(spa_vals) == len(fgw_vals):\n",
    "        corr, _ = pearsonr(spa_vals, fgw_vals)\n",
    "        print(f\"Correlation between methods: {corr:.3f}\")\n",
    "\n",
    "def simple_recommendation():\n",
    "    \"\"\"Give a simple recommendation based on basic checks\"\"\"\n",
    "    \n",
    "    print(f\"\\n🎯 SIMPLE RECOMMENDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    scores_spa = []\n",
    "    scores_fgw = []\n",
    "    \n",
    "    for i in range(3):\n",
    "        D_spa = D_induced_spaotsc_all[i]\n",
    "        D_fgw = D_induced_all[i]\n",
    "        \n",
    "        # Score based on simple criteria\n",
    "        spa_score = 0\n",
    "        fgw_score = 0\n",
    "        \n",
    "        # 1. Has proper diagonal (should be 0)\n",
    "        if np.allclose(np.diag(D_spa), 0, atol=1e-5):\n",
    "            spa_score += 1\n",
    "        if np.allclose(np.diag(D_fgw), 0, atol=1e-5):\n",
    "            fgw_score += 1\n",
    "            \n",
    "        # 2. Is symmetric\n",
    "        if np.allclose(D_spa, D_spa.T, rtol=1e-5):\n",
    "            spa_score += 1\n",
    "        if np.allclose(D_fgw, D_fgw.T, rtol=1e-5):\n",
    "            fgw_score += 1\n",
    "            \n",
    "        # 3. Has reasonable range (not all zeros, not too extreme)\n",
    "        spa_range = D_spa.max() - D_spa.min()\n",
    "        fgw_range = D_fgw.max() - D_fgw.min()\n",
    "        \n",
    "        if 0.1 < spa_range < 10:  # reasonable range\n",
    "            spa_score += 1\n",
    "        if 0.1 < fgw_range < 10:\n",
    "            fgw_score += 1\n",
    "            \n",
    "        # 4. Not too sparse (has enough variation)\n",
    "        spa_nonzero = np.sum(D_spa > 1e-6) / D_spa.size\n",
    "        fgw_nonzero = np.sum(D_fgw > 1e-6) / D_fgw.size\n",
    "        \n",
    "        if spa_nonzero > 0.5:  # at least 50% non-zero\n",
    "            spa_score += 1\n",
    "        if fgw_nonzero > 0.5:\n",
    "            fgw_score += 1\n",
    "        \n",
    "        scores_spa.append(spa_score)\n",
    "        scores_fgw.append(fgw_score)\n",
    "        \n",
    "        print(f\"Dataset {i+1}: SpaOTsc={spa_score}/4, FGW={fgw_score}/4\")\n",
    "    \n",
    "    total_spa = sum(scores_spa)\n",
    "    total_fgw = sum(scores_fgw)\n",
    "    \n",
    "    print(f\"\\nTotal scores: SpaOTsc={total_spa}/12, FGW={total_fgw}/12\")\n",
    "    \n",
    "    if total_spa > total_fgw:\n",
    "        print(\"🏆 Recommendation: Use SpaOTsc D_induced\")\n",
    "    elif total_fgw > total_spa:\n",
    "        print(\"🏆 Recommendation: Use FGW D_induced\")\n",
    "    else:\n",
    "        print(\"🤔 Recommendation: Both methods seem similar\")\n",
    "        \n",
    "    return total_spa, total_fgw\n",
    "\n",
    "# 🚀 RUN THE TESTS\n",
    "print(\"🧪 RUNNING SIMPLE TESTS...\")\n",
    "print(\"This will help you decide which D_induced is better!\")\n",
    "print()\n",
    "\n",
    "# Uncomment to run:\n",
    "test_which_d_induced_is_better()\n",
    "spa_score, fgw_score = simple_recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scikit-learn ≥1.2, the SMACOF core lives in sklearn.manifold._mds\n",
    "from sklearn.manifold._mds import smacof_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold.smacof import smacof_single\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import smacof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
