{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "from core_models_et_p1 import SharedEncoder, train_encoder\n",
    "from ssl_utils import set_seed\n",
    "import utils_et as uet\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-SLIDE ALIGNMENT: Mouse Liver\")\n",
    "print(\"ST training = ST1, ST2, ST3 | Inference target = ST4\")\n",
    "print(\"Same patient (mouse) — cross-slide gap only\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load data\n",
    "ST_PATHS = {\n",
    "    'liver_ST1': '/home/ehtesamul/sc_st/data/liver/stadata1.h5ad',\n",
    "    'liver_ST2': '/home/ehtesamul/sc_st/data/liver/stadata2.h5ad',\n",
    "    'liver_ST3': '/home/ehtesamul/sc_st/data/liver/stadata3.h5ad',\n",
    "}\n",
    "INF_PATHS = {\n",
    "    'liver_ST4': '/home/ehtesamul/sc_st/data/liver/stadata4.h5ad',\n",
    "}\n",
    "\n",
    "st_data = {}\n",
    "for name, path in ST_PATHS.items():\n",
    "    st_data[name] = sc.read_h5ad(path)\n",
    "    print(f\"  {name}: {st_data[name].n_obs} spots\")\n",
    "\n",
    "inf_data = {}\n",
    "for name, path in INF_PATHS.items():\n",
    "    inf_data[name] = sc.read_h5ad(path)\n",
    "    print(f\"  {name}: {inf_data[name].n_obs} spots\")\n",
    "\n",
    "# Normalize\n",
    "all_data = list(st_data.values()) + list(inf_data.values())\n",
    "for adata in all_data:\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "# Common genes\n",
    "common_genes = sorted(set.intersection(*[set(a.var_names) for a in all_data]))\n",
    "n_genes = len(common_genes)\n",
    "print(f\"\\n✓ Common genes: {n_genes}\")\n",
    "\n",
    "# Extract expression\n",
    "def extract_expr(adata, genes):\n",
    "    X = adata[:, genes].X\n",
    "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "X_st = {name: extract_expr(st_data[name], common_genes) for name in ST_PATHS}\n",
    "X_inf = {name: extract_expr(inf_data[name], common_genes) for name in INF_PATHS}\n",
    "\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    print(f\"  {name}: {X.shape}\")\n",
    "\n",
    "labels_str = list(ST_PATHS.keys()) + list(INF_PATHS.keys())\n",
    "\n",
    "colors_map = {\n",
    "    'liver_ST1': '#e74c3c',\n",
    "    'liver_ST2': '#3498db',\n",
    "    'liver_ST3': '#2ecc71',\n",
    "    'liver_ST4': '#f39c12',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ST domain (training slides with coords)\n",
    "st_expr = torch.tensor(\n",
    "    np.vstack([X_st[n] for n in ST_PATHS]),\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "st_coords_list = [st_data[n].obsm['spatial'] for n in ST_PATHS]\n",
    "st_coords_raw = torch.tensor(\n",
    "    np.vstack(st_coords_list), dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "ns = [X_st[n].shape[0] for n in ST_PATHS]\n",
    "slide_ids = torch.tensor(\n",
    "    np.concatenate([np.full(n, i, dtype=int) for i, n in enumerate(ns)]),\n",
    "    dtype=torch.long, device=device\n",
    ")\n",
    "\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"✓ ST expr: {st_expr.shape}\")\n",
    "for i, (name, n) in enumerate(zip(ST_PATHS, ns)):\n",
    "    print(f\"  - {name}: {n} spots (slide {i})\")\n",
    "print(f\"✓ ST coords: {st_coords.shape} (canonicalized per-slide)\")\n",
    "\n",
    "# Inference domain (ST4 treated as \"SC\")\n",
    "inf_expr = torch.tensor(X_inf['liver_ST4'], dtype=torch.float32, device=device)\n",
    "n_inf = X_inf['liver_ST4'].shape[0]\n",
    "inf_slide_ids = torch.zeros(n_inf, dtype=torch.long, device=device)\n",
    "inf_patient_ids = torch.zeros(n_inf, dtype=torch.long, device=device)  # same patient\n",
    "\n",
    "print(f\"\\n✓ Inference expr: {inf_expr.shape}\")\n",
    "print(f\"  - liver_ST4: {n_inf} spots (inference domain)\")\n",
    "\n",
    "# Source IDs for adversary\n",
    "st_source_ids = torch.tensor(\n",
    "    np.concatenate([np.full(n, i, dtype=int) for i, n in enumerate(ns)]),\n",
    "    dtype=torch.long\n",
    ")\n",
    "inf_source_ids = torch.full((n_inf,), len(ns), dtype=torch.long)\n",
    "\n",
    "print(f\"\\n✓ ST source IDs: {st_source_ids.unique().tolist()}, counts: {ns}\")\n",
    "print(f\"✓ Inf source IDs: {inf_source_ids.unique().tolist()}, count: {n_inf}\")\n",
    "print(f\"\\nTotal for Stage A: {st_expr.shape[0] + n_inf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# SPATIAL InfoNCE + ALL-LOSS DIAGNOSTICS\n",
    "# ===========================================================================\n",
    "# Add after Cell 1 (data prep). Uses: st_expr, inf_expr, st_coords,\n",
    "# slide_ids, n_genes, device, SEED, X_ssl is built below.\n",
    "# ===========================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core_models_et_p1 import SharedEncoder\n",
    "from ssl_utils import (\n",
    "    set_seed, vicreg_loss, compute_local_alignment_loss,\n",
    "    coral_loss, mmd_rbf_loss,\n",
    "    precompute_spatial_nce_structures, compute_spatial_infonce_loss,\n",
    ")\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---- build concat pool + domain labels ----\n",
    "n_st = st_expr.shape[0]\n",
    "n_sc = inf_expr.shape[0]\n",
    "X_ssl = torch.cat([st_expr, inf_expr], dim=0)\n",
    "domain_ids = torch.cat([\n",
    "    torch.zeros(n_st, dtype=torch.long, device=device),\n",
    "    torch.ones(n_sc, dtype=torch.long, device=device),\n",
    "])\n",
    "\n",
    "# ---- precompute spatial NCE structures ----\n",
    "nce_data = precompute_spatial_nce_structures(\n",
    "    st_coords=st_coords, st_gene_expr=st_expr, slide_ids=slide_ids,\n",
    "    k_phys=20, far_mult=4.0, n_hard=20, device=device,\n",
    ")\n",
    "pos_idx  = nce_data['pos_idx']\n",
    "far_mask = nce_data['far_mask']\n",
    "hard_neg = nce_data['hard_neg']\n",
    "r_pos    = nce_data['r_pos']\n",
    "\n",
    "# ---- tiny helpers ----\n",
    "def augment_expr(X, drop=0.2, gauss=0.01, jitter=0.1):\n",
    "    m = torch.bernoulli(torch.full_like(X, 1.0 - drop))\n",
    "    X = X * m + torch.randn_like(X) * gauss\n",
    "    s = 1.0 + (torch.rand(X.shape[0], 1, device=X.device) * 2 - 1) * jitter\n",
    "    return X * s\n",
    "\n",
    "def enc_grad_norm(enc):\n",
    "    return sum(p.grad.data.norm(2).item() ** 2\n",
    "               for p in enc.parameters() if p.grad is not None) ** 0.5\n",
    "\n",
    "def balanced_batch(n_st, n_sc, bs, device):\n",
    "    h = bs // 2\n",
    "    i_st = torch.randperm(n_st, device=device)[:h]\n",
    "    i_sc = torch.randperm(n_sc, device=device)[:bs - h] + n_st\n",
    "    idx = torch.cat([i_st, i_sc])\n",
    "    return idx[torch.randperm(idx.shape[0], device=device)]\n",
    "\n",
    "vicreg_fn = vicreg_loss(\n",
    "    lambda_inv=25.0, lambda_var=50.0, lambda_cov=1.0,\n",
    "    gamma=1.0, eps=1e-4, use_projector=False, float32_stats=True,\n",
    ")\n",
    "\n",
    "# ======================================================================\n",
    "# CHECK C — Index-set sanity\n",
    "# ======================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"CHECK C: Index-Set Sanity (positives / hard-negs / overlap)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "coords_np = st_coords.cpu().numpy()\n",
    "rng = np.random.default_rng(SEED)\n",
    "anchors = rng.choice(n_st, size=min(300, n_st), replace=False)\n",
    "\n",
    "pos_d, hard_d, overlaps = [], [], []\n",
    "for i in anchors:\n",
    "    pg = pos_idx[i]; pg = pg[pg >= 0].cpu().numpy()\n",
    "    hg = hard_neg[i]; hg = hg[hg >= 0].cpu().numpy()\n",
    "    if len(pg): pos_d.extend(np.linalg.norm(coords_np[pg] - coords_np[i], axis=1))\n",
    "    if len(hg): hard_d.extend(np.linalg.norm(coords_np[hg] - coords_np[i], axis=1))\n",
    "    overlaps.append(len(set(pg) & set(hg)))\n",
    "\n",
    "pos_d, hard_d = np.asarray(pos_d), np.asarray(hard_d)\n",
    "r_far = [r * 4.0 for r in r_pos]\n",
    "\n",
    "print(f\"\\n  Sampled {len(anchors)} anchors\")\n",
    "print(f\"  POSITIVES  — n={len(pos_d):,}  median_dist={np.median(pos_d):.4f}  \"\n",
    "      f\"max={np.max(pos_d):.4f}\")\n",
    "print(f\"  HARD_NEG   — n={len(hard_d):,}  median_dist={np.median(hard_d):.4f}  \"\n",
    "      f\"min={np.min(hard_d):.4f}\")\n",
    "print(f\"  r_pos/slide: {[f'{r:.4f}' for r in r_pos]}\")\n",
    "print(f\"  r_far/slide: {[f'{r:.4f}' for r in r_far]}\")\n",
    "leak = (hard_d < min(r_far)).sum() if len(hard_d) else 0\n",
    "print(f\"  Hard negs below r_far (LEAK): {leak}/{len(hard_d)}  \"\n",
    "      f\"{'✓ OK' if leak == 0 else '⚠️ BUG'}\")\n",
    "print(f\"  Pos ∩ Neg overlap total: {sum(overlaps)}  \"\n",
    "      f\"{'✓ zero' if sum(overlaps) == 0 else '⚠️ OVERLAP!'}\")\n",
    "no_pos = (pos_idx[:, 0] < 0).sum().item()\n",
    "no_neg = (hard_neg[:, 0] < 0).sum().item()\n",
    "print(f\"  Spots w/ no positives: {no_pos}/{n_st}   no hard_neg: {no_neg}/{n_st}\")\n",
    "\n",
    "# ======================================================================\n",
    "# CHECK A — Per-loss gradient norms (single batch)\n",
    "# ======================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECK A: Per-Loss Gradient Norms (single batch, bs=256)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "set_seed(SEED)\n",
    "enc_a = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128],\n",
    "                      dropout=0.1).to(device).train()\n",
    "BS = 256\n",
    "idx = balanced_batch(n_st, n_sc, BS, device)\n",
    "X_b = X_ssl[idx]; s_b = domain_ids[idx]\n",
    "is_sc = (idx >= n_st); is_st = (s_b == 0)\n",
    "\n",
    "results = {}  # name → (loss_val, grad_norm)\n",
    "\n",
    "# --- VICReg ---\n",
    "enc_a.zero_grad()\n",
    "y1, y2 = enc_a(augment_expr(X_b)), enc_a(augment_expr(X_b))\n",
    "lv, _ = vicreg_fn(y1, y2)\n",
    "lv.backward(); results['VICReg'] = (lv.item(), enc_grad_norm(enc_a))\n",
    "\n",
    "# --- Spatial InfoNCE ---\n",
    "enc_a.zero_grad()\n",
    "z = enc_a(X_b)\n",
    "ln = compute_spatial_infonce_loss(\n",
    "    z=z, batch_idx=idx, pos_idx=pos_idx, far_mask=far_mask,\n",
    "    hard_neg=hard_neg, tau=0.1, n_rand_neg=128, is_st_mask=is_st)\n",
    "ln.backward(); results['SpatialNCE'] = (ln.item(), enc_grad_norm(enc_a))\n",
    "\n",
    "# --- Local Alignment ---\n",
    "enc_a.zero_grad()\n",
    "z = enc_a(X_b)\n",
    "zst, zsc = z[~is_sc], z[is_sc]; xst, xsc = X_b[~is_sc], X_b[is_sc]\n",
    "ll = compute_local_alignment_loss(z_sc=zsc, z_st=zst, x_sc=xsc, x_st=xst,\n",
    "                                  tau_z=0.07, bidirectional=True\n",
    "     ) if zst.shape[0] > 8 and zsc.shape[0] > 8 else torch.tensor(0., device=device)\n",
    "ll.backward(); results['LocalAlign'] = (ll.item(), enc_grad_norm(enc_a))\n",
    "\n",
    "# --- CORAL ---\n",
    "enc_a.zero_grad()\n",
    "z = enc_a(X_b); zst, zsc = z[~is_sc], z[is_sc]\n",
    "lc = coral_loss(zst, zsc) if zst.shape[0] > 4 and zsc.shape[0] > 4 \\\n",
    "     else torch.tensor(0., device=device)\n",
    "lc.backward(); results['CORAL'] = (lc.item(), enc_grad_norm(enc_a))\n",
    "\n",
    "# --- MMD ---\n",
    "enc_a.zero_grad()\n",
    "z = enc_a(X_b); zst, zsc = F.normalize(z[~is_sc], 1), F.normalize(z[is_sc], 1)\n",
    "lm, _ = mmd_rbf_loss(zst, zsc, return_sigma=True) \\\n",
    "        if zst.shape[0] > 4 and zsc.shape[0] > 4 \\\n",
    "        else (torch.tensor(0., device=device), 1.0)\n",
    "lm.backward(); results['MMD'] = (lm.item(), enc_grad_norm(enc_a))\n",
    "\n",
    "# --- Full combined (weights = your training config) ---\n",
    "enc_a.zero_grad()\n",
    "y1, y2 = enc_a(augment_expr(X_b)), enc_a(augment_expr(X_b))\n",
    "lv_f, _ = vicreg_fn(y1, y2)\n",
    "z_f = enc_a(X_b)\n",
    "ln_f = compute_spatial_infonce_loss(z=z_f, batch_idx=idx, pos_idx=pos_idx,\n",
    "    far_mask=far_mask, hard_neg=hard_neg, tau=0.1, n_rand_neg=128, is_st_mask=is_st)\n",
    "zst_f, zsc_f = z_f[~is_sc], z_f[is_sc]\n",
    "xst_f, xsc_f = X_b[~is_sc], X_b[is_sc]\n",
    "ll_f = compute_local_alignment_loss(z_sc=zsc_f, z_st=zst_f, x_sc=xsc_f,\n",
    "    x_st=xst_f, tau_z=0.07, bidirectional=True\n",
    ") if zst_f.shape[0] > 8 and zsc_f.shape[0] > 8 else torch.tensor(0., device=device)\n",
    "lc_f = coral_loss(zst_f, zsc_f) if zst_f.shape[0] > 4 and zsc_f.shape[0] > 4 \\\n",
    "       else torch.tensor(0., device=device)\n",
    "zst_n, zsc_n = F.normalize(zst_f, 1), F.normalize(zsc_f, 1)\n",
    "lm_f, _ = mmd_rbf_loss(zst_n, zsc_n, return_sigma=True)\n",
    "L = lv_f + 3.0*ln_f + 3.0*ll_f + lc_f + 30.0*lm_f   # no adv here\n",
    "L.backward(); results['FULL'] = (L.item(), enc_grad_norm(enc_a))\n",
    "\n",
    "print(f\"\\n  {'Loss':<16s} {'value':>10s} {'‖∇θ‖':>12s} {'% of Full':>10s}\")\n",
    "print(f\"  {'-'*16} {'-'*10} {'-'*12} {'-'*10}\")\n",
    "gn_full = results['FULL'][1]\n",
    "for name in ['VICReg','SpatialNCE','LocalAlign','CORAL','MMD','FULL']:\n",
    "    val, gn = results[name]\n",
    "    pct = 100*gn/max(gn_full,1e-10)\n",
    "    flag = \" ← NO-OP!\" if gn < 1e-8 and name != 'FULL' else \"\"\n",
    "    print(f\"  {name:<16s} {val:>10.4f} {gn:>12.6f} {pct:>9.1f}%{flag}\")\n",
    "\n",
    "gn_nce = results['SpatialNCE'][1]\n",
    "if gn_nce < 1e-8:\n",
    "    print(\"\\n  ⚠️  SpatialNCE grad is ZERO — loss is disconnected!\")\n",
    "else:\n",
    "    print(f\"\\n  ✓ SpatialNCE grad live (ratio vs VICReg: \"\n",
    "          f\"{gn_nce/max(results['VICReg'][1],1e-10):.3f})\")\n",
    "\n",
    "# ======================================================================\n",
    "# CHECK B — Loss scale + similarity gap (200 steps)\n",
    "# ======================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHECK B: Loss Scale + Similarity Gap (200 training steps)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "set_seed(SEED)\n",
    "enc_b = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128],\n",
    "                      dropout=0.1).to(device).train()\n",
    "opt_b = torch.optim.Adam(enc_b.parameters(), lr=1e-4)\n",
    "\n",
    "N_STEPS = 200\n",
    "log = defaultdict(list)\n",
    "\n",
    "for step in range(N_STEPS):\n",
    "    idx = balanced_batch(n_st, n_sc, 256, device)\n",
    "    X_b = X_ssl[idx]; s_b = domain_ids[idx]\n",
    "    is_sc_b = (idx >= n_st); is_st_b = (s_b == 0)\n",
    "\n",
    "    # VICReg\n",
    "    y1, y2 = enc_b(augment_expr(X_b)), enc_b(augment_expr(X_b))\n",
    "    l_vic, sv = vicreg_fn(y1, y2)\n",
    "\n",
    "    # Clean embeddings\n",
    "    z = enc_b(X_b)\n",
    "    z_n = F.normalize(z, dim=1)\n",
    "\n",
    "    # --- Spatial NCE with per-anchor similarity logging ---\n",
    "    # one-time global→local map\n",
    "    max_g = max(idx.max().item(), int(pos_idx.max().item()),\n",
    "                int(hard_neg.max().item())) + 1\n",
    "    g2l = torch.full((max_g,), -1, dtype=torch.long, device=device)\n",
    "    g2l[idx] = torch.arange(len(idx), device=device)\n",
    "\n",
    "    sp_list, sh_list, sr_list = [], [], []\n",
    "    st_locals = torch.where(is_st_b)[0]\n",
    "\n",
    "    for li in st_locals:\n",
    "        gi = idx[li].item()\n",
    "        za = z_n[li]\n",
    "        pg = pos_idx[gi]; pg = pg[pg >= 0]\n",
    "        if pg.numel() == 0: continue\n",
    "        pl = g2l[pg]; pl = pl[pl >= 0]\n",
    "        if pl.numel() == 0: continue\n",
    "\n",
    "        hg = hard_neg[gi]; hg = hg[hg >= 0]\n",
    "        hl = g2l[hg.clamp(max=max_g-1)]; hl = hl[hl >= 0]\n",
    "\n",
    "        fb = far_mask[gi][idx]; fb[li] = False\n",
    "        fc = torch.where(fb)[0]\n",
    "        rn = fc[torch.randperm(fc.numel(), device=device)[:128]] \\\n",
    "             if fc.numel() > 128 else fc\n",
    "\n",
    "        sp_list.append((za @ z_n[pl].T).mean().item())\n",
    "        if hl.numel(): sh_list.append((za @ z_n[hl].T).mean().item())\n",
    "        if rn.numel(): sr_list.append((za @ z_n[rn].T).mean().item())\n",
    "\n",
    "    l_nce = compute_spatial_infonce_loss(\n",
    "        z=z, batch_idx=idx, pos_idx=pos_idx, far_mask=far_mask,\n",
    "        hard_neg=hard_neg, tau=0.1, n_rand_neg=128, is_st_mask=is_st_b)\n",
    "\n",
    "    # Other losses\n",
    "    zst, zsc = z[~is_sc_b], z[is_sc_b]\n",
    "    xst, xsc = X_b[~is_sc_b], X_b[is_sc_b]\n",
    "    ok = zst.shape[0] > 8 and zsc.shape[0] > 8\n",
    "    l_la  = compute_local_alignment_loss(z_sc=zsc, z_st=zst, x_sc=xsc,\n",
    "                x_st=xst, tau_z=0.07, bidirectional=True) if ok \\\n",
    "            else torch.tensor(0., device=device)\n",
    "    l_cor = coral_loss(zst, zsc) if ok else torch.tensor(0., device=device)\n",
    "    zst_n, zsc_n = F.normalize(zst, 1), F.normalize(zsc, 1)\n",
    "    l_mmd, sig = mmd_rbf_loss(zst_n, zsc_n, return_sigma=True) if ok \\\n",
    "                 else (torch.tensor(0., device=device), 1.0)\n",
    "\n",
    "    L = l_vic + 3.0*l_nce + 3.0*l_la + l_cor + 30.0*l_mmd\n",
    "    opt_b.zero_grad(); L.backward(); opt_b.step()\n",
    "\n",
    "    # log\n",
    "    log['loss_total'].append(L.item())\n",
    "    log['loss_vic'].append(l_vic.item())\n",
    "    log['loss_nce'].append(l_nce.item())\n",
    "    log['loss_la'].append(l_la.item())\n",
    "    log['loss_coral'].append(l_cor.item())\n",
    "    log['loss_mmd'].append(l_mmd.item() if torch.is_tensor(l_mmd) else l_mmd)\n",
    "    log['inv'].append(sv['inv']); log['var'].append(sv['var'])\n",
    "    log['cov'].append(sv['cov']); log['std_min'].append(sv['std_min'])\n",
    "    sp = np.mean(sp_list) if sp_list else float('nan')\n",
    "    sh = np.mean(sh_list) if sh_list else float('nan')\n",
    "    sr = np.mean(sr_list) if sr_list else float('nan')\n",
    "    log['sim_pos'].append(sp); log['sim_hard'].append(sh)\n",
    "    log['sim_rand'].append(sr); log['n_anchors'].append(len(sp_list))\n",
    "\n",
    "    if step % 50 == 0 or step < 3:\n",
    "        gap = sp - sh if not np.isnan(sh) else float('nan')\n",
    "        print(f\"  {step:>3d} | tot={L.item():.3f} vic={l_vic.item():.3f} \"\n",
    "              f\"nce={l_nce.item():.4f} la={l_la.item():.4f} \"\n",
    "              f\"cor={l_cor.item():.5f} mmd={l_mmd.item():.5f} | \"\n",
    "              f\"sim p/h/r={sp:+.4f}/{sh:+.4f}/{sr:+.4f} gap={gap:+.4f} \"\n",
    "              f\"| anchors={len(sp_list)}\")\n",
    "\n",
    "# summary\n",
    "print(f\"\\n  --- 200-step summary ---\")\n",
    "for k in ['loss_nce','sim_pos','sim_hard','sim_rand']:\n",
    "    v = log[k]\n",
    "    print(f\"  {k:<12s}  start={v[0]:.4f}  end={v[-1]:.4f}  \"\n",
    "          f\"Δ={v[-1]-v[0]:+.4f}\")\n",
    "gap0 = log['sim_pos'][0] - log['sim_hard'][0]\n",
    "gapN = log['sim_pos'][-1] - log['sim_hard'][-1]\n",
    "print(f\"  gap(p-h)    start={gap0:+.4f}  end={gapN:+.4f}  Δ={gapN-gap0:+.4f}\")\n",
    "print(f\"  avg anchors/batch: {np.mean(log['n_anchors']):.1f}\")\n",
    "\n",
    "if np.mean(log['n_anchors']) < 5:\n",
    "    print(\"  ⚠️  Very few in-batch anchors — positives rarely in same batch\")\n",
    "if abs(log['loss_nce'][-1] - log['loss_nce'][0]) < 0.01:\n",
    "    print(\"  ⚠️  NCE loss barely moved — check gradient flow\")\n",
    "if gapN > gap0 + 0.02:\n",
    "    print(\"  ✓ Gap is GROWING — NCE is learning spatial locality\")\n",
    "else:\n",
    "    print(\"  ⚠️  Gap not growing — NCE may be dominated by other losses\")\n",
    "\n",
    "# ======================================================================\n",
    "# PLOTS\n",
    "# ======================================================================\n",
    "steps = list(range(N_STEPS))\n",
    "fig, axes = plt.subplots(2, 4, figsize=(22, 9))\n",
    "fig.suptitle('Spatial InfoNCE + All-Loss Diagnostics (200 steps)',\n",
    "             fontweight='bold', fontsize=14)\n",
    "\n",
    "axes[0,0].plot(steps, log['loss_total'], lw=2)\n",
    "axes[0,0].set_title('Total Loss'); axes[0,0].grid(alpha=.3)\n",
    "\n",
    "axes[0,1].plot(steps, log['loss_vic'], c='tab:blue', lw=1.5)\n",
    "axes[0,1].set_title('VICReg'); axes[0,1].grid(alpha=.3)\n",
    "\n",
    "axes[0,2].plot(steps, log['loss_nce'], c='tab:red', lw=2)\n",
    "axes[0,2].set_title('Spatial InfoNCE'); axes[0,2].grid(alpha=.3)\n",
    "\n",
    "axes[0,3].plot(steps, log['loss_la'], label='LocalAlign', c='tab:green')\n",
    "axes[0,3].plot(steps, log['loss_coral'], label='CORAL', c='tab:orange')\n",
    "axes[0,3].plot(steps, log['loss_mmd'], label='MMD', c='tab:purple')\n",
    "axes[0,3].set_title('Alignment Losses'); axes[0,3].legend(fontsize=7)\n",
    "axes[0,3].grid(alpha=.3)\n",
    "\n",
    "axes[1,0].plot(steps, log['sim_pos'], c='green', lw=2, label='pos')\n",
    "axes[1,0].plot(steps, log['sim_hard'], c='red', lw=2, label='hard_neg')\n",
    "axes[1,0].plot(steps, log['sim_rand'], c='gray', lw=1.5, ls='--', label='rand_neg')\n",
    "axes[1,0].set_title('Similarity (KEY)'); axes[1,0].legend(fontsize=7)\n",
    "axes[1,0].set_ylabel('cosine sim'); axes[1,0].grid(alpha=.3)\n",
    "\n",
    "gap_curve = [p-h for p,h in zip(log['sim_pos'], log['sim_hard'])]\n",
    "axes[1,1].plot(steps, gap_curve, c='darkblue', lw=2)\n",
    "axes[1,1].axhline(0, c='red', ls='--', alpha=.5)\n",
    "axes[1,1].set_title('Gap: sim(pos)−sim(hard)'); axes[1,1].grid(alpha=.3)\n",
    "\n",
    "axes[1,2].plot(steps, log['inv'], label='inv')\n",
    "axes[1,2].plot(steps, log['var'], label='var')\n",
    "axes[1,2].plot(steps, log['cov'], label='cov')\n",
    "axes[1,2].set_title('VICReg Components'); axes[1,2].legend(fontsize=7)\n",
    "axes[1,2].grid(alpha=.3)\n",
    "\n",
    "axes[1,3].plot(steps, log['std_min'], c='purple', lw=2)\n",
    "axes[1,3].axhline(0.1, c='red', ls='--', alpha=.5, label='collapse')\n",
    "axes[1,3].set_title('std_min (collapse?)'); axes[1,3].legend(fontsize=7)\n",
    "axes[1,3].grid(alpha=.3)\n",
    "\n",
    "for ax in axes.flat: ax.set_xlabel('Step', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/ehtesamul/sc_st/model/gems_liver_crossslide/nce_diagnostics.png',\n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "# print(\"Saved: gems_liver_crossslide/nce_diagnostics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING ENCODER — CROSS-SLIDE (SAME PATIENT)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "set_seed(SEED)\n",
    "encoder = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n",
    "\n",
    "outdir = '/home/ehtesamul/sc_st/model/gems_liver_crossslide'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "encoder, projector, discriminator, hist = train_encoder(\n",
    "    inference_dropout_prob=0.5,\n",
    "    model=encoder,\n",
    "    st_gene_expr=st_expr,\n",
    "    st_coords=st_coords,\n",
    "    sc_gene_expr=inf_expr,\n",
    "    slide_ids=slide_ids,\n",
    "    sc_slide_ids=inf_slide_ids,\n",
    "    sc_patient_ids=inf_patient_ids,\n",
    "    n_epochs=2000,\n",
    "    batch_size=256,\n",
    "    lr=1e-4,\n",
    "    device=device,\n",
    "    outf=outdir,\n",
    "    st_source_ids=st_source_ids,\n",
    "    sc_source_ids=inf_source_ids,\n",
    "    use_source_adversary=False,\n",
    "    source_coral_weight=75.0,\n",
    "    stageA_obj='vicreg_adv',\n",
    "    vicreg_lambda_inv=25.0,\n",
    "    vicreg_lambda_var=50.0,\n",
    "    vicreg_lambda_cov=1.0,\n",
    "    vicreg_gamma=1.0,\n",
    "    vicreg_eps=1e-4,\n",
    "    vicreg_project_dim=256,\n",
    "    vicreg_use_projector=False,\n",
    "    vicreg_float32_stats=True,\n",
    "    vicreg_ddp_gather=False,\n",
    "    aug_gene_dropout=0.25,\n",
    "    aug_gauss_std=0.01,\n",
    "    aug_scale_jitter=0.1,\n",
    "    adv_slide_weight=75.0,\n",
    "    patient_coral_weight=0.0,\n",
    "    mmd_weight=30.0,\n",
    "    mmd_use_l2norm=True,\n",
    "    mmd_ramp=True,\n",
    "    adv_warmup_epochs=50,\n",
    "    adv_ramp_epochs=200,\n",
    "    grl_alpha_max=1.0,\n",
    "    disc_hidden=512,\n",
    "    disc_dropout=0.1,\n",
    "    stageA_balanced_slides=True,\n",
    "    adv_representation_mode='clean',\n",
    "    adv_use_layernorm=False,\n",
    "    adv_log_diagnostics=True,\n",
    "    adv_log_grad_norms=False,\n",
    "    use_local_align=True,\n",
    "    return_aux=True,\n",
    "    local_align_bidirectional=True,\n",
    "    local_align_weight=0.0,\n",
    "    local_align_tau_z=0.07,\n",
    "    seed=SEED,\n",
    "    use_best_checkpoint=True,\n",
    "    coral_raw_weight=2.0,\n",
    "    knn_weight=0.0,              # OFF — this preserves expression neighborhoods (bad for liver)\n",
    "    spatial_nce_weight=3.0,       # ON — enforces physical neighborhoods\n",
    "    spatial_nce_k_phys=20,\n",
    "    spatial_nce_far_mult=4.0,\n",
    "    spatial_nce_n_hard=20,\n",
    "    spatial_nce_tau=0.1,\n",
    "    spatial_nce_n_rand_neg=128,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n",
    "torch.save(encoder.state_dict(), f'{outdir}/encoder_final_trained.pt')\n",
    "print(f\"✓ Saved to: {outdir}/encoder_final_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPUTING EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    Z_st = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "            for name, X in X_st.items()}\n",
    "    Z_inf = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "             for name, X in X_inf.items()}\n",
    "\n",
    "for name, Z in {**Z_st, **Z_inf}.items():\n",
    "    print(f\"  {name}: {Z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained encoder\n",
    "encoder = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n",
    "encoder.load_state_dict(torch.load(\n",
    "    '/home/ehtesamul/sc_st/model/gems_liver_crossslide/encoder_final_trained.pt',\n",
    "    map_location=device\n",
    "))\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Compute embeddings\n",
    "with torch.no_grad():\n",
    "    Z_st = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "            for name, X in X_st.items()}\n",
    "    Z_inf = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "             for name, X in X_inf.items()}\n",
    "\n",
    "for name, Z in {**Z_st, **Z_inf}.items():\n",
    "    print(f\"  {name}: {Z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis A: Embedding kNN vs Physical kNN ===\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS A: Embedding kNN vs Physical Neighborhoods (ST only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in ST_PATHS:\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n = Z.shape[0]\n",
    "\n",
    "    tree_phys = cKDTree(coords)\n",
    "    tree_emb = cKDTree(Z)\n",
    "\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    for k in [10, 20, 50]:\n",
    "        # physical distances of embedding kNN\n",
    "        _, emb_idx = tree_emb.query(Z, k=k + 1)\n",
    "        emb_idx = emb_idx[:, 1:]  # exclude self\n",
    "        emb_phys_dists = np.array([\n",
    "            np.median(np.linalg.norm(coords[emb_idx[i]] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "\n",
    "        # physical distances of true physical kNN\n",
    "        _, phys_idx = tree_phys.query(coords, k=k + 1)\n",
    "        phys_idx = phys_idx[:, 1:]\n",
    "        phys_phys_dists = np.array([\n",
    "            np.median(np.linalg.norm(coords[phys_idx[i]] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "\n",
    "        # random baseline\n",
    "        rand_dists = np.array([\n",
    "            np.median(np.linalg.norm(coords[rng.choice(n, k, replace=False)] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "\n",
    "        print(f\"\\n  {name} | k={k}\")\n",
    "        print(f\"    Physical kNN median dist:  {np.median(phys_phys_dists):.2f}\")\n",
    "        print(f\"    Embedding kNN median dist: {np.median(emb_phys_dists):.2f}\")\n",
    "        print(f\"    Random median dist:        {np.median(rand_dists):.2f}\")\n",
    "\n",
    "    # Histogram for k=20\n",
    "    _, emb_idx = tree_emb.query(Z, k=21)\n",
    "    emb_idx = emb_idx[:, 1:]\n",
    "    _, phys_idx = tree_phys.query(coords, k=21)\n",
    "    phys_idx = phys_idx[:, 1:]\n",
    "\n",
    "    emb_d = [np.median(np.linalg.norm(coords[emb_idx[i]] - coords[i], axis=1)) for i in range(n)]\n",
    "    phys_d = [np.median(np.linalg.norm(coords[phys_idx[i]] - coords[i], axis=1)) for i in range(n)]\n",
    "    rand_d = [np.median(np.linalg.norm(coords[rng.choice(n, 20, replace=False)] - coords[i], axis=1)) for i in range(n)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.hist(phys_d, bins=50, alpha=0.5, label='Physical kNN', density=True)\n",
    "    ax.hist(emb_d, bins=50, alpha=0.5, label='Embedding kNN', density=True)\n",
    "    ax.hist(rand_d, bins=50, alpha=0.5, label='Random', density=True)\n",
    "    ax.set_xlabel('Median physical distance to k=20 neighbors')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{name}: kNN Physical Distance Distributions')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis B: Moran's I on Embedding Dimensions (no libpysal) ===\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS B: Spatial Autocorrelation of Embeddings (Moran's I)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def morans_i_fast(x, idx, n):\n",
    "    \"\"\"Moran's I with precomputed kNN indices.\"\"\"\n",
    "    xm = x - x.mean()\n",
    "    denom = np.sum(xm ** 2)\n",
    "    if denom < 1e-12:\n",
    "        return 0.0, 1.0\n",
    "    k = idx.shape[1]\n",
    "    W = n * k\n",
    "    num = np.sum(xm[:, None] * xm[idx]).sum() if False else sum(np.sum(xm[i] * xm[idx[i]]) for i in range(n))\n",
    "    # vectorized version:\n",
    "    num = np.einsum('i,ij->', xm, xm[idx])\n",
    "    I = (n / W) * (num / denom)\n",
    "    EI = -1.0 / (n - 1)\n",
    "    # Variance (normality assumption, simplified)\n",
    "    VI = max(1.0 / (n - 1), 1e-10)  # conservative approx\n",
    "    z = (I - EI) / np.sqrt(VI / n)\n",
    "    p = 2 * norm.sf(abs(z))\n",
    "    return I, p\n",
    "\n",
    "for name in ST_PATHS:\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n, d = Z.shape\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    _, idx = tree.query(coords, k=11)\n",
    "    idx = idx[:, 1:]  # exclude self\n",
    "\n",
    "    morans = np.zeros(d)\n",
    "    pvals = np.zeros(d)\n",
    "    for j in range(d):\n",
    "        morans[j], pvals[j] = morans_i_fast(Z[:, j], idx, n)\n",
    "\n",
    "    _, pvals_fdr, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "    n_sig = np.sum(pvals_fdr < 0.05)\n",
    "\n",
    "    # Embedding norm\n",
    "    Z_norm = np.linalg.norm(Z, axis=1)\n",
    "    I_norm, p_norm = morans_i_fast(Z_norm, idx, n)\n",
    "\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Embedding dims: {d}\")\n",
    "    print(f\"    Spatially autocorrelated dims (FDR<0.05): {n_sig}/{d} ({100*n_sig/d:.1f}%)\")\n",
    "    print(f\"    Moran's I — mean: {morans.mean():.4f}, median: {np.median(morans):.4f}\")\n",
    "    print(f\"    Moran's I of embedding norm: {I_norm:.4f} (p={p_norm:.4e})\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.hist(morans, bins=30, edgecolor='k', alpha=0.7)\n",
    "    ax.axvline(0, color='red', linestyle='--', label='No autocorrelation')\n",
    "    ax.set_xlabel(\"Moran's I\")\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f\"{name}: Moran's I across {d} embedding dims\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis E: Distribution Checks ===\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS E: Distribution Checks (ST vs Inference)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "all_stats = {}\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    lib_size = X.sum(axis=1)\n",
    "    genes_detected = (X > 0).sum(axis=1)\n",
    "    sparsity = (X == 0).mean() * 100\n",
    "    all_stats[name] = {\n",
    "        'lib_size_median': np.median(lib_size),\n",
    "        'lib_size_std': np.std(lib_size),\n",
    "        'genes_detected_median': np.median(genes_detected),\n",
    "        'sparsity_pct': sparsity,\n",
    "    }\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Library size — median: {np.median(lib_size):.1f}, std: {np.std(lib_size):.1f}\")\n",
    "    print(f\"    Genes detected — median: {np.median(genes_detected):.0f}\")\n",
    "    print(f\"    Sparsity: {sparsity:.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Library size distributions\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    axes[0].hist(X.sum(axis=1), bins=50, alpha=0.5, label=name, density=True)\n",
    "axes[0].set_xlabel('Library size (post-norm)')\n",
    "axes[0].set_title('Library Size Distribution')\n",
    "axes[0].legend(fontsize=7)\n",
    "\n",
    "# Genes detected\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    axes[1].hist((X > 0).sum(axis=1), bins=50, alpha=0.5, label=name, density=True)\n",
    "axes[1].set_xlabel('Genes detected per spot')\n",
    "axes[1].set_title('Detected Genes Distribution')\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "# Per-gene mean expression\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    gene_means = X.mean(axis=0)\n",
    "    axes[2].hist(gene_means, bins=50, alpha=0.5, label=name, density=True)\n",
    "axes[2].set_xlabel('Per-gene mean expression')\n",
    "axes[2].set_title('Gene Mean Distribution')\n",
    "axes[2].legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis C: Spatially Variable Genes ===\n",
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS C: Spatially Variable Gene Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in list(ST_PATHS.keys())[:1]:\n",
    "    adata = st_data[name].copy()\n",
    "    adata = adata[:, common_genes]\n",
    "    coords = adata.obsm['spatial']\n",
    "    X_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    n = X_dense.shape[0]\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    _, idx = tree.query(coords, k=11)\n",
    "    idx = idx[:, 1:]\n",
    "\n",
    "    morans_vals = np.empty(X_dense.shape[1])\n",
    "    for g in range(X_dense.shape[1]):\n",
    "        x = X_dense[:, g]\n",
    "        xm = x - x.mean()\n",
    "        denom = np.sum(xm ** 2)\n",
    "        if denom < 1e-12:\n",
    "            morans_vals[g] = 0.0\n",
    "            continue\n",
    "        num = np.einsum('i,ij->', xm, xm[idx])\n",
    "        W = n * 10\n",
    "        morans_vals[g] = (n / W) * (num / denom)\n",
    "\n",
    "    svg_results = pd.DataFrame({'I': morans_vals}, index=common_genes)\n",
    "    svg_results = svg_results.sort_values('I', ascending=False)\n",
    "\n",
    "    n_sig_svg = (svg_results['I'] > 0.1).sum()\n",
    "    top300_mean_I = svg_results.head(300)['I'].mean()\n",
    "\n",
    "    detection_rate = (X_dense > 0).mean(axis=0)\n",
    "    n_ultra_sparse = (detection_rate < 0.01).sum()\n",
    "\n",
    "    print(f\"\\n  {name} ({len(common_genes)} common genes):\")\n",
    "    print(f\"    Genes with Moran's I > 0.1: {n_sig_svg}\")\n",
    "    print(f\"    Top 300 SVGs — mean Moran's I: {top300_mean_I:.4f}\")\n",
    "    print(f\"    Ultra-sparse genes (<1% detection): {n_ultra_sparse}/{len(common_genes)}\")\n",
    "    print(f\"\\n  Top 20 SVGs:\")\n",
    "    print(svg_results.head(20)[['I']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ablation 2: Embedding kNN under different transforms ===\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION 2: Embedding kNN with different similarity transforms\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "K_VALUES = [10, 20, 50]\n",
    "\n",
    "def compute_knn_phys_dist(Z_np, coords, k):\n",
    "    \"\"\"Median physical distance of embedding kNN neighbors.\"\"\"\n",
    "    tree = cKDTree(Z_np)\n",
    "    _, idx = tree.query(Z_np, k=k + 1)\n",
    "    idx = idx[:, 1:]\n",
    "    diffs = coords[idx] - coords[:, None, :]\n",
    "    dists = np.sqrt((diffs ** 2).sum(axis=2))\n",
    "    return np.median(dists, axis=1)  # per-spot median\n",
    "\n",
    "for name in list(ST_PATHS.keys())[:1]:  # run on first ST slide\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n = Z.shape[0]\n",
    "\n",
    "    # Baselines\n",
    "    tree_phys = cKDTree(coords)\n",
    "\n",
    "    transforms = {\n",
    "        'Raw embedding': Z,\n",
    "        'L2-normalized (cosine)': Z / (np.linalg.norm(Z, axis=1, keepdims=True) + 1e-8),\n",
    "        'Per-dim z-scored': (Z - Z.mean(0)) / (Z.std(0) + 1e-8),\n",
    "        'PCA(Z) → 32d': PCA(n_components=32, random_state=42).fit_transform(Z),\n",
    "        'PCA(Z) → 64d': PCA(n_components=64, random_state=42).fit_transform(Z),\n",
    "    }\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        print(f\"\\n  {name} | k={k}\")\n",
    "        print(f\"  {'Transform':<30s} {'Median phys dist':>18s}\")\n",
    "        print(f\"  {'-'*30} {'-'*18}\")\n",
    "\n",
    "        # Physical baseline\n",
    "        _, phys_idx = tree_phys.query(coords, k=k + 1)\n",
    "        phys_idx = phys_idx[:, 1:]\n",
    "        phys_d = np.median(np.sqrt(((coords[phys_idx] - coords[:, None, :]) ** 2).sum(2)), axis=1)\n",
    "        print(f\"  {'Physical kNN (best)':<30s} {np.median(phys_d):>18.2f}\")\n",
    "\n",
    "        for tname, Z_t in transforms.items():\n",
    "            emb_d = compute_knn_phys_dist(Z_t, coords, k)\n",
    "            print(f\"  {tname:<30s} {np.median(emb_d):>18.2f}\")\n",
    "\n",
    "        # Random baseline\n",
    "        rng = np.random.default_rng(42)\n",
    "        rand_d = np.array([\n",
    "            np.median(np.linalg.norm(coords[rng.choice(n, k, replace=False)] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "        print(f\"  {'Random (worst)':<30s} {np.median(rand_d):>18.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ablation 3: Identifiability — crop a single spatial region ===\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION 3: Embedding kNN within a spatial crop (single region)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in list(ST_PATHS.keys())[:1]:\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n = Z.shape[0]\n",
    "\n",
    "    # Crop: take a central region covering ~25% of spots\n",
    "    cx, cy = np.median(coords, axis=0)\n",
    "    dists_to_center = np.sqrt((coords[:, 0] - cx)**2 + (coords[:, 1] - cy)**2)\n",
    "    radius = np.quantile(dists_to_center, 0.50)  # ~25% area → 50% radius\n",
    "    mask = dists_to_center <= radius\n",
    "    n_crop = mask.sum()\n",
    "\n",
    "    coords_crop = coords[mask]\n",
    "    Z_crop = Z[mask]\n",
    "\n",
    "    print(f\"  {name}: cropped {n_crop}/{n} spots (radius={radius:.1f})\")\n",
    "\n",
    "    tree_phys = cKDTree(coords_crop)\n",
    "    tree_emb = cKDTree(Z_crop)\n",
    "\n",
    "    for k in [10, 20]:\n",
    "        if k >= n_crop:\n",
    "            continue\n",
    "\n",
    "        _, phys_idx = tree_phys.query(coords_crop, k=k + 1)\n",
    "        phys_idx = phys_idx[:, 1:]\n",
    "        phys_d = np.median(np.sqrt(((coords_crop[phys_idx] - coords_crop[:, None, :]) ** 2).sum(2)), axis=1)\n",
    "\n",
    "        _, emb_idx = tree_emb.query(Z_crop, k=k + 1)\n",
    "        emb_idx = emb_idx[:, 1:]\n",
    "        emb_d = np.median(np.sqrt(((coords_crop[emb_idx] - coords_crop[:, None, :]) ** 2).sum(2)), axis=1)\n",
    "\n",
    "        rng = np.random.default_rng(42)\n",
    "        rand_d = np.array([\n",
    "            np.median(np.linalg.norm(coords_crop[rng.choice(n_crop, k, replace=False)] - coords_crop[i], axis=1))\n",
    "            for i in range(n_crop)\n",
    "        ])\n",
    "\n",
    "        print(f\"\\n  Cropped region | k={k}\")\n",
    "        print(f\"    Physical kNN median dist:  {np.median(phys_d):.2f}\")\n",
    "        print(f\"    Embedding kNN median dist: {np.median(emb_d):.2f}\")\n",
    "        print(f\"    Random median dist:        {np.median(rand_d):.2f}\")\n",
    "        ratio = (np.median(emb_d) - np.median(phys_d)) / (np.median(rand_d) - np.median(phys_d) + 1e-8)\n",
    "        print(f\"    Emb→Phys ratio (0=perfect, 1=random): {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ablation 1: Physical vs Embedding patch construction (ST4) ===\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION 1: Physical vs Embedding patch graph quality (ST4)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "name = 'liver_ST4'\n",
    "coords = inf_data[name].obsm['spatial']\n",
    "Z = Z_inf[name].numpy()\n",
    "n = Z.shape[0]\n",
    "\n",
    "K_PATCH = 20  # typical miniset neighborhood size\n",
    "\n",
    "# === Mode P: Physical kNN graph ===\n",
    "tree_phys = cKDTree(coords)\n",
    "_, phys_idx = tree_phys.query(coords, k=K_PATCH + 1)\n",
    "phys_idx = phys_idx[:, 1:]\n",
    "\n",
    "# === Mode Z: Embedding kNN graph ===\n",
    "tree_emb = cKDTree(Z)\n",
    "_, emb_idx = tree_emb.query(Z, k=K_PATCH + 1)\n",
    "emb_idx = emb_idx[:, 1:]\n",
    "\n",
    "# --- Metric 1: Physical compactness of patches ---\n",
    "def patch_compactness(coords, idx):\n",
    "    \"\"\"Mean diameter of patches (max pairwise dist within each patch).\"\"\"\n",
    "    diameters = []\n",
    "    for i in range(len(idx)):\n",
    "        patch_coords = coords[idx[i]]\n",
    "        center = patch_coords.mean(axis=0)\n",
    "        dists = np.sqrt(((patch_coords - center) ** 2).sum(axis=1))\n",
    "        diameters.append(dists.max())\n",
    "    return np.array(diameters)\n",
    "\n",
    "phys_compact = patch_compactness(coords, phys_idx)\n",
    "emb_compact = patch_compactness(coords, emb_idx)\n",
    "\n",
    "print(f\"\\n  Patch compactness (physical radius of k={K_PATCH} neighborhood):\")\n",
    "print(f\"    Physical patches — median: {np.median(phys_compact):.2f}, mean: {np.mean(phys_compact):.2f}\")\n",
    "print(f\"    Embedding patches — median: {np.median(emb_compact):.2f}, mean: {np.mean(emb_compact):.2f}\")\n",
    "print(f\"    Ratio (emb/phys): {np.median(emb_compact) / np.median(phys_compact):.2f}x\")\n",
    "\n",
    "# --- Metric 2: Graph connectivity overlap ---\n",
    "# What fraction of embedding neighbors are also physical neighbors?\n",
    "overlap_frac = np.array([\n",
    "    len(set(phys_idx[i]) & set(emb_idx[i])) / K_PATCH\n",
    "    for i in range(n)\n",
    "])\n",
    "\n",
    "print(f\"\\n  Neighbor overlap (phys ∩ emb / k):\")\n",
    "print(f\"    Mean overlap: {overlap_frac.mean():.4f}\")\n",
    "print(f\"    Median overlap: {np.median(overlap_frac):.4f}\")\n",
    "print(f\"    % spots with zero overlap: {(overlap_frac == 0).mean() * 100:.1f}%\")\n",
    "\n",
    "# --- Metric 3: Pairwise distance preservation ---\n",
    "# For each patch, compute pairwise distances in coord space\n",
    "# Compare consistency between the two modes\n",
    "def pairwise_dist_stats(coords, idx):\n",
    "    \"\"\"Per-patch mean pairwise distance in physical space.\"\"\"\n",
    "    means = []\n",
    "    for i in range(len(idx)):\n",
    "        pc = coords[idx[i]]\n",
    "        d = np.sqrt(((pc[:, None] - pc[None, :]) ** 2).sum(axis=2))\n",
    "        means.append(d[np.triu_indices(len(pc), k=1)].mean())\n",
    "    return np.array(means)\n",
    "\n",
    "phys_pw = pairwise_dist_stats(coords, phys_idx)\n",
    "emb_pw = pairwise_dist_stats(coords, emb_idx)\n",
    "\n",
    "print(f\"\\n  Mean pairwise physical distance within patches:\")\n",
    "print(f\"    Physical patches: {np.median(phys_pw):.2f}\")\n",
    "print(f\"    Embedding patches: {np.median(emb_pw):.2f}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(phys_compact, bins=50, alpha=0.6, label='Physical', density=True)\n",
    "axes[0].hist(emb_compact, bins=50, alpha=0.6, label='Embedding', density=True)\n",
    "axes[0].set_xlabel('Patch radius')\n",
    "axes[0].set_title('Patch Compactness')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(overlap_frac, bins=50, edgecolor='k', alpha=0.7)\n",
    "axes[1].set_xlabel('Neighbor overlap fraction')\n",
    "axes[1].set_title(f'Physical ∩ Embedding (k={K_PATCH})')\n",
    "\n",
    "axes[2].hist(phys_pw, bins=50, alpha=0.6, label='Physical', density=True)\n",
    "axes[2].hist(emb_pw, bins=50, alpha=0.6, label='Embedding', density=True)\n",
    "axes[2].set_xlabel('Mean pairwise distance')\n",
    "axes[2].set_title('Within-Patch Distances')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle(f'{name}: Physical vs Embedding Patch Construction', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Summary verdict ---\n",
    "ratio = np.median(emb_compact) / np.median(phys_compact)\n",
    "print(f\"\\n  VERDICT:\")\n",
    "if ratio > 3.0 and np.median(overlap_frac) < 0.1:\n",
    "    print(f\"    Embedding patches are {ratio:.1f}x larger with {np.median(overlap_frac)*100:.1f}% overlap.\")\n",
    "    print(f\"    → Embedding kNN is NOT spatially local. Physical patches should be used for training.\")\n",
    "elif ratio > 1.5:\n",
    "    print(f\"    Embedding patches are {ratio:.1f}x larger — moderate mismatch.\")\n",
    "    print(f\"    → Consider hybrid approach or tighter embedding constraints.\")\n",
    "else:\n",
    "    print(f\"    Embedding patches are comparable ({ratio:.1f}x). kNN graph is reasonable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION: Domain Mixing Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_MAX = 5000\n",
    "K = 20\n",
    "set_seed(SEED)\n",
    "\n",
    "def subsample(X, n_max):\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "    if X.shape[0] <= n_max:\n",
    "        return X\n",
    "    return X[torch.randperm(X.shape[0])[:n_max]]\n",
    "\n",
    "Z_st_sub = {n: subsample(Z, N_MAX) for n, Z in Z_st.items()}\n",
    "Z_inf_sub = {n: subsample(Z, N_MAX) for n, Z in Z_inf.items()}\n",
    "\n",
    "Z_all = torch.cat(list(Z_st_sub.values()) + list(Z_inf_sub.values()), dim=0)\n",
    "Z_all_norm = F.normalize(Z_all, dim=1)\n",
    "N = Z_all.shape[0]\n",
    "\n",
    "# Domain labels\n",
    "domain_labels = []\n",
    "source_names = []\n",
    "for i, name in enumerate(list(Z_st_sub) + list(Z_inf_sub)):\n",
    "    n = (Z_st_sub if name in Z_st_sub else Z_inf_sub)[name].shape[0]\n",
    "    domain_labels.append(torch.full((n,), i, dtype=torch.long))\n",
    "    source_names.append(name)\n",
    "domain_labels = torch.cat(domain_labels)\n",
    "S = len(source_names)\n",
    "\n",
    "print(f\"Sources: {source_names}, Total: {N}, Domains: {S}\")\n",
    "\n",
    "# --- kNN Domain-Mixing ---\n",
    "print(f\"\\n[METRIC 1] kNN Domain-Mixing (k={K})\")\n",
    "D = torch.cdist(Z_all_norm, Z_all_norm)\n",
    "D.fill_diagonal_(float('inf'))\n",
    "_, knn_idx = torch.topk(D, k=K, dim=1, largest=False)\n",
    "knn_labels = domain_labels[knn_idx]\n",
    "\n",
    "p = torch.zeros(N, S)\n",
    "for s in range(S):\n",
    "    p[:, s] = (knn_labels == s).float().mean(dim=1)\n",
    "\n",
    "eps = 1e-10\n",
    "H_norm = (-torch.sum(p * torch.log(p + eps), dim=1) / np.log(S)).mean().item()\n",
    "iLISI_norm = ((1.0 / torch.sum(p ** 2, dim=1) - 1) / (S - 1)).mean().item()\n",
    "\n",
    "print(f\"  Normalized Neighbor Entropy: {H_norm:.4f}\")\n",
    "print(f\"  Normalized iLISI: {iLISI_norm:.4f}\")\n",
    "\n",
    "# --- Domain Classification (CV) ---\n",
    "print(f\"\\n[METRIC 2] Domain Classification (5-fold CV)\")\n",
    "Z_np, y_np = Z_all_norm.numpy(), domain_labels.numpy()\n",
    "clf = LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(clf, Z_np, y_np, cv=cv, scoring='balanced_accuracy')\n",
    "chance = 1.0 / S\n",
    "print(f\"  Balanced Acc: {cv_scores.mean():.4f} ± {cv_scores.std():.4f} (chance={chance:.4f})\")\n",
    "\n",
    "# --- Binary: Training ST vs Inference ST4 ---\n",
    "print(f\"\\n[METRIC 3] Binary ST(1-3) vs ST4 (5-fold CV)\")\n",
    "n_st_total = sum(Z_st_sub[n].shape[0] for n in Z_st_sub)\n",
    "y_binary = torch.cat([torch.zeros(n_st_total, dtype=torch.long),\n",
    "                       torch.ones(N - n_st_total, dtype=torch.long)]).numpy()\n",
    "cv_binary = cross_val_score(clf, Z_np, y_binary, cv=cv, scoring='balanced_accuracy')\n",
    "print(f\"  Balanced Acc (ST vs ST4): {cv_binary.mean():.4f} ± {cv_binary.std():.4f}\")\n",
    "\n",
    "# --- Centroid Analysis ---\n",
    "print(f\"\\n[CENTROID ANALYSIS]\")\n",
    "centroids = {}\n",
    "for name in Z_st_sub:\n",
    "    centroids[name] = Z_st_sub[name].mean(dim=0)\n",
    "for name in Z_inf_sub:\n",
    "    centroids[name] = Z_inf_sub[name].mean(dim=0)\n",
    "\n",
    "names = list(centroids)\n",
    "print(\"         \", \"  \".join([f\"{n:>12}\" for n in names]))\n",
    "for n1 in names:\n",
    "    row = f\"{n1:12s}\"\n",
    "    for n2 in names:\n",
    "        row += f\"  {(centroids[n1] - centroids[n2]).norm().item():12.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<40} {'Value':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Neighbor Entropy (k=20)':<40} {H_norm:<15.4f}\")\n",
    "print(f\"{'iLISI (k=20)':<40} {iLISI_norm:<15.4f}\")\n",
    "print(f\"{'Domain Class. Acc (CV)':<40} {cv_scores.mean():<15.4f}\")\n",
    "print(f\"{'ST vs ST4 Acc (CV)':<40} {cv_binary.mean():<15.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PCA COMPARISON: Raw vs Embeddings\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VIS = 3000\n",
    "set_seed(SEED)\n",
    "\n",
    "# Subsample matched\n",
    "def subsample_matched(X, Z, n_max):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32) if not isinstance(X, torch.Tensor) else X.float()\n",
    "    Z_t = Z.float() if isinstance(Z, torch.Tensor) else torch.tensor(Z, dtype=torch.float32)\n",
    "    X_t, Z_t = X_t.cpu(), Z_t.cpu()\n",
    "    n = min(X_t.shape[0], n_max)\n",
    "    idx = torch.randperm(X_t.shape[0])[:n]\n",
    "    return X_t[idx].numpy(), Z_t[idx].numpy()\n",
    "\n",
    "X_vis, Z_vis, labels_vis = [], [], []\n",
    "for name in labels_str:\n",
    "    src = X_st if name in X_st else X_inf\n",
    "    src_z = Z_st if name in Z_st else Z_inf\n",
    "    x_sub, z_sub = subsample_matched(src[name], src_z[name], N_VIS)\n",
    "    X_vis.append(x_sub)\n",
    "    Z_vis.append(z_sub)\n",
    "    labels_vis.extend([name] * x_sub.shape[0])\n",
    "\n",
    "X_vis = np.vstack(X_vis)\n",
    "Z_vis = np.vstack(Z_vis)\n",
    "labels_vis = np.array(labels_vis)\n",
    "\n",
    "pca_x = PCA(n_components=2).fit(X_vis)\n",
    "X_pca = pca_x.transform(X_vis)\n",
    "var_x = pca_x.explained_variance_ratio_\n",
    "\n",
    "pca_z = PCA(n_components=2).fit(Z_vis)\n",
    "Z_pca = pca_z.transform(Z_vis)\n",
    "var_z = pca_z.explained_variance_ratio_\n",
    "\n",
    "# plt.rcParams.update({'font.family': 'Arial', 'font.weight': 'bold',\n",
    "#                      'axes.labelweight': 'bold', 'axes.titleweight': 'bold'})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "titles = [\n",
    "    f\"(a) PCA of raw expression (log1p)\\nVar: PC1={var_x[0]:.1%}, PC2={var_x[1]:.1%}\",\n",
    "    f\"(b) PCA of encoder embeddings\\nVar: PC1={var_z[0]:.1%}, PC2={var_z[1]:.1%}\",\n",
    "]\n",
    "for ax, data, title in zip(axes, [X_pca, Z_pca], titles):\n",
    "    for label in labels_str:\n",
    "        mask = labels_vis == label\n",
    "        ax.scatter(data[mask, 0], data[mask, 1], c=colors_map[label], label=label,\n",
    "                   s=25, alpha=0.65, edgecolors='white', linewidths=0.5)\n",
    "    ax.set_xlabel('PC1', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('PC2', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_facecolor('#FAFAFA')\n",
    "    ax.tick_params(labelsize=11)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('#CCCCCC')\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "axes[0].legend(loc='best', fontsize=11, framealpha=0.98, edgecolor='#888888')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('liver_pca_comparison.svg', format='svg', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"t-SNE COMPARISON: Raw vs Embeddings\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# N_TSNE = 2000\n",
    "# set_seed(SEED)\n",
    "\n",
    "# X_tsne_all, Z_tsne_all, labels_tsne = [], [], []\n",
    "# for name in labels_str:\n",
    "#     src = X_st if name in X_st else X_inf\n",
    "#     src_z = Z_st if name in Z_st else Z_inf\n",
    "#     x_sub, z_sub = subsample_matched(src[name], src_z[name], N_TSNE)\n",
    "#     X_tsne_all.append(x_sub)\n",
    "#     Z_tsne_all.append(z_sub)\n",
    "#     labels_tsne.extend([name] * x_sub.shape[0])\n",
    "\n",
    "# X_tsne_all = np.vstack(X_tsne_all)\n",
    "# Z_tsne_all = np.vstack(Z_tsne_all)\n",
    "# labels_tsne = np.array(labels_tsne)\n",
    "# Z_tsne_norm = F.normalize(torch.tensor(Z_tsne_all), dim=1).numpy()\n",
    "\n",
    "# print(\"Computing t-SNE for raw expression...\")\n",
    "# X_tsne_proj = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000).fit_transform(X_tsne_all)\n",
    "\n",
    "# print(\"Computing t-SNE for embeddings...\")\n",
    "# Z_tsne_proj = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000).fit_transform(Z_tsne_norm)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "# for ax, data, title in zip(axes, [X_tsne_proj, Z_tsne_proj],\n",
    "#     ['(A) t-SNE on Raw Expression (Before)', '(B) t-SNE on Embeddings (After)']):\n",
    "#     for label in labels_str:\n",
    "#         mask = labels_tsne == label\n",
    "#         ax.scatter(data[mask, 0], data[mask, 1], c=colors_map[label],\n",
    "#                    label=label, alpha=0.5, s=20, edgecolors='none')\n",
    "#     ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "#     ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "#     ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "#     ax.legend(loc='best', fontsize=10, frameon=True)\n",
    "#     ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# plt.suptitle('Mouse Liver: Before vs After Alignment (Cross-Slide)',\n",
    "#              fontsize=16, fontweight='bold', y=0.98)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# EVALUATION METRICS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION: Liver Cross-Slide Encoder\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_MAX = 5000\n",
    "set_seed(SEED)\n",
    "\n",
    "def subsample(X, n_max, device='cpu'):\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        X = X.to(device)\n",
    "    if X.shape[0] <= n_max:\n",
    "        return X\n",
    "    return X[torch.randperm(X.shape[0], device=device)[:n_max]]\n",
    "\n",
    "Z_st_sub = {name: subsample(Z, N_MAX, 'cpu') for name, Z in Z_st.items()}\n",
    "Z_inf_sub = {name: subsample(Z, N_MAX, 'cpu') for name, Z in Z_inf.items()}\n",
    "\n",
    "print(\"Subsampled embeddings:\")\n",
    "for name, Z in {**Z_st_sub, **Z_inf_sub}.items():\n",
    "    print(f\"  {name}: {Z.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# TEST 1: ST(1-3) vs ST4 kNN mixing\n",
    "# ===================================================================\n",
    "print(\"\\n[ST-vs-ST4 MIXING] kNN domain distribution:\")\n",
    "\n",
    "Z_st_all = torch.cat(list(Z_st_sub.values()), dim=0)\n",
    "Z_inf_all = torch.cat(list(Z_inf_sub.values()), dim=0)\n",
    "Z_all = torch.cat([Z_st_all, Z_inf_all], dim=0)\n",
    "\n",
    "n_st = Z_st_all.shape[0]\n",
    "n_inf = Z_inf_all.shape[0]\n",
    "n_total = n_st + n_inf\n",
    "\n",
    "# Domain labels (0=ST training, 1=ST4 inference)\n",
    "domain_labels = torch.cat([\n",
    "    torch.zeros(n_st, dtype=torch.long),\n",
    "    torch.ones(n_inf, dtype=torch.long)\n",
    "])\n",
    "\n",
    "Z_all_norm = F.normalize(Z_all, dim=1)\n",
    "\n",
    "K = 20\n",
    "inf_start = n_st\n",
    "\n",
    "D_inf = torch.cdist(Z_all_norm[inf_start:], Z_all_norm)\n",
    "for i in range(n_inf):\n",
    "    D_inf[i, inf_start + i] = float('inf')\n",
    "\n",
    "_, knn_inf = torch.topk(D_inf, k=K, dim=1, largest=False)\n",
    "frac_same_inf = (domain_labels[knn_inf] == 1).float().mean().item()\n",
    "base_rate_inf = n_inf / n_total\n",
    "\n",
    "print(f\"  ST4 neighbors (K={K}):\")\n",
    "print(f\"    Same-domain (ST4) fraction: {frac_same_inf:.4f}\")\n",
    "print(f\"    Base rate (chance):         {base_rate_inf:.4f}\")\n",
    "\n",
    "if frac_same_inf < base_rate_inf + 0.10:\n",
    "    print(\"    ✓ EXCELLENT mixing (ST4 not clustering)\")\n",
    "elif frac_same_inf < base_rate_inf + 0.20:\n",
    "    print(\"    ✓ Good mixing\")\n",
    "else:\n",
    "    print(\"    ⚠️ ST4 may be clustering\")\n",
    "\n",
    "# ===================================================================\n",
    "# TEST 2: Linear Probe (4-class separability)\n",
    "# ===================================================================\n",
    "n_sources = len(ST_PATHS) + len(INF_PATHS)\n",
    "print(f\"\\n[{n_sources}-CLASS PROBE] Linear separability test:\")\n",
    "\n",
    "class_labels_list = []\n",
    "class_idx = 0\n",
    "for name in ST_PATHS:\n",
    "    n_cells = Z_st_sub[name].shape[0]\n",
    "    class_labels_list.append(torch.full((n_cells,), class_idx, dtype=torch.long))\n",
    "    class_idx += 1\n",
    "for name in INF_PATHS:\n",
    "    n_cells = Z_inf_sub[name].shape[0]\n",
    "    class_labels_list.append(torch.full((n_cells,), class_idx, dtype=torch.long))\n",
    "    class_idx += 1\n",
    "\n",
    "class_labels = torch.cat(class_labels_list)\n",
    "\n",
    "Z_np = Z_all_norm.numpy()\n",
    "y_np = class_labels.numpy()\n",
    "\n",
    "probe = LogisticRegression(max_iter=5000, random_state=42, class_weight='balanced')\n",
    "probe.fit(Z_np, y_np)\n",
    "pred = probe.predict(Z_np)\n",
    "bal_acc = balanced_accuracy_score(y_np, pred)\n",
    "chance = 1.0 / n_sources\n",
    "\n",
    "print(f\"  Balanced accuracy: {bal_acc:.4f} (chance={chance:.3f})\")\n",
    "\n",
    "if bal_acc < 0.30:\n",
    "    print(\"  ✓ EXCELLENT: Sources are very well-mixed\")\n",
    "elif bal_acc < 0.40:\n",
    "    print(\"  ✓ Good: Moderate mixing\")\n",
    "elif bal_acc < 0.50:\n",
    "    print(\"  ~ Partial alignment\")\n",
    "else:\n",
    "    print(\"  ⚠️ Sources are separable\")\n",
    "\n",
    "# ===================================================================\n",
    "# TEST 3: Centroid distances\n",
    "# ===================================================================\n",
    "print(\"\\n[CENTROID ANALYSIS] Cross-source distances:\")\n",
    "\n",
    "centroids = {}\n",
    "for name in ST_PATHS:\n",
    "    centroids[name] = Z_st_sub[name].mean(dim=0)\n",
    "for name in INF_PATHS:\n",
    "    centroids[name] = Z_inf_sub[name].mean(dim=0)\n",
    "\n",
    "names = list(centroids)\n",
    "print(\"\\nCentroid distance matrix:\")\n",
    "print(\"             \", \"  \".join([f\"{n:>12}\" for n in names]))\n",
    "\n",
    "for n1 in names:\n",
    "    row = f\"{n1:12s}\"\n",
    "    for n2 in names:\n",
    "        dist = (centroids[n1] - centroids[n2]).norm().item()\n",
    "        row += f\"  {dist:12.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# ST-to-ST vs ST-to-ST4 distances\n",
    "st_names = list(ST_PATHS)\n",
    "inf_names = list(INF_PATHS)\n",
    "\n",
    "st_to_st = []\n",
    "for i, n1 in enumerate(st_names):\n",
    "    for n2 in st_names[i + 1:]:\n",
    "        st_to_st.append((centroids[n1] - centroids[n2]).norm().item())\n",
    "\n",
    "st_to_inf = []\n",
    "for n1 in st_names:\n",
    "    for n2 in inf_names:\n",
    "        st_to_inf.append((centroids[n1] - centroids[n2]).norm().item())\n",
    "\n",
    "print(f\"\\nAverage distances:\")\n",
    "print(f\"  ST-to-ST:  {np.mean(st_to_st):.4f} ± {np.std(st_to_st):.4f}\")\n",
    "print(f\"  ST-to-ST4: {np.mean(st_to_inf):.4f} ± {np.std(st_to_inf):.4f}\")\n",
    "ratio = np.mean(st_to_inf) / np.mean(st_to_st)\n",
    "print(f\"  Ratio (ST4/ST): {ratio:.2f}x\")\n",
    "\n",
    "if ratio < 1.5:\n",
    "    print(\"  ✓ EXCELLENT: ST4 very close to training ST centroids\")\n",
    "elif ratio < 2.5:\n",
    "    print(\"  ✓ Good: Reasonable cross-slide distance\")\n",
    "else:\n",
    "    print(\"  ⚠️ Large slide gap remains\")\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LIVER ENCODER EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"✓ ST4 mixing:        same-domain frac = {frac_same_inf:.4f} (base = {base_rate_inf:.4f})\")\n",
    "print(f\"✓ {n_sources}-class probe:   balanced acc = {bal_acc:.4f} (chance = {chance:.3f})\")\n",
    "print(f\"✓ Centroid ratio:    ST-to-ST4 / ST-to-ST = {ratio:.2f}x\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
