{
 "cells": [
  {
   "cell_type": "code",
   "source": "# ===================================================================\n# ENCODER TRAINING v3 \u2014 FINAL (Phase 1 only + slide-scale reg)\n# ===================================================================\n# Decision: Trunk = Phase 1 checkpoint (VICReg + Spatial InfoNCE only)\n#           Domain alignment = post-hoc adapter (trained separately, trunk frozen)\n#\n# Changes from v2:\n#   - two_phase_training=False  (no Phase 2 \u2014 alignment broke locality)\n#   - n_epochs=1200  (Phase 1 saturates ~900-1000, extra buffer for best checkpoint)\n#   - slide_scale_weight=5.0  (equalize per-slide RMS norms during training)\n#   - adv_slide_weight=0.0  (GRL off, disc training skipped to save compute)\n#   - All CORAL/MMD/alignment weights=0  (no trunk alignment)\n#   - Everything else same as v2 (NCE=5.0, inv warmup, etc.)\n# ===================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING ENCODER v3 \u2014 FINAL (Phase 1 only)\")\nprint(\"=\" * 70)\n\nset_seed(SEED)\nencoder_v3 = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n\noutdir_v3 = '/home/ehtesamul/sc_st/model/gems_liver_crossslide_v3'\nos.makedirs(outdir_v3, exist_ok=True)\n\nencoder_v3, projector_v3, discriminator_v3, hist_v3 = train_encoder(\n    inference_dropout_prob=0.0,       # No inference dropout (no SC alignment phase)\n    model=encoder_v3,\n    st_gene_expr=st_expr,\n    st_coords=st_coords,\n    sc_gene_expr=inf_expr,\n    slide_ids=slide_ids,\n    sc_slide_ids=inf_slide_ids,\n    sc_patient_ids=inf_patient_ids,\n    n_epochs=1200,                    # Phase 1 only \u2014 saturates by ~1000\n    batch_size=256,\n    lr=1e-4,\n    device=device,\n    outf=outdir_v3,\n    st_source_ids=st_source_ids,\n    sc_source_ids=inf_source_ids,\n    use_source_adversary=False,\n    source_coral_weight=0.0,          # OFF \u2014 no trunk alignment\n    stageA_obj='vicreg_adv',\n    # ---- VICReg: same warmup as v2 ----\n    vicreg_lambda_inv=25.0,\n    vicreg_lambda_var=50.0,\n    vicreg_lambda_cov=1.0,\n    vicreg_inv_warmup_frac=0.3,\n    vicreg_inv_start=5.0,\n    vicreg_gamma=1.0,\n    vicreg_eps=1e-4,\n    vicreg_project_dim=256,\n    vicreg_use_projector=False,\n    vicreg_float32_stats=True,\n    vicreg_ddp_gather=False,\n    aug_gene_dropout=0.25,\n    aug_gauss_std=0.01,\n    aug_scale_jitter=0.1,\n    # ---- ALL alignment OFF (domain compat via post-hoc adapter) ----\n    adv_slide_weight=0.0,             # GRL off + disc training skipped\n    patient_coral_weight=0.0,\n    mmd_weight=0.0,                   # OFF \u2014 was 30.0 in v2\n    mmd_use_l2norm=True,\n    mmd_ramp=True,\n    adv_warmup_epochs=50,\n    adv_ramp_epochs=200,\n    grl_alpha_max=1.0,\n    disc_hidden=512,\n    disc_dropout=0.1,\n    stageA_balanced_slides=True,\n    adv_representation_mode='clean',\n    adv_use_layernorm=False,\n    adv_log_diagnostics=False,        # No disc = no diagnostics needed\n    adv_log_grad_norms=False,\n    use_local_align=False,            # OFF\n    return_aux=True,\n    local_align_bidirectional=True,\n    local_align_weight=0.0,\n    local_align_tau_z=0.07,\n    seed=SEED,\n    use_best_checkpoint=True,\n    coral_raw_weight=0.0,             # OFF\n    knn_weight=0.0,\n    # ---- Spatial NCE: same as v2 ----\n    spatial_nce_weight=5.0,\n    spatial_nce_k_phys=20,\n    spatial_nce_far_mult=4.0,\n    spatial_nce_n_hard=20,\n    spatial_nce_tau=0.1,\n    spatial_nce_n_rand_neg=128,\n    spatial_nce_n_anchors=64,\n    # ---- NO Phase 2 ----\n    two_phase_training=False,\n    # ---- Slide-scale regularizer (equalize per-slide norms) ----\n    slide_scale_weight=5.0,           # NEW: L_ss = \u03a3_s (log RMS(z_s) - log RMS(z_all))^2\n    # ---- Per-slide normalization (for diagnostics) ----\n    per_slide_norm=True,\n    per_slide_norm_target=1.0,\n)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"v3 TRAINING COMPLETE\")\nprint(\"=\" * 70)\ntorch.save(encoder_v3.state_dict(), f'{outdir_v3}/encoder_v3_final.pt')\nprint(f\"Saved: {outdir_v3}/encoder_v3_final.pt\")\n\n# Post-training: per-slide normalization report\nfrom core_models_et_p1 import normalize_embeddings_per_slide\nencoder_v3.eval()\nwith torch.no_grad():\n    z_parts = []\n    for i in range(0, st_expr.shape[0], 512):\n        z_parts.append(encoder_v3(st_expr[i:i+512]))\n    z_all = torch.cat(z_parts, dim=0)\n    z_normed = normalize_embeddings_per_slide(z_all, slide_ids, target_rms=1.0)\n\n    print(\"\\nPer-slide norm report (before \u2192 after normalization):\")\n    for sid in torch.unique(slide_ids):\n        mask = (slide_ids == sid)\n        nb = z_all[mask].norm(dim=1).mean().item()\n        na = z_normed[mask].norm(dim=1).mean().item()\n        print(f\"  Slide {sid.item()}: {nb:.2f} \u2192 {na:.2f}\")\n\n# Save normalized embeddings for adapter training\ntorch.save({\n    'z_raw': z_all.cpu(),\n    'z_normed': z_normed.cpu(),\n    'slide_ids': slide_ids.cpu(),\n}, f'{outdir_v3}/st_embeddings.pt')\nprint(f\"\\nSaved ST embeddings: {outdir_v3}/st_embeddings.pt\")\nprint(\"Next step: Train affine adapter with trunk frozen (see adapter_topology_eval.py)\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ===================================================================\n# ENCODER TRAINING v2 \u2014 with diagnostic improvements\n# ===================================================================\n# Changes from v1 (Cell 5):\n#   - adv_slide_weight=0.0  (GRL kept in code, just zeroed \u2014 preserves code for future use)\n#   - vicreg_inv_warmup_frac=0.3, vicreg_inv_start=5.0  (ramp \u03bb_inv over first 30% of Phase 1)\n#   - spatial_nce_weight=5.0  (strengthened from 3.0)\n#   - per_slide_norm=True  (track per-slide norms in diagnostics)\n#   - Now logs: overlap@20, hit@20, per-slide norms every 100 epochs\n# ===================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING ENCODER v2 \u2014 DIAGNOSTIC IMPROVEMENTS\")\nprint(\"=\" * 70)\n\nset_seed(SEED)\nencoder_v2 = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n\noutdir_v2 = '/home/ehtesamul/sc_st/model/gems_liver_crossslide_v2'\nos.makedirs(outdir_v2, exist_ok=True)\n\nencoder_v2, projector_v2, discriminator_v2, hist_v2 = train_encoder(\n    inference_dropout_prob=0.5,\n    model=encoder_v2,\n    st_gene_expr=st_expr,\n    st_coords=st_coords,\n    sc_gene_expr=inf_expr,\n    slide_ids=slide_ids,\n    sc_slide_ids=inf_slide_ids,\n    sc_patient_ids=inf_patient_ids,\n    n_epochs=2000,\n    batch_size=256,\n    lr=1e-4,\n    device=device,\n    outf=outdir_v2,\n    st_source_ids=st_source_ids,\n    sc_source_ids=inf_source_ids,\n    use_source_adversary=False,\n    source_coral_weight=75.0,\n    stageA_obj='vicreg_adv',\n    # ---- VICReg: ramp \u03bb_inv from 5 \u2192 25 over first 30% of Phase 1 ----\n    vicreg_lambda_inv=25.0,\n    vicreg_lambda_var=50.0,\n    vicreg_lambda_cov=1.0,\n    vicreg_inv_warmup_frac=0.3,   # NEW: ramp \u03bb_inv over first 30% of Phase 1\n    vicreg_inv_start=5.0,         # NEW: start \u03bb_inv low\n    vicreg_gamma=1.0,\n    vicreg_eps=1e-4,\n    vicreg_project_dim=256,\n    vicreg_use_projector=False,\n    vicreg_float32_stats=True,\n    vicreg_ddp_gather=False,\n    aug_gene_dropout=0.25,\n    aug_gauss_std=0.01,\n    aug_scale_jitter=0.1,\n    # ---- GRL: zeroed weight (code preserved for future use) ----\n    adv_slide_weight=0.0,         # CHANGED: was 75.0, now zeroed\n    patient_coral_weight=0.0,\n    mmd_weight=30.0,\n    mmd_use_l2norm=True,\n    mmd_ramp=True,\n    adv_warmup_epochs=50,\n    adv_ramp_epochs=200,\n    grl_alpha_max=1.0,\n    disc_hidden=512,\n    disc_dropout=0.1,\n    stageA_balanced_slides=True,\n    adv_representation_mode='clean',\n    adv_use_layernorm=False,\n    adv_log_diagnostics=True,\n    adv_log_grad_norms=False,\n    use_local_align=True,\n    return_aux=True,\n    local_align_bidirectional=True,\n    local_align_weight=0.0,\n    local_align_tau_z=0.07,\n    seed=SEED,\n    use_best_checkpoint=True,\n    coral_raw_weight=2.0,\n    knn_weight=0.0,\n    # ---- Spatial NCE: strengthened ----\n    spatial_nce_weight=5.0,       # CHANGED: was 3.0, now 5.0\n    spatial_nce_k_phys=20,\n    spatial_nce_far_mult=4.0,\n    spatial_nce_n_hard=20,\n    spatial_nce_tau=0.1,\n    spatial_nce_n_rand_neg=128,\n    spatial_nce_n_anchors=64,\n    # ---- Two-phase training ----\n    two_phase_training=True,\n    phase1_epochs=1000,\n    phase2_lr_factor=0.1,\n    # ---- Per-slide normalization (diagnostic tracking) ----\n    per_slide_norm=True,           # NEW: track per-slide norms\n    per_slide_norm_target=1.0,\n)\n\nprint(\"\\n\u2713 Training v2 complete!\")\ntorch.save(encoder_v2.state_dict(), f'{outdir_v2}/encoder_v2_final.pt')\nprint(f\"\u2713 Saved to: {outdir_v2}/encoder_v2_final.pt\")\n\n# Post-training: apply per-slide normalization and save\nfrom core_models_et_p1 import normalize_embeddings_per_slide\nencoder_v2.eval()\nwith torch.no_grad():\n    z_parts = []\n    for i in range(0, st_expr.shape[0], 512):\n        z_parts.append(encoder_v2(st_expr[i:i+512]))\n    z_all = torch.cat(z_parts, dim=0)\n    z_normed = normalize_embeddings_per_slide(z_all, slide_ids, target_rms=1.0)\n\n    # Report norm stats before/after\n    for sid in torch.unique(slide_ids):\n        mask = (slide_ids == sid)\n        norm_before = z_all[mask].norm(dim=1).mean().item()\n        norm_after = z_normed[mask].norm(dim=1).mean().item()\n        print(f\"  Slide {sid.item()}: norm {norm_before:.2f} \u2192 {norm_after:.2f}\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================================================================\n",
    "# SECTION D (cont): VISUALIZE ADAPTER RESULTS\n",
    "# ===================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "hist = adapter_results['history']\n",
    "epochs_h = hist['epoch']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Panel 1: CORAL + MMD loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(epochs_h, hist['loss_coral'], label='CORAL', color='blue', linewidth=2)\n",
    "ax.plot(epochs_h, hist['loss_mmd'], label='MMD', color='red', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Domain Alignment Losses')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Panel 2: Centroid distance\n",
    "ax = axes[0, 1]\n",
    "ax.plot(epochs_h, hist['domain_centroid_dist'], color='purple', linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('L2 Distance')\n",
    "ax.set_title('ST-SC Centroid Distance')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Panel 3: ST overlap@20 (should be flat)\n",
    "ax = axes[1, 0]\n",
    "ax.plot(epochs_h, hist['st_overlap_at_20'], color='green', linewidth=2, marker='o', markersize=3)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('overlap@20')\n",
    "ax.set_title('ST Spatial Locality (should be STABLE)')\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim([0, max(0.7, max(hist['st_overlap_at_20']) * 1.1)])\n",
    "\n",
    "# Panel 4: PCA of embeddings (before vs after adapter)\n",
    "ax = axes[1, 1]\n",
    "z_st = adapter_results['z_st_frozen'].cpu().numpy()\n",
    "z_sc_raw = encoder_for_adapter(inf_expr).detach().cpu().numpy() if False else None  # skip if not needed\n",
    "z_sc_adapted = adapter_results['z_sc_adapted'].cpu().numpy()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "Z_all_vis = np.vstack([z_st, z_sc_adapted])\n",
    "pca = PCA(n_components=2).fit(Z_all_vis)\n",
    "st_pca = pca.transform(z_st)\n",
    "sc_pca = pca.transform(z_sc_adapted)\n",
    "\n",
    "ax.scatter(st_pca[:, 0], st_pca[:, 1], c='steelblue', s=8, alpha=0.3, label='ST (frozen)')\n",
    "ax.scatter(sc_pca[:, 0], sc_pca[:, 1], c='orange', s=8, alpha=0.3, label='SC (adapted)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_title('PCA: ST (frozen) vs SC (adapted)')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('SC Adapter Training Results', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "res = adapter_results['results']\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SC ADAPTER EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  ST overlap@20:       {res['st_overlap_at_20']:.4f}  (should be ~0.686 for v3)\")\n",
    "print(f\"  Domain acc BEFORE:   {res['domain_acc_before']:.4f}  (closer to 0.5 = better mixing)\")\n",
    "print(f\"  Domain acc AFTER:    {res['domain_acc_after']:.4f}\")\n",
    "print(f\"  CORAL (final):       {res['final_coral']:.4f}\")\n",
    "print(f\"  MMD (final):         {res['final_mmd']:.4f}\")\n",
    "print(f\"  Centroid dist:       {res['final_centroid_dist']:.4f}\")\n",
    "print(\"=\" * 60)"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ===================================================================\n",
    "# SECTION D: SC ADAPTER TRAINING (ST-Anchored Alignment)  \u2014 v3\n",
    "# ===================================================================\n",
    "# v3 encoder is frozen. A per-dim affine adapter (z' = a*z + b) is\n",
    "# trained with CORAL + MMD to align SC embeddings to frozen ST space.\n",
    "# Affine adapter CANNOT scramble neighborhoods (topology-safe).\n",
    "# ST overlap@20 stays unchanged by design.\n",
    "# ===================================================================\n",
    "\n",
    "from sc_adapter import SCAdapter, train_sc_adapter\n",
    "from ssl_utils import precompute_spatial_nce_structures\n",
    "\n",
    "# Load the v3 encoder (Phase 1 only, slide-scale regularized)\n",
    "encoder_for_adapter = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n",
    "encoder_for_adapter.load_state_dict(torch.load(\n",
    "    '/home/ehtesamul/sc_st/model/gems_liver_crossslide_v3/encoder_v3_final.pt',\n",
    "    map_location=device\n",
    "))\n",
    "encoder_for_adapter.to(device)\n",
    "\n",
    "# Precompute physical kNN for overlap monitoring\n",
    "spatial_nce_data_full = precompute_spatial_nce_structures(\n",
    "    st_coords=st_coords, st_gene_expr=st_expr, slide_ids=slide_ids,\n",
    "    k_phys=20, far_mult=4.0, n_hard=20, device=device,\n",
    ")\n",
    "\n",
    "adapter_outdir = '/home/ehtesamul/sc_st/model/gems_liver_crossslide_v3/sc_adapter'\n",
    "\n",
    "adapter_results = train_sc_adapter(\n",
    "    encoder=encoder_for_adapter,\n",
    "    st_gene_expr=st_expr,\n",
    "    sc_gene_expr=inf_expr,\n",
    "    st_coords=st_coords,\n",
    "    slide_ids=slide_ids,\n",
    "    # Adapter config \u2014 affine (per-dim scale+shift, topology-safe)\n",
    "    adapter_mode='affine',\n",
    "    adapter_dropout=0.0,          # not used for affine, kept for API compat\n",
    "    # Training config\n",
    "    n_epochs=500,\n",
    "    batch_size=256,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    # Loss weights\n",
    "    coral_weight=10.0,\n",
    "    mmd_weight=10.0,\n",
    "    # Diagnostics\n",
    "    log_every=25,\n",
    "    device=device,\n",
    "    seed=SEED,\n",
    "    outf=adapter_outdir,\n",
    "    phys_knn_idx=spatial_nce_data_full['pos_idx'],\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "from core_models_et_p1 import SharedEncoder, train_encoder\n",
    "from ssl_utils import set_seed\n",
    "import utils_et as uet\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CROSS-SLIDE ALIGNMENT: Mouse Liver\")\n",
    "print(\"ST training = ST1, ST2, ST3 | Inference target = ST4\")\n",
    "print(\"Same patient (mouse) \u2014 cross-slide gap only\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load data\n",
    "ST_PATHS = {\n",
    "    'liver_ST1': '/home/ehtesamul/sc_st/data/liver/stadata1.h5ad',\n",
    "    'liver_ST2': '/home/ehtesamul/sc_st/data/liver/stadata2.h5ad',\n",
    "    'liver_ST3': '/home/ehtesamul/sc_st/data/liver/stadata3.h5ad',\n",
    "}\n",
    "INF_PATHS = {\n",
    "    'liver_ST4': '/home/ehtesamul/sc_st/data/liver/stadata4.h5ad',\n",
    "}\n",
    "\n",
    "st_data = {}\n",
    "for name, path in ST_PATHS.items():\n",
    "    st_data[name] = sc.read_h5ad(path)\n",
    "    print(f\"  {name}: {st_data[name].n_obs} spots\")\n",
    "\n",
    "inf_data = {}\n",
    "for name, path in INF_PATHS.items():\n",
    "    inf_data[name] = sc.read_h5ad(path)\n",
    "    print(f\"  {name}: {inf_data[name].n_obs} spots\")\n",
    "\n",
    "# Normalize\n",
    "all_data = list(st_data.values()) + list(inf_data.values())\n",
    "for adata in all_data:\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "# Common genes\n",
    "common_genes = sorted(set.intersection(*[set(a.var_names) for a in all_data]))\n",
    "n_genes = len(common_genes)\n",
    "print(f\"\\n\u2713 Common genes: {n_genes}\")\n",
    "\n",
    "# Extract expression\n",
    "def extract_expr(adata, genes):\n",
    "    X = adata[:, genes].X\n",
    "    return X.toarray() if hasattr(X, \"toarray\") else X\n",
    "\n",
    "X_st = {name: extract_expr(st_data[name], common_genes) for name in ST_PATHS}\n",
    "X_inf = {name: extract_expr(inf_data[name], common_genes) for name in INF_PATHS}\n",
    "\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    print(f\"  {name}: {X.shape}\")\n",
    "\n",
    "labels_str = list(ST_PATHS.keys()) + list(INF_PATHS.keys())\n",
    "\n",
    "colors_map = {\n",
    "    'liver_ST1': '#e74c3c',\n",
    "    'liver_ST2': '#3498db',\n",
    "    'liver_ST3': '#2ecc71',\n",
    "    'liver_ST4': '#f39c12',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPARING TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ST domain (training slides with coords)\n",
    "st_expr = torch.tensor(\n",
    "    np.vstack([X_st[n] for n in ST_PATHS]),\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "st_coords_list = [st_data[n].obsm['spatial'] for n in ST_PATHS]\n",
    "st_coords_raw = torch.tensor(\n",
    "    np.vstack(st_coords_list), dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "ns = [X_st[n].shape[0] for n in ST_PATHS]\n",
    "slide_ids = torch.tensor(\n",
    "    np.concatenate([np.full(n, i, dtype=int) for i, n in enumerate(ns)]),\n",
    "    dtype=torch.long, device=device\n",
    ")\n",
    "\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"\u2713 ST expr: {st_expr.shape}\")\n",
    "for i, (name, n) in enumerate(zip(ST_PATHS, ns)):\n",
    "    print(f\"  - {name}: {n} spots (slide {i})\")\n",
    "print(f\"\u2713 ST coords: {st_coords.shape} (canonicalized per-slide)\")\n",
    "\n",
    "# Inference domain (ST4 treated as \"SC\")\n",
    "inf_expr = torch.tensor(X_inf['liver_ST4'], dtype=torch.float32, device=device)\n",
    "n_inf = X_inf['liver_ST4'].shape[0]\n",
    "inf_slide_ids = torch.zeros(n_inf, dtype=torch.long, device=device)\n",
    "inf_patient_ids = torch.zeros(n_inf, dtype=torch.long, device=device)  # same patient\n",
    "\n",
    "print(f\"\\n\u2713 Inference expr: {inf_expr.shape}\")\n",
    "print(f\"  - liver_ST4: {n_inf} spots (inference domain)\")\n",
    "\n",
    "# Source IDs for adversary\n",
    "st_source_ids = torch.tensor(\n",
    "    np.concatenate([np.full(n, i, dtype=int) for i, n in enumerate(ns)]),\n",
    "    dtype=torch.long\n",
    ")\n",
    "inf_source_ids = torch.full((n_inf,), len(ns), dtype=torch.long)\n",
    "\n",
    "print(f\"\\n\u2713 ST source IDs: {st_source_ids.unique().tolist()}, counts: {ns}\")\n",
    "print(f\"\u2713 Inf source IDs: {inf_source_ids.unique().tolist()}, count: {n_inf}\")\n",
    "print(f\"\\nTotal for Stage A: {st_expr.shape[0] + n_inf}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ===================================================================\n# SPATIAL InfoNCE DIAGNOSTICS (SUPPORT-SET VERSION)\n# ===================================================================\n# CHECK A: Per-loss-component gradient norms (is spatial NCE \"live\"?)\n# CHECK B: Logit gap over 200 steps (sim_pos vs sim_hard vs sim_rand)\n# CHECK C: Index set sanity (pos/neg physical distances + overlap)\n# ===================================================================\n\nfrom ssl_utils import diagnose_spatial_infonce, set_seed\nfrom core_models_et_p1 import SharedEncoder\n\nprint(\"=\" * 70)\nprint(\"SPATIAL InfoNCE DIAGNOSTIC (support-set)\")\nprint(\"=\" * 70)\n\nset_seed(SEED)\ndiag_encoder = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n\ndiag_results = diagnose_spatial_infonce(\n    model=diag_encoder,\n    st_gene_expr=st_expr,\n    st_coords=st_coords,\n    sc_gene_expr=inf_expr,\n    slide_ids=slide_ids,\n    sc_slide_ids=inf_slide_ids,\n    spatial_nce_weight=3.0,\n    spatial_nce_k_phys=20,\n    spatial_nce_far_mult=4.0,\n    spatial_nce_n_hard=20,\n    spatial_nce_tau=0.1,\n    spatial_nce_n_rand_neg=128,\n    spatial_nce_n_anchors=64,\n    vicreg_lambda_inv=25.0,\n    vicreg_lambda_var=50.0,\n    vicreg_lambda_cov=1.0,\n    vicreg_gamma=1.0,\n    vicreg_eps=1e-4,\n    aug_gene_dropout=0.25,\n    aug_gauss_std=0.01,\n    aug_scale_jitter=0.1,\n    local_align_weight=0.0,\n    local_align_tau_z=0.07,\n    local_align_bidirectional=True,\n    batch_size=256,\n    n_diagnostic_steps=200,\n    lr=1e-4,\n    device=device,\n    seed=SEED,\n)\n\n# ===================================================================\n# PLOTS\n# ===================================================================\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlogs = diag_results['step_logs']\nsteps = np.array(logs['step'])\ncc = diag_results['check_c']\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# --- Panel 1: Similarity traces (Check B) ---\nax = axes[0, 0]\nax.plot(steps, logs['sim_pos_mean'], label='sim(anchor, pos)', color='green', linewidth=2)\nax.plot(steps, logs['sim_hard_mean'], label='sim(anchor, hard_neg)', color='red', linewidth=2)\nax.plot(steps, logs['sim_rand_mean'], label='sim(anchor, rand_neg)', color='blue', linewidth=2)\nax.fill_between(steps,\n    np.array(logs['sim_pos_mean']) - np.array(logs['sim_pos_std']),\n    np.array(logs['sim_pos_mean']) + np.array(logs['sim_pos_std']),\n    alpha=0.15, color='green')\nax.fill_between(steps,\n    np.array(logs['sim_hard_mean']) - np.array(logs['sim_hard_std']),\n    np.array(logs['sim_hard_mean']) + np.array(logs['sim_hard_std']),\n    alpha=0.15, color='red')\nax.set_xlabel('Step')\nax.set_ylabel('Cosine Similarity')\nax.set_title('CHECK B: Similarity Gap (support-set)')\nax.legend(fontsize=8)\nax.grid(alpha=0.3)\n\n# --- Panel 2: NCE loss over steps (Check B) ---\nax = axes[0, 1]\nax.plot(steps, logs['loss_nce'], label='L_spatialNCE', color='purple', linewidth=2)\nax.plot(steps, logs['loss_vicreg'], label='L_VICReg', color='orange', linewidth=2, alpha=0.7)\nax.set_xlabel('Step')\nax.set_ylabel('Loss')\nax.set_title('CHECK B: Loss Components')\nax.legend(fontsize=8)\nax.grid(alpha=0.3)\n\n# --- Panel 3: Gradient norms bar chart (Check A) ---\nax = axes[0, 2]\ngn = diag_results['grad_norms']\nnames_gn = list(gn.keys())\nvals_gn = [gn[k] for k in names_gn]\nbars = ax.bar(names_gn, vals_gn, color=['purple', 'orange', 'green', 'gray'])\nax.set_ylabel('||grad_theta||')\nax.set_title('CHECK A: Gradient Norms')\nfor bar, val in zip(bars, vals_gn):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n            f'{val:.4f}', ha='center', va='bottom', fontsize=8)\nax.grid(alpha=0.3, axis='y')\nplt.setp(ax.get_xticklabels(), rotation=15, ha='right', fontsize=8)\n\n# --- Panel 4: Physical distance distributions (Check C) ---\nax = axes[1, 0]\nif len(cc['pos_phys_dists']) > 0:\n    ax.hist(cc['pos_phys_dists'], bins=60, alpha=0.6, label='Positives', color='green', density=True)\nif len(cc['hard_phys_dists']) > 0:\n    ax.hist(cc['hard_phys_dists'], bins=60, alpha=0.6, label='Hard negatives', color='red', density=True)\nax.axvline(cc['r_far_threshold'], color='black', linestyle='--', linewidth=2,\n           label=f'r_far = {cc[\"r_far_threshold\"]:.3f}')\nax.set_xlabel('Physical distance to anchor')\nax.set_ylabel('Density')\nax.set_title('CHECK C: Pos vs Neg Physical Distances')\nax.legend(fontsize=8)\nax.grid(alpha=0.3)\n\n# --- Panel 5: Per-anchor counts (Check C) ---\nax = axes[1, 1]\nx_pos = np.arange(3)\nmeans = [cc['pos_counts'].mean(), cc['hard_counts'].mean(), np.mean(cc['far_counts'])]\nmins = [cc['pos_counts'].min(), cc['hard_counts'].min(), np.min(cc['far_counts'])]\nlabels_c = ['Positives\\n(phys neighbors)', 'Hard negatives\\n(expr-sim + far)', 'Far mask\\n(total far spots)']\nbar_colors = ['green', 'red', 'blue']\nbars = ax.bar(x_pos, means, color=bar_colors, alpha=0.7)\nfor i, (bar, m, mn) in enumerate(zip(bars, means, mins)):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n            f'mean={m:.1f}\\nmin={mn}', ha='center', va='bottom', fontsize=8)\nax.set_xticks(x_pos)\nax.set_xticklabels(labels_c, fontsize=8)\nax.set_ylabel('Count per anchor')\nax.set_title('CHECK C: Index Set Sizes')\nax.grid(alpha=0.3, axis='y')\n\n# --- Panel 6: Batch coverage (Check B) ---\nax = axes[1, 2]\nax.plot(steps, logs['n_active_anchors'], label='Active anchors', color='teal', linewidth=2)\nax2 = ax.twinx()\nax2.plot(steps, logs['n_pos_per_anchor'], label='pos/anchor', color='green', linestyle='--')\nax2.plot(steps, logs['n_hard_per_anchor'], label='hard/anchor', color='red', linestyle='--')\nax2.plot(steps, logs['n_rand_per_anchor'], label='rand/anchor', color='blue', linestyle='--')\nax.set_xlabel('Step')\nax.set_ylabel('Active Anchors')\nax2.set_ylabel('Neighbors per Anchor')\nax.set_title('CHECK B: Support-Set Coverage')\nlines1, labels1 = ax.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax.legend(lines1 + lines2, labels1 + labels2, fontsize=7, loc='center right')\nax.grid(alpha=0.3)\n\nplt.suptitle('Spatial InfoNCE Diagnostics \u2014 Support-Set (Checks A + B + C)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# ===================================================================\n# VERDICT\n# ===================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"DIAGNOSTIC VERDICT\")\nprint(\"=\" * 70)\n\n# Check A verdict\ngnce = gn.get('L_spatialNCE', 0)\ngvic = gn.get('L_VICReg', 0)\nprint(\"\\n[CHECK A] Gradient norms:\")\nif gnce < 1e-6:\n    print(\"  FAIL: Spatial NCE gradient is ZERO -- loss is disconnected\")\nelif gvic > 0 and gnce / gvic < 1e-3:\n    print(f\"  WARN: Spatial NCE gradient negligible vs VICReg (ratio={gnce/gvic:.6f})\")\nelse:\n    print(f\"  PASS: Spatial NCE is live (ratio NCE/VICReg = {gnce/gvic:.4f})\")\n\n# Check B verdict\nmean_active = np.mean(logs['n_active_anchors'])\nmean_pos = np.mean(logs['sim_pos_mean'])\nmean_hard = np.mean(logs['sim_hard_mean'])\nmean_rand = np.mean(logs['sim_rand_mean'])\nmean_n_pos = np.mean(logs['n_pos_per_anchor'])\nmean_n_hard = np.mean(logs['n_hard_per_anchor'])\nprint(f\"\\n[CHECK B] Loss scale (support-set):\")\nprint(f\"  Active anchors/step: {mean_active:.1f}\")\nprint(f\"  pos/anchor: {mean_n_pos:.1f}, hard/anchor: {mean_n_hard:.1f}\")\nprint(f\"  sim(pos)={mean_pos:.4f}, sim(hard)={mean_hard:.4f}, sim(rand)={mean_rand:.4f}\")\nprint(f\"  Gap pos-hard: {mean_pos - mean_hard:.4f}, Gap pos-rand: {mean_pos - mean_rand:.4f}\")\nif mean_active < 2:\n    print(\"  FAIL: Almost no anchors active -- loss is starved\")\nelif mean_pos > mean_hard + 0.1:\n    print(\"  INFO: Positives already more similar than hard negatives\")\nelif mean_pos < mean_hard:\n    print(\"  INFO: Positives less similar than hard negatives -- loss is working hard\")\n\n# Check C verdict\nprint(f\"\\n[CHECK C] Index set sanity:\")\nmax_overlap_ph = cc['overlap_pos_hard'].max()\nmax_overlap_any = cc['overlap_pos_any_neg'].max()\nn_leak_ph = sum(1 for x in cc['overlap_pos_hard'] if x > 0)\nn_leak_any = sum(1 for x in cc['overlap_pos_any_neg'] if x > 0)\nn_sampled = len(cc['overlap_pos_hard'])\nprint(f\"  |pos \u2229 hard_neg|: max={max_overlap_ph}, leaking anchors={n_leak_ph}/{n_sampled}\")\nprint(f\"  |pos \u2229 (hard \u222a far)|: max={max_overlap_any}, leaking anchors={n_leak_any}/{n_sampled}\")\nif max_overlap_ph > 0:\n    print(\"  FAIL: LEAKAGE -- some spots are both positive AND hard negative!\")\nelif max_overlap_any > 0:\n    print(\"  WARN: Some positives appear in the far set\")\nelse:\n    print(\"  PASS: Zero overlap between pos and neg index sets\")\n\nif len(cc['pos_phys_dists']) > 0:\n    pct_pos_ok = (cc['pos_phys_dists'] < cc['r_far_threshold']).mean() * 100\n    print(f\"  Pos below r_far: {pct_pos_ok:.1f}% (should be ~100%)\")\nif len(cc['hard_phys_dists']) > 0:\n    pct_hard_ok = (cc['hard_phys_dists'] >= cc['r_far_threshold']).mean() * 100\n    print(f\"  Hard neg above r_far: {pct_hard_ok:.1f}% (should be ~100%)\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING ENCODER \u2014 CROSS-SLIDE (SAME PATIENT)\")\nprint(\"=\" * 70)\n\nset_seed(SEED)\nencoder = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n\noutdir = '/home/ehtesamul/sc_st/model/gems_liver_crossslide'\nos.makedirs(outdir, exist_ok=True)\n\nencoder, projector, discriminator, hist = train_encoder(\n    inference_dropout_prob=0.5,\n    model=encoder,\n    st_gene_expr=st_expr,\n    st_coords=st_coords,\n    sc_gene_expr=inf_expr,\n    slide_ids=slide_ids,\n    sc_slide_ids=inf_slide_ids,\n    sc_patient_ids=inf_patient_ids,\n    n_epochs=2000,\n    batch_size=256,\n    lr=1e-4,\n    device=device,\n    outf=outdir,\n    st_source_ids=st_source_ids,\n    sc_source_ids=inf_source_ids,\n    use_source_adversary=False,\n    source_coral_weight=75.0,\n    stageA_obj='vicreg_adv',\n    vicreg_lambda_inv=25.0,\n    vicreg_lambda_var=50.0,\n    vicreg_lambda_cov=1.0,\n    vicreg_gamma=1.0,\n    vicreg_eps=1e-4,\n    vicreg_project_dim=256,\n    vicreg_use_projector=False,\n    vicreg_float32_stats=True,\n    vicreg_ddp_gather=False,\n    aug_gene_dropout=0.25,\n    aug_gauss_std=0.01,\n    aug_scale_jitter=0.1,\n    adv_slide_weight=75.0,\n    patient_coral_weight=0.0,\n    mmd_weight=30.0,\n    mmd_use_l2norm=True,\n    mmd_ramp=True,\n    adv_warmup_epochs=50,\n    adv_ramp_epochs=200,\n    grl_alpha_max=1.0,\n    disc_hidden=512,\n    disc_dropout=0.1,\n    stageA_balanced_slides=True,\n    adv_representation_mode='clean',\n    adv_use_layernorm=False,\n    adv_log_diagnostics=True,\n    adv_log_grad_norms=False,\n    use_local_align=True,\n    return_aux=True,\n    local_align_bidirectional=True,\n    local_align_weight=0.0,\n    local_align_tau_z=0.07,\n    seed=SEED,\n    use_best_checkpoint=True,\n    coral_raw_weight=2.0,\n    knn_weight=0.0,              # OFF \u2014 this preserves expression neighborhoods (bad for liver)\n    spatial_nce_weight=3.0,       # ON \u2014 enforces physical neighborhoods (support-set)\n    spatial_nce_k_phys=20,\n    spatial_nce_far_mult=4.0,\n    spatial_nce_n_hard=20,\n    spatial_nce_tau=0.1,\n    spatial_nce_n_rand_neg=128,\n    spatial_nce_n_anchors=64,     # anchors per step for support-set NCE\n    # ---- Two-phase training (ChatGPT Step 2) ----\n    two_phase_training=True,      # Phase 1: VICReg+NCE only, Phase 2: add alignment\n    phase1_epochs=1000,           # ~where overlap saturates based on prior runs\n    phase2_lr_factor=0.1,         # 10x lower LR in Phase 2 to preserve spatial geometry\n)\n\nprint(\"\\n\u2713 Training complete!\")\ntorch.save(encoder.state_dict(), f'{outdir}/encoder_final_trained.pt')\nprint(f\"\u2713 Saved to: {outdir}/encoder_final_trained.pt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPUTING EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    Z_st = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "            for name, X in X_st.items()}\n",
    "    Z_inf = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "             for name, X in X_inf.items()}\n",
    "\n",
    "for name, Z in {**Z_st, **Z_inf}.items():\n",
    "    print(f\"  {name}: {Z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained encoder\n",
    "encoder = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n",
    "encoder.load_state_dict(torch.load(\n",
    "    '/home/ehtesamul/sc_st/model/gems_liver_crossslide/encoder_final_trained.pt',\n",
    "    map_location=device\n",
    "))\n",
    "encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Compute embeddings\n",
    "with torch.no_grad():\n",
    "    Z_st = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "            for name, X in X_st.items()}\n",
    "    Z_inf = {name: encoder(torch.tensor(X, dtype=torch.float32, device=device)).cpu()\n",
    "             for name, X in X_inf.items()}\n",
    "\n",
    "for name, Z in {**Z_st, **Z_inf}.items():\n",
    "    print(f\"  {name}: {Z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis A: Embedding kNN vs Physical kNN ===\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS A: Embedding kNN vs Physical Neighborhoods (ST only)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in ST_PATHS:\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n = Z.shape[0]\n",
    "\n",
    "    tree_phys = cKDTree(coords)\n",
    "    tree_emb = cKDTree(Z)\n",
    "\n",
    "    rng = np.random.default_rng(SEED)\n",
    "\n",
    "    for k in [10, 20, 50]:\n",
    "        # physical distances of embedding kNN\n",
    "        _, emb_idx = tree_emb.query(Z, k=k + 1)\n",
    "        emb_idx = emb_idx[:, 1:]  # exclude self\n",
    "        emb_phys_dists = np.array([\n",
    "            np.median(np.linalg.norm(coords[emb_idx[i]] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "\n",
    "        # physical distances of true physical kNN\n",
    "        _, phys_idx = tree_phys.query(coords, k=k + 1)\n",
    "        phys_idx = phys_idx[:, 1:]\n",
    "        phys_phys_dists = np.array([\n",
    "            np.median(np.linalg.norm(coords[phys_idx[i]] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "\n",
    "        # random baseline\n",
    "        rand_dists = np.array([\n",
    "            np.median(np.linalg.norm(coords[rng.choice(n, k, replace=False)] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "\n",
    "        print(f\"\\n  {name} | k={k}\")\n",
    "        print(f\"    Physical kNN median dist:  {np.median(phys_phys_dists):.2f}\")\n",
    "        print(f\"    Embedding kNN median dist: {np.median(emb_phys_dists):.2f}\")\n",
    "        print(f\"    Random median dist:        {np.median(rand_dists):.2f}\")\n",
    "\n",
    "    # Histogram for k=20\n",
    "    _, emb_idx = tree_emb.query(Z, k=21)\n",
    "    emb_idx = emb_idx[:, 1:]\n",
    "    _, phys_idx = tree_phys.query(coords, k=21)\n",
    "    phys_idx = phys_idx[:, 1:]\n",
    "\n",
    "    emb_d = [np.median(np.linalg.norm(coords[emb_idx[i]] - coords[i], axis=1)) for i in range(n)]\n",
    "    phys_d = [np.median(np.linalg.norm(coords[phys_idx[i]] - coords[i], axis=1)) for i in range(n)]\n",
    "    rand_d = [np.median(np.linalg.norm(coords[rng.choice(n, 20, replace=False)] - coords[i], axis=1)) for i in range(n)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.hist(phys_d, bins=50, alpha=0.5, label='Physical kNN', density=True)\n",
    "    ax.hist(emb_d, bins=50, alpha=0.5, label='Embedding kNN', density=True)\n",
    "    ax.hist(rand_d, bins=50, alpha=0.5, label='Random', density=True)\n",
    "    ax.set_xlabel('Median physical distance to k=20 neighbors')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{name}: kNN Physical Distance Distributions')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis B: Moran's I on Embedding Dimensions (no libpysal) ===\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS B: Spatial Autocorrelation of Embeddings (Moran's I)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def morans_i_fast(x, idx, n):\n",
    "    \"\"\"Moran's I with precomputed kNN indices.\"\"\"\n",
    "    xm = x - x.mean()\n",
    "    denom = np.sum(xm ** 2)\n",
    "    if denom < 1e-12:\n",
    "        return 0.0, 1.0\n",
    "    k = idx.shape[1]\n",
    "    W = n * k\n",
    "    num = np.sum(xm[:, None] * xm[idx]).sum() if False else sum(np.sum(xm[i] * xm[idx[i]]) for i in range(n))\n",
    "    # vectorized version:\n",
    "    num = np.einsum('i,ij->', xm, xm[idx])\n",
    "    I = (n / W) * (num / denom)\n",
    "    EI = -1.0 / (n - 1)\n",
    "    # Variance (normality assumption, simplified)\n",
    "    VI = max(1.0 / (n - 1), 1e-10)  # conservative approx\n",
    "    z = (I - EI) / np.sqrt(VI / n)\n",
    "    p = 2 * norm.sf(abs(z))\n",
    "    return I, p\n",
    "\n",
    "for name in ST_PATHS:\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n, d = Z.shape\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    _, idx = tree.query(coords, k=11)\n",
    "    idx = idx[:, 1:]  # exclude self\n",
    "\n",
    "    morans = np.zeros(d)\n",
    "    pvals = np.zeros(d)\n",
    "    for j in range(d):\n",
    "        morans[j], pvals[j] = morans_i_fast(Z[:, j], idx, n)\n",
    "\n",
    "    _, pvals_fdr, _, _ = multipletests(pvals, method='fdr_bh')\n",
    "    n_sig = np.sum(pvals_fdr < 0.05)\n",
    "\n",
    "    # Embedding norm\n",
    "    Z_norm = np.linalg.norm(Z, axis=1)\n",
    "    I_norm, p_norm = morans_i_fast(Z_norm, idx, n)\n",
    "\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    Embedding dims: {d}\")\n",
    "    print(f\"    Spatially autocorrelated dims (FDR<0.05): {n_sig}/{d} ({100*n_sig/d:.1f}%)\")\n",
    "    print(f\"    Moran's I \u2014 mean: {morans.mean():.4f}, median: {np.median(morans):.4f}\")\n",
    "    print(f\"    Moran's I of embedding norm: {I_norm:.4f} (p={p_norm:.4e})\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3))\n",
    "    ax.hist(morans, bins=30, edgecolor='k', alpha=0.7)\n",
    "    ax.axvline(0, color='red', linestyle='--', label='No autocorrelation')\n",
    "    ax.set_xlabel(\"Moran's I\")\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f\"{name}: Moran's I across {d} embedding dims\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis E: Distribution Checks ===\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS E: Distribution Checks (ST vs Inference)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "all_stats = {}\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    lib_size = X.sum(axis=1)\n",
    "    genes_detected = (X > 0).sum(axis=1)\n",
    "    sparsity = (X == 0).mean() * 100\n",
    "    all_stats[name] = {\n",
    "        'lib_size_median': np.median(lib_size),\n",
    "        'lib_size_std': np.std(lib_size),\n",
    "        'genes_detected_median': np.median(genes_detected),\n",
    "        'sparsity_pct': sparsity,\n",
    "    }\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Library size \u2014 median: {np.median(lib_size):.1f}, std: {np.std(lib_size):.1f}\")\n",
    "    print(f\"    Genes detected \u2014 median: {np.median(genes_detected):.0f}\")\n",
    "    print(f\"    Sparsity: {sparsity:.1f}%\")\n",
    "    print()\n",
    "\n",
    "# Library size distributions\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    axes[0].hist(X.sum(axis=1), bins=50, alpha=0.5, label=name, density=True)\n",
    "axes[0].set_xlabel('Library size (post-norm)')\n",
    "axes[0].set_title('Library Size Distribution')\n",
    "axes[0].legend(fontsize=7)\n",
    "\n",
    "# Genes detected\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    axes[1].hist((X > 0).sum(axis=1), bins=50, alpha=0.5, label=name, density=True)\n",
    "axes[1].set_xlabel('Genes detected per spot')\n",
    "axes[1].set_title('Detected Genes Distribution')\n",
    "axes[1].legend(fontsize=7)\n",
    "\n",
    "# Per-gene mean expression\n",
    "for name, X in {**X_st, **X_inf}.items():\n",
    "    gene_means = X.mean(axis=0)\n",
    "    axes[2].hist(gene_means, bins=50, alpha=0.5, label=name, density=True)\n",
    "axes[2].set_xlabel('Per-gene mean expression')\n",
    "axes[2].set_title('Gene Mean Distribution')\n",
    "axes[2].legend(fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Analysis C: Spatially Variable Genes ===\n",
    "from scipy.spatial import cKDTree\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS C: Spatially Variable Gene Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in list(ST_PATHS.keys())[:1]:\n",
    "    adata = st_data[name].copy()\n",
    "    adata = adata[:, common_genes]\n",
    "    coords = adata.obsm['spatial']\n",
    "    X_dense = adata.X.toarray() if hasattr(adata.X, 'toarray') else adata.X\n",
    "    n = X_dense.shape[0]\n",
    "\n",
    "    tree = cKDTree(coords)\n",
    "    _, idx = tree.query(coords, k=11)\n",
    "    idx = idx[:, 1:]\n",
    "\n",
    "    morans_vals = np.empty(X_dense.shape[1])\n",
    "    for g in range(X_dense.shape[1]):\n",
    "        x = X_dense[:, g]\n",
    "        xm = x - x.mean()\n",
    "        denom = np.sum(xm ** 2)\n",
    "        if denom < 1e-12:\n",
    "            morans_vals[g] = 0.0\n",
    "            continue\n",
    "        num = np.einsum('i,ij->', xm, xm[idx])\n",
    "        W = n * 10\n",
    "        morans_vals[g] = (n / W) * (num / denom)\n",
    "\n",
    "    svg_results = pd.DataFrame({'I': morans_vals}, index=common_genes)\n",
    "    svg_results = svg_results.sort_values('I', ascending=False)\n",
    "\n",
    "    n_sig_svg = (svg_results['I'] > 0.1).sum()\n",
    "    top300_mean_I = svg_results.head(300)['I'].mean()\n",
    "\n",
    "    detection_rate = (X_dense > 0).mean(axis=0)\n",
    "    n_ultra_sparse = (detection_rate < 0.01).sum()\n",
    "\n",
    "    print(f\"\\n  {name} ({len(common_genes)} common genes):\")\n",
    "    print(f\"    Genes with Moran's I > 0.1: {n_sig_svg}\")\n",
    "    print(f\"    Top 300 SVGs \u2014 mean Moran's I: {top300_mean_I:.4f}\")\n",
    "    print(f\"    Ultra-sparse genes (<1% detection): {n_ultra_sparse}/{len(common_genes)}\")\n",
    "    print(f\"\\n  Top 20 SVGs:\")\n",
    "    print(svg_results.head(20)[['I']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ablation 2: Embedding kNN under different transforms ===\n",
    "from scipy.spatial import cKDTree\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION 2: Embedding kNN with different similarity transforms\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "K_VALUES = [10, 20, 50]\n",
    "\n",
    "def compute_knn_phys_dist(Z_np, coords, k):\n",
    "    \"\"\"Median physical distance of embedding kNN neighbors.\"\"\"\n",
    "    tree = cKDTree(Z_np)\n",
    "    _, idx = tree.query(Z_np, k=k + 1)\n",
    "    idx = idx[:, 1:]\n",
    "    diffs = coords[idx] - coords[:, None, :]\n",
    "    dists = np.sqrt((diffs ** 2).sum(axis=2))\n",
    "    return np.median(dists, axis=1)  # per-spot median\n",
    "\n",
    "for name in list(ST_PATHS.keys())[:1]:  # run on first ST slide\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n = Z.shape[0]\n",
    "\n",
    "    # Baselines\n",
    "    tree_phys = cKDTree(coords)\n",
    "\n",
    "    transforms = {\n",
    "        'Raw embedding': Z,\n",
    "        'L2-normalized (cosine)': Z / (np.linalg.norm(Z, axis=1, keepdims=True) + 1e-8),\n",
    "        'Per-dim z-scored': (Z - Z.mean(0)) / (Z.std(0) + 1e-8),\n",
    "        'PCA(Z) \u2192 32d': PCA(n_components=32, random_state=42).fit_transform(Z),\n",
    "        'PCA(Z) \u2192 64d': PCA(n_components=64, random_state=42).fit_transform(Z),\n",
    "    }\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        print(f\"\\n  {name} | k={k}\")\n",
    "        print(f\"  {'Transform':<30s} {'Median phys dist':>18s}\")\n",
    "        print(f\"  {'-'*30} {'-'*18}\")\n",
    "\n",
    "        # Physical baseline\n",
    "        _, phys_idx = tree_phys.query(coords, k=k + 1)\n",
    "        phys_idx = phys_idx[:, 1:]\n",
    "        phys_d = np.median(np.sqrt(((coords[phys_idx] - coords[:, None, :]) ** 2).sum(2)), axis=1)\n",
    "        print(f\"  {'Physical kNN (best)':<30s} {np.median(phys_d):>18.2f}\")\n",
    "\n",
    "        for tname, Z_t in transforms.items():\n",
    "            emb_d = compute_knn_phys_dist(Z_t, coords, k)\n",
    "            print(f\"  {tname:<30s} {np.median(emb_d):>18.2f}\")\n",
    "\n",
    "        # Random baseline\n",
    "        rng = np.random.default_rng(42)\n",
    "        rand_d = np.array([\n",
    "            np.median(np.linalg.norm(coords[rng.choice(n, k, replace=False)] - coords[i], axis=1))\n",
    "            for i in range(n)\n",
    "        ])\n",
    "        print(f\"  {'Random (worst)':<30s} {np.median(rand_d):>18.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ablation 3: Identifiability \u2014 crop a single spatial region ===\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION 3: Embedding kNN within a spatial crop (single region)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in list(ST_PATHS.keys())[:1]:\n",
    "    coords = st_data[name].obsm['spatial']\n",
    "    Z = Z_st[name].numpy()\n",
    "    n = Z.shape[0]\n",
    "\n",
    "    # Crop: take a central region covering ~25% of spots\n",
    "    cx, cy = np.median(coords, axis=0)\n",
    "    dists_to_center = np.sqrt((coords[:, 0] - cx)**2 + (coords[:, 1] - cy)**2)\n",
    "    radius = np.quantile(dists_to_center, 0.50)  # ~25% area \u2192 50% radius\n",
    "    mask = dists_to_center <= radius\n",
    "    n_crop = mask.sum()\n",
    "\n",
    "    coords_crop = coords[mask]\n",
    "    Z_crop = Z[mask]\n",
    "\n",
    "    print(f\"  {name}: cropped {n_crop}/{n} spots (radius={radius:.1f})\")\n",
    "\n",
    "    tree_phys = cKDTree(coords_crop)\n",
    "    tree_emb = cKDTree(Z_crop)\n",
    "\n",
    "    for k in [10, 20]:\n",
    "        if k >= n_crop:\n",
    "            continue\n",
    "\n",
    "        _, phys_idx = tree_phys.query(coords_crop, k=k + 1)\n",
    "        phys_idx = phys_idx[:, 1:]\n",
    "        phys_d = np.median(np.sqrt(((coords_crop[phys_idx] - coords_crop[:, None, :]) ** 2).sum(2)), axis=1)\n",
    "\n",
    "        _, emb_idx = tree_emb.query(Z_crop, k=k + 1)\n",
    "        emb_idx = emb_idx[:, 1:]\n",
    "        emb_d = np.median(np.sqrt(((coords_crop[emb_idx] - coords_crop[:, None, :]) ** 2).sum(2)), axis=1)\n",
    "\n",
    "        rng = np.random.default_rng(42)\n",
    "        rand_d = np.array([\n",
    "            np.median(np.linalg.norm(coords_crop[rng.choice(n_crop, k, replace=False)] - coords_crop[i], axis=1))\n",
    "            for i in range(n_crop)\n",
    "        ])\n",
    "\n",
    "        print(f\"\\n  Cropped region | k={k}\")\n",
    "        print(f\"    Physical kNN median dist:  {np.median(phys_d):.2f}\")\n",
    "        print(f\"    Embedding kNN median dist: {np.median(emb_d):.2f}\")\n",
    "        print(f\"    Random median dist:        {np.median(rand_d):.2f}\")\n",
    "        ratio = (np.median(emb_d) - np.median(phys_d)) / (np.median(rand_d) - np.median(phys_d) + 1e-8)\n",
    "        print(f\"    Emb\u2192Phys ratio (0=perfect, 1=random): {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ablation 1: Physical vs Embedding patch construction (ST4) ===\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ABLATION 1: Physical vs Embedding patch graph quality (ST4)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "name = 'liver_ST4'\n",
    "coords = inf_data[name].obsm['spatial']\n",
    "Z = Z_inf[name].numpy()\n",
    "n = Z.shape[0]\n",
    "\n",
    "K_PATCH = 20  # typical miniset neighborhood size\n",
    "\n",
    "# === Mode P: Physical kNN graph ===\n",
    "tree_phys = cKDTree(coords)\n",
    "_, phys_idx = tree_phys.query(coords, k=K_PATCH + 1)\n",
    "phys_idx = phys_idx[:, 1:]\n",
    "\n",
    "# === Mode Z: Embedding kNN graph ===\n",
    "tree_emb = cKDTree(Z)\n",
    "_, emb_idx = tree_emb.query(Z, k=K_PATCH + 1)\n",
    "emb_idx = emb_idx[:, 1:]\n",
    "\n",
    "# --- Metric 1: Physical compactness of patches ---\n",
    "def patch_compactness(coords, idx):\n",
    "    \"\"\"Mean diameter of patches (max pairwise dist within each patch).\"\"\"\n",
    "    diameters = []\n",
    "    for i in range(len(idx)):\n",
    "        patch_coords = coords[idx[i]]\n",
    "        center = patch_coords.mean(axis=0)\n",
    "        dists = np.sqrt(((patch_coords - center) ** 2).sum(axis=1))\n",
    "        diameters.append(dists.max())\n",
    "    return np.array(diameters)\n",
    "\n",
    "phys_compact = patch_compactness(coords, phys_idx)\n",
    "emb_compact = patch_compactness(coords, emb_idx)\n",
    "\n",
    "print(f\"\\n  Patch compactness (physical radius of k={K_PATCH} neighborhood):\")\n",
    "print(f\"    Physical patches \u2014 median: {np.median(phys_compact):.2f}, mean: {np.mean(phys_compact):.2f}\")\n",
    "print(f\"    Embedding patches \u2014 median: {np.median(emb_compact):.2f}, mean: {np.mean(emb_compact):.2f}\")\n",
    "print(f\"    Ratio (emb/phys): {np.median(emb_compact) / np.median(phys_compact):.2f}x\")\n",
    "\n",
    "# --- Metric 2: Graph connectivity overlap ---\n",
    "# What fraction of embedding neighbors are also physical neighbors?\n",
    "overlap_frac = np.array([\n",
    "    len(set(phys_idx[i]) & set(emb_idx[i])) / K_PATCH\n",
    "    for i in range(n)\n",
    "])\n",
    "\n",
    "print(f\"\\n  Neighbor overlap (phys \u2229 emb / k):\")\n",
    "print(f\"    Mean overlap: {overlap_frac.mean():.4f}\")\n",
    "print(f\"    Median overlap: {np.median(overlap_frac):.4f}\")\n",
    "print(f\"    % spots with zero overlap: {(overlap_frac == 0).mean() * 100:.1f}%\")\n",
    "\n",
    "# --- Metric 3: Pairwise distance preservation ---\n",
    "# For each patch, compute pairwise distances in coord space\n",
    "# Compare consistency between the two modes\n",
    "def pairwise_dist_stats(coords, idx):\n",
    "    \"\"\"Per-patch mean pairwise distance in physical space.\"\"\"\n",
    "    means = []\n",
    "    for i in range(len(idx)):\n",
    "        pc = coords[idx[i]]\n",
    "        d = np.sqrt(((pc[:, None] - pc[None, :]) ** 2).sum(axis=2))\n",
    "        means.append(d[np.triu_indices(len(pc), k=1)].mean())\n",
    "    return np.array(means)\n",
    "\n",
    "phys_pw = pairwise_dist_stats(coords, phys_idx)\n",
    "emb_pw = pairwise_dist_stats(coords, emb_idx)\n",
    "\n",
    "print(f\"\\n  Mean pairwise physical distance within patches:\")\n",
    "print(f\"    Physical patches: {np.median(phys_pw):.2f}\")\n",
    "print(f\"    Embedding patches: {np.median(emb_pw):.2f}\")\n",
    "\n",
    "# --- Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(phys_compact, bins=50, alpha=0.6, label='Physical', density=True)\n",
    "axes[0].hist(emb_compact, bins=50, alpha=0.6, label='Embedding', density=True)\n",
    "axes[0].set_xlabel('Patch radius')\n",
    "axes[0].set_title('Patch Compactness')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(overlap_frac, bins=50, edgecolor='k', alpha=0.7)\n",
    "axes[1].set_xlabel('Neighbor overlap fraction')\n",
    "axes[1].set_title(f'Physical \u2229 Embedding (k={K_PATCH})')\n",
    "\n",
    "axes[2].hist(phys_pw, bins=50, alpha=0.6, label='Physical', density=True)\n",
    "axes[2].hist(emb_pw, bins=50, alpha=0.6, label='Embedding', density=True)\n",
    "axes[2].set_xlabel('Mean pairwise distance')\n",
    "axes[2].set_title('Within-Patch Distances')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle(f'{name}: Physical vs Embedding Patch Construction', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Summary verdict ---\n",
    "ratio = np.median(emb_compact) / np.median(phys_compact)\n",
    "print(f\"\\n  VERDICT:\")\n",
    "if ratio > 3.0 and np.median(overlap_frac) < 0.1:\n",
    "    print(f\"    Embedding patches are {ratio:.1f}x larger with {np.median(overlap_frac)*100:.1f}% overlap.\")\n",
    "    print(f\"    \u2192 Embedding kNN is NOT spatially local. Physical patches should be used for training.\")\n",
    "elif ratio > 1.5:\n",
    "    print(f\"    Embedding patches are {ratio:.1f}x larger \u2014 moderate mismatch.\")\n",
    "    print(f\"    \u2192 Consider hybrid approach or tighter embedding constraints.\")\n",
    "else:\n",
    "    print(f\"    Embedding patches are comparable ({ratio:.1f}x). kNN graph is reasonable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION: Domain Mixing Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_MAX = 5000\n",
    "K = 20\n",
    "set_seed(SEED)\n",
    "\n",
    "def subsample(X, n_max):\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "    if X.shape[0] <= n_max:\n",
    "        return X\n",
    "    return X[torch.randperm(X.shape[0])[:n_max]]\n",
    "\n",
    "Z_st_sub = {n: subsample(Z, N_MAX) for n, Z in Z_st.items()}\n",
    "Z_inf_sub = {n: subsample(Z, N_MAX) for n, Z in Z_inf.items()}\n",
    "\n",
    "Z_all = torch.cat(list(Z_st_sub.values()) + list(Z_inf_sub.values()), dim=0)\n",
    "Z_all_norm = F.normalize(Z_all, dim=1)\n",
    "N = Z_all.shape[0]\n",
    "\n",
    "# Domain labels\n",
    "domain_labels = []\n",
    "source_names = []\n",
    "for i, name in enumerate(list(Z_st_sub) + list(Z_inf_sub)):\n",
    "    n = (Z_st_sub if name in Z_st_sub else Z_inf_sub)[name].shape[0]\n",
    "    domain_labels.append(torch.full((n,), i, dtype=torch.long))\n",
    "    source_names.append(name)\n",
    "domain_labels = torch.cat(domain_labels)\n",
    "S = len(source_names)\n",
    "\n",
    "print(f\"Sources: {source_names}, Total: {N}, Domains: {S}\")\n",
    "\n",
    "# --- kNN Domain-Mixing ---\n",
    "print(f\"\\n[METRIC 1] kNN Domain-Mixing (k={K})\")\n",
    "D = torch.cdist(Z_all_norm, Z_all_norm)\n",
    "D.fill_diagonal_(float('inf'))\n",
    "_, knn_idx = torch.topk(D, k=K, dim=1, largest=False)\n",
    "knn_labels = domain_labels[knn_idx]\n",
    "\n",
    "p = torch.zeros(N, S)\n",
    "for s in range(S):\n",
    "    p[:, s] = (knn_labels == s).float().mean(dim=1)\n",
    "\n",
    "eps = 1e-10\n",
    "H_norm = (-torch.sum(p * torch.log(p + eps), dim=1) / np.log(S)).mean().item()\n",
    "iLISI_norm = ((1.0 / torch.sum(p ** 2, dim=1) - 1) / (S - 1)).mean().item()\n",
    "\n",
    "print(f\"  Normalized Neighbor Entropy: {H_norm:.4f}\")\n",
    "print(f\"  Normalized iLISI: {iLISI_norm:.4f}\")\n",
    "\n",
    "# --- Domain Classification (CV) ---\n",
    "print(f\"\\n[METRIC 2] Domain Classification (5-fold CV)\")\n",
    "Z_np, y_np = Z_all_norm.numpy(), domain_labels.numpy()\n",
    "clf = LogisticRegression(max_iter=2000, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(clf, Z_np, y_np, cv=cv, scoring='balanced_accuracy')\n",
    "chance = 1.0 / S\n",
    "print(f\"  Balanced Acc: {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f} (chance={chance:.4f})\")\n",
    "\n",
    "# --- Binary: Training ST vs Inference ST4 ---\n",
    "print(f\"\\n[METRIC 3] Binary ST(1-3) vs ST4 (5-fold CV)\")\n",
    "n_st_total = sum(Z_st_sub[n].shape[0] for n in Z_st_sub)\n",
    "y_binary = torch.cat([torch.zeros(n_st_total, dtype=torch.long),\n",
    "                       torch.ones(N - n_st_total, dtype=torch.long)]).numpy()\n",
    "cv_binary = cross_val_score(clf, Z_np, y_binary, cv=cv, scoring='balanced_accuracy')\n",
    "print(f\"  Balanced Acc (ST vs ST4): {cv_binary.mean():.4f} \u00b1 {cv_binary.std():.4f}\")\n",
    "\n",
    "# --- Centroid Analysis ---\n",
    "print(f\"\\n[CENTROID ANALYSIS]\")\n",
    "centroids = {}\n",
    "for name in Z_st_sub:\n",
    "    centroids[name] = Z_st_sub[name].mean(dim=0)\n",
    "for name in Z_inf_sub:\n",
    "    centroids[name] = Z_inf_sub[name].mean(dim=0)\n",
    "\n",
    "names = list(centroids)\n",
    "print(\"         \", \"  \".join([f\"{n:>12}\" for n in names]))\n",
    "for n1 in names:\n",
    "    row = f\"{n1:12s}\"\n",
    "    for n2 in names:\n",
    "        row += f\"  {(centroids[n1] - centroids[n2]).norm().item():12.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# --- Summary ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metric':<40} {'Value':<15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Neighbor Entropy (k=20)':<40} {H_norm:<15.4f}\")\n",
    "print(f\"{'iLISI (k=20)':<40} {iLISI_norm:<15.4f}\")\n",
    "print(f\"{'Domain Class. Acc (CV)':<40} {cv_scores.mean():<15.4f}\")\n",
    "print(f\"{'ST vs ST4 Acc (CV)':<40} {cv_binary.mean():<15.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PCA COMPARISON: Raw vs Embeddings\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_VIS = 3000\n",
    "set_seed(SEED)\n",
    "\n",
    "# Subsample matched\n",
    "def subsample_matched(X, Z, n_max):\n",
    "    X_t = torch.tensor(X, dtype=torch.float32) if not isinstance(X, torch.Tensor) else X.float()\n",
    "    Z_t = Z.float() if isinstance(Z, torch.Tensor) else torch.tensor(Z, dtype=torch.float32)\n",
    "    X_t, Z_t = X_t.cpu(), Z_t.cpu()\n",
    "    n = min(X_t.shape[0], n_max)\n",
    "    idx = torch.randperm(X_t.shape[0])[:n]\n",
    "    return X_t[idx].numpy(), Z_t[idx].numpy()\n",
    "\n",
    "X_vis, Z_vis, labels_vis = [], [], []\n",
    "for name in labels_str:\n",
    "    src = X_st if name in X_st else X_inf\n",
    "    src_z = Z_st if name in Z_st else Z_inf\n",
    "    x_sub, z_sub = subsample_matched(src[name], src_z[name], N_VIS)\n",
    "    X_vis.append(x_sub)\n",
    "    Z_vis.append(z_sub)\n",
    "    labels_vis.extend([name] * x_sub.shape[0])\n",
    "\n",
    "X_vis = np.vstack(X_vis)\n",
    "Z_vis = np.vstack(Z_vis)\n",
    "labels_vis = np.array(labels_vis)\n",
    "\n",
    "pca_x = PCA(n_components=2).fit(X_vis)\n",
    "X_pca = pca_x.transform(X_vis)\n",
    "var_x = pca_x.explained_variance_ratio_\n",
    "\n",
    "pca_z = PCA(n_components=2).fit(Z_vis)\n",
    "Z_pca = pca_z.transform(Z_vis)\n",
    "var_z = pca_z.explained_variance_ratio_\n",
    "\n",
    "# plt.rcParams.update({'font.family': 'Arial', 'font.weight': 'bold',\n",
    "#                      'axes.labelweight': 'bold', 'axes.titleweight': 'bold'})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "titles = [\n",
    "    f\"(a) PCA of raw expression (log1p)\\nVar: PC1={var_x[0]:.1%}, PC2={var_x[1]:.1%}\",\n",
    "    f\"(b) PCA of encoder embeddings\\nVar: PC1={var_z[0]:.1%}, PC2={var_z[1]:.1%}\",\n",
    "]\n",
    "for ax, data, title in zip(axes, [X_pca, Z_pca], titles):\n",
    "    for label in labels_str:\n",
    "        mask = labels_vis == label\n",
    "        ax.scatter(data[mask, 0], data[mask, 1], c=colors_map[label], label=label,\n",
    "                   s=25, alpha=0.65, edgecolors='white', linewidths=0.5)\n",
    "    ax.set_xlabel('PC1', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('PC2', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_facecolor('#FAFAFA')\n",
    "    ax.tick_params(labelsize=11)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('#CCCCCC')\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "axes[0].legend(loc='best', fontsize=11, framealpha=0.98, edgecolor='#888888')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('liver_pca_comparison.svg', format='svg', bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\n\" + \"=\" * 70)\n",
    "# print(\"t-SNE COMPARISON: Raw vs Embeddings\")\n",
    "# print(\"=\" * 70)\n",
    "\n",
    "# N_TSNE = 2000\n",
    "# set_seed(SEED)\n",
    "\n",
    "# X_tsne_all, Z_tsne_all, labels_tsne = [], [], []\n",
    "# for name in labels_str:\n",
    "#     src = X_st if name in X_st else X_inf\n",
    "#     src_z = Z_st if name in Z_st else Z_inf\n",
    "#     x_sub, z_sub = subsample_matched(src[name], src_z[name], N_TSNE)\n",
    "#     X_tsne_all.append(x_sub)\n",
    "#     Z_tsne_all.append(z_sub)\n",
    "#     labels_tsne.extend([name] * x_sub.shape[0])\n",
    "\n",
    "# X_tsne_all = np.vstack(X_tsne_all)\n",
    "# Z_tsne_all = np.vstack(Z_tsne_all)\n",
    "# labels_tsne = np.array(labels_tsne)\n",
    "# Z_tsne_norm = F.normalize(torch.tensor(Z_tsne_all), dim=1).numpy()\n",
    "\n",
    "# print(\"Computing t-SNE for raw expression...\")\n",
    "# X_tsne_proj = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000).fit_transform(X_tsne_all)\n",
    "\n",
    "# print(\"Computing t-SNE for embeddings...\")\n",
    "# Z_tsne_proj = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000).fit_transform(Z_tsne_norm)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "# fig.patch.set_facecolor('white')\n",
    "\n",
    "# for ax, data, title in zip(axes, [X_tsne_proj, Z_tsne_proj],\n",
    "#     ['(A) t-SNE on Raw Expression (Before)', '(B) t-SNE on Embeddings (After)']):\n",
    "#     for label in labels_str:\n",
    "#         mask = labels_tsne == label\n",
    "#         ax.scatter(data[mask, 0], data[mask, 1], c=colors_map[label],\n",
    "#                    label=label, alpha=0.5, s=20, edgecolors='none')\n",
    "#     ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "#     ax.set_xlabel('t-SNE 1', fontsize=12)\n",
    "#     ax.set_ylabel('t-SNE 2', fontsize=12)\n",
    "#     ax.legend(loc='best', fontsize=10, frameon=True)\n",
    "#     ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# plt.suptitle('Mouse Liver: Before vs After Alignment (Cross-Slide)',\n",
    "#              fontsize=16, fontweight='bold', y=0.98)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# EVALUATION METRICS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATION: Liver Cross-Slide Encoder\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "N_MAX = 5000\n",
    "set_seed(SEED)\n",
    "\n",
    "def subsample(X, n_max, device='cpu'):\n",
    "    if not isinstance(X, torch.Tensor):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        X = X.to(device)\n",
    "    if X.shape[0] <= n_max:\n",
    "        return X\n",
    "    return X[torch.randperm(X.shape[0], device=device)[:n_max]]\n",
    "\n",
    "Z_st_sub = {name: subsample(Z, N_MAX, 'cpu') for name, Z in Z_st.items()}\n",
    "Z_inf_sub = {name: subsample(Z, N_MAX, 'cpu') for name, Z in Z_inf.items()}\n",
    "\n",
    "print(\"Subsampled embeddings:\")\n",
    "for name, Z in {**Z_st_sub, **Z_inf_sub}.items():\n",
    "    print(f\"  {name}: {Z.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# TEST 1: ST(1-3) vs ST4 kNN mixing\n",
    "# ===================================================================\n",
    "print(\"\\n[ST-vs-ST4 MIXING] kNN domain distribution:\")\n",
    "\n",
    "Z_st_all = torch.cat(list(Z_st_sub.values()), dim=0)\n",
    "Z_inf_all = torch.cat(list(Z_inf_sub.values()), dim=0)\n",
    "Z_all = torch.cat([Z_st_all, Z_inf_all], dim=0)\n",
    "\n",
    "n_st = Z_st_all.shape[0]\n",
    "n_inf = Z_inf_all.shape[0]\n",
    "n_total = n_st + n_inf\n",
    "\n",
    "# Domain labels (0=ST training, 1=ST4 inference)\n",
    "domain_labels = torch.cat([\n",
    "    torch.zeros(n_st, dtype=torch.long),\n",
    "    torch.ones(n_inf, dtype=torch.long)\n",
    "])\n",
    "\n",
    "Z_all_norm = F.normalize(Z_all, dim=1)\n",
    "\n",
    "K = 20\n",
    "inf_start = n_st\n",
    "\n",
    "D_inf = torch.cdist(Z_all_norm[inf_start:], Z_all_norm)\n",
    "for i in range(n_inf):\n",
    "    D_inf[i, inf_start + i] = float('inf')\n",
    "\n",
    "_, knn_inf = torch.topk(D_inf, k=K, dim=1, largest=False)\n",
    "frac_same_inf = (domain_labels[knn_inf] == 1).float().mean().item()\n",
    "base_rate_inf = n_inf / n_total\n",
    "\n",
    "print(f\"  ST4 neighbors (K={K}):\")\n",
    "print(f\"    Same-domain (ST4) fraction: {frac_same_inf:.4f}\")\n",
    "print(f\"    Base rate (chance):         {base_rate_inf:.4f}\")\n",
    "\n",
    "if frac_same_inf < base_rate_inf + 0.10:\n",
    "    print(\"    \u2713 EXCELLENT mixing (ST4 not clustering)\")\n",
    "elif frac_same_inf < base_rate_inf + 0.20:\n",
    "    print(\"    \u2713 Good mixing\")\n",
    "else:\n",
    "    print(\"    \u26a0\ufe0f ST4 may be clustering\")\n",
    "\n",
    "# ===================================================================\n",
    "# TEST 2: Linear Probe (4-class separability)\n",
    "# ===================================================================\n",
    "n_sources = len(ST_PATHS) + len(INF_PATHS)\n",
    "print(f\"\\n[{n_sources}-CLASS PROBE] Linear separability test:\")\n",
    "\n",
    "class_labels_list = []\n",
    "class_idx = 0\n",
    "for name in ST_PATHS:\n",
    "    n_cells = Z_st_sub[name].shape[0]\n",
    "    class_labels_list.append(torch.full((n_cells,), class_idx, dtype=torch.long))\n",
    "    class_idx += 1\n",
    "for name in INF_PATHS:\n",
    "    n_cells = Z_inf_sub[name].shape[0]\n",
    "    class_labels_list.append(torch.full((n_cells,), class_idx, dtype=torch.long))\n",
    "    class_idx += 1\n",
    "\n",
    "class_labels = torch.cat(class_labels_list)\n",
    "\n",
    "Z_np = Z_all_norm.numpy()\n",
    "y_np = class_labels.numpy()\n",
    "\n",
    "probe = LogisticRegression(max_iter=5000, random_state=42, class_weight='balanced')\n",
    "probe.fit(Z_np, y_np)\n",
    "pred = probe.predict(Z_np)\n",
    "bal_acc = balanced_accuracy_score(y_np, pred)\n",
    "chance = 1.0 / n_sources\n",
    "\n",
    "print(f\"  Balanced accuracy: {bal_acc:.4f} (chance={chance:.3f})\")\n",
    "\n",
    "if bal_acc < 0.30:\n",
    "    print(\"  \u2713 EXCELLENT: Sources are very well-mixed\")\n",
    "elif bal_acc < 0.40:\n",
    "    print(\"  \u2713 Good: Moderate mixing\")\n",
    "elif bal_acc < 0.50:\n",
    "    print(\"  ~ Partial alignment\")\n",
    "else:\n",
    "    print(\"  \u26a0\ufe0f Sources are separable\")\n",
    "\n",
    "# ===================================================================\n",
    "# TEST 3: Centroid distances\n",
    "# ===================================================================\n",
    "print(\"\\n[CENTROID ANALYSIS] Cross-source distances:\")\n",
    "\n",
    "centroids = {}\n",
    "for name in ST_PATHS:\n",
    "    centroids[name] = Z_st_sub[name].mean(dim=0)\n",
    "for name in INF_PATHS:\n",
    "    centroids[name] = Z_inf_sub[name].mean(dim=0)\n",
    "\n",
    "names = list(centroids)\n",
    "print(\"\\nCentroid distance matrix:\")\n",
    "print(\"             \", \"  \".join([f\"{n:>12}\" for n in names]))\n",
    "\n",
    "for n1 in names:\n",
    "    row = f\"{n1:12s}\"\n",
    "    for n2 in names:\n",
    "        dist = (centroids[n1] - centroids[n2]).norm().item()\n",
    "        row += f\"  {dist:12.4f}\"\n",
    "    print(row)\n",
    "\n",
    "# ST-to-ST vs ST-to-ST4 distances\n",
    "st_names = list(ST_PATHS)\n",
    "inf_names = list(INF_PATHS)\n",
    "\n",
    "st_to_st = []\n",
    "for i, n1 in enumerate(st_names):\n",
    "    for n2 in st_names[i + 1:]:\n",
    "        st_to_st.append((centroids[n1] - centroids[n2]).norm().item())\n",
    "\n",
    "st_to_inf = []\n",
    "for n1 in st_names:\n",
    "    for n2 in inf_names:\n",
    "        st_to_inf.append((centroids[n1] - centroids[n2]).norm().item())\n",
    "\n",
    "print(f\"\\nAverage distances:\")\n",
    "print(f\"  ST-to-ST:  {np.mean(st_to_st):.4f} \u00b1 {np.std(st_to_st):.4f}\")\n",
    "print(f\"  ST-to-ST4: {np.mean(st_to_inf):.4f} \u00b1 {np.std(st_to_inf):.4f}\")\n",
    "ratio = np.mean(st_to_inf) / np.mean(st_to_st)\n",
    "print(f\"  Ratio (ST4/ST): {ratio:.2f}x\")\n",
    "\n",
    "if ratio < 1.5:\n",
    "    print(\"  \u2713 EXCELLENT: ST4 very close to training ST centroids\")\n",
    "elif ratio < 2.5:\n",
    "    print(\"  \u2713 Good: Reasonable cross-slide distance\")\n",
    "else:\n",
    "    print(\"  \u26a0\ufe0f Large slide gap remains\")\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LIVER ENCODER EVALUATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\u2713 ST4 mixing:        same-domain frac = {frac_same_inf:.4f} (base = {base_rate_inf:.4f})\")\n",
    "print(f\"\u2713 {n_sources}-class probe:   balanced acc = {bal_acc:.4f} (chance = {chance:.3f})\")\n",
    "print(f\"\u2713 Centroid ratio:    ST-to-ST4 / ST-to-ST = {ratio:.2f}x\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===================================================================\n# METRIC CONSISTENCY DIAGNOSTIC v4 \u2014 FULL CHAIN VERIFICATION\n# ===================================================================\n# v2 proved overlap@20 = 0.02 on saved model (random-level).\n# v3 tested cosine vs euclidean + checkpoint integrity.\n# v4 adds:\n#   - Compare encoder_final_new.pt vs encoder_final_trained.pt\n#   - Test both files against the same overlap metric\n#   - Check if the model learned ANYTHING (weight diff + loss comparison)\n# ===================================================================\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom scipy.spatial import cKDTree\nimport os\nfrom ssl_utils import (set_seed, precompute_spatial_nce_structures,\n                       compute_knn_locality_metrics, compute_spatial_infonce_supportset)\nfrom core_models_et_p1 import SharedEncoder\n\nprint(\"=\" * 70)\nprint(\"DIAGNOSTIC v4: FULL CHECKPOINT CHAIN VERIFICATION\")\nprint(\"=\" * 70)\n\nK = 20\nCKPT_DIR = '/home/ehtesamul/sc_st/model/gems_liver_crossslide'\n\n# Precompute pos_idx (same as training uses)\nspatial_nce_data = precompute_spatial_nce_structures(\n    st_coords=st_coords, st_gene_expr=st_expr, slide_ids=slide_ids,\n    k_phys=20, far_mult=4.0, n_hard=20, device=device,\n)\npos_idx = spatial_nce_data['pos_idx']\n\n# ===================================================================\n# TEST 0: Check which checkpoint files exist\n# ===================================================================\nprint(\"\\n[TEST 0] Checkpoint files on disk\")\nprint(\"-\" * 60)\nfor fname in ['encoder_final_new.pt', 'encoder_final_trained.pt']:\n    fpath = os.path.join(CKPT_DIR, fname)\n    if os.path.exists(fpath):\n        fsize = os.path.getsize(fpath) / 1024\n        mtime = os.path.getmtime(fpath)\n        import datetime\n        mtime_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')\n        print(f\"  {fname}: {fsize:.1f} KB, modified {mtime_str}\")\n    else:\n        print(f\"  {fname}: NOT FOUND\")\n\n# ===================================================================\n# TEST 1: Load BOTH checkpoint files and compare weights\n# ===================================================================\nprint(\"\\n[TEST 1] Compare encoder_final_new.pt vs encoder_final_trained.pt\")\nprint(\"-\" * 60)\n\nmodels = {}\nfor fname in ['encoder_final_new.pt', 'encoder_final_trained.pt']:\n    fpath = os.path.join(CKPT_DIR, fname)\n    if not os.path.exists(fpath):\n        print(f\"  SKIP: {fname} not found\")\n        continue\n    m = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\n    m.load_state_dict(torch.load(fpath, map_location=device))\n    m.to(device).eval()\n    models[fname] = m\n\nif len(models) == 2:\n    sd1 = models['encoder_final_new.pt'].state_dict()\n    sd2 = models['encoder_final_trained.pt'].state_dict()\n    max_diff = 0.0\n    for k in sd1:\n        d = (sd1[k].cpu().float() - sd2[k].cpu().float()).abs().max().item()\n        max_diff = max(max_diff, d)\n    print(f\"  Max param diff between files: {max_diff:.2e}\")\n    if max_diff < 1e-6:\n        print(\"  IDENTICAL \u2014 both files have the same weights\")\n    else:\n        print(\"  DIFFERENT \u2014 files diverge!\")\n\n# ===================================================================\n# TEST 2: Random model (same seed as training init)\n# ===================================================================\nprint(\"\\n[TEST 2] Random model baseline\")\nprint(\"-\" * 60)\nset_seed(SEED)\nencoder_random = SharedEncoder(n_genes=n_genes, n_embedding=[512, 256, 128], dropout=0.1)\nencoder_random.to(device).eval()\n\n# ===================================================================\n# TEST 3: Compare trained vs random weights\n# ===================================================================\nprint(\"\\n[TEST 3] Trained vs Random weight comparison\")\nprint(\"-\" * 60)\n\n# Use whichever trained file exists\ntrained_key = 'encoder_final_trained.pt' if 'encoder_final_trained.pt' in models else list(models.keys())[0]\nencoder_trained = models[trained_key]\n\nsd_random = encoder_random.state_dict()\nsd_trained = encoder_trained.state_dict()\n\nparams_differ = 0\nparams_same = 0\ntotal_l2_diff = 0.0\nfor k in sd_random:\n    r = sd_random[k].cpu().float()\n    t = sd_trained[k].cpu().float()\n    max_diff = (r - t).abs().max().item()\n    l2_diff = (r - t).norm().item()\n    total_l2_diff += l2_diff\n    if max_diff > 1e-6:\n        params_differ += 1\n        if params_differ <= 5:\n            print(f\"  {k}: DIFFERENT \u2014 max_diff={max_diff:.6f}, L2_diff={l2_diff:.4f}\")\n            print(f\"    random[:5]: {r.flatten()[:5].tolist()}\")\n            print(f\"    trained[:5]: {t.flatten()[:5].tolist()}\")\n    else:\n        params_same += 1\n\nprint(f\"\\n  Summary: {params_differ} different, {params_same} same, total L2 diff={total_l2_diff:.4f}\")\nif params_differ == 0:\n    print(\"  *** CRITICAL: Trained model == Random init! Checkpoint is CORRUPT ***\")\nelse:\n    print(f\"  OK: Model has learned (weights changed)\")\n\n# ===================================================================\n# TEST 4: Overlap@20 with EUCLIDEAN vs COSINE kNN\n# ===================================================================\nprint(\"\\n[TEST 4] Overlap@20 \u2014 Euclidean vs Cosine kNN\")\nprint(\"-\" * 60)\n\nslide_names_list = list(ST_PATHS.keys())\noffset = 0\nslide_ranges = {}\nfor i, name in enumerate(slide_names_list):\n    slide_ranges[name] = (offset, offset + ns[i])\n    offset += ns[i]\n\nfor enc_name, enc_model in [(\"RANDOM\", encoder_random), (\"TRAINED\", encoder_trained)]:\n    enc_model.eval()\n    with torch.no_grad():\n        z_parts = []\n        for ci in range(0, st_expr.shape[0], 512):\n            z_parts.append(enc_model(st_expr[ci:ci+512]))\n        z_all = torch.cat(z_parts, dim=0)\n\n    z_all_norm = F.normalize(z_all, dim=1)\n\n    print(f\"\\n  === {enc_name} ===\")\n\n    # Embedding norm stats\n    norms = z_all.norm(dim=1)\n    print(f\"    Norms: mean={norms.mean():.3f}, std={norms.std():.3f}, \"\n          f\"min={norms.min():.3f}, max={norms.max():.3f}, CV={norms.std()/norms.mean():.3f}\")\n\n    for si, name in enumerate(slide_names_list):\n        s, e = slide_ranges[name]\n        n_s = e - s\n        z_slide = z_all[s:e]\n        z_slide_norm = z_all_norm[s:e]\n\n        # Euclidean kNN\n        dists_euc = torch.cdist(z_slide, z_slide)\n        dists_euc.fill_diagonal_(float('inf'))\n        _, euc_knn = torch.topk(dists_euc, k=K, dim=1, largest=False)\n        euc_knn_global = euc_knn + s\n\n        # Cosine kNN\n        sims_cos = z_slide_norm @ z_slide_norm.T\n        sims_cos.fill_diagonal_(-float('inf'))\n        _, cos_knn = torch.topk(sims_cos, k=K, dim=1, largest=True)\n        cos_knn_global = cos_knn + s\n\n        # Physical kNN\n        phys_knn_global = pos_idx[s:e]\n\n        overlaps_euc = []\n        overlaps_cos = []\n        euc_cos_agree = []\n\n        for i in range(n_s):\n            euc_set = set(euc_knn_global[i].cpu().tolist())\n            cos_set = set(cos_knn_global[i].cpu().tolist())\n            phys_set = set(phys_knn_global[i].cpu().tolist())\n            phys_set.discard(-1)\n            if len(phys_set) > 0:\n                overlaps_euc.append(len(euc_set & phys_set) / len(phys_set))\n                overlaps_cos.append(len(cos_set & phys_set) / len(phys_set))\n            euc_cos_agree.append(len(euc_set & cos_set) / K)\n\n        print(f\"\\n    {name} (n={n_s}):\")\n        print(f\"      Euclidean overlap@{K}: {np.mean(overlaps_euc):.4f}\")\n        print(f\"      Cosine overlap@{K}:    {np.mean(overlaps_cos):.4f}\")\n        print(f\"      Euc-Cos agreement:     {np.mean(euc_cos_agree):.4f}\")\n\n    # Also test with the EXACT same function used during training\n    ov = compute_knn_locality_metrics(\n        model=enc_model, st_gene_expr=st_expr,\n        st_coords=st_coords, slide_ids=slide_ids,\n        phys_knn_idx=pos_idx, k=20, n_sample=300,\n    )\n    print(f\"\\n    compute_knn_locality_metrics: overlap={ov['overlap_mean']:.4f}, \"\n          f\"emb_phys_dist={ov['emb_phys_dist_median']:.4f}\")\n\n# ===================================================================\n# TEST 5: NCE loss comparison (trained should be LOWER than random)\n# ===================================================================\nprint(\"\\n\\n[TEST 5] NCE Loss: Random vs Trained\")\nprint(\"-\" * 60)\n\nfor enc_name, enc_model in [(\"RANDOM\", encoder_random), (\"TRAINED\", encoder_trained)]:\n    enc_model.eval()\n    with torch.no_grad():\n        z_parts = []\n        for ci in range(0, st_expr.shape[0], 512):\n            z_parts.append(enc_model(st_expr[ci:ci+512]))\n        z_cache = torch.cat(z_parts, dim=0)\n\n    enc_model.train()\n    loss_nce = compute_spatial_infonce_supportset(\n        model=enc_model,\n        st_gene_expr=st_expr,\n        pos_idx=pos_idx,\n        far_mask=spatial_nce_data['far_mask'],\n        hard_neg=spatial_nce_data['hard_neg'],\n        slide_ids=slide_ids,\n        tau=0.1,\n        n_rand_neg=128,\n        n_anchors_per_step=64,\n        slide_override=0,\n        z_cache=z_cache.detach(),\n        n_hard_mine=20,\n    )\n    enc_model.eval()\n    print(f\"  {enc_name}: NCE loss = {loss_nce.item():.4f}\")\n\n# ===================================================================\n# TEST 6: Training history check (if available)\n# ===================================================================\nprint(\"\\n\\n[TEST 6] Training history check\")\nprint(\"-\" * 60)\nhist_path = os.path.join(CKPT_DIR, 'stageA_vicreg_history.json')\nif os.path.exists(hist_path):\n    import json\n    with open(hist_path) as f:\n        hist = json.load(f)\n    # Check the overlap history\n    ov_S = hist.get('locality_overlap_after_S', [])\n    ov_M = hist.get('locality_overlap_after_M', [])\n    epochs = hist.get('epoch', [])\n\n    # Non-zero overlap values (computed every 100 epochs)\n    nonzero_S = [(e, v) for e, v in zip(epochs, ov_S) if v > 0.001]\n    nonzero_M = [(e, v) for e, v in zip(epochs, ov_M) if v > 0.001]\n\n    print(f\"  History length: {len(epochs)} epochs\")\n    print(f\"  Non-zero overlap measurements: {len(nonzero_S)} (after S), {len(nonzero_M)} (after M)\")\n    if nonzero_M:\n        print(f\"\\n  overlap@20 after Step M (every 100 epochs):\")\n        for e, v in nonzero_M:\n            print(f\"    epoch {e:5d}: {v:.4f}\")\n        best_e, best_v = max(nonzero_M, key=lambda x: x[1])\n        print(f\"\\n  Best overlap: {best_v:.4f} at epoch {best_e}\")\nelse:\n    print(\"  History file not found\")\n\n# ===================================================================\n# SUMMARY\n# ===================================================================\nprint(\"\\n\\n\" + \"=\" * 70)\nprint(\"DIAGNOSTIC v4 SUMMARY\")\nprint(\"=\" * 70)\nprint(\"\"\"\nKEY TESTS:\n  TEST 1: Are encoder_final_new.pt and encoder_final_trained.pt identical?\n  TEST 3: Are trained weights different from random init?\n  TEST 4: Cosine vs Euclidean overlap \u2014 which is higher?\n  TEST 5: NCE loss \u2014 is trained lower than random?\n  TEST 6: Training history \u2014 what overlap was recorded?\n\nINTERPRETATION:\n  If TEST 3 says identical    \u2192 checkpoint save/restore is broken\n  If TEST 4 cosine >> euclid  \u2192 metric should use cosine (NCE trains cosine)\n  If TEST 5 losses are same   \u2192 model didn't learn spatial structure\n  If TEST 6 shows 0.69        \u2192 training metric was live, bug is in save/load\n\"\"\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "source": "# ===================================================================\n# SECTION B+C: BUG VERIFICATION & METRIC CONSISTENCY\n# ===================================================================\n# B1) Bug #1: coordinate mismatch after subsampling\n# B2) Bug #2: subsampling destroys spatial resolution\n# C)  Per-slide overlap@20, p25/median physical distance\n# ===================================================================\n\nfrom verify_bugs_and_metrics import run_all_checks\n\nCKPT_DIR = '/home/ehtesamul/sc_st/model/gems_liver_crossslide'\nckpt_path = f'{CKPT_DIR}/encoder_final_trained.pt'\n\nverification_results = run_all_checks(\n    st_expr=st_expr,\n    st_coords=st_coords,\n    slide_ids=slide_ids,\n    inf_expr=inf_expr,\n    n_genes=n_genes,\n    ns=ns,\n    slide_names=list(ST_PATHS.keys()),\n    ckpt_path=ckpt_path,\n    device=device,\n    seed=SEED,\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}