{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# k-NN PRESERVATION ANALYSIS (HSCC SLIDE 3)\n",
    "# ===================================================================\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import torch\n",
    "import scanpy as sc \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils_et as uet\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION ANALYSIS - HSCC SLIDE 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load your inference results\n",
    "# Adjust the timestamp to match your actual saved file\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_hscc_output_anchored\"  # UPDATE THIS PATH\n",
    "timestamp = \"20260119_181640\"  # UPDATE THIS, e.g., \"20251208_143052\"\n",
    "\n",
    "# Load processed results\n",
    "results_path = f\"{output_dir}/st3_inference_processed_{timestamp}.pt\"\n",
    "results = torch.load(results_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "# Extract predicted coordinates and EDM\n",
    "coords_pred = results['coords_canon']  # Shape: (n_spots, 2)\n",
    "D_edm_pred = results['D_edm']  # Shape: (n_spots, n_spots)\n",
    "\n",
    "# Load ground truth from stadata3\n",
    "stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "gt_coords = stadata3.obsm['spatial']  # Shape: (n_spots, 2)\n",
    "\n",
    "# Convert to tensor and canonicalize (single slide â†’ all slide_ids = 0)\n",
    "gt_coords_tensor = torch.tensor(gt_coords, dtype=torch.float32)\n",
    "slide_ids = torch.zeros(gt_coords_tensor.shape[0], dtype=torch.long)\n",
    "gt_coords_tensor, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    gt_coords_tensor, slide_ids\n",
    ")\n",
    "\n",
    "# Convert back to numpy for downstream analysis\n",
    "gt_coords = gt_coords_tensor.cpu().numpy()\n",
    "\n",
    "n_spots = gt_coords.shape[0]\n",
    "print(f\"Number of spots: {n_spots}\")\n",
    "print(f\"Ground truth coords: {gt_coords.shape}\")\n",
    "print(f\"Predicted coords: {coords_pred.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE k-NN PRESERVATION\n",
    "# ===================================================================\n",
    "\n",
    "def compute_knn_preservation(coords_gt, coords_pred, k=10):\n",
    "    \"\"\"\n",
    "    Compute k-nearest neighbor preservation.\n",
    "    Returns average number of preserved neighbors per spot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coords_gt : ndarray, shape (n, 2)\n",
    "        Ground truth coordinates\n",
    "    coords_pred : ndarray, shape (n, 2)  \n",
    "        Predicted coordinates\n",
    "    k : int\n",
    "        Number of neighbors\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_overlap : float\n",
    "        Mean number of preserved neighbors\n",
    "    overlaps : list\n",
    "        Per-spot overlap counts\n",
    "    \"\"\"\n",
    "    n = coords_gt.shape[0]\n",
    "    \n",
    "    # Build k-NN for ground truth\n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_gt)\n",
    "    _, indices_gt = nbrs_gt.kneighbors(coords_gt)\n",
    "    indices_gt = indices_gt[:, 1:]  # Remove self\n",
    "    \n",
    "    # Build k-NN for predicted\n",
    "    nbrs_pred = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_pred)\n",
    "    _, indices_pred = nbrs_pred.kneighbors(coords_pred)\n",
    "    indices_pred = indices_pred[:, 1:]  # Remove self\n",
    "    \n",
    "    # Compute overlap for each spot\n",
    "    overlaps = np.array([\n",
    "        len(set(indices_gt[i]) & set(indices_pred[i]))\n",
    "        for i in range(n)\n",
    "    ])\n",
    "    \n",
    "    return overlaps.mean(), overlaps\n",
    "\n",
    "# Compute for k=10 and k=20\n",
    "knn_k10, overlaps_k10 = compute_knn_preservation(gt_coords, coords_pred, k=10)\n",
    "knn_k20, overlaps_k20 = compute_knn_preservation(gt_coords, coords_pred, k=20)\n",
    "\n",
    "print(f\"\\nk-NN Preservation Results:\")\n",
    "print(f\"  k=10: {knn_k10:.2f} / 10  ({knn_k10/10*100:.1f}% neighbors preserved)\")\n",
    "print(f\"  k=20: {knn_k20:.2f} / 20  ({knn_k20/20*100:.1f}% neighbors preserved)\")\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE EDM CORRELATIONS FOR COMPARISON\n",
    "# ===================================================================\n",
    "\n",
    "# Compute ground truth EDM\n",
    "gt_edm = squareform(pdist(gt_coords, 'euclidean'))\n",
    "\n",
    "# Extract upper triangle distances\n",
    "triu_indices = np.triu_indices(n_spots, k=1)\n",
    "gt_distances = gt_edm[triu_indices]\n",
    "pred_distances = D_edm_pred[triu_indices]\n",
    "\n",
    "# Scale alignment\n",
    "scale = np.median(gt_distances) / np.median(pred_distances)\n",
    "pred_distances_scaled = pred_distances * scale\n",
    "\n",
    "# Correlations\n",
    "pearson_corr, _ = pearsonr(gt_distances, pred_distances_scaled)\n",
    "spearman_corr, _ = spearmanr(gt_distances, pred_distances_scaled)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  EDM Pearson:  {pearson_corr:.4f}  (global distances)\")\n",
    "print(f\"  EDM Spearman: {spearman_corr:.4f}  (global distances)\")\n",
    "print(f\"  k-NN@10:      {knn_k10/10:.4f}  (local neighborhoods)\")\n",
    "print(f\"  k-NN@20:      {knn_k20/20:.4f}  (local neighborhoods)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if knn_k10/10 < 0.3:\n",
    "    print(\"\\nâš ï¸ SEVERE local scrambling detected!\")\n",
    "    print(\"   < 30% of neighbors preserved - local structure heavily disrupted\")\n",
    "elif knn_k10/10 < 0.5:\n",
    "    print(\"\\nâš ï¸ MODERATE local scrambling detected\")\n",
    "    print(\"   30-50% of neighbors preserved - significant local disruption\")\n",
    "elif knn_k10/10 < 0.7:\n",
    "    print(\"\\nâœ“ MILD local scrambling\")\n",
    "    print(\"   50-70% of neighbors preserved - some local structure retained\")\n",
    "else:\n",
    "    print(\"\\nâœ“âœ“ GOOD local preservation\")\n",
    "    print(\"   > 70% of neighbors preserved - local structure mostly intact\")\n",
    "\n",
    "if pearson_corr > 0.6 and knn_k10/10 < 0.4:\n",
    "    print(\"\\nðŸ” DIAGNOSIS: High EDM correlation but low k-NN preservation\")\n",
    "    print(\"   â†’ Global geometry preserved, but specific neighbor relationships lost\")\n",
    "    print(\"   â†’ Consistent with context-dependent coordinate generation\")\n",
    "\n",
    "# ===================================================================\n",
    "# DISTANCE-DEPENDENT ERROR ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTANCE-DEPENDENT ERROR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_bins = 20\n",
    "bins = np.linspace(0, np.percentile(gt_distances, 95), n_bins + 1)\n",
    "bin_idx = np.digitize(gt_distances, bins)\n",
    "\n",
    "print(f\"\\n{'Bin':<5} {'Mean Dist':<12} {'RMSE':<12} {'Rel Error':<12} {'N Points':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for b in range(1, n_bins + 1):\n",
    "    mask = bin_idx == b\n",
    "    if mask.sum() > 100:\n",
    "        gt_bin = gt_distances[mask]\n",
    "        pred_bin = pred_distances_scaled[mask]\n",
    "        \n",
    "        rmse = np.sqrt(((gt_bin - pred_bin) ** 2).mean())\n",
    "        mean_dist = gt_bin.mean()\n",
    "        rel_err = rmse / mean_dist\n",
    "        n_points = mask.sum()\n",
    "        \n",
    "        print(f\"{b:<5} {mean_dist:<12.4f} {rmse:<12.4f} {rel_err:<12.4f} {n_points:<10}\")\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATION: k-NN PRESERVATION HEATMAP\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZING k-NN PRESERVATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "\n",
    "# Plot 1: Ground truth coordinates\n",
    "ax = axes[0]\n",
    "if 'celltype' in stadata3.obs.columns:\n",
    "    cell_types = stadata3.obs['celltype']\n",
    "    unique_types = cell_types.unique()\n",
    "    cmap = plt.cm.tab10 if len(unique_types) <= 10 else plt.cm.tab20\n",
    "    \n",
    "    for i, ct in enumerate(unique_types):\n",
    "        mask = cell_types == ct\n",
    "        ax.scatter(gt_coords[mask, 0], gt_coords[mask, 1],\n",
    "                  c=[cmap(i / len(unique_types))], label=ct, s=20, alpha=0.7)\n",
    "else:\n",
    "    ax.scatter(gt_coords[:, 0], gt_coords[:, 1], s=20, alpha=0.7, c='blue')\n",
    "\n",
    "ax.set_title('Ground Truth (Slide 3)', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('X', fontsize=14)\n",
    "ax.set_ylabel('Y', fontsize=14)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predicted coordinates\n",
    "ax = axes[1]\n",
    "if 'celltype' in stadata3.obs.columns:\n",
    "    for i, ct in enumerate(unique_types):\n",
    "        mask = cell_types == ct\n",
    "        ax.scatter(coords_pred[mask, 0], coords_pred[mask, 1],\n",
    "                  c=[cmap(i / len(unique_types))], label=ct, s=20, alpha=0.7)\n",
    "else:\n",
    "    ax.scatter(coords_pred[:, 0], coords_pred[:, 1], s=20, alpha=0.7, c='red')\n",
    "\n",
    "ax.set_title('GEMS Predicted', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('X', fontsize=14)\n",
    "ax.set_ylabel('Y', fontsize=14)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: k-NN preservation per spot\n",
    "ax = axes[2]\n",
    "scatter = ax.scatter(coords_pred[:, 0], coords_pred[:, 1],\n",
    "                    c=overlaps_k10, cmap='RdYlGn', s=20, vmin=0, vmax=10, alpha=0.8)\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Neighbors Preserved (k=10)', fontsize=12)\n",
    "ax.set_title(f'k-NN@10 Preservation\\nMean: {knn_k10:.2f}/10 ({knn_k10/10*100:.1f}%)', \n",
    "            fontsize=16, weight='bold')\n",
    "ax.set_xlabel('X', fontsize=14)\n",
    "ax.set_ylabel('Y', fontsize=14)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "if 'celltype' in stadata3.obs.columns and len(unique_types) <= 15:\n",
    "    handles, labels = axes[1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1.0, 0.5),\n",
    "              fontsize=10, title='Cell Type', title_fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "knn_plot_path = f\"{output_dir}/hscc_knn_preservation_{timestamp}.png\"\n",
    "# plt.savefig(knn_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ“ Saved k-NN preservation plot: {knn_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "\n",
    "from core_models_et_p3 import GEMSModel, infer_anchor_train_from_checkpoint\n",
    "from core_models_et_p1 import STSetDataset, SCSetDataset\n",
    "import utils_et as uet\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ===================================================================\n",
    "# 1. LOAD HSCC DATA\n",
    "# ===================================================================\n",
    "print(\"Loading HSCC data...\")\n",
    "\n",
    "# Load SC data\n",
    "scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "\n",
    "# Load 3 ST slides (first 2 for training, 3rd for inference)\n",
    "stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')  # For inference\n",
    "\n",
    "# Normalize\n",
    "print(\"Normalizing data...\")\n",
    "for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "# Create cell types for SC data\n",
    "print(\"Creating cell types...\")\n",
    "scadata.obs['celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A', 'celltype'] = 'DC'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C', 'celltype'] = 'DC'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC', 'celltype'] = 'DC'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='PDC', 'celltype'] = 'PDC'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC', 'celltype'] = 'DC'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='LC', 'celltype'] = 'DC'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='Mac', 'celltype'] = 'Myeloid cell'\n",
    "scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell', 'celltype'] = 'T cell'\n",
    "scadata.obs.loc[scadata.obs['level2_celltype']=='TSK', 'celltype'] = 'TSK'\n",
    "scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff', 'Tumor_KC_Cyc']), 'celltype'] = 'NonTSK'\n",
    "\n",
    "# Get common genes\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata1.var_names) & \n",
    "                     set(stadata2.var_names) & set(stadata3.var_names)))\n",
    "n_genes = len(common)\n",
    "\n",
    "print(f\"âœ“ SC data: {scadata.shape[0]} cells Ã— {scadata.shape[1]} genes\")\n",
    "print(f\"âœ“ ST slide 1: {stadata1.shape[0]} spots Ã— {stadata1.shape[1]} genes\")\n",
    "print(f\"âœ“ ST slide 2: {stadata2.shape[0]} spots Ã— {stadata2.shape[1]} genes\")\n",
    "print(f\"âœ“ ST slide 3 (inference): {stadata3.shape[0]} spots Ã— {stadata3.shape[1]} genes\")\n",
    "print(f\"âœ“ Common genes: {n_genes}\")\n",
    "\n",
    "# Extract tensors for training slides\n",
    "X_st1 = stadata1[:, common].X\n",
    "X_st2 = stadata2[:, common].X\n",
    "if hasattr(X_st1, \"toarray\"): X_st1 = X_st1.toarray()\n",
    "if hasattr(X_st2, \"toarray\"): X_st2 = X_st2.toarray()\n",
    "\n",
    "# Combine training ST data\n",
    "st_expr_combined = torch.tensor(np.vstack([X_st1, X_st2]), dtype=torch.float32, device=device)\n",
    "\n",
    "# ST coordinates from 2 training slides\n",
    "st_coords1 = stadata1.obsm['spatial']\n",
    "st_coords2 = stadata2.obsm['spatial']\n",
    "st_coords_raw = torch.tensor(np.vstack([st_coords1, st_coords2]), dtype=torch.float32, device=device)\n",
    "\n",
    "# Slide IDs (0 for slide1, 1 for slide2)\n",
    "slide_ids = torch.tensor(\n",
    "    np.concatenate([\n",
    "        np.zeros(X_st1.shape[0], dtype=int),\n",
    "        np.ones(X_st2.shape[0], dtype=int)\n",
    "    ]),\n",
    "    dtype=torch.long, device=device\n",
    ")\n",
    "\n",
    "# Per-slide canonicalization\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"âœ“ Combined ST data: {st_expr_combined.shape[0]} spots\")\n",
    "print(f\"âœ“ Slide IDs: slide 0 ({(slide_ids==0).sum()} spots), slide 1 ({(slide_ids==1).sum()} spots)\")\n",
    "print(f\"âœ“ Coordinates canonicalized: scale={st_scale.cpu().numpy()}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. LOAD TRAINED MODEL\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING TRAINED MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_hscc_output_anchored\"\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "# Auto-detect anchor mode from checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "base_h_dim = 128  # Last dimension of n_embedding\n",
    "anchor_train_detected = infer_anchor_train_from_checkpoint(checkpoint, base_h_dim)\n",
    "print(f\"âœ“ Detected anchor_train={anchor_train_detected}\")\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=128,\n",
    "    dist_bins=24,\n",
    "    device=device,\n",
    "    anchor_train=anchor_train_detected,\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "# Restore EDM parameters\n",
    "if 'sigma_data' in checkpoint:\n",
    "    model.sigma_data = checkpoint['sigma_data']\n",
    "if 'sigma_min' in checkpoint:\n",
    "    model.sigma_min = checkpoint['sigma_min']\n",
    "if 'sigma_max' in checkpoint:\n",
    "    model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.generator.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "print(f\"âœ“ Model loaded from {checkpoint_path}\")\n",
    "print(f\"âœ“ anchor_train={model.anchor_train}\")\n",
    "print(f\"âœ“ sigma_data={getattr(model, 'sigma_data', 'N/A')}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. RUN STAGE B ON TRAINING SLIDES\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING STAGE B\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Separate coordinates for each training slide\n",
    "st1_coords_canon = st_coords[slide_ids == 0]\n",
    "st1_expr = st_expr_combined[slide_ids == 0]\n",
    "st2_coords_canon = st_coords[slide_ids == 1]\n",
    "st2_expr = st_expr_combined[slide_ids == 1]\n",
    "\n",
    "slides_dict = {\n",
    "    0: (st1_coords_canon, st1_expr),\n",
    "    1: (st2_coords_canon, st2_expr)\n",
    "}\n",
    "\n",
    "model.train_stageB(\n",
    "    slides=slides_dict,\n",
    "    outdir=f\"{output_dir}/stageB_targets\"\n",
    ")\n",
    "print(\"âœ“ Stage B complete\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. CREATE MINI-SET DATASETS (FROM TRAINING SLIDES)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING MINI-SET DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "st_gene_expr_dict_cpu = {\n",
    "    0: st1_expr.cpu(),\n",
    "    1: st2_expr.cpu()\n",
    "}\n",
    "\n",
    "st_dataset = STSetDataset(\n",
    "    targets_dict=model.targets_dict,\n",
    "    encoder=model.encoder,\n",
    "    st_gene_expr_dict=st_gene_expr_dict_cpu,\n",
    "    n_min=128,\n",
    "    n_max=384,\n",
    "    D_latent=model.D_latent,\n",
    "    num_samples=20,  # Sample from both slides\n",
    "    knn_k=12,\n",
    "    device=device,\n",
    "    landmarks_L=0,\n",
    "    pool_mult=2.0,\n",
    "    stochastic_tau=1.0,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ ST dataset: {len(st_dataset)} mini-sets (from 2 training slides)\")\n",
    "\n",
    "# ===================================================================\n",
    "# 5. SAMPLE MINI-SETS\n",
    "# ===================================================================\n",
    "print(\"\\nSampling mini-sets...\")\n",
    "\n",
    "st_minisets = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(st_dataset)):\n",
    "        st_minisets.append(st_dataset[i])\n",
    "\n",
    "print(f\"âœ“ Sampled {len(st_minisets)} ST mini-sets\")\n",
    "print(f\"\\nST mini-set #0 keys: {list(st_minisets[0].keys())}\")\n",
    "print(f\"  Z_set shape: {st_minisets[0]['Z_set'].shape}\")\n",
    "print(f\"  V_target shape: {st_minisets[0]['V_target'].shape}\")\n",
    "print(f\"  n points: {st_minisets[0]['n']}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. PLOT MINI-SETS\n",
    "# ===================================================================\n",
    "print(\"\\nPlotting mini-sets...\")\n",
    "\n",
    "n_st_plots = 6\n",
    "max_cols = 3\n",
    "\n",
    "n_rows_st = (n_st_plots + max_cols - 1) // max_cols\n",
    "n_cols_st = min(n_st_plots, max_cols)\n",
    "\n",
    "fig_st, axes_st = plt.subplots(n_rows_st, n_cols_st, \n",
    "                                figsize=(6.5*n_cols_st, 6*n_rows_st))\n",
    "axes_st = np.atleast_2d(axes_st).reshape(n_rows_st, n_cols_st)\n",
    "\n",
    "for i in range(n_st_plots):\n",
    "    row, col = i // max_cols, i % max_cols\n",
    "    ax = axes_st[row, col]\n",
    "    coords = st_minisets[i]['V_target'].cpu().numpy()\n",
    "    n = st_minisets[i]['n']\n",
    "    ax.scatter(coords[:n, 0], coords[:n, 1], s=20, alpha=0.7)\n",
    "    ax.set_title(f'ST Mini-set {i} (n={n})', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i in range(n_st_plots, n_rows_st * n_cols_st):\n",
    "    row, col = i // max_cols, i % max_cols\n",
    "    axes_st[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ MINI-SETS READY!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CREATE SC MINISETS FOR INFERENCE SLIDE (stadata3)\n",
    "# ===================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"PREPARING INFERENCE DATA (SLIDE 3)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract inference slide expression\n",
    "X_st3 = stadata3[:, common].X\n",
    "if hasattr(X_st3, \"toarray\"):\n",
    "    X_st3 = X_st3.toarray()\n",
    "st3_expr = torch.tensor(X_st3, dtype=torch.float32, device=device)\n",
    "\n",
    "print(f\"âœ“ Inference slide (ST3): {st3_expr.shape[0]} spots Ã— {st3_expr.shape[1]} genes\")\n",
    "\n",
    "# Check if GT coordinates exist in obs columns (new_x, new_y)\n",
    "if 'new_x' in stadata3.obs.columns and 'new_y' in stadata3.obs.columns:\n",
    "    print(\"âœ“ Ground truth coordinates available in obs (new_x, new_y)\")\n",
    "    gt_coords_available = True\n",
    "    gt_coords_raw_st3 = stadata3.obs[['new_x', 'new_y']].values\n",
    "    print(f\"âœ“ GT coords shape: {gt_coords_raw_st3.shape}\")\n",
    "    print(f\"âœ“ GT coords range: X=[{gt_coords_raw_st3[:,0].min():.3f}, {gt_coords_raw_st3[:,0].max():.3f}], \"\n",
    "          f\"Y=[{gt_coords_raw_st3[:,1].min():.3f}, {gt_coords_raw_st3[:,1].max():.3f}]\")\n",
    "elif 'spatial_gt' in stadata3.obsm:\n",
    "    print(\"âœ“ Ground truth coordinates available in obsm['spatial_gt']\")\n",
    "    gt_coords_available = True\n",
    "    gt_coords_raw_st3 = stadata3.obsm['spatial_gt']\n",
    "    print(f\"âœ“ GT coords shape: {gt_coords_raw_st3.shape}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No ground truth coordinates for slide 3\")\n",
    "    gt_coords_available = False\n",
    "    gt_coords_raw_st3 = None\n",
    "\n",
    "print(\"\\nâœ“ Inference data ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "from core_models_et_p3 import GEMSModel, infer_anchor_train_from_checkpoint\n",
    "import utils_et as uet\n",
    "\n",
    "# ===================================================================\n",
    "# PATHS AND CONFIG\n",
    "# ===================================================================\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_hscc_output_anchored\"\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA AND MODEL FOR INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# LOAD AND NORMALIZE GT COORDS (IF AVAILABLE)\n",
    "# ===================================================================\n",
    "gt_coords_norm = None\n",
    "if gt_coords_available and gt_coords_raw_st3 is not None:\n",
    "    gt_coords_tensor = torch.tensor(gt_coords_raw_st3, dtype=torch.float32, device=device)\n",
    "    slide_ids_gt = torch.zeros(gt_coords_tensor.shape[0], dtype=torch.long, device=device)\n",
    "    \n",
    "    gt_coords_norm, gt_mu, gt_scale = uet.canonicalize_st_coords_per_slide(\n",
    "        gt_coords_tensor, slide_ids_gt\n",
    "    )\n",
    "    \n",
    "    gt_coords_np = gt_coords_norm.cpu().numpy()\n",
    "    print(f\"âœ“ GT coords normalized: scale={gt_scale[0].item():.4f}\")\n",
    "    print(f\"âœ“ GT coords RMS: {gt_coords_norm.pow(2).mean().sqrt().item():.4f}\")\n",
    "    print(f\"âœ“ GT coords range: X=[{gt_coords_np[:,0].min():.3f}, {gt_coords_np[:,0].max():.3f}], \"\n",
    "          f\"Y=[{gt_coords_np[:,1].min():.3f}, {gt_coords_np[:,1].max():.3f}]\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GT coords available - debug_knn will be disabled\")\n",
    "\n",
    "# ===================================================================\n",
    "# MODEL ALREADY LOADED FROM CELL 1, BUT RELOAD CHECKPOINT PARAMS\n",
    "# ===================================================================\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "if 'sigma_data' in checkpoint:\n",
    "    model.sigma_data = checkpoint['sigma_data']\n",
    "if 'sigma_min' in checkpoint:\n",
    "    model.sigma_min = checkpoint['sigma_min']\n",
    "if 'sigma_max' in checkpoint:\n",
    "    model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "print(f\"âœ“ Model ready for inference\")\n",
    "print(f\"  sigma_data={getattr(model, 'sigma_data', 'N/A')}\")\n",
    "print(f\"  sigma_min={getattr(model, 'sigma_min', 'N/A')}\")\n",
    "print(f\"  sigma_max={getattr(model, 'sigma_max', 'N/A')}\")\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE CORAL TRANSFORMATION\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING CORAL TRANSFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare ST gene expression dict from training slides\n",
    "st_gene_expr_dict = {\n",
    "    0: st1_expr.cpu(),\n",
    "    1: st2_expr.cpu()\n",
    "}\n",
    "\n",
    "# # Compute ST context distribution\n",
    "# print(\"--- Computing ST context distribution (from 2 training slides) ---\")\n",
    "# model.compute_coral_params_from_st(\n",
    "#     st_gene_expr_dict=st_gene_expr_dict,\n",
    "#     n_samples=2000,\n",
    "#     n_min=96,\n",
    "#     n_max=384,\n",
    "# )\n",
    "\n",
    "# # Build CORAL transform using inference slide expression\n",
    "# print(\"--- Building CORAL transformation for inference slide ---\")\n",
    "# model.build_coral_transform(\n",
    "#     sc_gene_expr=st3_expr,  # Use inference slide as \"SC\" data\n",
    "#     n_samples=2000,\n",
    "#     n_min=96,\n",
    "#     n_max=384,\n",
    "#     shrink=0.01,\n",
    "#     eps=1e-5,\n",
    "# )\n",
    "\n",
    "# print(\"âœ“ CORAL transformation ready!\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# RUN INFERENCE (PATCHWISE)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING INFERENCE ON SLIDE 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_cells = st3_expr.shape[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model.infer_sc_patchwise(\n",
    "        sc_gene_expr=st3_expr,\n",
    "        n_timesteps_sample=500,\n",
    "        return_coords=True,\n",
    "        patch_size=638,\n",
    "        coverage_per_cell=1.0,\n",
    "        n_align_iters=1,\n",
    "        eta=0.0,\n",
    "        guidance_scale=2.0,\n",
    "        gt_coords=gt_coords_norm,\n",
    "        debug_knn=(gt_coords_norm is not None),  # Only if GT available\n",
    "        debug_max_patches=15,\n",
    "        debug_k_list=(10, 20),\n",
    "        pool_mult=2.0,\n",
    "        stochastic_tau=0.8,\n",
    "        tau_mode=\"adaptive_kth\",\n",
    "        ensure_connected=True,\n",
    "        local_refine=False,\n",
    "        # ========== ANCHORED INFERENCE ==========\n",
    "        inference_mode=\"anchored\",\n",
    "        anchor_sampling_mode=\"align_vote_only\",\n",
    "        commit_frac=0.6,\n",
    "        seq_align_dim=32,\n",
    "        coldstart_diag=True\n",
    "    )\n",
    "\n",
    "D_edm_pred = results['D_edm'].cpu().numpy()\n",
    "coords_pred = results['coords_canon'].cpu().numpy()\n",
    "print(f\"\\nâœ“ Inference complete!\")\n",
    "print(f\"  Predicted EDM: {D_edm_pred.shape}\")\n",
    "print(f\"  Predicted coords: {coords_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPUTE GROUND TRUTH EDM AND METRICS (IF GT AVAILABLE)\n",
    "# ===================================================================\n",
    "if gt_coords_norm is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPUTING METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gt_edm = squareform(pdist(gt_coords_np, 'euclidean'))\n",
    "    print(f\"âœ“ Ground truth EDM: {gt_edm.shape}\")\n",
    "    \n",
    "    # Extract upper triangle distances\n",
    "    triu_indices = np.triu_indices(n_cells, k=1)\n",
    "    gt_distances = gt_edm[triu_indices]\n",
    "    pred_distances = D_edm_pred[triu_indices]\n",
    "    \n",
    "    # Scale alignment\n",
    "    scale = np.median(gt_distances) / np.median(pred_distances)\n",
    "    pred_distances_scaled = pred_distances * scale\n",
    "    \n",
    "    # Correlations\n",
    "    pearson_corr, _ = pearsonr(gt_distances, pred_distances_scaled)\n",
    "    spearman_corr, _ = spearmanr(gt_distances, pred_distances_scaled)\n",
    "    \n",
    "    print(f\"\\nPearson Correlation:  {pearson_corr:.4f}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    print(f\"Scale factor: {scale:.4f}\")\n",
    "    \n",
    "    # ===================================================================\n",
    "    # VISUALIZATIONS\n",
    "    # ===================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING PLOTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Plot 1: Coordinate comparison\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    axes[0].scatter(gt_coords_np[:, 0], gt_coords_np[:, 1], s=5, alpha=0.6, c='blue')\n",
    "    axes[0].set_title('Ground Truth Coordinates (Slide 3)', fontsize=14, weight='bold')\n",
    "    axes[0].set_aspect('equal')\n",
    "    \n",
    "    axes[1].scatter(coords_pred[:, 0], coords_pred[:, 1], s=5, alpha=0.6, c='red')\n",
    "    axes[1].set_title('Predicted Coordinates (Slide 3)', fontsize=14, weight='bold')\n",
    "    axes[1].set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 2: Distance scatter - Pearson and Spearman\n",
    "    sample_size = min(50000, len(gt_distances))\n",
    "    sample_idx = np.random.choice(len(gt_distances), sample_size, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Pearson\n",
    "    ax = axes[0]\n",
    "    ax.scatter(gt_distances[sample_idx], pred_distances_scaled[sample_idx], alpha=0.2, s=5)\n",
    "    ax.set_title(f'Distance Correlation (Pearson)\\nÏ = {pearson_corr:.4f}', fontsize=16, weight='bold')\n",
    "    ax.set_xlabel('Ground Truth Distance', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Distance (scaled)', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.75, label='Ideal')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Spearman\n",
    "    ax = axes[1]\n",
    "    ax.scatter(gt_distances[sample_idx], pred_distances_scaled[sample_idx], alpha=0.2, s=5)\n",
    "    ax.set_title(f'Distance Correlation (Spearman)\\nÏ = {spearman_corr:.4f}', fontsize=16, weight='bold')\n",
    "    ax.set_xlabel('Ground Truth Distance', fontsize=12)\n",
    "    ax.set_ylabel('Predicted Distance (scaled)', fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "    ax.plot(lims, lims, 'r--', alpha=0.75, label='Ideal')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot 3: Distance distributions\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.histplot(gt_distances, color='blue', label='Ground Truth', stat='density', bins=100, alpha=0.5, ax=ax)\n",
    "    sns.histplot(pred_distances_scaled, color='red', label='Predicted (scaled)', stat='density', bins=100, alpha=0.5, ax=ax)\n",
    "    ax.set_title('Distance Distribution Comparison', fontsize=16, weight='bold')\n",
    "    ax.set_xlabel('Distance')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # ===================================================================\n",
    "    # EDM HEATMAP COMPARISON\n",
    "    # ===================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EDM HEATMAP VISUALIZATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    def normalize_matrix(matrix):\n",
    "        min_val = matrix.min()\n",
    "        max_val = matrix.max()\n",
    "        return (matrix - min_val) / (max_val - min_val)\n",
    "    \n",
    "    gt_edm_norm = normalize_matrix(gt_edm)\n",
    "    pred_edm_norm = normalize_matrix(D_edm_pred)\n",
    "    \n",
    "    # Sample cells\n",
    "    sample_size = min(500, n_cells)\n",
    "    sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "    sample_indices = np.sort(sample_indices)\n",
    "    \n",
    "    print(f\"\\nCreating EDM heatmaps with {sample_size} sampled cells...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    fig.suptitle('EDM Comparison: Ground Truth vs. Predicted', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Ground Truth EDM\n",
    "    im1 = axes[0].imshow(gt_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "    axes[0].set_title('Ground Truth EDM (Normalized)', fontsize=14)\n",
    "    axes[0].set_xlabel('Cell Index (Sampled)', fontsize=12)\n",
    "    axes[0].set_ylabel('Cell Index (Sampled)', fontsize=12)\n",
    "    fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Predicted EDM\n",
    "    im2 = axes[1].imshow(pred_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "    axes[1].set_title('Predicted EDM (Normalized)', fontsize=14)\n",
    "    axes[1].set_xlabel('Cell Index (Sampled)', fontsize=12)\n",
    "    fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "    \n",
    "    # ===================================================================\n",
    "    # ANISOTROPY ANALYSIS\n",
    "    # ===================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EIGENVALUE ANISOTROPY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    def compute_anisotropy(coords):\n",
    "        X = coords.astype(float)\n",
    "        Xc = X - X.mean(axis=0, keepdims=True)\n",
    "        cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "        eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "        eigvals = eigvals[::-1]\n",
    "        lam1, lam2 = eigvals[0], eigvals[1]\n",
    "        ratio = lam1 / (lam2 + 1e-12)\n",
    "        return lam1, lam2, ratio\n",
    "    \n",
    "    # Ground truth\n",
    "    lam1_gt, lam2_gt, ratio_gt = compute_anisotropy(gt_coords_np)\n",
    "    print(f\"\\nGround Truth Coordinates:\")\n",
    "    print(f\"  Î»â‚ = {lam1_gt:.4f},  Î»â‚‚ = {lam2_gt:.4f}\")\n",
    "    print(f\"  Î»â‚/Î»â‚‚ = {ratio_gt:.2f}\")\n",
    "    \n",
    "    if ratio_gt < 5:\n",
    "        print(f\"  â†’ GENUINELY 2D âœ“\")\n",
    "    elif ratio_gt < 20:\n",
    "        print(f\"  â†’ Anisotropic but still 2D-ish\")\n",
    "    else:\n",
    "        print(f\"  â†’ EFFECTIVELY 1D (very elongated) âœ—\")\n",
    "    \n",
    "    # Predicted\n",
    "    lam1_pred, lam2_pred, ratio_pred = compute_anisotropy(coords_pred)\n",
    "    print(f\"\\nPredicted Coordinates:\")\n",
    "    print(f\"  Î»â‚ = {lam1_pred:.4f},  Î»â‚‚ = {lam2_pred:.4f}\")\n",
    "    print(f\"  Î»â‚/Î»â‚‚ = {ratio_pred:.2f}\")\n",
    "    \n",
    "    if ratio_pred < 5:\n",
    "        print(f\"  â†’ GENUINELY 2D âœ“\")\n",
    "    elif ratio_pred < 20:\n",
    "        print(f\"  â†’ Anisotropic but still 2D-ish\")\n",
    "    else:\n",
    "        print(f\"  â†’ EFFECTIVELY 1D (very elongated) âœ—\")\n",
    "    \n",
    "    # Anisotropy visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Bar comparison\n",
    "    ax = axes[0, 0]\n",
    "    methods = ['Ground Truth', 'Predicted']\n",
    "    ratios = [ratio_gt, ratio_pred]\n",
    "    colors = ['blue', 'red']\n",
    "    \n",
    "    bars = ax.bar(methods, ratios, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "    ax.axhline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "    ax.axhline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "    \n",
    "    ax.set_ylabel('$\\\\lambda_1/\\\\lambda_2$ (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Anisotropy Comparison: Ground Truth vs Predicted', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, ratio in zip(bars, ratios):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{ratio:.2f}',\n",
    "                ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Eigenvalue scatter\n",
    "    ax = axes[0, 1]\n",
    "    ax.scatter(lam2_gt, lam1_gt, c='blue', s=300, marker='o',\n",
    "              edgecolors='darkblue', linewidth=2, label='Ground Truth', zorder=5)\n",
    "    ax.scatter(lam2_pred, lam1_pred, c='red', s=300, marker='*',\n",
    "              edgecolors='darkred', linewidth=2, label='Predicted', zorder=5)\n",
    "    \n",
    "    min_val = min(lam2_gt, lam2_pred)\n",
    "    max_val = max(lam1_gt, lam1_pred)\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'k--',\n",
    "            linewidth=2, label='$\\\\lambda_1 = \\\\lambda_2$', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('$\\\\lambda_2$ (Smaller Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('$\\\\lambda_1$ (Larger Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "    ax.set_title('Eigenvalue Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Ground truth coordinates\n",
    "    ax = axes[1, 0]\n",
    "    ax.scatter(gt_coords_np[:, 0], gt_coords_np[:, 1], alpha=0.5, s=10,\n",
    "              c='blue', edgecolors='none')\n",
    "    ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(f'Ground Truth\\n$\\\\lambda_1/\\\\lambda_2$ = {ratio_gt:.2f}',\n",
    "                 fontsize=14, fontweight='bold', color='blue')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # Plot 4: Predicted coordinates\n",
    "    ax = axes[1, 1]\n",
    "    ax.scatter(coords_pred[:, 0], coords_pred[:, 1], alpha=0.5, s=10,\n",
    "              c='red', edgecolors='none')\n",
    "    ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "    ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "    ax.set_title(f'Predicted\\n$\\\\lambda_1/\\\\lambda_2$ = {ratio_pred:.2f}',\n",
    "                 fontsize=14, fontweight='bold', color='red')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No ground truth coordinates available - skipping metric computation\")\n",
    "    print(\"   Only showing predicted coordinates...\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.scatter(coords_pred[:, 0], coords_pred[:, 1], s=5, alpha=0.6, c='red')\n",
    "    ax.set_title('Predicted Coordinates (Slide 3)', fontsize=14, weight='bold')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CELL TYPE VISUALIZATION (GROUND TRUTH vs PREDICTED)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CELL TYPE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if stadata3 has level1/level2 celltype columns like scadata\n",
    "print(\"\\nAvailable columns in stadata3.obs:\")\n",
    "print(list(stadata3.obs.columns))\n",
    "\n",
    "# If stadata3 has level1_celltype and level2_celltype, apply same logic as scadata\n",
    "if 'level1_celltype' in stadata3.obs.columns and 'level2_celltype' in stadata3.obs.columns:\n",
    "    print(\"\\nâœ“ Found level1_celltype and level2_celltype columns\")\n",
    "    print(\"   Applying cell type mapping from run_hscc_gems.py...\")\n",
    "    \n",
    "    stadata3.obs['celltype'] = stadata3.obs['level1_celltype'].astype(str)\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='CLEC9A', 'celltype'] = 'DC'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='CD1C', 'celltype'] = 'DC'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='ASDC', 'celltype'] = 'DC'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='PDC', 'celltype'] = 'PDC'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='MDSC', 'celltype'] = 'DC'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='LC', 'celltype'] = 'DC'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='Mac', 'celltype'] = 'Myeloid cell'\n",
    "    stadata3.obs.loc[stadata3.obs['level1_celltype']=='Tcell', 'celltype'] = 'T cell'\n",
    "    stadata3.obs.loc[stadata3.obs['level2_celltype']=='TSK', 'celltype'] = 'TSK'\n",
    "    stadata3.obs.loc[stadata3.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff', 'Tumor_KC_Cyc']), 'celltype'] = 'NonTSK'\n",
    "    \n",
    "    cell_type_col = 'celltype'\n",
    "    cell_types = stadata3.obs[cell_type_col].values\n",
    "    \n",
    "    print(f\"âœ“ Cell types created from level1/level2 annotations\")\n",
    "\n",
    "# Otherwise, use simple spatial regions for visualization\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ST data doesn't have cell-level annotations\")\n",
    "    print(\"   Using spatial regions (grid quadrants) for visualization...\")\n",
    "    \n",
    "    # Create simple spatial regions based on x,y position\n",
    "    if 'new_x' in stadata3.obs.columns and 'new_y' in stadata3.obs.columns:\n",
    "        x_vals = stadata3.obs['new_x'].values\n",
    "        y_vals = stadata3.obs['new_y'].values\n",
    "        \n",
    "        # Divide into 4 quadrants\n",
    "        x_median = np.median(x_vals)\n",
    "        y_median = np.median(y_vals)\n",
    "        \n",
    "        regions = []\n",
    "        for x, y in zip(x_vals, y_vals):\n",
    "            if x < x_median and y < y_median:\n",
    "                regions.append('Region 1 (Lower-Left)')\n",
    "            elif x >= x_median and y < y_median:\n",
    "                regions.append('Region 2 (Lower-Right)')\n",
    "            elif x < x_median and y >= y_median:\n",
    "                regions.append('Region 3 (Upper-Left)')\n",
    "            else:\n",
    "                regions.append('Region 4 (Upper-Right)')\n",
    "        \n",
    "        stadata3.obs['spatial_region'] = regions\n",
    "        cell_type_col = 'spatial_region'\n",
    "        cell_types = stadata3.obs[cell_type_col].values\n",
    "        \n",
    "        print(f\"âœ“ Created 4 spatial regions based on grid position\")\n",
    "    else:\n",
    "        print(\"   Cannot create spatial regions - no coordinates available\")\n",
    "        print(\"   Skipping cell type visualization\")\n",
    "        cell_types = None\n",
    "\n",
    "# Only proceed with visualization if we have cell types\n",
    "if cell_types is not None:\n",
    "    # Get unique types\n",
    "    unique_types = np.unique(cell_types)\n",
    "    n_types = len(unique_types)\n",
    "    \n",
    "    print(f\"\\nFound {n_types} unique categories:\")\n",
    "    for i, ct in enumerate(unique_types):\n",
    "        count = (cell_types == ct).sum()\n",
    "        print(f\"  {i+1}. {ct}: {count} spots ({count/len(cell_types)*100:.1f}%)\")\n",
    "    \n",
    "    # Create colormap\n",
    "    if n_types <= 10:\n",
    "        cmap = plt.cm.tab10\n",
    "    elif n_types <= 20:\n",
    "        cmap = plt.cm.tab20\n",
    "    else:\n",
    "        cmap = plt.cm.gist_ncar\n",
    "    \n",
    "    type_to_color = {ct: cmap(i / max(n_types, 2)) for i, ct in enumerate(unique_types)}\n",
    "    \n",
    "    # ===================================================================\n",
    "    # CREATE VISUALIZATION\n",
    "    # ===================================================================\n",
    "    if gt_coords_norm is not None:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot 1: Ground Truth\n",
    "        ax = axes[0]\n",
    "        for ct in unique_types:\n",
    "            mask = cell_types == ct\n",
    "            ax.scatter(gt_coords_np[mask, 0], gt_coords_np[mask, 1],\n",
    "                      c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "        \n",
    "        ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "        ax.set_title(f'Ground Truth - {cell_type_col.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Predicted\n",
    "        ax = axes[1]\n",
    "        for ct in unique_types:\n",
    "            mask = cell_types == ct\n",
    "            ax.scatter(coords_pred[mask, 0], coords_pred[mask, 1],\n",
    "                      c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "        \n",
    "        ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "        ax.set_title(f'Predicted - {cell_type_col.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        if n_types <= 15:\n",
    "            handles, labels = axes[1].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1.0, 0.5),\n",
    "                      fontsize=10, title=cell_type_col.replace('_', ' ').title(), \n",
    "                      title_fontsize=12, frameon=True)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Only predicted\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        for ct in unique_types:\n",
    "            mask = cell_types == ct\n",
    "            ax.scatter(coords_pred[mask, 0], coords_pred[mask, 1],\n",
    "                      c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "        \n",
    "        ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "        ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "        ax.set_title(f'Predicted - {cell_type_col.replace(\"_\", \" \").title()}', fontsize=14, fontweight='bold')\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        if n_types <= 15:\n",
    "            ax.legend(loc='best', fontsize=10, title=cell_type_col.replace('_', ' ').title(), frameon=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CELL TYPE VISUALIZATION COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Skipping cell type visualization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# k-NN PRESERVATION ANALYSIS (ENHANCED)\n",
    "# ===================================================================\n",
    "if gt_coords_norm is not None:\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    from scipy.stats import spearmanr\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"k-NN PRESERVATION ANALYSIS (ENHANCED)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # --- Helper functions ---\n",
    "    def knn_sets(coords, k):\n",
    "        \"\"\"Get k-NN indices for all points.\"\"\"\n",
    "        nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords)\n",
    "        _, idx = nbrs.kneighbors(coords)\n",
    "        return idx[:, 1:]\n",
    "    \n",
    "    def knn_overlap_frac(idx_gt, idx_pr):\n",
    "        \"\"\"Compute per-point overlap fraction: |intersection| / k\"\"\"\n",
    "        n, k = idx_gt.shape\n",
    "        out = np.empty(n, dtype=np.float32)\n",
    "        for i in range(n):\n",
    "            out[i] = len(set(idx_gt[i]).intersection(idx_pr[i])) / k\n",
    "        return out\n",
    "    \n",
    "    def knn_jaccard(idx_gt, idx_pr):\n",
    "        \"\"\"Compute per-point Jaccard: |intersection| / |union|\"\"\"\n",
    "        n, k = idx_gt.shape\n",
    "        out = np.empty(n, dtype=np.float32)\n",
    "        for i in range(n):\n",
    "            a = set(idx_gt[i])\n",
    "            b = set(idx_pr[i])\n",
    "            out[i] = len(a & b) / max(1, len(a | b))\n",
    "        return out\n",
    "    \n",
    "    def compute_hits(gt_knn_k, pred_knn_m):\n",
    "        \"\"\"Compute h_i(k,m) = |G_i^(k) âˆ© P_i^(m)| for each point i.\"\"\"\n",
    "        n = gt_knn_k.shape[0]\n",
    "        hits = np.zeros(n, dtype=np.int32)\n",
    "        for i in range(n):\n",
    "            gt_set = set(gt_knn_k[i])\n",
    "            pred_set = set(pred_knn_m[i])\n",
    "            hits[i] = len(gt_set & pred_set)\n",
    "        return hits\n",
    "    \n",
    "    def recall_at_k(gt_coords, pred_coords, k=10):\n",
    "        \"\"\"Recall@k = mean(h_i(k,k) / k)\"\"\"\n",
    "        gt_knn = knn_sets(gt_coords, k)\n",
    "        pred_knn = knn_sets(pred_coords, k)\n",
    "        hits = compute_hits(gt_knn, pred_knn)\n",
    "        return hits.mean() / k, hits / k\n",
    "    \n",
    "    def nearmiss_at_m(gt_coords, pred_coords, k_base=10, m=20):\n",
    "        \"\"\"NearMiss@m(k) = mean(h_i(k,m) / k)\"\"\"\n",
    "        gt_knn = knn_sets(gt_coords, k_base)\n",
    "        pred_knn = knn_sets(pred_coords, m)\n",
    "        hits = compute_hits(gt_knn, pred_knn)\n",
    "        return hits.mean() / k_base, hits / k_base\n",
    "    \n",
    "    def soft_weighted_jaccard(gt_coords, pr_coords, k=20, tau=None):\n",
    "        \"\"\"Compute soft distance-weighted Jaccard.\"\"\"\n",
    "        nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(gt_coords)\n",
    "        d_gt, idx_gt = nbrs_gt.kneighbors(gt_coords)\n",
    "        d_gt, idx_gt = d_gt[:, 1:], idx_gt[:, 1:]\n",
    "        \n",
    "        nbrs_pr = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(pr_coords)\n",
    "        d_pr, idx_pr = nbrs_pr.kneighbors(pr_coords)\n",
    "        d_pr, idx_pr = d_pr[:, 1:], idx_pr[:, 1:]\n",
    "        \n",
    "        if tau is None:\n",
    "            tau = np.median(d_gt[:, -1]) + 1e-12\n",
    "        \n",
    "        n = gt_coords.shape[0]\n",
    "        out = np.empty(n, dtype=np.float32)\n",
    "        \n",
    "        for i in range(n):\n",
    "            wgt = {int(j): np.exp(-float(d)/tau) for j, d in zip(idx_gt[i], d_gt[i])}\n",
    "            wpr = {int(j): np.exp(-float(d)/tau) for j, d in zip(idx_pr[i], d_pr[i])}\n",
    "            \n",
    "            keys = set(wgt.keys()) | set(wpr.keys())\n",
    "            num = 0.0\n",
    "            den = 0.0\n",
    "            for j in keys:\n",
    "                a = wgt.get(j, 0.0)\n",
    "                b = wpr.get(j, 0.0)\n",
    "                num += min(a, b)\n",
    "                den += max(a, b)\n",
    "            out[i] = num / max(1e-12, den)\n",
    "        \n",
    "        return out, tau\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 1) HARD kNN OVERLAP + JACCARD (k=10, 20, 50)\n",
    "    # ===================================================================\n",
    "    print(\"\\n--- Hard k-NN Metrics ---\")\n",
    "    \n",
    "    idx_gt_10 = knn_sets(gt_coords_np, 10)\n",
    "    idx_pr_10 = knn_sets(coords_pred, 10)\n",
    "    idx_gt_20 = knn_sets(gt_coords_np, 20)\n",
    "    idx_pr_20 = knn_sets(coords_pred, 20)\n",
    "    idx_gt_50 = knn_sets(gt_coords_np, min(50, n_cells-1))\n",
    "    idx_pr_50 = knn_sets(coords_pred, min(50, n_cells-1))\n",
    "    \n",
    "    for k, ig, ip in [(10, idx_gt_10, idx_pr_10), (20, idx_gt_20, idx_pr_20), (min(50, n_cells-1), idx_gt_50, idx_pr_50)]:\n",
    "        ov = knn_overlap_frac(ig, ip)\n",
    "        jc = knn_jaccard(ig, ip)\n",
    "        print(f\"[KNN] k={k:2d}: overlap mean={ov.mean():.3f} p50={np.median(ov):.3f} | \"\n",
    "              f\"jaccard mean={jc.mean():.3f} p50={np.median(jc):.3f}\")\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 2) RECALL@k AND NEARMISS@m METRICS\n",
    "    # ===================================================================\n",
    "    print(\"\\n--- Recall and NearMiss Metrics ---\")\n",
    "    \n",
    "    recall_10, recall_10_per_point = recall_at_k(gt_coords_np, coords_pred, k=10)\n",
    "    print(f\"[RECALL@10] mean={recall_10:.4f} p50={np.median(recall_10_per_point):.4f}\")\n",
    "    \n",
    "    nearmiss_20, nearmiss_20_per_point = nearmiss_at_m(gt_coords_np, coords_pred, k_base=10, m=20)\n",
    "    print(f\"[NEARMISS@20] (k_base=10) mean={nearmiss_20:.4f} p50={np.median(nearmiss_20_per_point):.4f}\")\n",
    "    \n",
    "    nearmiss_50, nearmiss_50_per_point = nearmiss_at_m(gt_coords_np, coords_pred, k_base=10, m=min(50, n_cells-1))\n",
    "    print(f\"[NEARMISS@50] (k_base=10) mean={nearmiss_50:.4f} p50={np.median(nearmiss_50_per_point):.4f}\")\n",
    "    \n",
    "    # ===================================================================\n",
    "    # 3) SOFT WEIGHTED JACCARD\n",
    "    # ===================================================================\n",
    "    print(\"\\n--- Soft Weighted Jaccard ---\")\n",
    "    \n",
    "    sj20, tau20 = soft_weighted_jaccard(gt_coords_np, coords_pred, k=20, tau=None)\n",
    "    sj50, tau50 = soft_weighted_jaccard(gt_coords_np, coords_pred, k=min(50, n_cells-1), tau=None)\n",
    "    print(f\"[SOFT-JACCARD] k=20 tau={tau20:.6f}: mean={sj20.mean():.3f} p50={np.median(sj20):.3f}\")\n",
    "    print(f\"[SOFT-JACCARD] k=50 tau={tau50:.6f}: mean={sj50.mean():.3f} p50={np.median(sj50):.3f}\")\n",
    "    \n",
    "    # ===================================================================\n",
    "    # SUMMARY\n",
    "    # ===================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    ov_10 = knn_overlap_frac(idx_gt_10, idx_pr_10).mean()\n",
    "    jc_10 = knn_jaccard(idx_gt_10, idx_pr_10).mean()\n",
    "    \n",
    "    print(f\"\\n  Global Metrics:\")\n",
    "    print(f\"    EDM Pearson:      {pearson_corr:.4f}\")\n",
    "    print(f\"    EDM Spearman:     {spearman_corr:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Local Metrics (Hard):\")\n",
    "    print(f\"    kNN@10 overlap:   {ov_10:.4f}\")\n",
    "    print(f\"    kNN@10 Jaccard:   {jc_10:.4f}\")\n",
    "    print(f\"    Recall@10:        {recall_10:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Local Metrics (Tolerant):\")\n",
    "    print(f\"    NearMiss@20:      {nearmiss_20:.4f}\")\n",
    "    print(f\"    NearMiss@50:      {nearmiss_50:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Local Metrics (Stable):\")\n",
    "    print(f\"    Soft Jaccard@20:  {sj20.mean():.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"k-NN ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No ground truth coordinates available - skipping k-NN preservation analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
