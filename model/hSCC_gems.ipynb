{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from threadpoolctl import threadpool_limits\n",
    "threadpool_limits(limits=1, user_api='blas')\n",
    "\n",
    "import torch\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import GEMS components\n",
    "from core_models_et_p1 import SharedEncoder, STStageBPrecomputer, STSetDataset\n",
    "from core_models_et_p2 import SetEncoderContext, DiffusionScoreNet, sample_sc_edm_anchored\n",
    "from core_models_et_p3 import GEMSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_process_cscc_data():\n",
    "    \"\"\"\n",
    "    Load and process the cSCC dataset with multiple ST replicates.\n",
    "    Follows exact structure from hSCC.ipynb Cell 6\n",
    "    \"\"\"\n",
    "    print(\"Loading cSCC data...\")\n",
    "    \n",
    "    # Load SC data\n",
    "    scadata = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/scP2.h5ad')\n",
    "    \n",
    "    # Load all 3 ST datasets\n",
    "    stadata1 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2.h5ad')\n",
    "    stadata2 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep2.h5ad')\n",
    "    stadata3 = sc.read_h5ad('/home/ehtesamul/sc_st/data/cSCC/processed/stP2rep3.h5ad')\n",
    "    \n",
    "    # Normalize and log transform\n",
    "    for adata in [scadata, stadata1, stadata2, stadata3]:\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "    \n",
    "    # Create rough cell types for SC data\n",
    "    scadata.obs['rough_celltype'] = scadata.obs['level1_celltype'].astype(str)\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CLEC9A', 'rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='CD1C', 'rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='ASDC', 'rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='PDC', 'rough_celltype'] = 'PDC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='MDSC', 'rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='LC', 'rough_celltype'] = 'DC'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Mac', 'rough_celltype'] = 'Myeloid cell'\n",
    "    scadata.obs.loc[scadata.obs['level1_celltype']=='Tcell', 'rough_celltype'] = 'T cell'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype']=='TSK', 'rough_celltype'] = 'TSK'\n",
    "    scadata.obs.loc[scadata.obs['level2_celltype'].isin(['Tumor_KC_Basal', 'Tumor_KC_Diff', 'Tumor_KC_Cyc']), 'rough_celltype'] = 'NonTSK'\n",
    "    \n",
    "    return scadata, stadata1, stadata2, stadata3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata):\n",
    "    \"\"\"\n",
    "    Combine all ST datasets for diffusion training while maintaining gene alignment.\n",
    "    Follows exact structure from hSCC.ipynb\n",
    "    \"\"\"\n",
    "    print(\"Preparing combined ST data for diffusion training...\")\n",
    "    \n",
    "    # Get common genes between SC and all ST datasets\n",
    "    sc_genes = set(scadata.var_names)\n",
    "    st1_genes = set(stadata1.var_names)\n",
    "    st2_genes = set(stadata2.var_names)\n",
    "    st3_genes = set(stadata3.var_names)\n",
    "    \n",
    "    common_genes = sorted(list(sc_genes & st1_genes & st2_genes & st3_genes))\n",
    "    print(f\"Common genes across all datasets: {len(common_genes)}\")\n",
    "    \n",
    "    # Extract aligned expression data\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    st1_expr = stadata1[:, common_genes].X\n",
    "    st2_expr = stadata2[:, common_genes].X\n",
    "    st3_expr = stadata3[:, common_genes].X\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    if hasattr(st1_expr, 'toarray'):\n",
    "        st1_expr = st1_expr.toarray()\n",
    "    if hasattr(st2_expr, 'toarray'):\n",
    "        st2_expr = st2_expr.toarray()\n",
    "    if hasattr(st3_expr, 'toarray'):\n",
    "        st3_expr = st3_expr.toarray()\n",
    "    \n",
    "    # Get spatial coordinates\n",
    "    st1_coords = stadata1.obsm['spatial']\n",
    "    st2_coords = stadata2.obsm['spatial']\n",
    "    st3_coords = stadata3.obsm['spatial']\n",
    "    \n",
    "    # Combine ST data\n",
    "    X_st_combined = np.vstack([st1_expr, st2_expr, st3_expr])\n",
    "    Y_st_combined = np.vstack([st1_coords, st2_coords, st3_coords])\n",
    "    \n",
    "    # Create dataset labels\n",
    "    dataset_labels = np.concatenate([\n",
    "        np.zeros(st1_expr.shape[0], dtype=int),\n",
    "        np.ones(st2_expr.shape[0], dtype=int),\n",
    "        np.full(st3_expr.shape[0], 2, dtype=int)\n",
    "    ])\n",
    "    \n",
    "    st_coords_list = [st1_coords, st2_coords, st3_coords]\n",
    "    \n",
    "    return sc_expr, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensors_for_gems(scadata, stadata1, stadata2, stadata3, common_genes):\n",
    "    \"\"\"\n",
    "    Convert AnnData to PyTorch tensors with slide IDs for GEMS training.\n",
    "    \"\"\"\n",
    "    # SC expression\n",
    "    sc_expr = scadata[:, common_genes].X\n",
    "    if hasattr(sc_expr, 'toarray'):\n",
    "        sc_expr = sc_expr.toarray()\n",
    "    \n",
    "    # ST expression and coordinates (combined)\n",
    "    st_expr_list = []\n",
    "    st_coords_list = []\n",
    "    slide_ids_list = []\n",
    "    slides_dict = {}\n",
    "    st_gene_expr_dict = {}\n",
    "    \n",
    "    stadatas = [stadata1, stadata2, stadata3]\n",
    "    \n",
    "    for slide_id, stadata in enumerate(stadatas):\n",
    "        st_expr = stadata[:, common_genes].X\n",
    "        if hasattr(st_expr, 'toarray'):\n",
    "            st_expr = st_expr.toarray()\n",
    "        \n",
    "        st_coords = stadata.obsm['spatial']\n",
    "        n_spots = st_expr.shape[0]\n",
    "        \n",
    "        st_expr_list.append(st_expr)\n",
    "        st_coords_list.append(st_coords)\n",
    "        slide_ids_list.append(np.full(n_spots, slide_id))\n",
    "        \n",
    "        # For Stage B\n",
    "        slides_dict[slide_id] = (\n",
    "            torch.tensor(st_coords, dtype=torch.float32),\n",
    "            torch.tensor(st_expr, dtype=torch.float32)\n",
    "        )\n",
    "        st_gene_expr_dict[slide_id] = torch.tensor(st_expr, dtype=torch.float32)\n",
    "    \n",
    "    # Combine\n",
    "    sc_expr = torch.tensor(sc_expr, dtype=torch.float32)\n",
    "    st_expr_combined = torch.tensor(np.vstack(st_expr_list), dtype=torch.float32)\n",
    "    st_coords_combined = torch.tensor(np.vstack(st_coords_list), dtype=torch.float32)\n",
    "    slide_ids = torch.tensor(np.concatenate(slide_ids_list), dtype=torch.long)\n",
    "    \n",
    "    return sc_expr, st_expr_combined, st_coords_combined, slide_ids, slides_dict, st_gene_expr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRAINING PIPELINE\n",
    "# ==============================================================================\n",
    "\n",
    "def train_gems_hscc(\n",
    "    sc_expr,\n",
    "    st_expr_combined,\n",
    "    st_coords_combined,\n",
    "    slide_ids,\n",
    "    slides_dict,\n",
    "    st_gene_expr_dict,\n",
    "    n_genes,\n",
    "    output_dir='gems_hscc_p2_output',\n",
    "    device='cuda'\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete GEMS training pipeline for hSCC P2 data.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GEMS hSCC P2 TRAINING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"SC cells: {sc_expr.shape[0]}\")\n",
    "    print(f\"ST spots (total): {st_expr_combined.shape[0]}\")\n",
    "    print(f\"Number of slides: {len(slides_dict)}\")\n",
    "    print(f\"Genes: {n_genes}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = GEMSModel(\n",
    "        n_genes=n_genes,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        D_latent=16,\n",
    "        c_dim=256,\n",
    "        n_heads=4,\n",
    "        isab_m=64,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE A: Train Shared Encoder\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STAGE A: Training Shared Encoder (Multi-Slide)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "\n",
    "    model.train_stageA(\n",
    "        st_gene_expr=st_expr_combined,\n",
    "        st_coords=st_coords_combined,\n",
    "        sc_gene_expr=sc_expr,\n",
    "        slide_ids=slide_ids,\n",
    "        n_epochs=1200,\n",
    "        batch_size=256,\n",
    "        lr=0.002,  # Your old lr_e      \n",
    "        sigma=None,     \n",
    "        alpha=0.8,\n",
    "        mmdbatch=1.0,            \n",
    "        ratio_start=0.0,   \n",
    "        ratio_end=1.0,\n",
    "        \n",
    "        outf=output_dir\n",
    "    )\n",
    "        \n",
    "    # ========================================================================\n",
    "    # STAGE B: Precompute Geometric Targets\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STAGE B: Precomputing Geometric Targets\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    slides_dict_device = {\n",
    "        sid: (coords.to(device), expr.to(device))\n",
    "        for sid, (coords, expr) in slides_dict.items()\n",
    "    }\n",
    "    \n",
    "    model.train_stageB(\n",
    "        slides=slides_dict_device,\n",
    "        outdir=Path(output_dir) / 'stage_b_cache'\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STAGE C: Train Diffusion Generator\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STAGE C: Training Diffusion Generator\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    st_gene_expr_dict_device = {\n",
    "        sid: expr.to(device)\n",
    "        for sid, expr in st_gene_expr_dict.items()\n",
    "    }\n",
    "    \n",
    "    model.train_stageC(\n",
    "        st_gene_expr_dict=st_gene_expr_dict_device,\n",
    "        n_min=128,\n",
    "        n_max=512,\n",
    "        num_samples=3000,\n",
    "        n_epochs=10,\n",
    "        batch_size=8,\n",
    "        lr=1e-4,\n",
    "        n_timesteps=1000,\n",
    "        sigma_min=0.01,\n",
    "        sigma_max=50.0,\n",
    "        loss_weights={'alpha': 0.1, 'beta': 1.0, 'gamma': 0.5, 'eta': 0.5},\n",
    "        outf=output_dir\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(Path(output_dir) / 'gems_model_hscc_p2.pt')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "hSCC P2 GEMS TRAINING - START\n",
      "======================================================================\n",
      "\n",
      "Step 1: Loading data...\n",
      "Loading cSCC data...\n",
      "\n",
      "Data loaded successfully:\n",
      "  SC cells: 2688\n",
      "  ST slide 1: 666 spots\n",
      "  ST slide 2: 646 spots\n",
      "  ST slide 3: 638 spots\n",
      "\n",
      "Step 2: Preparing data for GEMS...\n",
      "Preparing combined ST data for diffusion training...\n",
      "Common genes across all datasets: 2000\n",
      "\n",
      "Data preparation complete:\n",
      "  SC expression: torch.Size([2688, 2000])\n",
      "  ST expression (combined): torch.Size([1950, 2000])\n",
      "  ST coordinates (combined): torch.Size([1950, 2])\n",
      "  Slide IDs: torch.Size([1950])\n",
      "  Common genes: 2000\n",
      "\n",
      "Step 3: Training GEMS model...\n",
      "\n",
      "======================================================================\n",
      "GEMS hSCC P2 TRAINING PIPELINE\n",
      "======================================================================\n",
      "Device: cuda\n",
      "SC cells: 2688\n",
      "ST spots (total): 1950\n",
      "Number of slides: 3\n",
      "Genes: 2000\n",
      "GEMS Model initialized:\n",
      "  Encoder: 2000 → [512, 256, 128]\n",
      "  D_latent: 16\n",
      "  Context dim: 256\n",
      "  ISAB inducing points: 64\n",
      "\n",
      "======================================================================\n",
      "STAGE A: Training Shared Encoder (Multi-Slide)\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "STAGE A: Training Shared Encoder\n",
      "============================================================\n",
      "Auto-computed sigma = 0.1034\n",
      "Training encoder for 1200 epochs...\n",
      "Epoch 0/1200 | Loss: 3.7872 | Pred: 3.7810 | Circle: 4.6428 | MMD: 0.0078\n",
      "Epoch 100/1200 | Loss: 1.8362 | Pred: 1.5903 | Circle: 2.3004 | MMD: 0.0078\n",
      "Epoch 200/1200 | Loss: 1.7582 | Pred: 1.3989 | Circle: 1.6945 | MMD: 0.0078\n",
      "Epoch 300/1200 | Loss: 1.6442 | Pred: 1.2616 | Circle: 1.2041 | MMD: 0.0078\n",
      "Epoch 400/1200 | Loss: 1.5630 | Pred: 1.2028 | Circle: 0.8494 | MMD: 0.0078\n",
      "Epoch 500/1200 | Loss: 1.5232 | Pred: 1.2147 | Circle: 0.5804 | MMD: 0.0078\n",
      "Epoch 600/1200 | Loss: 1.5070 | Pred: 1.1919 | Circle: 0.4941 | MMD: 0.0078\n",
      "Epoch 700/1200 | Loss: 1.4830 | Pred: 1.2100 | Circle: 0.3659 | MMD: 0.0078\n",
      "Epoch 800/1200 | Loss: 1.4457 | Pred: 1.2133 | Circle: 0.2713 | MMD: 0.0078\n",
      "Epoch 900/1200 | Loss: 1.4334 | Pred: 1.2354 | Circle: 0.2046 | MMD: 0.0078\n",
      "Epoch 1000/1200 | Loss: 1.3698 | Pred: 1.1743 | Circle: 0.1892 | MMD: 0.0078\n",
      "Epoch 1100/1200 | Loss: 1.3701 | Pred: 1.1914 | Circle: 0.1725 | MMD: 0.0078\n",
      "Encoder training complete!\n",
      "Stage A complete. Encoder frozen.\n",
      "\n",
      "======================================================================\n",
      "STAGE B: Precomputing Geometric Targets\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "STAGE B: Precomputing Geometric Targets\n",
      "============================================================\n",
      "Precomputing targets for slide 0...\n",
      "  Saved to gems_hscc_p2_output/stage_b_cache/slide_0_targets.pt\n",
      "Precomputing targets for slide 1...\n",
      "  Saved to gems_hscc_p2_output/stage_b_cache/slide_1_targets.pt\n",
      "Precomputing targets for slide 2...\n",
      "  Saved to gems_hscc_p2_output/stage_b_cache/slide_2_targets.pt\n",
      "Stage B precomputation complete!\n",
      "Stage B complete. Targets cached.\n",
      "\n",
      "======================================================================\n",
      "STAGE C: Training Diffusion Generator\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "STAGE C: Training Diffusion Generator\n",
      "============================================================\n",
      "Created dataset with 3000 mini-sets\n",
      "Training Stage C for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/375 [00:00<01:26,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Conditioning] Epoch 0, Batch 0: ||eps_c - eps_u|| / ||eps_u|| = 0.7957 (should be > 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   6%|▌         | 22/375 [00:03<00:57,  6.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 107\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Train GEMS\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 3: Training GEMS model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gems_hscc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43msc_expr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msc_expr_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mst_expr_combined\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mst_expr_combined\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mst_coords_combined\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mst_coords_combined\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslide_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslide_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslides_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslides_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mst_gene_expr_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mst_gene_expr_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_genes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_genes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# ========================================================================\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 4: Running inference on SC data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 95\u001b[0m, in \u001b[0;36mtrain_gems_hscc\u001b[0;34m(sc_expr, st_expr_combined, st_coords_combined, slide_ids, slides_dict, st_gene_expr_dict, n_genes, output_dir, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m     90\u001b[0m st_gene_expr_dict_device \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     91\u001b[0m     sid: expr\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sid, expr \u001b[38;5;129;01min\u001b[39;00m st_gene_expr_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     93\u001b[0m }\n\u001b[0;32m---> 95\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_stageC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mst_gene_expr_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mst_gene_expr_dict_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m    111\u001b[0m model\u001b[38;5;241m.\u001b[39msave(Path(output_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgems_model_hscc_p2.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/sc_st/model/core_models_et_p3.py:252\u001b[0m, in \u001b[0;36mGEMSModel.train_stageC\u001b[0;34m(self, st_gene_expr_dict, n_min, n_max, num_samples, n_epochs, batch_size, lr, n_timesteps, sigma_min, sigma_max, loss_weights, outf)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated dataset with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m mini-sets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m \u001b[43mtrain_stageC_diffusion_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_encoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscore_net\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_net\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigma_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutf\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStage C complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/sc_st/model/core_models_et_p2.py:458\u001b[0m, in \u001b[0;36mtrain_stageC_diffusion_generator\u001b[0;34m(context_encoder, generator, score_net, dataset, n_epochs, batch_size, lr, n_timesteps, sigma_min, sigma_max, loss_weights, device, outf)\u001b[0m\n\u001b[1;32m    454\u001b[0m batch_losses_this_epoch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    457\u001b[0m \u001b[38;5;66;03m# for batch in dataloader:\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    459\u001b[0m     Z_set \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ_set\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    460\u001b[0m     V_target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV_target\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ehtesamenv_gains/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ehtesamenv_gains/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/ehtesamenv_gains/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/ehtesamenv_gains/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/ehtesamenv_gains/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/sc_st/model/core_models_et_p1.py:502\u001b[0m, in \u001b[0;36mSTSetDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    499\u001b[0m L_subset \u001b[38;5;241m=\u001b[39m uet\u001b[38;5;241m.\u001b[39mcompute_graph_laplacian(edge_index, edge_weight, n)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# Recompute distance histogram for subset\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m d_95 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantile(D_subset[torch\u001b[38;5;241m.\u001b[39mtriu(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_subset\u001b[49m\u001b[43m)\u001b[49m, diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()], \u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m    503\u001b[0m bins \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, d_95, \u001b[38;5;241m64\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    504\u001b[0m H_subset \u001b[38;5;241m=\u001b[39m uet\u001b[38;5;241m.\u001b[39mcompute_distance_hist(D_subset, bins)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# INFERENCE\n",
    "# ==============================================================================\n",
    "\n",
    "def infer_sc_coordinates(model, sc_expr, device='cuda'):\n",
    "    \"\"\"\n",
    "    Infer SC coordinates using trained GEMS model.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SC COORDINATE INFERENCE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = model.infer_sc(\n",
    "        sc_gene_expr=sc_expr.to(device),\n",
    "        n_samples=1,\n",
    "        n_timesteps_sample=300,\n",
    "        return_coords=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# VISUALIZATION - Following hSCC.ipynb style\n",
    "# ==============================================================================\n",
    "\n",
    "def visualize_results_hscc(scadata, save_dir='figures'):\n",
    "    \"\"\"\n",
    "    Visualize inferred SC coordinates following hSCC.ipynb style.\n",
    "    \"\"\"\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    sc.settings.set_figure_params(format='svg')\n",
    "    \n",
    "    n_groups = scadata.obs[\"rough_celltype\"].nunique()\n",
    "    my_tab20 = sns.color_palette(\"tab20\", n_colors=n_groups).as_hex()\n",
    "    \n",
    "    # Plot averaged coordinates\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sc.pl.embedding(\n",
    "        scadata, \n",
    "        basis='gems_coords_avg', \n",
    "        color='rough_celltype',\n",
    "        size=85, \n",
    "        title='SC GEMS Coordinates (Averaged)',\n",
    "        palette=my_tab20, \n",
    "        legend_loc='right margin', \n",
    "        legend_fontsize=10,\n",
    "        save='_hscc_p2_gems_avg.svg'\n",
    "    )\n",
    "    \n",
    "    print(f\"Figures saved to {save_dir}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Main execution for hSCC Patient 2 training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configuration\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    output_dir = 'gems_hscc_p2_output'\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"hSCC P2 GEMS TRAINING - START\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Load and process data\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 1: Loading data...\")\n",
    "    scadata, stadata1, stadata2, stadata3 = load_and_process_cscc_data()\n",
    "    \n",
    "    print(f\"\\nData loaded successfully:\")\n",
    "    print(f\"  SC cells: {scadata.shape[0]}\")\n",
    "    print(f\"  ST slide 1: {stadata1.shape[0]} spots\")\n",
    "    print(f\"  ST slide 2: {stadata2.shape[0]} spots\")\n",
    "    print(f\"  ST slide 3: {stadata3.shape[0]} spots\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Prepare data for GEMS\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 2: Preparing data for GEMS...\")\n",
    "    sc_expr, X_st_combined, Y_st_combined, dataset_labels, common_genes, st_coords_list = \\\n",
    "        prepare_combined_st_for_diffusion(stadata1, stadata2, stadata3, scadata)\n",
    "    \n",
    "    sc_expr_tensor, st_expr_combined, st_coords_combined, slide_ids, slides_dict, st_gene_expr_dict = \\\n",
    "        prepare_tensors_for_gems(scadata, stadata1, stadata2, stadata3, common_genes)\n",
    "    \n",
    "    n_genes = len(common_genes)\n",
    "    \n",
    "    print(f\"\\nData preparation complete:\")\n",
    "    print(f\"  SC expression: {sc_expr_tensor.shape}\")\n",
    "    print(f\"  ST expression (combined): {st_expr_combined.shape}\")\n",
    "    print(f\"  ST coordinates (combined): {st_coords_combined.shape}\")\n",
    "    print(f\"  Slide IDs: {slide_ids.shape}\")\n",
    "    print(f\"  Common genes: {n_genes}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Train GEMS\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 3: Training GEMS model...\")\n",
    "    model = train_gems_hscc(\n",
    "        sc_expr=sc_expr_tensor,\n",
    "        st_expr_combined=st_expr_combined,\n",
    "        st_coords_combined=st_coords_combined,\n",
    "        slide_ids=slide_ids,\n",
    "        slides_dict=slides_dict,\n",
    "        st_gene_expr_dict=st_gene_expr_dict,\n",
    "        n_genes=n_genes,\n",
    "        output_dir=output_dir,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Inference\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 4: Running inference on SC data...\")\n",
    "    # results = infer_sc_coordinates(model, sc_expr_tensor, device=device)\n",
    "    \n",
    "    print(\"\\nStep 4: Running inference on SC data...\")\n",
    "\n",
    "    # Clear cache before inference\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    #Use batched inference\n",
    "    results = model.infer_sc_batched(\n",
    "        sc_gene_expr=sc_expr_tensor.to(device),\n",
    "        n_timesteps_sample=250,\n",
    "        return_coords=True,\n",
    "        batch_size=512  # Start here, reduce to 256/128 if still OOM\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInference complete:\")\n",
    "    print(f\"  D_edm shape: {results['D_edm'].shape}\")\n",
    "    if 'coords_canon' in results:\n",
    "        print(f\"  Coordinates shape: {results['coords_canon'].shape}\")\n",
    "    \n",
    "    # Add coordinates to scadata\n",
    "    coords_avg = results['coords_canon'].cpu().numpy()\n",
    "    scadata.obsm['gems_coords_avg'] = coords_avg\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Visualization\n",
    "    # ========================================================================\n",
    "    print(\"\\nStep 5: Visualizing results...\")\n",
    "    visualize_results_hscc(scadata, save_dir='figures')\n",
    "    \n",
    "    # Save scadata with GEMS coordinates\n",
    "    scadata.write_h5ad(Path(output_dir) / 'scadata_with_gems_coords.h5ad')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"hSCC P2 GEMS TRAINING - COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nOutputs:\")\n",
    "    print(f\"  Model: {output_dir}/gems_model_hscc_p2.pt\")\n",
    "    print(f\"  Cached targets: {output_dir}/stage_b_cache/\")\n",
    "    print(f\"  SC data with coords: {output_dir}/scadata_with_gems_coords.h5ad\")\n",
    "    print(f\"  Figures: figures/\")\n",
    "    print(\"\\nCoordinates stored in scadata.obsm['gems_coords_avg']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== COORDINATE CHECK ===\")\n",
    "print(f\"Keys in results: {results.keys()}\")\n",
    "print(f\"D_edm shape: {results['D_edm'].shape}\")\n",
    "print(f\"D_edm min/max: {results['D_edm'].min():.4f} / {results['D_edm'].max():.4f}\")\n",
    "\n",
    "if 'coords_canon' in results:\n",
    "    coords = results['coords_canon'].cpu().numpy()\n",
    "    print(f\"\\nCoordinates shape: {coords.shape}\")\n",
    "    print(f\"X range: [{coords[:, 0].min():.4f}, {coords[:, 0].max():.4f}]\")\n",
    "    print(f\"Y range: [{coords[:, 1].min():.4f}, {coords[:, 1].max():.4f}]\")\n",
    "    print(f\"\\nFirst 5 coordinates:\\n{coords[:5]}\")\n",
    "    \n",
    "    # Save as CSV to check\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(coords, columns=['X', 'Y']).to_csv('coords_check.csv', index=False)\n",
    "    print(\"\\nSaved to coords_check.csv\")\n",
    "else:\n",
    "    print(\"WARNING: No coords_canon in results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 7: VISUALIZE GEMS COORDINATES WITH CELL TYPES\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: Visualizing GEMS Coordinates\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Verify coordinates are present\n",
    "if 'gems_coords_avg' not in scadata.obsm:\n",
    "    raise ValueError(\"gems_coords_avg not found in scadata.obsm. Run inference first!\")\n",
    "\n",
    "# Check for NaNs in coordinates\n",
    "coords = scadata.obsm['gems_coords_avg']\n",
    "if np.isnan(coords).any():\n",
    "    print(\"WARNING: NaN values detected in coordinates!\")\n",
    "    print(f\"  NaN count: {np.isnan(coords).sum()}\")\n",
    "    print(\"  Skipping visualization.\")\n",
    "else:\n",
    "    print(f\"Coordinates shape: {coords.shape}\")\n",
    "    print(f\"X range: [{coords[:, 0].min():.4f}, {coords[:, 0].max():.4f}]\")\n",
    "    print(f\"Y range: [{coords[:, 1].min():.4f}, {coords[:, 1].max():.4f}]\")\n",
    "    \n",
    "    # Set up plotting parameters\n",
    "    sc.settings.set_figure_params(dpi=300, frameon=False, format='png')\n",
    "    \n",
    "    # Get number of unique cell types and create palette\n",
    "    n_celltypes = scadata.obs['rough_celltype'].nunique()\n",
    "    my_tab20 = sns.color_palette(\"tab20\", n_colors=n_celltypes).as_hex()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot with scanpy\n",
    "    sc.pl.embedding(\n",
    "        scadata,\n",
    "        basis='gems_coords_avg',\n",
    "        color='rough_celltype',\n",
    "        size=80,\n",
    "        title='SC GEMS Coordinates (Averaged) - cSCC P2',\n",
    "        palette=my_tab20,\n",
    "        legend_loc='right margin',\n",
    "        legend_fontsize=10,\n",
    "        frameon=False,\n",
    "        ax=ax,\n",
    "        show=False\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/gems_coords_avg_hscc_p2.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nFigure saved to figures/gems_coords_avg_hscc_p2.png\")\n",
    "\n",
    "# ==============================================================================\n",
    "# OPTIONAL: Save detailed statistics\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Coordinate Statistics\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if not np.isnan(coords).any():\n",
    "    # Per cell type statistics\n",
    "    for celltype in scadata.obs['rough_celltype'].unique():\n",
    "        mask = scadata.obs['rough_celltype'] == celltype\n",
    "        ct_coords = coords[mask]\n",
    "        print(f\"\\n{celltype}:\")\n",
    "        print(f\"  Count: {mask.sum()}\")\n",
    "        print(f\"  X: [{ct_coords[:, 0].min():.3f}, {ct_coords[:, 0].max():.3f}]\")\n",
    "        print(f\"  Y: [{ct_coords[:, 1].min():.3f}, {ct_coords[:, 1].max():.3f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VISUALIZATION COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
