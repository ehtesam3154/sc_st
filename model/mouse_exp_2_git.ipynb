{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "from core_models_et_p1 import STSetDataset, SCSetDataset\n",
    "import utils_et as uet\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ===================================================================\n",
    "# 1. LOAD DATA\n",
    "# ===================================================================\n",
    "print(\"Loading data...\")\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "sc_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st2_counts_et.csv'\n",
    "sc_meta = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st2_metadata_et.csv'\n",
    "\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "sc_expr_df = pd.read_csv(sc_counts, index_col=0)\n",
    "sc_meta_df = pd.read_csv(sc_meta, index_col=0)\n",
    "scadata = ad.AnnData(X=sc_expr_df.values.T)\n",
    "scadata.obs_names = sc_expr_df.columns\n",
    "scadata.var_names = sc_expr_df.index\n",
    "scadata.obsm['spatial_gt'] = sc_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_st = stadata[:, common].X\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_st, \"toarray\"): X_st = X_st.toarray()\n",
    "if hasattr(X_sc, \"toarray\"): X_sc = X_sc.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32, device=device)\n",
    "\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, _, _ = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"✓ ST: {st_expr.shape[0]} spots × {st_expr.shape[1]} genes\")\n",
    "print(f\"✓ SC: {sc_expr.shape[0]} cells × {sc_expr.shape[1]} genes\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. LOAD TRAINED MODEL\n",
    "# ===================================================================\n",
    "print(\"\\nLoading trained model...\")\n",
    "checkpoint_path = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output/phase1_st_checkpoint.pt\"\n",
    "# checkpoint_path = '/home/ehtesamul/sc_st/model/gems_v2_output/final_checkpoint_20251205_233814.pt'\n",
    "\n",
    "n_genes = len(common)\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=device,\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=0,\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.generator.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "print(\"✓ Model loaded\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. RUN STAGE B\n",
    "# ===================================================================\n",
    "print(\"\\nRunning Stage B...\")\n",
    "slides_dict = {0: (st_coords, st_expr)}\n",
    "model.train_stageB(\n",
    "    slides=slides_dict,\n",
    "    outdir='temp_miniset_cache'\n",
    ")\n",
    "print(\"✓ Stage B complete\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4. CREATE MINI-SET DATASETS\n",
    "# ===================================================================\n",
    "print(\"\\nCreating mini-set datasets...\")\n",
    "\n",
    "st_gene_expr_dict_cpu = {0: st_expr.cpu()}\n",
    "st_dataset = STSetDataset(\n",
    "    targets_dict=model.targets_dict,\n",
    "    encoder=model.encoder,\n",
    "    st_gene_expr_dict=st_gene_expr_dict_cpu,\n",
    "    n_min=128,\n",
    "    n_max=384,\n",
    "    D_latent=model.D_latent,\n",
    "    num_samples=15,\n",
    "    knn_k=12,\n",
    "    device=device,\n",
    "    landmarks_L=0,\n",
    "    pool_mult=2.0,\n",
    "    stochastic_tau=1.0,\n",
    ")\n",
    "\n",
    "sc_gene_expr_cpu = sc_expr.cpu()\n",
    "# sc_dataset = SCSetDataset(\n",
    "#     sc_gene_expr=sc_gene_expr_cpu,\n",
    "#     encoder=model.encoder,\n",
    "#     n_min=128,\n",
    "#     n_max=384,\n",
    "#     n_large_max=384,\n",
    "#     num_samples=25,\n",
    "#     device=device,\n",
    "#     landmarks_L=0\n",
    "# )\n",
    "\n",
    "\n",
    "sc_dataset = SCSetDataset(\n",
    "    sc_gene_expr=sc_gene_expr_cpu,\n",
    "    encoder=model.encoder,\n",
    "    n_min=384,\n",
    "    n_max=512,\n",
    "    num_samples=25,\n",
    "    device=device,\n",
    "    landmarks_L=0,\n",
    "    pool_mult=2.0,       # same style as ST\n",
    "    stochastic_tau=1.0,  # same style as ST\n",
    "    knn_k=12,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"✓ ST dataset: {len(st_dataset)} mini-sets\")\n",
    "print(f\"✓ SC dataset: {len(sc_dataset)} mini-sets\")\n",
    "\n",
    "# ===================================================================\n",
    "# 5. SAMPLE MINI-SETS\n",
    "# ===================================================================\n",
    "print(\"\\nSampling mini-sets...\")\n",
    "\n",
    "st_minisets = []\n",
    "sc_minisets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(st_dataset)):\n",
    "        st_minisets.append(st_dataset[i])\n",
    "    \n",
    "    for i in range(len(sc_dataset)):\n",
    "        sc_minisets.append(sc_dataset[i])\n",
    "\n",
    "print(f\"✓ Sampled {len(st_minisets)} ST mini-sets\")\n",
    "print(f\"✓ Sampled {len(sc_minisets)} SC mini-sets\")\n",
    "\n",
    "# Check structure\n",
    "print(f\"\\nST mini-set #0 keys: {list(st_minisets[0].keys())}\")\n",
    "print(f\"SC mini-set #0 keys: {list(sc_minisets[0].keys())}\")\n",
    "\n",
    "print(f\"\\nSample ST mini-set #0:\")\n",
    "print(f\"  Z_set shape: {st_minisets[0]['Z_set'].shape}\")\n",
    "print(f\"  V_target shape: {st_minisets[0]['V_target'].shape}\")\n",
    "print(f\"  n points: {st_minisets[0]['n']}\")\n",
    "\n",
    "# print(f\"\\nSample SC mini-set #0 (overlapping pair A+B):\")\n",
    "# print(f\"  Z_A shape: {sc_minisets[0]['Z_A'].shape}\")\n",
    "# print(f\"  Z_B shape: {sc_minisets[0]['Z_B'].shape}\")\n",
    "# print(f\"  n_A: {sc_minisets[0]['n_A']}, n_B: {sc_minisets[0]['n_B']}\")\n",
    "# print(f\"  shared_A length: {len(sc_minisets[0]['shared_A'])}\")\n",
    "# print(f\"  shared_B length: {len(sc_minisets[0]['shared_B'])}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. PLOT MINI-SETS\n",
    "# ===================================================================\n",
    "print(\"\\nPlotting mini-sets...\")\n",
    "\n",
    "# PARAMETERS - Adjust as needed\n",
    "n_st_plots = 5  # Number of ST mini-sets to plot\n",
    "n_sc_plots = 10  # Number of SC mini-sets to plot\n",
    "max_cols = 3    # Maximum plots per row\n",
    "\n",
    "# Plot ST mini-sets\n",
    "if n_st_plots > 0:\n",
    "    n_rows_st = (n_st_plots + max_cols - 1) // max_cols\n",
    "    n_cols_st = min(n_st_plots, max_cols)\n",
    "    \n",
    "    fig_st, axes_st = plt.subplots(n_rows_st, n_cols_st, \n",
    "                                    figsize=(6.5*n_cols_st, 6*n_rows_st))\n",
    "    axes_st = np.atleast_2d(axes_st).reshape(n_rows_st, n_cols_st)\n",
    "    \n",
    "    for i in range(n_st_plots):\n",
    "        row, col = i // max_cols, i % max_cols\n",
    "        ax = axes_st[row, col]\n",
    "        coords = st_minisets[i]['V_target'].cpu().numpy()\n",
    "        n = st_minisets[i]['n']\n",
    "        ax.scatter(coords[:n, 0], coords[:n, 1], s=20, alpha=0.7)\n",
    "        ax.set_title(f'ST Mini-set {i} (n={n})', fontsize=10)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_st_plots, n_rows_st * n_cols_st):\n",
    "        row, col = i // max_cols, i % max_cols\n",
    "        axes_st[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('st_minisets_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Plot SC mini-sets\n",
    "if n_sc_plots > 0:\n",
    "    n_rows_sc = (n_sc_plots + max_cols - 1) // max_cols\n",
    "    n_cols_sc = min(n_sc_plots, max_cols)\n",
    "    \n",
    "    fig_sc, axes_sc = plt.subplots(n_rows_sc, n_cols_sc,\n",
    "                                    figsize=(6.5*n_cols_sc, 6*n_rows_sc))\n",
    "    axes_sc = np.atleast_2d(axes_sc).reshape(n_rows_sc, n_cols_sc)\n",
    "    \n",
    "    for i in range(n_sc_plots):\n",
    "        row, col = i // max_cols, i % max_cols\n",
    "        ax = axes_sc[row, col]\n",
    "        # indices_A = sc_minisets[i]['global_indices_A'].cpu().numpy()\n",
    "        # n_A = sc_minisets[i]['n_A']\n",
    "        # coords_gt = scadata.obsm['spatial_gt'][indices_A[:n_A]]\n",
    "        # ax.scatter(coords_gt[:, 0], coords_gt[:, 1], s=20, alpha=0.7, c='orange')\n",
    "        # ax.set_title(f'SC Mini-set {i} - Set A (n={n_A})', fontsize=10)\n",
    "        # ax.set_aspect('equal')\n",
    "        # ax.grid(True, alpha=0.3)\n",
    "        indices = sc_minisets[i]['global_indices'].cpu().numpy()\n",
    "        n = sc_minisets[i]['n']\n",
    "        coords_gt = scadata.obsm['spatial_gt'][indices[:n]]\n",
    "        ax.scatter(coords_gt[:, 0], coords_gt[:, 1], s=20, alpha=0.7, c='orange')\n",
    "        ax.set_title(f'SC Mini-set {i} (n={n})', fontsize=10)\n",
    "\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_sc_plots, n_rows_sc * n_cols_sc):\n",
    "        row, col = i // max_cols, i % max_cols\n",
    "        axes_sc[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('sc_minisets_visualization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n✓ MINI-SETS READY FOR INFERENCE!\")\n",
    "print(\"\\nNote: SC mini-sets contain overlapping pairs (A, B)\")\n",
    "print(\"      Each has Z_A, Z_B embeddings and global_indices_A, global_indices_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE GLOBAL kNN ON ST COORDINATES\n",
    "# ===================================================================\n",
    "k_global = 15\n",
    "\n",
    "st_coords_cpu = st_coords.cpu().numpy()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k_global + 1, algorithm='ball_tree').fit(st_coords_cpu)\n",
    "distances, global_knn_indices = nbrs.kneighbors(st_coords_cpu)\n",
    "global_knn_indices = global_knn_indices[:, 1:]  # Remove self\n",
    "\n",
    "print(f\"✓ Computed global {k_global}-NN for {st_coords_cpu.shape[0]} ST spots\")\n",
    "\n",
    "# ===================================================================\n",
    "# CHECK NEIGHBOR COVERAGE IN EACH MINISET\n",
    "# ===================================================================\n",
    "coverage_ratios = []\n",
    "\n",
    "for miniset_idx, miniset in enumerate(st_minisets):\n",
    "    global_idx = miniset['overlap_info']['indices'].cpu().numpy()\n",
    "    \n",
    "    for local_i, global_i in enumerate(global_idx):\n",
    "        true_neighbors = set(global_knn_indices[global_i])\n",
    "        miniset_cells = set(global_idx)\n",
    "        neighbors_in_miniset = true_neighbors & miniset_cells\n",
    "        coverage = len(neighbors_in_miniset) / k_global\n",
    "        coverage_ratios.append(coverage)\n",
    "\n",
    "coverage_ratios = np.array(coverage_ratios)\n",
    "\n",
    "# ===================================================================\n",
    "# STATISTICS\n",
    "# ===================================================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"NEIGHBOR COVERAGE STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total cell appearances: {len(coverage_ratios)}\")\n",
    "print(f\"Mean coverage: {coverage_ratios.mean():.3f} ({coverage_ratios.mean() * k_global:.1f}/{k_global})\")\n",
    "print(f\"Median coverage: {np.median(coverage_ratios):.3f} ({np.median(coverage_ratios) * k_global:.1f}/{k_global})\")\n",
    "print(f\"Min coverage: {coverage_ratios.min():.3f} ({coverage_ratios.min() * k_global:.1f}/{k_global})\")\n",
    "print(f\"Max coverage: {coverage_ratios.max():.3f} ({coverage_ratios.max() * k_global:.1f}/{k_global})\")\n",
    "\n",
    "print(f\"\\nCoverage distribution:\")\n",
    "for threshold in [0, 2, 5, 8, 10, 12, 14]:\n",
    "    count = np.sum(coverage_ratios >= threshold / k_global)\n",
    "    pct = 100 * count / len(coverage_ratios)\n",
    "    print(f\"  ≥{threshold:2d}/{k_global}: {count:6d} ({pct:5.1f}%)\")\n",
    "\n",
    "# ===================================================================\n",
    "# PLOT\n",
    "# ===================================================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.hist(coverage_ratios * k_global, bins=np.arange(0, k_global + 2) - 0.5, \n",
    "        edgecolor='black', alpha=0.7)\n",
    "ax.axvline(np.mean(coverage_ratios) * k_global, color='red', linestyle='--', \n",
    "           linewidth=2, label=f'Mean: {coverage_ratios.mean() * k_global:.1f}')\n",
    "ax.axvline(np.median(coverage_ratios) * k_global, color='orange', linestyle='--', \n",
    "           linewidth=2, label=f'Median: {np.median(coverage_ratios) * k_global:.1f}')\n",
    "ax.set_xlabel(f'Number of true {k_global}-NN present in miniset', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)\n",
    "ax.set_title('ST Miniset Neighbor Coverage', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "from core_models_et_p1 import STSetDataset, SCSetDataset\n",
    "import utils_et as uet\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# ===================================================================\n",
    "# 1. LOAD DATA\n",
    "# ===================================================================\n",
    "print(\"Loading data...\")\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "sc_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st2_counts_et.csv'\n",
    "sc_meta = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st2_metadata_et.csv'\n",
    "\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "sc_expr_df = pd.read_csv(sc_counts, index_col=0)\n",
    "sc_meta_df = pd.read_csv(sc_meta, index_col=0)\n",
    "scadata = ad.AnnData(X=sc_expr_df.values.T)\n",
    "scadata.obs_names = sc_expr_df.columns\n",
    "scadata.var_names = sc_expr_df.index\n",
    "scadata.obsm['spatial_gt'] = sc_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_st = stadata[:, common].X\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_st, \"toarray\"): X_st = X_st.toarray()\n",
    "if hasattr(X_sc, \"toarray\"): X_sc = X_sc.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32, device=device)\n",
    "\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, _, _ = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"✓ ST: {st_expr.shape[0]} spots × {st_expr.shape[1]} genes\")\n",
    "print(f\"✓ SC: {sc_expr.shape[0]} cells × {sc_expr.shape[1]} genes\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. LOAD TRAINED MODEL\n",
    "# ===================================================================\n",
    "print(\"\\nLoading trained model...\")\n",
    "# checkpoint_path = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output/phase2_sc_finetuned_checkpoint.pt\"\n",
    "checkpoint_path = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output/phase1_st_checkpoint.pt\"\n",
    "\n",
    "\n",
    "n_genes = len(common)\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=device,\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.generator.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE CORAL TRANSFORMATION\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING CORAL TRANSFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Prepare ST gene expression dict\n",
    "st_gene_expr_dict = {0: st_expr.cpu()}\n",
    "\n",
    "# 2. Load Stage B targets (needed for compute_coral_params_from_st)\n",
    "targets_path = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output/stageB_targets/targets_dict.pt\"\n",
    "if os.path.exists(targets_path):\n",
    "    model.targets_dict = torch.load(targets_path, map_location='cpu')\n",
    "    print(f\"✓ Loaded targets_dict from {targets_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  Targets not found at {targets_path}\")\n",
    "    print(\"   Running Stage B precomputation...\")\n",
    "    slides_dict = {0: (st_coords, st_expr)}\n",
    "    model.train_stageB(slides=slides_dict, outdir=\"/home/ehtesamul/sc_st/model/gems_mousebrain_output/stageB_targets\")\n",
    "    print(\"✓ Stage B complete\")\n",
    "\n",
    "# 3. Compute CORAL parameters\n",
    "print(\"\\n--- Computing ST context distribution ---\")\n",
    "model.compute_coral_params_from_st(\n",
    "    st_gene_expr_dict=st_gene_expr_dict,\n",
    "    n_samples=2000,\n",
    "    n_min=96,\n",
    "    n_max=384,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Building CORAL transformation ---\")\n",
    "model.build_coral_transform(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_samples=2000,\n",
    "    n_min=96,\n",
    "    n_max=384,\n",
    "    shrink=0.01,\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "print(\"✓ CORAL transformation ready!\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MINI-SET INFERENCE AND ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# HELPER FUNCTION: k-NN PRESERVATION\n",
    "# ===================================================================\n",
    "def compute_knn_preservation(coords_gt, coords_pred, k=10):\n",
    "    \"\"\"Compute k-nearest neighbor preservation.\"\"\"\n",
    "    n = coords_gt.shape[0]\n",
    "    \n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_gt)\n",
    "    _, indices_gt = nbrs_gt.kneighbors(coords_gt)\n",
    "    indices_gt = indices_gt[:, 1:]\n",
    "    \n",
    "    nbrs_pred = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_pred)\n",
    "    _, indices_pred = nbrs_pred.kneighbors(coords_pred)\n",
    "    indices_pred = indices_pred[:, 1:]\n",
    "    \n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        gt_neighbors = set(indices_gt[i])\n",
    "        pred_neighbors = set(indices_pred[i])\n",
    "        overlap = len(gt_neighbors.intersection(pred_neighbors))\n",
    "        overlaps.append(overlap)\n",
    "    \n",
    "    return np.mean(overlaps), overlaps\n",
    "\n",
    "# ===================================================================\n",
    "# HELPER FUNCTION: COMPUTE ANISOTROPY\n",
    "# ===================================================================\n",
    "def compute_anisotropy(coords):\n",
    "    \"\"\"Compute eigenvalue anisotropy ratio λ1/λ2\"\"\"\n",
    "    X = coords.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    eigvals = eigvals[::-1]\n",
    "    \n",
    "    lam1, lam2 = eigvals[0], eigvals[1]\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    \n",
    "    return lam1, lam2, ratio\n",
    "\n",
    "# ===================================================================\n",
    "# ANALYZE ST MINI-SETS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYZING ST MINI-SETS (5 samples)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "st_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(5):\n",
    "        print(f\"\\n--- ST Mini-set {idx} ---\")\n",
    "        \n",
    "        miniset = st_minisets[idx]\n",
    "        n = miniset['n']\n",
    "        \n",
    "        # Ground truth coordinates\n",
    "        coords_gt = miniset['V_target'].cpu().numpy()[:n]\n",
    "        \n",
    "        # Extract gene expression for inference\n",
    "        # Use overlap_info to get original indices\n",
    "        slide_id = 0  # We only have one slide\n",
    "        # indices = miniset['overlap_info']['indices'][slide_id][:n]\n",
    "        indices = miniset['overlap_info']['indices'][:n]\n",
    "        slide_id = miniset['overlap_info']['slide_id']\n",
    "        gene_expr = st_gene_expr_dict_cpu[slide_id][indices]\n",
    "        \n",
    "        print(f\"  n_spots: {n}\")\n",
    "        print(f\"  Running inference with patch_size={n}, coverage=1.0, iters=1...\")\n",
    "        \n",
    "        if 'sigma_data' in checkpoint:\n",
    "            model.sigma_data = checkpoint['sigma_data']\n",
    "        if 'sigma_min' in checkpoint:\n",
    "            model.sigma_min = checkpoint['sigma_min']\n",
    "        if 'sigma_max' in checkpoint:\n",
    "            model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = model.infer_sc_patchwise(\n",
    "                sc_gene_expr=gene_expr,\n",
    "                n_timesteps_sample=500,\n",
    "                return_coords=True,\n",
    "                patch_size=n,\n",
    "                coverage_per_cell=1.0,\n",
    "                n_align_iters=1,\n",
    "                eta=0.0,\n",
    "                guidance_scale=2.0,   # <-- changed from 4.0 to 2.0\n",
    "                debug_flag=False\n",
    "            )\n",
    "\n",
    "        # Extract predictions\n",
    "        D_edm_pred = results['D_edm'].cpu().numpy()\n",
    "        coords_pred = results['coords_canon'].cpu().numpy()\n",
    "        \n",
    "        # Compute ground truth EDM\n",
    "        gt_edm = squareform(pdist(coords_gt, 'euclidean'))\n",
    "        \n",
    "        # Extract upper triangle\n",
    "        triu_indices = np.triu_indices(n, k=1)\n",
    "        gt_distances = gt_edm[triu_indices]\n",
    "        pred_distances = D_edm_pred[triu_indices]\n",
    "        \n",
    "        # Scale alignment\n",
    "        scale = np.median(gt_distances) / (np.median(pred_distances) + 1e-12)\n",
    "        pred_distances_scaled = pred_distances * scale\n",
    "        \n",
    "        # Correlations\n",
    "        pearson_corr, _ = pearsonr(gt_distances, pred_distances_scaled)\n",
    "        spearman_corr, _ = spearmanr(gt_distances, pred_distances_scaled)\n",
    "        \n",
    "        # k-NN preservation\n",
    "        knn_k10, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(10, n-1))\n",
    "        knn_k20, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(20, n-1))\n",
    "        \n",
    "        # Anisotropy\n",
    "        lam1_gt, lam2_gt, ratio_gt = compute_anisotropy(coords_gt)\n",
    "        lam1_pred, lam2_pred, ratio_pred = compute_anisotropy(coords_pred)\n",
    "        \n",
    "        # Store results\n",
    "        st_results.append({\n",
    "            'idx': idx,\n",
    "            'n': n,\n",
    "            'coords_gt': coords_gt,\n",
    "            'coords_pred': coords_pred,\n",
    "            'pearson': pearson_corr,\n",
    "            'spearman': spearman_corr,\n",
    "            'knn_k10': knn_k10 / min(10, n-1),\n",
    "            'knn_k20': knn_k20 / min(20, n-1),\n",
    "            'scale': scale,\n",
    "            'ratio_gt': ratio_gt,\n",
    "            'ratio_pred': ratio_pred,\n",
    "            'gt_distances': gt_distances,\n",
    "            'pred_distances_scaled': pred_distances_scaled,\n",
    "        })\n",
    "        \n",
    "        print(f\"  Pearson:  {pearson_corr:.4f}\")\n",
    "        print(f\"  Spearman: {spearman_corr:.4f}\")\n",
    "        print(f\"  k-NN@10:  {knn_k10/min(10, n-1):.4f}\")\n",
    "        print(f\"  k-NN@20:  {knn_k20/min(20, n-1):.4f}\")\n",
    "        print(f\"  λ₁/λ₂ GT:   {ratio_gt:.2f}\")\n",
    "        print(f\"  λ₁/λ₂ Pred: {ratio_pred:.2f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# ANALYZE SC MINI-SETS (FIXED: Canonicalize GT coords)\n",
    "# ===================================================================\n",
    "# ===================================================================\n",
    "# CONFIGURATION\n",
    "# ===================================================================\n",
    "N_SC_SAMPLES = 10  # Change this to analyze any number of samples\n",
    "\n",
    "# ===================================================================\n",
    "# ANALYZE SC MINI-SETS (FIXED: Canonicalize GT coords)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"ANALYZING SC MINI-SETS ({N_SC_SAMPLES} samples) - FIXED COORDINATE SPACE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sc_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(N_SC_SAMPLES):\n",
    "        print(f\"\\n--- SC Mini-set {idx} ---\")\n",
    "        \n",
    "        miniset = sc_minisets[idx]\n",
    "        n = miniset['n']\n",
    "\n",
    "        indices = miniset['global_indices'].cpu().numpy()[:n]\n",
    "        coords_gt_raw = scadata.obsm['spatial_gt'][indices]\n",
    "\n",
    "        coords_gt_tensor = torch.tensor(coords_gt_raw, dtype=torch.float32, device=device)\n",
    "        slide_ids_mini = torch.zeros(n, dtype=torch.long, device=device)\n",
    "        coords_gt_canon, gt_mu, gt_scale = uet.canonicalize_st_coords_per_slide(\n",
    "            coords_gt_tensor, slide_ids_mini\n",
    "        )\n",
    "        coords_gt = coords_gt_canon.cpu().numpy()\n",
    "        \n",
    "        print(f\"  [DEBUG] GT raw range: X=[{coords_gt_raw[:,0].min():.2f}, {coords_gt_raw[:,0].max():.2f}]\")\n",
    "        print(f\"  [DEBUG] GT canon range: X=[{coords_gt[:,0].min():.3f}, {coords_gt[:,0].max():.3f}]\")\n",
    "        print(f\"  [DEBUG] GT scale factor: {gt_scale[0].item():.4f}\")\n",
    "        \n",
    "        gene_expr = sc_gene_expr_cpu[indices]\n",
    "        \n",
    "        print(f\"  n_cells: {n}\")\n",
    "        print(f\"  Running inference...\")\n",
    "\n",
    "        if 'sigma_data' in checkpoint:\n",
    "            model.sigma_data = checkpoint['sigma_data']\n",
    "        if 'sigma_min' in checkpoint:\n",
    "            model.sigma_min = checkpoint['sigma_min']\n",
    "        if 'sigma_max' in checkpoint:\n",
    "            model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = model.infer_sc_patchwise(\n",
    "                sc_gene_expr=gene_expr,\n",
    "                n_timesteps_sample=500,\n",
    "                return_coords=True,\n",
    "                patch_size=n,\n",
    "                coverage_per_cell=1.0,\n",
    "                n_align_iters=1,\n",
    "                eta=0.0,\n",
    "                guidance_scale=2.0,\n",
    "                debug_flag=False\n",
    "            )\n",
    "        \n",
    "        D_edm_pred = results['D_edm'].cpu().numpy()\n",
    "        coords_pred = results['coords_canon'].cpu().numpy()\n",
    "        \n",
    "        print(f\"  [DEBUG] Pred range: X=[{coords_pred[:,0].min():.3f}, {coords_pred[:,0].max():.3f}]\")\n",
    "        \n",
    "        gt_edm = squareform(pdist(coords_gt, 'euclidean'))\n",
    "        \n",
    "        triu_indices = np.triu_indices(n, k=1)\n",
    "        gt_distances = gt_edm[triu_indices]\n",
    "        pred_distances = D_edm_pred[triu_indices]\n",
    "        \n",
    "        scale = np.median(gt_distances) / (np.median(pred_distances) + 1e-12)\n",
    "        pred_distances_scaled = pred_distances * scale\n",
    "        \n",
    "        pearson_corr, _ = pearsonr(gt_distances, pred_distances_scaled)\n",
    "        spearman_corr, _ = spearmanr(gt_distances, pred_distances_scaled)\n",
    "        \n",
    "        knn_k10, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(10, n-1))\n",
    "        knn_k20, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(20, n-1))\n",
    "        \n",
    "        lam1_gt, lam2_gt, ratio_gt = compute_anisotropy(coords_gt)\n",
    "        lam1_pred, lam2_pred, ratio_pred = compute_anisotropy(coords_pred)\n",
    "        \n",
    "        sc_results.append({\n",
    "            'idx': idx,\n",
    "            'n': n,\n",
    "            'coords_gt': coords_gt,\n",
    "            'coords_gt_raw': coords_gt_raw,\n",
    "            'coords_pred': coords_pred,\n",
    "            'pearson': pearson_corr,\n",
    "            'spearman': spearman_corr,\n",
    "            'knn_k10': knn_k10 / min(10, n-1),\n",
    "            'knn_k20': knn_k20 / min(20, n-1),\n",
    "            'scale': scale,\n",
    "            'ratio_gt': ratio_gt,\n",
    "            'ratio_pred': ratio_pred,\n",
    "            'gt_distances': gt_distances,\n",
    "            'pred_distances_scaled': pred_distances_scaled,\n",
    "        })\n",
    "        \n",
    "        print(f\"  Pearson:  {pearson_corr:.4f}\")\n",
    "        print(f\"  Spearman: {spearman_corr:.4f}\")\n",
    "        print(f\"  k-NN@10:  {knn_k10/min(10, n-1):.4f}\")\n",
    "        print(f\"  k-NN@20:  {knn_k20/min(20, n-1):.4f}\")\n",
    "        print(f\"  λ₁/λ₂ GT:   {ratio_gt:.2f}\")\n",
    "        print(f\"  λ₁/λ₂ Pred: {ratio_pred:.2f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- ST MINI-SETS (n=5) ---\")\n",
    "print(f\"Pearson:       {np.mean([r['pearson'] for r in st_results]):.4f} ± {np.std([r['pearson'] for r in st_results]):.4f}\")\n",
    "print(f\"Spearman:      {np.mean([r['spearman'] for r in st_results]):.4f} ± {np.std([r['spearman'] for r in st_results]):.4f}\")\n",
    "print(f\"k-NN@10:       {np.mean([r['knn_k10'] for r in st_results]):.4f} ± {np.std([r['knn_k10'] for r in st_results]):.4f}\")\n",
    "print(f\"k-NN@20:       {np.mean([r['knn_k20'] for r in st_results]):.4f} ± {np.std([r['knn_k20'] for r in st_results]):.4f}\")\n",
    "print(f\"Anisotropy GT: {np.mean([r['ratio_gt'] for r in st_results]):.2f} ± {np.std([r['ratio_gt'] for r in st_results]):.2f}\")\n",
    "print(f\"Anisotropy PR: {np.mean([r['ratio_pred'] for r in st_results]):.2f} ± {np.std([r['ratio_pred'] for r in st_results]):.2f}\")\n",
    "\n",
    "print(f\"\\n--- SC MINI-SETS (n={N_SC_SAMPLES}) ---\")\n",
    "print(f\"Pearson:       {np.mean([r['pearson'] for r in sc_results]):.4f} ± {np.std([r['pearson'] for r in sc_results]):.4f}\")\n",
    "print(f\"Spearman:      {np.mean([r['spearman'] for r in sc_results]):.4f} ± {np.std([r['spearman'] for r in sc_results]):.4f}\")\n",
    "print(f\"k-NN@10:       {np.mean([r['knn_k10'] for r in sc_results]):.4f} ± {np.std([r['knn_k10'] for r in sc_results]):.4f}\")\n",
    "print(f\"k-NN@20:       {np.mean([r['knn_k20'] for r in sc_results]):.4f} ± {np.std([r['knn_k20'] for r in sc_results]):.4f}\")\n",
    "print(f\"Anisotropy GT: {np.mean([r['ratio_gt'] for r in sc_results]):.2f} ± {np.std([r['ratio_gt'] for r in sc_results]):.2f}\")\n",
    "print(f\"Anisotropy PR: {np.mean([r['ratio_pred'] for r in sc_results]):.2f} ± {np.std([r['ratio_pred'] for r in sc_results]):.2f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATION: COMPARISON PLOTS (FIXED)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING COMPARISON PLOTS (FIXED COORDINATE SPACE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "MAX_COLS = 3\n",
    "n_sc = len(sc_results)\n",
    "n_st = len(st_results)\n",
    "n_sc_cols = min(n_sc, MAX_COLS)\n",
    "n_st_cols = min(n_st, MAX_COLS)\n",
    "n_sc_rows = int(np.ceil(n_sc / MAX_COLS))\n",
    "n_st_rows = int(np.ceil(n_st / MAX_COLS))\n",
    "\n",
    "total_rows = 2 * n_st_rows + 2 * n_sc_rows + 1\n",
    "total_cols = MAX_COLS\n",
    "fig = plt.figure(figsize=(10 * total_cols, 7 * total_rows))\n",
    "\n",
    "current_row = 0\n",
    "\n",
    "# ST GT (canonical)\n",
    "for i, result in enumerate(st_results):\n",
    "    row = i // MAX_COLS\n",
    "    col = i % MAX_COLS\n",
    "    ax = plt.subplot(total_rows, total_cols, row * total_cols + col + 1)\n",
    "    coords = result['coords_gt']\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=20, alpha=0.7, c='blue')\n",
    "    ax.set_title(f'ST {i} GT (canon)\\nn={result[\"n\"]}', fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "current_row += n_st_rows\n",
    "\n",
    "# ST Pred (canonical)\n",
    "for i, result in enumerate(st_results):\n",
    "    row = i // MAX_COLS\n",
    "    col = i % MAX_COLS\n",
    "    ax = plt.subplot(total_rows, total_cols, (current_row + row) * total_cols + col + 1)\n",
    "    coords = result['coords_pred']\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=20, alpha=0.7, c='red')\n",
    "    ax.set_title(f'ST {i} Pred\\nkNN={result[\"knn_k10\"]:.3f}', fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "current_row += n_st_rows\n",
    "\n",
    "# Separator row\n",
    "ax = plt.subplot(total_rows, total_cols, current_row * total_cols + 2)\n",
    "ax.text(0.5, 0.5, 'SC RESULTS BELOW', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "current_row += 1\n",
    "\n",
    "# SC GT (canonical)\n",
    "for i, result in enumerate(sc_results):\n",
    "    row = i // MAX_COLS\n",
    "    col = i % MAX_COLS\n",
    "    ax = plt.subplot(total_rows, total_cols, (current_row + row) * total_cols + col + 1)\n",
    "    coords = result['coords_gt']\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=20, alpha=0.7, c='green')\n",
    "    ax.set_title(f'SC {i} GT (canon)\\nn={result[\"n\"]}', fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "current_row += n_sc_rows\n",
    "\n",
    "# SC Pred (canonical)\n",
    "for i, result in enumerate(sc_results):\n",
    "    row = i // MAX_COLS\n",
    "    col = i % MAX_COLS\n",
    "    ax = plt.subplot(total_rows, total_cols, (current_row + row) * total_cols + col + 1)\n",
    "    coords = result['coords_pred']\n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=20, alpha=0.7, c='orange')\n",
    "    ax.set_title(f'SC {i} Pred\\nkNN={result[\"knn_k10\"]:.3f}', fontsize=9)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Mini-set Inference: ST (n={n_st}) vs SC (n={n_sc}) - All in Canonical Space', \n",
    "             fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# METRIC COMPARISON BAR PLOTS\n",
    "# ===================================================================\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "metrics = ['pearson', 'spearman', 'knn_k10', 'knn_k20', 'ratio_gt', 'ratio_pred']\n",
    "titles = ['Pearson Correlation', 'Spearman Correlation', 'k-NN@10 Preservation', \n",
    "          'k-NN@20 Preservation', 'Anisotropy (GT)', 'Anisotropy (Pred)']\n",
    "\n",
    "for ax, metric, title in zip(axes.flat, metrics, titles):\n",
    "    st_vals = [r[metric] for r in st_results]\n",
    "    sc_vals = [r[metric] for r in sc_results]\n",
    "    \n",
    "    x_st = np.arange(len(st_vals))\n",
    "    x_sc = np.arange(len(sc_vals))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x_st - width/2, st_vals, width, label='ST', alpha=0.7, color='blue')\n",
    "    ax.bar(x_sc + width/2, sc_vals, width, label='SC', alpha=0.7, color='orange')\n",
    "    \n",
    "    ax.set_xlabel('Mini-set Index')\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xticks(np.arange(max(len(st_vals), len(sc_vals))))\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CHECK 1: CONDITIONING SENSITIVITY TEST\n",
    "# Tests if SC conditioning is being used or ignored\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 1: CONDITIONING SENSITIVITY TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def run_conditioning_sensitivity_test(model, sc_minisets, scadata, sc_gene_expr_cpu, \n",
    "                                       checkpoint, device, n_tests=3):\n",
    "    \"\"\"\n",
    "    For the same SC miniset, compare inference with:\n",
    "    - Real expression\n",
    "    - Shuffled expression (permuted rows)\n",
    "    - Mean expression (all cells = mean vector)\n",
    "    \n",
    "    If outputs are similar, conditioning is being ignored.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Set sigma params\n",
    "    if 'sigma_data' in checkpoint:\n",
    "        model.sigma_data = checkpoint['sigma_data']\n",
    "    if 'sigma_min' in checkpoint:\n",
    "        model.sigma_min = checkpoint['sigma_min']\n",
    "    if 'sigma_max' in checkpoint:\n",
    "        model.sigma_max = checkpoint['sigma_max']\n",
    "    \n",
    "    for idx in range(n_tests):\n",
    "        print(f\"\\n--- SC Mini-set {idx} ---\")\n",
    "        \n",
    "        miniset = sc_minisets[idx]\n",
    "        n_A = miniset['n_A']\n",
    "        indices_A = miniset['global_indices_A'].cpu().numpy()[:n_A]\n",
    "        \n",
    "        # Get GT coords (canonicalized)\n",
    "        coords_gt_raw = scadata.obsm['spatial_gt'][indices_A]\n",
    "        coords_gt_tensor = torch.tensor(coords_gt_raw, dtype=torch.float32, device=device)\n",
    "        slide_ids_mini = torch.zeros(n_A, dtype=torch.long, device=device)\n",
    "        coords_gt_canon, _, _ = uet.canonicalize_st_coords_per_slide(coords_gt_tensor, slide_ids_mini)\n",
    "        coords_gt = coords_gt_canon.cpu().numpy()\n",
    "        \n",
    "        # Real expression\n",
    "        gene_expr_real = sc_gene_expr_cpu[indices_A].clone()\n",
    "        \n",
    "        # Shuffled expression (permute rows)\n",
    "        perm = torch.randperm(n_A)\n",
    "        gene_expr_shuffled = gene_expr_real[perm].clone()\n",
    "        \n",
    "        # Mean expression (all cells = mean vector)\n",
    "        gene_expr_mean = gene_expr_real.mean(dim=0, keepdim=True).expand(n_A, -1).clone()\n",
    "        \n",
    "        # Fixed noise seed for fair comparison\n",
    "        torch.manual_seed(42 + idx)\n",
    "        np.random.seed(42 + idx)\n",
    "        \n",
    "        conditions = [\n",
    "            ('REAL', gene_expr_real),\n",
    "            ('SHUFFLED', gene_expr_shuffled),\n",
    "            ('MEAN', gene_expr_mean),\n",
    "        ]\n",
    "        \n",
    "        condition_results = {}\n",
    "        \n",
    "        for cond_name, gene_expr in conditions:\n",
    "            # Reset seed for each condition (same noise)\n",
    "            torch.manual_seed(42 + idx)\n",
    "            np.random.seed(42 + idx)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                inf_results = model.infer_sc_patchwise(\n",
    "                    sc_gene_expr=gene_expr,\n",
    "                    n_timesteps_sample=500,\n",
    "                    return_coords=True,\n",
    "                    patch_size=n_A,\n",
    "                    coverage_per_cell=1.0,\n",
    "                    n_align_iters=1,\n",
    "                    eta=0.0,\n",
    "                    guidance_scale=2.0,\n",
    "                    debug_flag=False\n",
    "                )\n",
    "            \n",
    "            coords_pred = inf_results['coords_canon'].cpu().numpy()\n",
    "            D_pred = inf_results['D_edm'].cpu().numpy()\n",
    "            \n",
    "            # Compute metrics\n",
    "            gt_edm = squareform(pdist(coords_gt, 'euclidean'))\n",
    "            triu_idx = np.triu_indices(n_A, k=1)\n",
    "            \n",
    "            # Correlations\n",
    "            spear, _ = spearmanr(gt_edm[triu_idx], D_pred[triu_idx])\n",
    "            \n",
    "            # kNN\n",
    "            knn_k10, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(10, n_A-1))\n",
    "            knn_k10_norm = knn_k10 / min(10, n_A-1)\n",
    "            \n",
    "            condition_results[cond_name] = {\n",
    "                'coords': coords_pred,\n",
    "                'D_pred': D_pred,\n",
    "                'spearman': spear,\n",
    "                'knn10': knn_k10_norm,\n",
    "            }\n",
    "            \n",
    "            print(f\"  [{cond_name}] Spearman={spear:.4f}, kNN@10={knn_k10_norm:.4f}\")\n",
    "        \n",
    "        # Compute similarity between conditions\n",
    "        D_real = condition_results['REAL']['D_pred']\n",
    "        D_shuffled = condition_results['SHUFFLED']['D_pred']\n",
    "        D_mean = condition_results['MEAN']['D_pred']\n",
    "        \n",
    "        # Correlation between distance matrices\n",
    "        corr_real_shuffled, _ = pearsonr(D_real[triu_idx], D_shuffled[triu_idx])\n",
    "        corr_real_mean, _ = pearsonr(D_real[triu_idx], D_mean[triu_idx])\n",
    "        \n",
    "        print(f\"\\n[COND-SENSITIVITY-{idx}] Checking if conditioning matters...\")\n",
    "        print(f\"[COND-SENSITIVITY-{idx}] D_pred correlation REAL vs SHUFFLED: {corr_real_shuffled:.4f}\")\n",
    "        print(f\"[COND-SENSITIVITY-{idx}] D_pred correlation REAL vs MEAN: {corr_real_mean:.4f}\")\n",
    "        \n",
    "        if corr_real_shuffled > 0.95 and corr_real_mean > 0.95:\n",
    "            print(f\"[COND-SENSITIVITY-{idx}] ⚠️ HIGH CORRELATION → CONDITIONING IS BEING IGNORED!\")\n",
    "        elif corr_real_shuffled > 0.8 or corr_real_mean > 0.8:\n",
    "            print(f\"[COND-SENSITIVITY-{idx}] ⚠️ MODERATE CORRELATION → WEAK CONDITIONING\")\n",
    "        else:\n",
    "            print(f\"[COND-SENSITIVITY-{idx}] ✓ LOW CORRELATION → CONDITIONING IS ACTIVE\")\n",
    "        \n",
    "        # Store for summary\n",
    "        results.append({\n",
    "            'idx': idx,\n",
    "            'n': n_A,\n",
    "            'corr_real_shuffled': corr_real_shuffled,\n",
    "            'corr_real_mean': corr_real_mean,\n",
    "            'spear_real': condition_results['REAL']['spearman'],\n",
    "            'spear_shuffled': condition_results['SHUFFLED']['spearman'],\n",
    "            'spear_mean': condition_results['MEAN']['spearman'],\n",
    "            'knn_real': condition_results['REAL']['knn10'],\n",
    "            'knn_shuffled': condition_results['SHUFFLED']['knn10'],\n",
    "            'knn_mean': condition_results['MEAN']['knn10'],\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[COND-SENSITIVITY-SUMMARY]\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    avg_corr_shuffled = np.mean([r['corr_real_shuffled'] for r in results])\n",
    "    avg_corr_mean = np.mean([r['corr_real_mean'] for r in results])\n",
    "    \n",
    "    print(f\"[COND-SENSITIVITY-SUMMARY] Avg D_pred corr REAL vs SHUFFLED: {avg_corr_shuffled:.4f}\")\n",
    "    print(f\"[COND-SENSITIVITY-SUMMARY] Avg D_pred corr REAL vs MEAN: {avg_corr_mean:.4f}\")\n",
    "    \n",
    "    print(f\"\\n[COND-SENSITIVITY-SUMMARY] Metric comparison:\")\n",
    "    print(f\"  Spearman: REAL={np.mean([r['spear_real'] for r in results]):.4f} \"\n",
    "          f\"SHUFFLED={np.mean([r['spear_shuffled'] for r in results]):.4f} \"\n",
    "          f\"MEAN={np.mean([r['spear_mean'] for r in results]):.4f}\")\n",
    "    print(f\"  kNN@10:   REAL={np.mean([r['knn_real'] for r in results]):.4f} \"\n",
    "          f\"SHUFFLED={np.mean([r['knn_shuffled'] for r in results]):.4f} \"\n",
    "          f\"MEAN={np.mean([r['knn_mean'] for r in results]):.4f}\")\n",
    "    \n",
    "    if avg_corr_shuffled > 0.9:\n",
    "        print(f\"\\n[COND-SENSITIVITY-DIAGNOSIS] 🚨 SC CONDITIONING IS EFFECTIVELY IGNORED\")\n",
    "        print(f\"[COND-SENSITIVITY-DIAGNOSIS] The model produces nearly identical outputs regardless of input\")\n",
    "        print(f\"[COND-SENSITIVITY-DIAGNOSIS] → Fix: Align SC context to ST manifold, or include SC in training\")\n",
    "    elif avg_corr_shuffled > 0.7:\n",
    "        print(f\"\\n[COND-SENSITIVITY-DIAGNOSIS] ⚠️ SC CONDITIONING IS WEAK\")\n",
    "        print(f\"[COND-SENSITIVITY-DIAGNOSIS] The model partially uses conditioning but falls back to prior\")\n",
    "    else:\n",
    "        print(f\"\\n[COND-SENSITIVITY-DIAGNOSIS] ✓ SC CONDITIONING IS ACTIVE\")\n",
    "        print(f\"[COND-SENSITIVITY-DIAGNOSIS] Low kNN may be due to other issues (margin/density)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run Check 1\n",
    "check1_results = run_conditioning_sensitivity_test(\n",
    "    model, sc_minisets, scadata, sc_gene_expr_cpu, checkpoint, device, n_tests=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CHECK 2: EMBEDDING/CONTEXT STATISTICS - ST vs SC\n",
    "# Compare Z embeddings and context H between ST and SC\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 2: EMBEDDING/CONTEXT STATISTICS - ST vs SC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_embedding_stats(model, st_minisets, sc_minisets, st_gene_expr_dict_cpu, \n",
    "                            sc_gene_expr_cpu, device, n_samples=5):\n",
    "    \"\"\"\n",
    "    Compare Z (encoder output) and H (context encoder output) statistics\n",
    "    between ST and SC minisets.\n",
    "    \"\"\"\n",
    "    st_Z_stats = []\n",
    "    st_H_stats = []\n",
    "    sc_Z_stats = []\n",
    "    sc_H_stats = []\n",
    "    \n",
    "    model.encoder.eval()\n",
    "    model.context_encoder.eval()\n",
    "    \n",
    "    print(\"\\n--- Computing ST embedding statistics ---\")\n",
    "    for idx in range(min(n_samples, len(st_minisets))):\n",
    "        miniset = st_minisets[idx]\n",
    "        n = miniset['n']\n",
    "        indices = miniset['overlap_info']['indices'][:n]\n",
    "        slide_id = miniset['overlap_info']['slide_id']\n",
    "        gene_expr = st_gene_expr_dict_cpu[slide_id][indices].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encoder output Z\n",
    "            Z = model.encoder(gene_expr)  # (n, D_z)\n",
    "            \n",
    "            # Context encoder output H\n",
    "            Z_batch = Z.unsqueeze(0)  # (1, n, D_z)\n",
    "            mask = torch.ones(1, n, dtype=torch.bool, device=device)\n",
    "            H = model.context_encoder(Z_batch, mask)  # (1, n, c_dim)\n",
    "            H = H.squeeze(0)  # (n, c_dim)\n",
    "            \n",
    "            # Compute statistics\n",
    "            Z_norms = Z.norm(dim=1)  # (n,)\n",
    "            H_norms = H.norm(dim=1)  # (n,)\n",
    "            \n",
    "            # Cosine similarity within set\n",
    "            Z_normed = F.normalize(Z, dim=1)\n",
    "            Z_cos_sim = (Z_normed @ Z_normed.T)\n",
    "            Z_cos_sim_offdiag = Z_cos_sim[torch.triu(torch.ones(n, n, dtype=torch.bool), diagonal=1)]\n",
    "            \n",
    "            H_normed = F.normalize(H, dim=1)\n",
    "            H_cos_sim = (H_normed @ H_normed.T)\n",
    "            H_cos_sim_offdiag = H_cos_sim[torch.triu(torch.ones(n, n, dtype=torch.bool), diagonal=1)]\n",
    "            \n",
    "            st_Z_stats.append({\n",
    "                'norm_mean': Z_norms.mean().item(),\n",
    "                'norm_std': Z_norms.std().item(),\n",
    "                'cos_sim_mean': Z_cos_sim_offdiag.mean().item(),\n",
    "                'cos_sim_std': Z_cos_sim_offdiag.std().item(),\n",
    "            })\n",
    "            st_H_stats.append({\n",
    "                'norm_mean': H_norms.mean().item(),\n",
    "                'norm_std': H_norms.std().item(),\n",
    "                'cos_sim_mean': H_cos_sim_offdiag.mean().item(),\n",
    "                'cos_sim_std': H_cos_sim_offdiag.std().item(),\n",
    "            })\n",
    "    \n",
    "    print(\"--- Computing SC embedding statistics ---\")\n",
    "    for idx in range(min(n_samples, len(sc_minisets))):\n",
    "        miniset = sc_minisets[idx]\n",
    "        n_A = miniset['n_A']\n",
    "        indices_A = miniset['global_indices_A'].cpu().numpy()[:n_A]\n",
    "        gene_expr = sc_gene_expr_cpu[indices_A].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encoder output Z\n",
    "            Z = model.encoder(gene_expr)  # (n, D_z)\n",
    "            \n",
    "            # Context encoder output H\n",
    "            Z_batch = Z.unsqueeze(0)  # (1, n, D_z)\n",
    "            mask = torch.ones(1, n_A, dtype=torch.bool, device=device)\n",
    "            H = model.context_encoder(Z_batch, mask)  # (1, n, c_dim)\n",
    "            H = H.squeeze(0)  # (n, c_dim)\n",
    "            \n",
    "            # Compute statistics\n",
    "            Z_norms = Z.norm(dim=1)\n",
    "            H_norms = H.norm(dim=1)\n",
    "            \n",
    "            # Cosine similarity within set\n",
    "            Z_normed = F.normalize(Z, dim=1)\n",
    "            Z_cos_sim = (Z_normed @ Z_normed.T)\n",
    "            Z_cos_sim_offdiag = Z_cos_sim[torch.triu(torch.ones(n_A, n_A, dtype=torch.bool), diagonal=1)]\n",
    "            \n",
    "            H_normed = F.normalize(H, dim=1)\n",
    "            H_cos_sim = (H_normed @ H_normed.T)\n",
    "            H_cos_sim_offdiag = H_cos_sim[torch.triu(torch.ones(n_A, n_A, dtype=torch.bool), diagonal=1)]\n",
    "            \n",
    "            sc_Z_stats.append({\n",
    "                'norm_mean': Z_norms.mean().item(),\n",
    "                'norm_std': Z_norms.std().item(),\n",
    "                'cos_sim_mean': Z_cos_sim_offdiag.mean().item(),\n",
    "                'cos_sim_std': Z_cos_sim_offdiag.std().item(),\n",
    "            })\n",
    "            sc_H_stats.append({\n",
    "                'norm_mean': H_norms.mean().item(),\n",
    "                'norm_std': H_norms.std().item(),\n",
    "                'cos_sim_mean': H_cos_sim_offdiag.mean().item(),\n",
    "                'cos_sim_std': H_cos_sim_offdiag.std().item(),\n",
    "            })\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[EMBED-STATS] Z (Encoder Output) Statistics\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    st_Z_norm_mean = np.mean([s['norm_mean'] for s in st_Z_stats])\n",
    "    st_Z_norm_std = np.mean([s['norm_std'] for s in st_Z_stats])\n",
    "    st_Z_cos_mean = np.mean([s['cos_sim_mean'] for s in st_Z_stats])\n",
    "    st_Z_cos_std = np.mean([s['cos_sim_std'] for s in st_Z_stats])\n",
    "    \n",
    "    sc_Z_norm_mean = np.mean([s['norm_mean'] for s in sc_Z_stats])\n",
    "    sc_Z_norm_std = np.mean([s['norm_std'] for s in sc_Z_stats])\n",
    "    sc_Z_cos_mean = np.mean([s['cos_sim_mean'] for s in sc_Z_stats])\n",
    "    sc_Z_cos_std = np.mean([s['cos_sim_std'] for s in sc_Z_stats])\n",
    "    \n",
    "    print(f\"[EMBED-STATS-Z] ST: ||Z|| mean={st_Z_norm_mean:.4f} std={st_Z_norm_std:.4f}\")\n",
    "    print(f\"[EMBED-STATS-Z] SC: ||Z|| mean={sc_Z_norm_mean:.4f} std={sc_Z_norm_std:.4f}\")\n",
    "    print(f\"[EMBED-STATS-Z] ST: cos_sim mean={st_Z_cos_mean:.4f} std={st_Z_cos_std:.4f}\")\n",
    "    print(f\"[EMBED-STATS-Z] SC: cos_sim mean={sc_Z_cos_mean:.4f} std={sc_Z_cos_std:.4f}\")\n",
    "    \n",
    "    # Check for distribution shift\n",
    "    norm_ratio = sc_Z_norm_mean / (st_Z_norm_mean + 1e-8)\n",
    "    cos_diff = abs(sc_Z_cos_mean - st_Z_cos_mean)\n",
    "    \n",
    "    print(f\"\\n[EMBED-STATS-Z-SHIFT] ||Z|| ratio (SC/ST): {norm_ratio:.4f}\")\n",
    "    print(f\"[EMBED-STATS-Z-SHIFT] cos_sim difference: {cos_diff:.4f}\")\n",
    "    \n",
    "    if abs(norm_ratio - 1.0) > 0.3:\n",
    "        print(f\"[EMBED-STATS-Z-SHIFT] ⚠️ LARGE NORM SHIFT between ST and SC Z embeddings\")\n",
    "    if cos_diff > 0.15:\n",
    "        print(f\"[EMBED-STATS-Z-SHIFT] ⚠️ LARGE COS_SIM SHIFT between ST and SC Z embeddings\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[EMBED-STATS] H (Context Encoder Output) Statistics\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    st_H_norm_mean = np.mean([s['norm_mean'] for s in st_H_stats])\n",
    "    st_H_norm_std = np.mean([s['norm_std'] for s in st_H_stats])\n",
    "    st_H_cos_mean = np.mean([s['cos_sim_mean'] for s in st_H_stats])\n",
    "    st_H_cos_std = np.mean([s['cos_sim_std'] for s in st_H_stats])\n",
    "    \n",
    "    sc_H_norm_mean = np.mean([s['norm_mean'] for s in sc_H_stats])\n",
    "    sc_H_norm_std = np.mean([s['norm_std'] for s in sc_H_stats])\n",
    "    sc_H_cos_mean = np.mean([s['cos_sim_mean'] for s in sc_H_stats])\n",
    "    sc_H_cos_std = np.mean([s['cos_sim_std'] for s in sc_H_stats])\n",
    "    \n",
    "    print(f\"[EMBED-STATS-H] ST: ||H|| mean={st_H_norm_mean:.4f} std={st_H_norm_std:.4f}\")\n",
    "    print(f\"[EMBED-STATS-H] SC: ||H|| mean={sc_H_norm_mean:.4f} std={sc_H_norm_std:.4f}\")\n",
    "    print(f\"[EMBED-STATS-H] ST: cos_sim mean={st_H_cos_mean:.4f} std={st_H_cos_std:.4f}\")\n",
    "    print(f\"[EMBED-STATS-H] SC: cos_sim mean={sc_H_cos_mean:.4f} std={sc_H_cos_std:.4f}\")\n",
    "    \n",
    "    # Check for distribution shift\n",
    "    H_norm_ratio = sc_H_norm_mean / (st_H_norm_mean + 1e-8)\n",
    "    H_cos_diff = abs(sc_H_cos_mean - st_H_cos_mean)\n",
    "    \n",
    "    print(f\"\\n[EMBED-STATS-H-SHIFT] ||H|| ratio (SC/ST): {H_norm_ratio:.4f}\")\n",
    "    print(f\"[EMBED-STATS-H-SHIFT] cos_sim difference: {H_cos_diff:.4f}\")\n",
    "    \n",
    "    if abs(H_norm_ratio - 1.0) > 0.3:\n",
    "        print(f\"[EMBED-STATS-H-SHIFT] ⚠️ LARGE NORM SHIFT between ST and SC context embeddings\")\n",
    "    if H_cos_diff > 0.15:\n",
    "        print(f\"[EMBED-STATS-H-SHIFT] ⚠️ LARGE COS_SIM SHIFT between ST and SC context embeddings\")\n",
    "    \n",
    "    # Diagnosis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[EMBED-STATS-DIAGNOSIS]\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if abs(norm_ratio - 1.0) > 0.3 or abs(H_norm_ratio - 1.0) > 0.3:\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] 🚨 SIGNIFICANT DISTRIBUTION SHIFT detected\")\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] SC embeddings have different scale than ST\")\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] → The diffusion model may treat SC as OOD\")\n",
    "    elif cos_diff > 0.15 or H_cos_diff > 0.15:\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] ⚠️ MODERATE DISTRIBUTION SHIFT detected\")\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] SC tokens have different similarity structure than ST\")\n",
    "    else:\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] ✓ Embedding distributions appear similar\")\n",
    "        print(\"[EMBED-STATS-DIAGNOSIS] → Issue may be elsewhere (margin/density)\")\n",
    "    \n",
    "    return {\n",
    "        'st_Z': st_Z_stats, 'st_H': st_H_stats,\n",
    "        'sc_Z': sc_Z_stats, 'sc_H': sc_H_stats,\n",
    "    }\n",
    "\n",
    "# Need F.normalize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Run Check 2\n",
    "check2_results = compute_embedding_stats(\n",
    "    model, st_minisets, sc_minisets, st_gene_expr_dict_cpu, sc_gene_expr_cpu, device, n_samples=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CHECK 3: SC MINISETS USING TRUE SPATIAL NEIGHBORS\n",
    "# Build SC minisets using GT spatial proximity (like ST sampling)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHECK 3: SC MINISETS WITH TRUE SPATIAL NEIGHBORS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def build_sc_spatial_minisets(scadata, sc_gene_expr_cpu, n_minisets=5, n_min=128, n_max=256):\n",
    "    \"\"\"\n",
    "    Build SC minisets using TRUE spatial neighbors (like ST sampling).\n",
    "    This tests if the problem is patch composition vs conditioning.\n",
    "    \"\"\"\n",
    "    gt_coords = scadata.obsm['spatial_gt']\n",
    "    n_cells = gt_coords.shape[0]\n",
    "    \n",
    "    # Build spatial distance matrix\n",
    "    from scipy.spatial.distance import cdist\n",
    "    D_spatial = cdist(gt_coords, gt_coords)\n",
    "    \n",
    "    minisets = []\n",
    "    \n",
    "    for i in range(n_minisets):\n",
    "        # Random patch size\n",
    "        n = np.random.randint(n_min, n_max + 1)\n",
    "        n = min(n, n_cells)\n",
    "        \n",
    "        # Random center\n",
    "        center_idx = np.random.randint(0, n_cells)\n",
    "        \n",
    "        # Get n-1 nearest spatial neighbors\n",
    "        dists_from_center = D_spatial[center_idx]\n",
    "        sorted_indices = np.argsort(dists_from_center)\n",
    "        \n",
    "        # Take center + nearest neighbors\n",
    "        indices = sorted_indices[:n]\n",
    "        \n",
    "        minisets.append({\n",
    "            'indices': indices,\n",
    "            'n': n,\n",
    "            'center_idx': center_idx,\n",
    "        })\n",
    "        \n",
    "        print(f\"[SC-SPATIAL-MINISET-{i}] n={n}, center={center_idx}, \"\n",
    "              f\"max_dist_from_center={dists_from_center[indices[-1]]:.4f}\")\n",
    "    \n",
    "    return minisets\n",
    "\n",
    "def run_sc_spatial_inference(model, sc_spatial_minisets, scadata, sc_gene_expr_cpu, \n",
    "                              checkpoint, device):\n",
    "    \"\"\"\n",
    "    Run inference on SC minisets built from spatial neighbors.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    if 'sigma_data' in checkpoint:\n",
    "        model.sigma_data = checkpoint['sigma_data']\n",
    "    if 'sigma_min' in checkpoint:\n",
    "        model.sigma_min = checkpoint['sigma_min']\n",
    "    if 'sigma_max' in checkpoint:\n",
    "        model.sigma_max = checkpoint['sigma_max']\n",
    "    \n",
    "    for idx, miniset in enumerate(sc_spatial_minisets):\n",
    "        indices = miniset['indices']\n",
    "        n = miniset['n']\n",
    "        \n",
    "        print(f\"\\n--- SC Spatial Miniset {idx} (n={n}) ---\")\n",
    "        \n",
    "        # Get GT coords (canonicalized)\n",
    "        coords_gt_raw = scadata.obsm['spatial_gt'][indices]\n",
    "        coords_gt_tensor = torch.tensor(coords_gt_raw, dtype=torch.float32, device=device)\n",
    "        slide_ids_mini = torch.zeros(n, dtype=torch.long, device=device)\n",
    "        coords_gt_canon, _, _ = uet.canonicalize_st_coords_per_slide(coords_gt_tensor, slide_ids_mini)\n",
    "        coords_gt = coords_gt_canon.cpu().numpy()\n",
    "        \n",
    "        # Get gene expression\n",
    "        gene_expr = sc_gene_expr_cpu[indices]\n",
    "        \n",
    "        # Run inference\n",
    "        torch.manual_seed(42 + idx)\n",
    "        np.random.seed(42 + idx)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inf_results = model.infer_sc_patchwise(\n",
    "                sc_gene_expr=gene_expr,\n",
    "                n_timesteps_sample=500,\n",
    "                return_coords=True,\n",
    "                patch_size=n,\n",
    "                coverage_per_cell=1.0,\n",
    "                n_align_iters=1,\n",
    "                eta=0.0,\n",
    "                guidance_scale=2.0,\n",
    "                debug_flag=False\n",
    "            )\n",
    "        \n",
    "        coords_pred = inf_results['coords_canon'].cpu().numpy()\n",
    "        D_pred = inf_results['D_edm'].cpu().numpy()\n",
    "        \n",
    "        # Compute metrics\n",
    "        gt_edm = squareform(pdist(coords_gt, 'euclidean'))\n",
    "        triu_idx = np.triu_indices(n, k=1)\n",
    "        \n",
    "        spear, _ = spearmanr(gt_edm[triu_idx], D_pred[triu_idx])\n",
    "        pear, _ = pearsonr(gt_edm[triu_idx], D_pred[triu_idx])\n",
    "        \n",
    "        knn_k10, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(10, n-1))\n",
    "        knn_k20, _ = compute_knn_preservation(coords_gt, coords_pred, k=min(20, n-1))\n",
    "        \n",
    "        results.append({\n",
    "            'idx': idx,\n",
    "            'n': n,\n",
    "            'spearman': spear,\n",
    "            'pearson': pear,\n",
    "            'knn10': knn_k10 / min(10, n-1),\n",
    "            'knn20': knn_k20 / min(20, n-1),\n",
    "            'coords_gt': coords_gt,\n",
    "            'coords_pred': coords_pred,\n",
    "        })\n",
    "        \n",
    "        print(f\"[SC-SPATIAL-{idx}] Spearman={spear:.4f}, Pearson={pear:.4f}\")\n",
    "        print(f\"[SC-SPATIAL-{idx}] kNN@10={knn_k10/min(10, n-1):.4f}, kNN@20={knn_k20/min(20, n-1):.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"[SC-SPATIAL-SUMMARY] SC Minisets with TRUE Spatial Neighbors\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    avg_spear = np.mean([r['spearman'] for r in results])\n",
    "    avg_knn10 = np.mean([r['knn10'] for r in results])\n",
    "    avg_knn20 = np.mean([r['knn20'] for r in results])\n",
    "    \n",
    "    print(f\"[SC-SPATIAL-SUMMARY] Avg Spearman: {avg_spear:.4f}\")\n",
    "    print(f\"[SC-SPATIAL-SUMMARY] Avg kNN@10: {avg_knn10:.4f}\")\n",
    "    print(f\"[SC-SPATIAL-SUMMARY] Avg kNN@20: {avg_knn20:.4f}\")\n",
    "    \n",
    "    # Compare with random SC minisets\n",
    "    print(\"\\n[SC-SPATIAL-VS-RANDOM] Comparison with random SC minisets:\")\n",
    "    if 'sc_results' in dir():\n",
    "        random_knn10 = np.mean([r['knn_k10'] for r in sc_results])\n",
    "        print(f\"[SC-SPATIAL-VS-RANDOM] Spatial kNN@10: {avg_knn10:.4f}\")\n",
    "        print(f\"[SC-SPATIAL-VS-RANDOM] Random kNN@10:  {random_knn10:.4f}\")\n",
    "        \n",
    "        if avg_knn10 > random_knn10 + 0.1:\n",
    "            print(f\"[SC-SPATIAL-VS-RANDOM] ✓ Spatial selection helps (+{avg_knn10-random_knn10:.3f})\")\n",
    "        else:\n",
    "            print(f\"[SC-SPATIAL-VS-RANDOM] ✗ Spatial selection doesn't help much\")\n",
    "            print(f\"[SC-SPATIAL-VS-RANDOM] → Problem is NOT patch composition, it's CONDITIONING\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Build SC spatial minisets\n",
    "sc_spatial_minisets = build_sc_spatial_minisets(\n",
    "    scadata, sc_gene_expr_cpu, n_minisets=5, n_min=128, n_max=256\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "check3_results = run_sc_spatial_inference(\n",
    "    model, sc_spatial_minisets, scadata, sc_gene_expr_cpu, checkpoint, device\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "\n",
    "for i, res in enumerate(check3_results):\n",
    "    # GT\n",
    "    ax = axes[0, i]\n",
    "    ax.scatter(res['coords_gt'][:, 0], res['coords_gt'][:, 1], s=15, alpha=0.7, c='green')\n",
    "    ax.set_title(f'SC-Spatial {i} GT\\nn={res[\"n\"]}', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pred\n",
    "    ax = axes[1, i]\n",
    "    ax.scatter(res['coords_pred'][:, 0], res['coords_pred'][:, 1], s=15, alpha=0.7, c='orange')\n",
    "    ax.set_title(f'SC-Spatial {i} Pred\\nkNN={res[\"knn10\"]:.3f}', fontsize=10)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CHECK 3: SC Minisets with TRUE Spatial Neighbors', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('check3_sc_spatial_minisets.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# HYPOTHESIS TEST: ST vs SC Context Distribution Shift\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING: ST vs SC Context Distribution Shift\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Collect context tokens from ST mini-sets\n",
    "st_contexts = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(st_minisets)):\n",
    "        miniset = st_minisets[idx]\n",
    "        n = miniset['n']\n",
    "        \n",
    "        # Get gene expression\n",
    "        slide_id = miniset['overlap_info']['slide_id']\n",
    "        indices = miniset['overlap_info']['indices'][:n]\n",
    "        gene_expr = st_gene_expr_dict_cpu[slide_id][indices].to(device)\n",
    "        \n",
    "        # Encode: gene_expr → Z embeddings\n",
    "        Z = model.encoder(gene_expr)  # (n, h_dim)\n",
    "        \n",
    "        # Create mask (all ones since no padding)\n",
    "        mask = torch.ones(n, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Context encoder: Z → H (context tokens)\n",
    "        Z_batch = Z.unsqueeze(0)  # (1, n, h_dim)\n",
    "        mask_batch = mask.unsqueeze(0)  # (1, n)\n",
    "        H = model.context_encoder(Z_batch, mask_batch)  # (1, n, c_dim)\n",
    "        \n",
    "        # Store flattened context tokens\n",
    "        st_contexts.append(H.squeeze(0).cpu())  # (n, c_dim)\n",
    "\n",
    "# Collect context tokens from SC mini-sets\n",
    "sc_contexts = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(sc_minisets)):\n",
    "        miniset = sc_minisets[idx]\n",
    "        n_A = miniset['n_A']\n",
    "        \n",
    "        # Get gene expression\n",
    "        indices_A = miniset['global_indices_A'].cpu().numpy()[:n_A]\n",
    "        gene_expr = sc_gene_expr_cpu[indices_A].to(device)\n",
    "        \n",
    "        # Encode: gene_expr → Z embeddings\n",
    "        Z = model.encoder(gene_expr)  # (n_A, h_dim)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = torch.ones(n_A, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Context encoder: Z → H\n",
    "        Z_batch = Z.unsqueeze(0)  # (1, n_A, h_dim)\n",
    "        mask_batch = mask.unsqueeze(0)  # (1, n_A)\n",
    "        H = model.context_encoder(Z_batch, mask_batch)  # (1, n_A, c_dim)\n",
    "        \n",
    "        # Store flattened context tokens\n",
    "        sc_contexts.append(H.squeeze(0).cpu())  # (n_A, c_dim)\n",
    "\n",
    "# Concatenate all context tokens\n",
    "st_H = torch.cat(st_contexts, dim=0).numpy()  # (N_st, c_dim)\n",
    "sc_H = torch.cat(sc_contexts, dim=0).numpy()  # (N_sc, c_dim)\n",
    "\n",
    "print(f\"\\nST contexts: {st_H.shape}\")\n",
    "print(f\"SC contexts: {sc_H.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. COMPUTE STATISTICS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Context Token Statistics ---\")\n",
    "\n",
    "# Per-dimension statistics\n",
    "st_mean = st_H.mean(axis=0)\n",
    "st_std = st_H.std(axis=0)\n",
    "st_norm = np.linalg.norm(st_H, axis=1)\n",
    "\n",
    "sc_mean = sc_H.mean(axis=0)\n",
    "sc_std = sc_H.std(axis=0)\n",
    "sc_norm = np.linalg.norm(sc_H, axis=1)\n",
    "\n",
    "print(f\"\\nST - Mean of means: {st_mean.mean():.6f}\")\n",
    "print(f\"ST - Mean of stds:  {st_std.mean():.6f}\")\n",
    "print(f\"ST - Mean norm:     {st_norm.mean():.6f} ± {st_norm.std():.6f}\")\n",
    "\n",
    "print(f\"\\nSC - Mean of means: {sc_mean.mean():.6f}\")\n",
    "print(f\"SC - Mean of stds:  {sc_std.mean():.6f}\")\n",
    "print(f\"SC - Mean norm:     {sc_norm.mean():.6f} ± {sc_norm.std():.6f}\")\n",
    "\n",
    "# Norm difference\n",
    "norm_diff = np.abs(st_norm.mean() - sc_norm.mean()) / st_norm.mean()\n",
    "print(f\"\\n[Norm difference: {norm_diff*100:.2f}%]\")\n",
    "\n",
    "# Mean vector cosine similarity\n",
    "cos_sim = np.dot(st_mean, sc_mean) / (np.linalg.norm(st_mean) * np.linalg.norm(sc_mean))\n",
    "print(f\"[Mean vector cosine similarity: {cos_sim:.4f}]\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. LINEAR CLASSIFIER TEST\n",
    "# ===================================================================\n",
    "print(\"\\n--- Linear Classifier Test (ST vs SC) ---\")\n",
    "\n",
    "# Prepare data\n",
    "X = np.vstack([st_H, sc_H])\n",
    "y = np.array([0]*len(st_H) + [1]*len(sc_H))  # 0=ST, 1=SC\n",
    "\n",
    "# Shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "acc = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nClassifier Accuracy: {acc:.4f}\")\n",
    "print(f\"Classifier AUC:      {auc:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if auc > 0.85:\n",
    "    print(\"\\n⚠️  HIGH AUC (>0.85): Strong distribution shift detected!\")\n",
    "    print(\"   The score net sees ST vs SC contexts as very different.\")\n",
    "    print(\"   This supports the 'conditioning shift' hypothesis.\")\n",
    "elif auc > 0.70:\n",
    "    print(\"\\n⚠️  MODERATE AUC (0.70-0.85): Moderate distribution shift.\")\n",
    "    print(\"   ST and SC contexts are distinguishable but not wildly different.\")\n",
    "elif auc > 0.60:\n",
    "    print(\"\\n✓  LOW AUC (0.60-0.70): Mild distribution shift.\")\n",
    "    print(\"   Contexts are similar. Shift may not be the main issue.\")\n",
    "else:\n",
    "    print(\"\\n✓  VERY LOW AUC (<0.60): Contexts are indistinguishable.\")\n",
    "    print(\"   Conditioning shift is NOT the problem.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. VISUALIZE (Optional)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Visualization ---\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA projection\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:len(st_H), 0], X_pca[:len(st_H), 1], \n",
    "           alpha=0.5, s=10, label='ST contexts', c='blue')\n",
    "plt.scatter(X_pca[len(st_H):, 0], X_pca[len(st_H):, 1], \n",
    "           alpha=0.5, s=10, label='SC contexts', c='red')\n",
    "plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}%)')\n",
    "plt.title('Context Token Distribution: ST vs SC')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('context_shift_pca.png', dpi=150)\n",
    "print(\"✓ Saved: context_shift_pca.png\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: ST vs SC Context Classification')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('context_shift_roc.png', dpi=150)\n",
    "print(\"✓ Saved: context_shift_roc.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# CORAL TRANSFORMATION + RE-TEST\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING CORAL TRANSFORMATION TO SC CONTEXTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: Compute Covariance Statistics\n",
    "# ===================================================================\n",
    "print(\"\\nComputing ST and SC covariance statistics...\")\n",
    "\n",
    "# Flatten all ST context tokens into (T_st, c_dim)\n",
    "st_H_flat = st_H  # Already (N_st, c_dim)\n",
    "\n",
    "# Flatten all SC context tokens into (T_sc, c_dim)  \n",
    "sc_H_flat = sc_H  # Already (N_sc, c_dim)\n",
    "\n",
    "# Compute statistics\n",
    "mu_st = torch.tensor(st_H_flat.mean(axis=0), dtype=torch.float32, device=device)\n",
    "mu_sc = torch.tensor(sc_H_flat.mean(axis=0), dtype=torch.float32, device=device)\n",
    "\n",
    "# Compute covariance matrices\n",
    "st_H_centered = st_H_flat - mu_st.cpu().numpy()\n",
    "sc_H_centered = sc_H_flat - mu_sc.cpu().numpy()\n",
    "\n",
    "cov_st = torch.tensor(\n",
    "    (st_H_centered.T @ st_H_centered) / (st_H_flat.shape[0] - 1),\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "cov_sc = torch.tensor(\n",
    "    (sc_H_centered.T @ sc_H_centered) / (sc_H_flat.shape[0] - 1),\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "print(f\"✓ mu_st shape: {mu_st.shape}\")\n",
    "print(f\"✓ mu_sc shape: {mu_sc.shape}\")\n",
    "print(f\"✓ cov_st shape: {cov_st.shape}\")\n",
    "print(f\"✓ cov_sc shape: {cov_sc.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: Build CORAL Transform\n",
    "# ===================================================================\n",
    "print(\"\\nBuilding CORAL transform...\")\n",
    "\n",
    "def sqrtm_psd(C, eps=1e-5):\n",
    "    \"\"\"Compute matrix square root of PSD matrix.\"\"\"\n",
    "    evals, evecs = torch.linalg.eigh(C)\n",
    "    evals = torch.clamp(evals, min=eps)\n",
    "    return (evecs * torch.sqrt(evals)) @ evecs.T\n",
    "\n",
    "def invsqrtm_psd(C, eps=1e-5):\n",
    "    \"\"\"Compute inverse matrix square root of PSD matrix.\"\"\"\n",
    "    evals, evecs = torch.linalg.eigh(C)\n",
    "    evals = torch.clamp(evals, min=eps)\n",
    "    return (evecs * (1.0 / torch.sqrt(evals))) @ evecs.T\n",
    "\n",
    "# Add shrinkage for numerical stability\n",
    "shrink = 0.01\n",
    "D = cov_sc.shape[0]\n",
    "I = torch.eye(D, device=device, dtype=torch.float32)\n",
    "\n",
    "cov_sc_shrunk = (1 - shrink) * cov_sc + shrink * I\n",
    "cov_st_shrunk = (1 - shrink) * cov_st + shrink * I\n",
    "\n",
    "# Compute transform matrices\n",
    "A = invsqrtm_psd(cov_sc_shrunk, eps=1e-5)  # C_sc^{-1/2}\n",
    "B = sqrtm_psd(cov_st_shrunk, eps=1e-5)     # C_st^{1/2}\n",
    "\n",
    "print(f\"✓ A (C_sc^{{-1/2}}) shape: {A.shape}\")\n",
    "print(f\"✓ B (C_st^{{1/2}}) shape: {B.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: Apply CORAL to SC Contexts\n",
    "# ===================================================================\n",
    "print(\"\\nApplying CORAL transformation to SC contexts...\")\n",
    "\n",
    "sc_H_torch = torch.tensor(sc_H, dtype=torch.float32, device=device)\n",
    "\n",
    "# Transform: (x - mu_sc) @ A @ B + mu_st\n",
    "sc_H_centered_torch = sc_H_torch - mu_sc\n",
    "sc_H_transformed_torch = sc_H_centered_torch @ A @ B + mu_st\n",
    "sc_H_transformed = sc_H_transformed_torch.cpu().numpy()\n",
    "\n",
    "print(f\"✓ SC contexts transformed: {sc_H_transformed.shape}\")\n",
    "\n",
    "# Verify transformation statistics\n",
    "print(\"\\n--- Post-CORAL SC Statistics ---\")\n",
    "sc_transformed_mean = sc_H_transformed.mean(axis=0)\n",
    "sc_transformed_std = sc_H_transformed.std(axis=0)\n",
    "sc_transformed_norm = np.linalg.norm(sc_H_transformed, axis=1)\n",
    "\n",
    "print(f\"SC (transformed) - Mean of means: {sc_transformed_mean.mean():.6f}\")\n",
    "print(f\"SC (transformed) - Mean of stds:  {sc_transformed_std.mean():.6f}\")\n",
    "print(f\"SC (transformed) - Mean norm:     {sc_transformed_norm.mean():.6f} ± {sc_transformed_norm.std():.6f}\")\n",
    "\n",
    "print(f\"\\nST - Mean of means: {st_mean.mean():.6f} (reference)\")\n",
    "print(f\"ST - Mean of stds:  {st_std.mean():.6f} (reference)\")\n",
    "print(f\"ST - Mean norm:     {st_norm.mean():.6f} ± {st_norm.std():.6f} (reference)\")\n",
    "\n",
    "# Cosine similarity of mean vectors (should be closer to 1.0 now)\n",
    "cos_sim_after = np.dot(st_mean, sc_transformed_mean) / (\n",
    "    np.linalg.norm(st_mean) * np.linalg.norm(sc_transformed_mean)\n",
    ")\n",
    "print(f\"\\n[Post-CORAL mean vector cosine similarity: {cos_sim_after:.4f}]\")\n",
    "print(f\"[Pre-CORAL mean vector cosine similarity:  {cos_sim:.4f}]\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: Re-run Linear Classifier Test\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RE-RUNNING LINEAR CLASSIFIER TEST (POST-CORAL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data with transformed SC contexts\n",
    "X_post = np.vstack([st_H, sc_H_transformed])\n",
    "y_post = np.array([0]*len(st_H) + [1]*len(sc_H_transformed))\n",
    "\n",
    "# Split\n",
    "X_train_post, X_test_post, y_train_post, y_test_post = train_test_split(\n",
    "    X_post, y_post, test_size=0.3, random_state=42, stratify=y_post\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler_post = StandardScaler()\n",
    "X_train_post_scaled = scaler_post.fit_transform(X_train_post)\n",
    "X_test_post_scaled = scaler_post.transform(X_test_post)\n",
    "\n",
    "# Train classifier\n",
    "clf_post = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_post.fit(X_train_post_scaled, y_train_post)\n",
    "\n",
    "# Predict\n",
    "y_pred_proba_post = clf_post.predict_proba(X_test_post_scaled)[:, 1]\n",
    "auc_post = roc_auc_score(y_test_post, y_pred_proba_post)\n",
    "acc_post = clf_post.score(X_test_post_scaled, y_test_post)\n",
    "\n",
    "print(f\"\\nPOST-CORAL Classifier Accuracy: {acc_post:.4f}\")\n",
    "print(f\"POST-CORAL Classifier AUC:      {auc_post:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: PRE-CORAL vs POST-CORAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PRE-CORAL  AUC: {auc:.4f}\")\n",
    "print(f\"POST-CORAL AUC: {auc_post:.4f}\")\n",
    "print(f\"AUC Reduction:  {(auc - auc_post):.4f} ({((auc - auc_post)/auc)*100:.1f}%)\")\n",
    "\n",
    "# Interpretation\n",
    "if auc_post < 0.60:\n",
    "    print(\"\\n✓ SUCCESS: AUC < 0.60 → ST and SC contexts are now indistinguishable!\")\n",
    "    print(\"  CORAL successfully aligned the conditioning distributions.\")\n",
    "    print(\"  → Use CORAL at inference to fix SC local scrambling.\")\n",
    "elif auc_post < 0.70:\n",
    "    print(\"\\n✓ GOOD: AUC 0.60-0.70 → Distributions much more similar.\")\n",
    "    print(\"  CORAL helped but some residual shift remains.\")\n",
    "    print(\"  → Try CORAL + small adapter for best results.\")\n",
    "elif auc_post < 0.85:\n",
    "    print(\"\\n⚠️  PARTIAL: AUC 0.70-0.85 → Shift reduced but still significant.\")\n",
    "    print(\"  CORAL alone may not be sufficient.\")\n",
    "    print(\"  → Consider training a small SC→ST adapter.\")\n",
    "else:\n",
    "    print(\"\\n⚠️  NO EFFECT: AUC still >0.85 → CORAL did not resolve shift.\")\n",
    "    print(\"  The shift is not purely covariance-based.\")\n",
    "    print(\"  → Need learned adapter or SC fine-tuning with geometry losses.\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: Visualize Post-CORAL Distribution\n",
    "# ===================================================================\n",
    "print(\"\\n--- Post-CORAL Visualization ---\")\n",
    "\n",
    "# PCA on post-CORAL data\n",
    "X_post_pca = pca.transform(X_post)  # Use same PCA as before for comparison\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pre-CORAL\n",
    "axes[0].scatter(X_pca[:len(st_H), 0], X_pca[:len(st_H), 1],\n",
    "               alpha=0.5, s=10, label='ST contexts', c='blue')\n",
    "axes[0].scatter(X_pca[len(st_H):, 0], X_pca[len(st_H):, 1],\n",
    "               alpha=0.5, s=10, label='SC contexts (original)', c='red')\n",
    "axes[0].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "axes[0].set_ylabel(f'PC2 ({explained_var[1]*100:.1f}%)')\n",
    "axes[0].set_title(f'PRE-CORAL (AUC={auc:.3f})')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Post-CORAL\n",
    "axes[1].scatter(X_post_pca[:len(st_H), 0], X_post_pca[:len(st_H), 1],\n",
    "               alpha=0.5, s=10, label='ST contexts', c='blue')\n",
    "axes[1].scatter(X_post_pca[len(st_H):, 0], X_post_pca[len(st_H):, 1],\n",
    "               alpha=0.5, s=10, label='SC contexts (CORAL)', c='green')\n",
    "axes[1].set_xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "axes[1].set_ylabel(f'PC2 ({explained_var[1]*100:.1f}%)')\n",
    "axes[1].set_title(f'POST-CORAL (AUC={auc_post:.3f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('context_shift_before_after_coral.png', dpi=150)\n",
    "print(\"✓ Saved: context_shift_before_after_coral.png\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curves comparison\n",
    "fpr_post, tpr_post, _ = roc_curve(y_test_post, y_pred_proba_post)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'Pre-CORAL (AUC={auc:.3f})', color='red')\n",
    "plt.plot(fpr_post, tpr_post, linewidth=2, label=f'Post-CORAL (AUC={auc_post:.3f})', color='green')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves: ST vs SC Classification')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('context_shift_roc_comparison.png', dpi=150)\n",
    "print(\"✓ Saved: context_shift_roc_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CORAL ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: Save CORAL Transform for Inference\n",
    "# ===================================================================\n",
    "print(\"\\nSaving CORAL transform parameters...\")\n",
    "\n",
    "coral_params = {\n",
    "    'mu_st': mu_st.cpu(),\n",
    "    'mu_sc': mu_sc.cpu(),\n",
    "    'cov_st': cov_st.cpu(),\n",
    "    'cov_sc': cov_sc.cpu(),\n",
    "    'A': A.cpu(),\n",
    "    'B': B.cpu(),\n",
    "    'shrink': shrink,\n",
    "}\n",
    "\n",
    "# torch.save(coral_params, 'coral_transform_params.pt')\n",
    "print(\"✓ Saved: coral_transform_params.pt\")\n",
    "print(\"\\nYou can load this in your inference code and apply:\")\n",
    "print(\"  sc_ctx_transformed = (sc_ctx - mu_sc) @ A @ B + mu_st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# HYPOTHESIS TEST: ST vs SC Context Distribution Shift\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TESTING: ST vs SC Context Distribution Shift\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Collect context tokens from ST mini-sets\n",
    "st_contexts = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(st_minisets)):\n",
    "        miniset = st_minisets[idx]\n",
    "        n = miniset['n']\n",
    "        \n",
    "        # Get gene expression\n",
    "        slide_id = miniset['overlap_info']['slide_id']\n",
    "        indices = miniset['overlap_info']['indices'][:n]\n",
    "        gene_expr = st_gene_expr_dict_cpu[slide_id][indices].to(device)\n",
    "        \n",
    "        # Encode: gene_expr → Z embeddings\n",
    "        Z = model.encoder(gene_expr)  # (n, h_dim)\n",
    "        \n",
    "        # Create mask (all ones since no padding)\n",
    "        mask = torch.ones(n, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Context encoder: Z → H (context tokens)\n",
    "        Z_batch = Z.unsqueeze(0)  # (1, n, h_dim)\n",
    "        mask_batch = mask.unsqueeze(0)  # (1, n)\n",
    "        H = model.context_encoder(Z_batch, mask_batch)  # (1, n, c_dim)\n",
    "        \n",
    "        # Store flattened context tokens\n",
    "        st_contexts.append(H.squeeze(0).cpu())  # (n, c_dim)\n",
    "\n",
    "# Collect context tokens from SC mini-sets\n",
    "sc_contexts = []\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(sc_minisets)):\n",
    "        miniset = sc_minisets[idx]\n",
    "        n_A = miniset['n_A']\n",
    "        \n",
    "        # Get gene expression\n",
    "        indices_A = miniset['global_indices_A'].cpu().numpy()[:n_A]\n",
    "        gene_expr = sc_gene_expr_cpu[indices_A].to(device)\n",
    "        \n",
    "        # Encode: gene_expr → Z embeddings\n",
    "        Z = model.encoder(gene_expr)  # (n_A, h_dim)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = torch.ones(n_A, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Context encoder: Z → H\n",
    "        Z_batch = Z.unsqueeze(0)  # (1, n_A, h_dim)\n",
    "        mask_batch = mask.unsqueeze(0)  # (1, n_A)\n",
    "        H = model.context_encoder(Z_batch, mask_batch)  # (1, n_A, c_dim)\n",
    "        \n",
    "        # Store flattened context tokens\n",
    "        sc_contexts.append(H.squeeze(0).cpu())  # (n_A, c_dim)\n",
    "\n",
    "# Concatenate all context tokens\n",
    "st_H = torch.cat(st_contexts, dim=0).numpy()  # (N_st, c_dim)\n",
    "sc_H = torch.cat(sc_contexts, dim=0).numpy()  # (N_sc, c_dim)\n",
    "\n",
    "print(f\"\\nST contexts: {st_H.shape}\")\n",
    "print(f\"SC contexts: {sc_H.shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. COMPUTE STATISTICS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Context Token Statistics ---\")\n",
    "\n",
    "# Per-dimension statistics\n",
    "st_mean = st_H.mean(axis=0)\n",
    "st_std = st_H.std(axis=0)\n",
    "st_norm = np.linalg.norm(st_H, axis=1)\n",
    "\n",
    "sc_mean = sc_H.mean(axis=0)\n",
    "sc_std = sc_H.std(axis=0)\n",
    "sc_norm = np.linalg.norm(sc_H, axis=1)\n",
    "\n",
    "print(f\"\\nST - Mean of means: {st_mean.mean():.6f}\")\n",
    "print(f\"ST - Mean of stds:  {st_std.mean():.6f}\")\n",
    "print(f\"ST - Mean norm:     {st_norm.mean():.6f} ± {st_norm.std():.6f}\")\n",
    "\n",
    "print(f\"\\nSC - Mean of means: {sc_mean.mean():.6f}\")\n",
    "print(f\"SC - Mean of stds:  {sc_std.mean():.6f}\")\n",
    "print(f\"SC - Mean norm:     {sc_norm.mean():.6f} ± {sc_norm.std():.6f}\")\n",
    "\n",
    "# Norm difference\n",
    "norm_diff = np.abs(st_norm.mean() - sc_norm.mean()) / st_norm.mean()\n",
    "print(f\"\\n[Norm difference: {norm_diff*100:.2f}%]\")\n",
    "\n",
    "# Mean vector cosine similarity\n",
    "cos_sim = np.dot(st_mean, sc_mean) / (np.linalg.norm(st_mean) * np.linalg.norm(sc_mean))\n",
    "print(f\"[Mean vector cosine similarity: {cos_sim:.4f}]\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. LINEAR CLASSIFIER TEST\n",
    "# ===================================================================\n",
    "print(\"\\n--- Linear Classifier Test (ST vs SC) ---\")\n",
    "\n",
    "# Prepare data\n",
    "X = np.vstack([st_H, sc_H])\n",
    "y = np.array([0]*len(st_H) + [1]*len(sc_H))  # 0=ST, 1=SC\n",
    "\n",
    "# Shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_proba = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "acc = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nClassifier Accuracy: {acc:.4f}\")\n",
    "print(f\"Classifier AUC:      {auc:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if auc > 0.85:\n",
    "    print(\"\\n⚠️  HIGH AUC (>0.85): Strong distribution shift detected!\")\n",
    "    print(\"   The score net sees ST vs SC contexts as very different.\")\n",
    "    print(\"   This supports the 'conditioning shift' hypothesis.\")\n",
    "elif auc > 0.70:\n",
    "    print(\"\\n⚠️  MODERATE AUC (0.70-0.85): Moderate distribution shift.\")\n",
    "    print(\"   ST and SC contexts are distinguishable but not wildly different.\")\n",
    "elif auc > 0.60:\n",
    "    print(\"\\n✓  LOW AUC (0.60-0.70): Mild distribution shift.\")\n",
    "    print(\"   Contexts are similar. Shift may not be the main issue.\")\n",
    "else:\n",
    "    print(\"\\n✓  VERY LOW AUC (<0.60): Contexts are indistinguishable.\")\n",
    "    print(\"   Conditioning shift is NOT the problem.\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3. VISUALIZE (Optional)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Visualization ---\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCA projection\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:len(st_H), 0], X_pca[:len(st_H), 1], \n",
    "           alpha=0.5, s=10, label='ST contexts', c='blue')\n",
    "plt.scatter(X_pca[len(st_H):, 0], X_pca[len(st_H):, 1], \n",
    "           alpha=0.5, s=10, label='SC contexts', c='red')\n",
    "plt.xlabel(f'PC1 ({explained_var[0]*100:.1f}%)')\n",
    "plt.ylabel(f'PC2 ({explained_var[1]*100:.1f}%)')\n",
    "plt.title('Context Token Distribution: ST vs SC')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('context_shift_pca.png', dpi=150)\n",
    "print(\"✓ Saved: context_shift_pca.png\")\n",
    "plt.close()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC: ST vs SC Context Classification')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('context_shift_roc.png', dpi=150)\n",
    "print(\"✓ Saved: context_shift_roc.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PATCHWISE INFERENCE WITH GLOBAL SCALE\n",
    "# ===================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "from core_models_et_p3 import GEMSModel\n",
    "import utils_et as uet\n",
    "\n",
    "# ===================================================================\n",
    "# PATHS AND CONFIG\n",
    "# ===================================================================\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "# checkpoint_path = f\"{output_dir}/phase2_sc_finetuned_checkpoint.pt\"\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA AND MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "adata_sc = sc.read_h5ad(f\"{output_dir}/scadata_with_gems_20260112_023604.h5ad\")\n",
    "\n",
    "if hasattr(adata_sc, 'raw') and adata_sc.raw is not None:\n",
    "    sc_expr = torch.tensor(adata_sc.raw.X.toarray() if hasattr(adata_sc.raw.X, 'toarray') else adata_sc.raw.X, dtype=torch.float32)\n",
    "else:\n",
    "    sc_expr = torch.tensor(adata_sc.X.toarray() if hasattr(adata_sc.X, 'toarray') else adata_sc.X, dtype=torch.float32)\n",
    "\n",
    "# ===================================================================\n",
    "# LOAD AND NORMALIZE GT COORDS\n",
    "# ===================================================================\n",
    "\n",
    "# Load raw GT coords (numpy)\n",
    "gt_coords_raw = adata_sc.obsm['spatial_gt']\n",
    "n_cells, n_genes = sc_expr.shape\n",
    "print(f\"✓ Loaded SC data: {n_cells} cells × {n_genes} genes\")\n",
    "print(f\"✓ Ground truth coords (raw): {gt_coords_raw.shape}\")\n",
    "\n",
    "# Convert to tensor for normalization\n",
    "gt_coords_tensor = torch.tensor(gt_coords_raw, dtype=torch.float32, device=device)\n",
    "slide_ids = torch.zeros(gt_coords_tensor.shape[0], dtype=torch.long, device=device)\n",
    "\n",
    "# Canonicalize (same as training)\n",
    "gt_coords_norm, gt_mu, gt_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    gt_coords_tensor, slide_ids\n",
    ")\n",
    "\n",
    "# Keep BOTH versions:\n",
    "# - gt_coords_norm: tensor on device (for torch operations, passing to inference)\n",
    "# - gt_coords_np: numpy on CPU (for scipy operations like pdist)\n",
    "gt_coords_np = gt_coords_norm.cpu().numpy()\n",
    "\n",
    "print(f\"✓ GT coords normalized: scale={gt_scale[0].item():.4f}\")\n",
    "print(f\"✓ GT coords RMS: {gt_coords_norm.pow(2).mean().sqrt().item():.4f}\")\n",
    "print(f\"✓ GT coords range: X=[{gt_coords_np[:,0].min():.3f}, {gt_coords_np[:,0].max():.3f}], \"\n",
    "      f\"Y=[{gt_coords_np[:,1].min():.3f}, {gt_coords_np[:,1].max():.3f}]\")\n",
    "\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "print(f\"✓ Loaded checkpoint\")\n",
    "\n",
    "# Initialize model (from run_mouse_brain_2.py config)\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    dist_bins=24,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.generator.eval()\n",
    "model.score_net.eval()\n",
    "print(f\"✓ Model loaded and set to eval mode\")\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE CORAL TRANSFORMATION\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING CORAL TRANSFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Load ST data (you need this to compute ST context distribution)\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "# Get common genes\n",
    "common = sorted(list(set(adata_sc.var_names) & set(stadata.var_names)))\n",
    "X_st = stadata[:, common].X\n",
    "if hasattr(X_st, \"toarray\"): \n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, _, _ = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"✓ Loaded ST data: {st_expr.shape[0]} spots × {st_expr.shape[1]} genes\")\n",
    "\n",
    "# 2. Prepare ST gene expression dict\n",
    "st_gene_expr_dict = {0: st_expr.cpu()}\n",
    "\n",
    "# 3. Load Stage B targets (needed for compute_coral_params_from_st)\n",
    "# This should be in your checkpoint or output directory\n",
    "targets_path = f\"{output_dir}/stageB_targets/targets_dict.pt\"\n",
    "if os.path.exists(targets_path):\n",
    "    model.targets_dict = torch.load(targets_path, map_location='cpu')\n",
    "    print(f\"✓ Loaded targets_dict from {targets_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  Targets not found at {targets_path}\")\n",
    "    print(\"   Running Stage B precomputation...\")\n",
    "    slides_dict = {0: (st_coords, st_expr)}\n",
    "    model.train_stageB(slides=slides_dict, outdir=f\"{output_dir}/stageB_targets\")\n",
    "    print(\"✓ Stage B complete\")\n",
    "\n",
    "# 4. Compute CORAL parameters\n",
    "print(\"\\n--- Computing ST context distribution ---\")\n",
    "model.compute_coral_params_from_st(\n",
    "    st_gene_expr_dict=st_gene_expr_dict,\n",
    "    n_samples=2000,\n",
    "    n_min=96,\n",
    "    n_max=384,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Building CORAL transformation ---\")\n",
    "model.build_coral_transform(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_samples=2000,\n",
    "    n_min=96,\n",
    "    n_max=384,\n",
    "    shrink=0.01,\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "print(\"✓ CORAL transformation ready!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# RUN INFERENCE (NEW GLOBAL SCALE VERSION)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING INFERENCE (GLOBAL SCALE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ADD THIS after loading checkpoint and before calling inference\n",
    "if 'sigma_data' in checkpoint:\n",
    "    model.sigma_data = checkpoint['sigma_data']\n",
    "if 'sigma_min' in checkpoint:\n",
    "    model.sigma_min = checkpoint['sigma_min']\n",
    "if 'sigma_max' in checkpoint:\n",
    "    model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model.infer_sc_patchwise(\n",
    "        sc_gene_expr=sc_expr,\n",
    "        n_timesteps_sample=600,\n",
    "        return_coords=True,\n",
    "        patch_size=224,\n",
    "        coverage_per_cell=8.0,\n",
    "        n_align_iters=10,\n",
    "        eta=0.0,\n",
    "        guidance_scale=2.0,\n",
    "        gt_coords=gt_coords_norm,\n",
    "        debug_knn=True,\n",
    "        debug_max_patches=15,\n",
    "        debug_k_list=(10, 20),\n",
    "        pool_mult=2.0,\n",
    "        stochastic_tau=1.0,\n",
    "        tau_mode=\"adaptive_kth\",\n",
    "        ensure_connected=True,\n",
    "        local_refine=False,\n",
    "        anchor_sampling_mode=\"seq_align_only\",  # NEW PARAMETER\n",
    "        commit_frac=0.75,\n",
    "        seq_align_dim=32,\n",
    "    )\n",
    "\n",
    "\n",
    "D_edm_pred = results['D_edm'].cpu().numpy()\n",
    "coords_pred = results['coords_canon'].cpu().numpy()\n",
    "print(f\"\\n✓ Inference complete!\")\n",
    "print(f\"  Predicted EDM: {D_edm_pred.shape}\")\n",
    "print(f\"  Predicted coords: {coords_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPUTE GROUND TRUTH EDM\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gt_edm = squareform(pdist(gt_coords_np, 'euclidean'))\n",
    "print(f\"✓ Ground truth EDM: {gt_edm.shape}\")\n",
    "\n",
    "# Extract upper triangle distances\n",
    "triu_indices = np.triu_indices(n_cells, k=1)\n",
    "gt_distances = gt_edm[triu_indices]\n",
    "pred_distances = D_edm_pred[triu_indices]\n",
    "\n",
    "# Scale alignment\n",
    "scale = np.median(gt_distances) / np.median(pred_distances)\n",
    "pred_distances_scaled = pred_distances * scale\n",
    "\n",
    "# Correlations\n",
    "pearson_corr, _ = pearsonr(gt_distances, pred_distances_scaled)\n",
    "spearman_corr, _ = spearmanr(gt_distances, pred_distances_scaled)\n",
    "\n",
    "print(f\"\\nPearson Correlation:  {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "print(f\"Scale factor: {scale:.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING PLOTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot 1: Coordinate comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(gt_coords_np[:, 0], gt_coords_np[:, 1], s=5, alpha=0.6, c='blue')\n",
    "axes[0].set_title('Ground Truth Coordinates', fontsize=14, weight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "axes[1].scatter(coords_pred[:, 0], coords_pred[:, 1], s=5, alpha=0.6, c='red')\n",
    "axes[1].set_title('Predicted Coordinates (Global Scale)', fontsize=14, weight='bold')\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Distance scatter - Pearson and Spearman\n",
    "sample_size = 50000\n",
    "sample_idx = np.random.choice(len(gt_distances), sample_size, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Pearson correlation plot\n",
    "ax = axes[0]\n",
    "ax.scatter(gt_distances[sample_idx], pred_distances_scaled[sample_idx], alpha=0.2, s=5)\n",
    "ax.set_title(f'Distance Correlation (Pearson)\\nρ = {pearson_corr:.4f}', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Ground Truth Distance', fontsize=12)\n",
    "ax.set_ylabel('Predicted Distance (scaled)', fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.75, label='Ideal')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "\n",
    "# Spearman correlation plot\n",
    "ax = axes[1]\n",
    "ax.scatter(gt_distances[sample_idx], pred_distances_scaled[sample_idx], alpha=0.2, s=5)\n",
    "ax.set_title(f'Distance Correlation (Spearman)\\nρ = {spearman_corr:.4f}', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Ground Truth Distance', fontsize=12)\n",
    "ax.set_ylabel('Predicted Distance (scaled)', fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]), max(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.75, label='Ideal')\n",
    "ax.set_aspect('equal')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Distance distributions\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(gt_distances, color='blue', label='Ground Truth', stat='density', bins=100, alpha=0.5, ax=ax)\n",
    "sns.histplot(pred_distances_scaled, color='red', label='Predicted (scaled)', stat='density', bins=100, alpha=0.5, ax=ax)\n",
    "ax.set_title('Distance Distribution Comparison', fontsize=16, weight='bold')\n",
    "ax.set_xlabel('Distance')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# EDM HEATMAP COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EDM HEATMAP VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    return (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "# Normalize EDMs\n",
    "gt_edm_norm = normalize_matrix(gt_edm)\n",
    "pred_edm_norm = normalize_matrix(D_edm_pred)\n",
    "\n",
    "# Sample cells for visualization\n",
    "sample_size = min(838, n_cells)\n",
    "sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "print(f\"\\nCreating EDM heatmaps with {sample_size} sampled cells...\")\n",
    "\n",
    "# Create side-by-side heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('EDM Comparison: Ground Truth vs. Predicted', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Ground Truth EDM\n",
    "im1 = axes[0].imshow(gt_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[0].set_title('Ground Truth EDM (Normalized)', fontsize=14)\n",
    "axes[0].set_xlabel('Cell Index (Sampled)', fontsize=12)\n",
    "axes[0].set_ylabel('Cell Index (Sampled)', fontsize=12)\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Predicted EDM\n",
    "im2 = axes[1].imshow(pred_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[1].set_title('Predicted EDM (Normalized)', fontsize=14)\n",
    "axes[1].set_xlabel('Cell Index (Sampled)', fontsize=12)\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EDM HEATMAP VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# ANISOTROPY ANALYSIS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EIGENVALUE ANISOTROPY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_anisotropy(coords):\n",
    "    \"\"\"Compute eigenvalue anisotropy ratio λ1/λ2\"\"\"\n",
    "    X = coords.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    eigvals = eigvals[::-1]\n",
    "    \n",
    "    lam1, lam2 = eigvals[0], eigvals[1]\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    \n",
    "    return lam1, lam2, ratio\n",
    "\n",
    "# Compute anisotropy for ground truth\n",
    "lam1_gt, lam2_gt, ratio_gt = compute_anisotropy(gt_coords_np)\n",
    "print(f\"\\nGround Truth Coordinates:\")\n",
    "print(f\"  λ₁ = {lam1_gt:.4f},  λ₂ = {lam2_gt:.4f}\")\n",
    "print(f\"  λ₁/λ₂ = {ratio_gt:.2f}\")\n",
    "\n",
    "if ratio_gt < 5:\n",
    "    print(f\"  → GENUINELY 2D ✓\")\n",
    "elif ratio_gt < 20:\n",
    "    print(f\"  → Anisotropic but still 2D-ish\")\n",
    "else:\n",
    "    print(f\"  → EFFECTIVELY 1D (very elongated) ✗\")\n",
    "\n",
    "# Compute anisotropy for predicted\n",
    "lam1_pred, lam2_pred, ratio_pred = compute_anisotropy(coords_pred)\n",
    "print(f\"\\nPredicted Coordinates (Global Scale):\")\n",
    "print(f\"  λ₁ = {lam1_pred:.4f},  λ₂ = {lam2_pred:.4f}\")\n",
    "print(f\"  λ₁/λ₂ = {ratio_pred:.2f}\")\n",
    "\n",
    "if ratio_pred < 5:\n",
    "    print(f\"  → GENUINELY 2D ✓\")\n",
    "elif ratio_pred < 20:\n",
    "    print(f\"  → Anisotropic but still 2D-ish\")\n",
    "else:\n",
    "    print(f\"  → EFFECTIVELY 1D (very elongated) ✗\")\n",
    "\n",
    "# ===================================================================\n",
    "# ANISOTROPY VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n=== Creating Anisotropy Plots ===\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Bar comparison\n",
    "ax = axes[0, 0]\n",
    "methods = ['Ground Truth', 'Predicted']\n",
    "ratios = [ratio_gt, ratio_pred]\n",
    "colors = ['blue', 'red']\n",
    "\n",
    "bars = ax.bar(methods, ratios, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.axhline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axhline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_ylabel('$\\\\lambda_1/\\\\lambda_2$ (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Anisotropy Comparison: Ground Truth vs Predicted', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, ratio in zip(bars, ratios):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{ratio:.2f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 2: Eigenvalue scatter\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(lam2_gt, lam1_gt, c='blue', s=300, marker='o', \n",
    "          edgecolors='darkblue', linewidth=2, label='Ground Truth', zorder=5)\n",
    "ax.scatter(lam2_pred, lam1_pred, c='red', s=300, marker='*', \n",
    "          edgecolors='darkred', linewidth=2, label='Predicted', zorder=5)\n",
    "\n",
    "min_val = min(lam2_gt, lam2_pred)\n",
    "max_val = max(lam1_gt, lam1_pred)\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'k--', \n",
    "        linewidth=2, label='$\\\\lambda_1 = \\\\lambda_2$', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('$\\\\lambda_2$ (Smaller Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('$\\\\lambda_1$ (Larger Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Eigenvalue Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Ground truth coordinates with anisotropy\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(gt_coords_np[:, 0], gt_coords_np[:, 1], alpha=0.5, s=10, \n",
    "          c='blue', edgecolors='none')\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'Ground Truth\\n$\\\\lambda_1/\\\\lambda_2$ = {ratio_gt:.2f}', \n",
    "             fontsize=14, fontweight='bold', color='blue')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 4: Predicted coordinates with anisotropy\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(coords_pred[:, 0], coords_pred[:, 1], alpha=0.5, s=10, \n",
    "          c='red', edgecolors='none')\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'Predicted (Global Scale)\\n$\\\\lambda_1/\\\\lambda_2$ = {ratio_pred:.2f}', \n",
    "             fontsize=14, fontweight='bold', color='red')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "anisotropy_path = os.path.join(output_dir, f'patchwise_anisotropy_{timestamp}.png')\n",
    "# plt.savefig(anisotropy_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved anisotropy plot: {anisotropy_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# DETAILED COMPARISON TABLE\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANISOTROPY COMPARISON TABLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<35} {'Ground Truth':<20} {'Predicted':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'λ₁ (larger eigenvalue)':<35} {lam1_gt:.4f}            {lam1_pred:.4f}\")\n",
    "print(f\"{'λ₂ (smaller eigenvalue)':<35} {lam2_gt:.4f}            {lam2_pred:.4f}\")\n",
    "print(f\"{'λ₁/λ₂ ratio':<35} {ratio_gt:.2f}                {ratio_pred:.2f}\")\n",
    "print(f\"{'Difference in λ₁/λ₂':<35} {abs(ratio_gt - ratio_pred):.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ratio_diff = abs(ratio_gt - ratio_pred)\n",
    "\n",
    "if ratio_diff < 2:\n",
    "    print(f\"\\n✓ EXCELLENT: Anisotropy closely matches ground truth (diff = {ratio_diff:.2f})\")\n",
    "elif ratio_diff < 5:\n",
    "    print(f\"\\n✓ GOOD: Moderate anisotropy difference (diff = {ratio_diff:.2f})\")\n",
    "else:\n",
    "    print(f\"\\n⚠ WARNING: Large anisotropy difference (diff = {ratio_diff:.2f})\")\n",
    "\n",
    "if ratio_pred < 5 and ratio_gt < 5:\n",
    "    print(f\"\\n✓ Both ground truth and predicted preserve 2D geometry\")\n",
    "elif ratio_pred < 5:\n",
    "    print(f\"\\n✓ Predicted successfully preserves 2D geometry\")\n",
    "    print(f\"  Ground truth is more anisotropic (λ₁/λ₂ = {ratio_gt:.2f})\")\n",
    "elif ratio_gt < 5:\n",
    "    print(f\"\\n⚠ Ground truth is 2D but predicted shows elongation\")\n",
    "    print(f\"  Predicted λ₁/λ₂ = {ratio_pred:.2f}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Both show anisotropic structure\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANISOTROPY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# CELL TYPE VISUALIZATION (GROUND TRUTH vs PREDICTED)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CELL TYPE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check what cell type column exists\n",
    "print(\"\\nAvailable columns in adata_sc.obs:\")\n",
    "print(list(adata_sc.obs.columns))\n",
    "\n",
    "# Find cell type column\n",
    "cell_type_col = None\n",
    "for col in ['cell_type', 'celltype', 'cluster', 'annotation', 'cell_ontology_class']:\n",
    "    if col in adata_sc.obs.columns:\n",
    "        cell_type_col = col\n",
    "        break\n",
    "\n",
    "if cell_type_col is None:\n",
    "    print(\"\\nWARNING: No cell type column found. Using first categorical column or creating dummy labels.\")\n",
    "    categorical_cols = adata_sc.obs.select_dtypes(include=['category', 'object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        cell_type_col = categorical_cols[0]\n",
    "    else:\n",
    "        adata_sc.obs['cell_type'] = 'Unknown'\n",
    "        cell_type_col = 'cell_type'\n",
    "\n",
    "print(f\"\\nUsing cell type column: '{cell_type_col}'\")\n",
    "cell_types = adata_sc.obs[cell_type_col].values\n",
    "\n",
    "# Get unique cell types\n",
    "unique_types = np.unique(cell_types)\n",
    "n_types = len(unique_types)\n",
    "\n",
    "print(f\"Found {n_types} unique cell types:\")\n",
    "for i, ct in enumerate(unique_types):\n",
    "    count = (cell_types == ct).sum()\n",
    "    print(f\"  {i+1}. {ct}: {count} cells\")\n",
    "\n",
    "# Create colormap\n",
    "if n_types <= 10:\n",
    "    cmap = plt.cm.tab10\n",
    "elif n_types <= 20:\n",
    "    cmap = plt.cm.tab20\n",
    "else:\n",
    "    cmap = plt.cm.gist_ncar\n",
    "\n",
    "# Map cell types to colors\n",
    "type_to_color = {ct: cmap(i / n_types) for i, ct in enumerate(unique_types)}\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Ground Truth\n",
    "ax = axes[0]\n",
    "for ct in unique_types:\n",
    "    mask = cell_types == ct\n",
    "    ax.scatter(gt_coords_np[mask, 0], gt_coords_np[mask, 1], \n",
    "              c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Ground Truth - Cell Types', fontsize=14, fontweight='bold')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Predicted\n",
    "ax = axes[1]\n",
    "for ct in unique_types:\n",
    "    mask = cell_types == ct\n",
    "    ax.scatter(coords_pred[mask, 0], coords_pred[mask, 1], \n",
    "              c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Predicted (Global Scale) - Cell Types', fontsize=14, fontweight='bold')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend outside the plot\n",
    "if n_types <= 15:\n",
    "    handles, labels = axes[1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1.0, 0.5), \n",
    "              fontsize=10, title='Cell Type', title_fontsize=12, frameon=True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "celltype_path = os.path.join(output_dir, f'patchwise_celltype_{timestamp}.png')\n",
    "# plt.savefig(celltype_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved cell type visualization: {celltype_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CELL TYPE VISUALIZATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# k-NN PRESERVATION ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_knn_preservation(coords_gt, coords_pred, k=10):\n",
    "    \"\"\"\n",
    "    Compute k-nearest neighbor preservation.\n",
    "    Returns average number of preserved neighbors per cell.\n",
    "    \"\"\"\n",
    "    n = coords_gt.shape[0]\n",
    "\n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_gt)\n",
    "    _, indices_gt = nbrs_gt.kneighbors(coords_gt)\n",
    "    indices_gt = indices_gt[:, 1:]\n",
    "\n",
    "    nbrs_pred = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_pred)\n",
    "    _, indices_pred = nbrs_pred.kneighbors(coords_pred)\n",
    "    indices_pred = indices_pred[:, 1:]\n",
    "\n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        gt_neighbors = set(indices_gt[i])\n",
    "        pred_neighbors = set(indices_pred[i])\n",
    "        overlap = len(gt_neighbors.intersection(pred_neighbors))\n",
    "        overlaps.append(overlap)\n",
    "\n",
    "    return np.mean(overlaps), overlaps\n",
    "\n",
    "knn_k10, overlaps_k10 = compute_knn_preservation(gt_coords_np, coords_pred, k=10)\n",
    "knn_k20, overlaps_k20 = compute_knn_preservation(gt_coords_np, coords_pred, k=20)\n",
    "\n",
    "print(f\"\\nk-NN Preservation Results:\")\n",
    "print(f\" k=10: {knn_k10:.2f} / 10 ({knn_k10/10*100:.1f}% neighbors preserved)\")\n",
    "print(f\" k=20: {knn_k20:.2f} / 20 ({knn_k20/20*100:.1f}% neighbors preserved)\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\" EDM Pearson: {pearson_corr:.4f} (global distances)\")\n",
    "print(f\" EDM Spearman: {spearman_corr:.4f} (global distances)\")\n",
    "print(f\" k-NN@10: {knn_k10/10:.4f} (local neighborhoods)\")\n",
    "print(f\" k-NN@20: {knn_k20/20:.4f} (local neighborhoods)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "# ===================================================================\n",
    "# k-NN PRESERVATION ANALYSIS (ENHANCED)\n",
    "# ===================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION ANALYSIS (ENHANCED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def knn_sets(coords, k):\n",
    "    \"\"\"Get k-NN indices for all points.\"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords)\n",
    "    _, idx = nbrs.kneighbors(coords)\n",
    "    return idx[:, 1:]\n",
    "\n",
    "def knn_overlap_frac(idx_gt, idx_pr):\n",
    "    \"\"\"Compute per-point overlap fraction: |intersection| / k\"\"\"\n",
    "    n, k = idx_gt.shape\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        out[i] = len(set(idx_gt[i]).intersection(idx_pr[i])) / k\n",
    "    return out\n",
    "\n",
    "def knn_jaccard(idx_gt, idx_pr):\n",
    "    \"\"\"Compute per-point Jaccard: |intersection| / |union|\"\"\"\n",
    "    n, k = idx_gt.shape\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        a = set(idx_gt[i])\n",
    "        b = set(idx_pr[i])\n",
    "        out[i] = len(a & b) / max(1, len(a | b))\n",
    "    return out\n",
    "\n",
    "def kth_neighbor_radius(coords, k):\n",
    "    \"\"\"Get distance to k-th neighbor for all points.\"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords)\n",
    "    d, _ = nbrs.kneighbors(coords)\n",
    "    return d[:, k]\n",
    "\n",
    "def local_spearman_within_radius(gt_coords, pr_coords, R, min_n=10, fallback_k=30):\n",
    "    \"\"\"Compute per-point local Spearman correlation within radius R.\"\"\"\n",
    "    nbrs_gt = NearestNeighbors(radius=R, algorithm='ball_tree').fit(gt_coords)\n",
    "    ind = nbrs_gt.radius_neighbors(gt_coords, return_distance=False)\n",
    "\n",
    "    nbrs_gt_k = NearestNeighbors(n_neighbors=fallback_k+1, algorithm='ball_tree').fit(gt_coords)\n",
    "    _, idx_k = nbrs_gt_k.kneighbors(gt_coords)\n",
    "    idx_k = idx_k[:, 1:]\n",
    "\n",
    "    n = gt_coords.shape[0]\n",
    "    vals = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        neigh = ind[i]\n",
    "        neigh = neigh[neigh != i]\n",
    "        if neigh.shape[0] < min_n:\n",
    "            neigh = idx_k[i]\n",
    "\n",
    "        d_gt = np.linalg.norm(gt_coords[neigh] - gt_coords[i], axis=1)\n",
    "        d_pr = np.linalg.norm(pr_coords[neigh] - pr_coords[i], axis=1)\n",
    "\n",
    "        if np.std(d_gt) < 1e-12 or np.std(d_pr) < 1e-12:\n",
    "            continue\n",
    "\n",
    "        vals[i] = spearmanr(d_gt, d_pr).correlation\n",
    "\n",
    "    return vals\n",
    "\n",
    "def soft_weighted_jaccard(gt_coords, pr_coords, k=20, tau=None):\n",
    "    \"\"\"Compute soft distance-weighted Jaccard (gives partial credit for near-misses).\"\"\"\n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(gt_coords)\n",
    "    d_gt, idx_gt = nbrs_gt.kneighbors(gt_coords)\n",
    "    d_gt, idx_gt = d_gt[:, 1:], idx_gt[:, 1:]\n",
    "\n",
    "    nbrs_pr = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(pr_coords)\n",
    "    d_pr, idx_pr = nbrs_pr.kneighbors(pr_coords)\n",
    "    d_pr, idx_pr = d_pr[:, 1:], idx_pr[:, 1:]\n",
    "\n",
    "    if tau is None:\n",
    "        tau = np.median(d_gt[:, -1]) + 1e-12\n",
    "\n",
    "    n = gt_coords.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        wgt = {int(j): np.exp(-float(d)/tau) for j, d in zip(idx_gt[i], d_gt[i])}\n",
    "        wpr = {int(j): np.exp(-float(d)/tau) for j, d in zip(idx_pr[i], d_pr[i])}\n",
    "\n",
    "        keys = set(wgt.keys()) | set(wpr.keys())\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for j in keys:\n",
    "            a = wgt.get(j, 0.0)\n",
    "            b = wpr.get(j, 0.0)\n",
    "            num += min(a, b)\n",
    "            den += max(a, b)\n",
    "        out[i] = num / max(1e-12, den)\n",
    "\n",
    "    return out, tau\n",
    "\n",
    "def compute_hits(gt_knn_k, pred_knn_m):\n",
    "    \"\"\"\n",
    "    Compute h_i(k,m) = |G_i^(k) ∩ P_i^(m)| for each point i.\n",
    "    \n",
    "    Args:\n",
    "        gt_knn_k: (N, k) array of GT top-k neighbor indices\n",
    "        pred_knn_m: (N, m) array of predicted top-m neighbor indices\n",
    "    \n",
    "    Returns:\n",
    "        hits: (N,) array of hit counts per point\n",
    "    \"\"\"\n",
    "    n = gt_knn_k.shape[0]\n",
    "    k = gt_knn_k.shape[1]\n",
    "    hits = np.zeros(n, dtype=np.int32)\n",
    "    \n",
    "    for i in range(n):\n",
    "        gt_set = set(gt_knn_k[i])\n",
    "        pred_set = set(pred_knn_m[i])\n",
    "        hits[i] = len(gt_set & pred_set)\n",
    "    \n",
    "    return hits\n",
    "\n",
    "def recall_at_k(gt_coords, pred_coords, k=10):\n",
    "    \"\"\"\n",
    "    Recall@k = mean(h_i(k,k) / k)\n",
    "    Strict metric: fraction of GT top-k recovered in predicted top-k.\n",
    "    \"\"\"\n",
    "    gt_knn = knn_sets(gt_coords, k)\n",
    "    pred_knn = knn_sets(pred_coords, k)\n",
    "    hits = compute_hits(gt_knn, pred_knn)\n",
    "    return hits.mean() / k, hits / k\n",
    "\n",
    "def nearmiss_at_m(gt_coords, pred_coords, k_base=10, m=20):\n",
    "    \"\"\"\n",
    "    NearMiss@m(k) = mean(h_i(k,m) / k)\n",
    "    Tolerant metric: fraction of GT top-k that appear in predicted top-m.\n",
    "    \"\"\"\n",
    "    gt_knn = knn_sets(gt_coords, k_base)\n",
    "    pred_knn = knn_sets(pred_coords, m)\n",
    "    hits = compute_hits(gt_knn, pred_knn)\n",
    "    return hits.mean() / k_base, hits / k_base\n",
    "\n",
    "# ===================================================================\n",
    "# 1) HARD kNN OVERLAP + JACCARD (k=10, 20, 50)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Hard k-NN Metrics ---\")\n",
    "\n",
    "idx_gt_10 = knn_sets(gt_coords_np, 10)\n",
    "idx_pr_10 = knn_sets(coords_pred, 10)\n",
    "idx_gt_20 = knn_sets(gt_coords_np, 20)\n",
    "idx_pr_20 = knn_sets(coords_pred, 20)\n",
    "idx_gt_50 = knn_sets(gt_coords_np, 50)\n",
    "idx_pr_50 = knn_sets(coords_pred, 50)\n",
    "\n",
    "for k, ig, ip in [(10, idx_gt_10, idx_pr_10), (20, idx_gt_20, idx_pr_20), (50, idx_gt_50, idx_pr_50)]:\n",
    "    ov = knn_overlap_frac(ig, ip)\n",
    "    jc = knn_jaccard(ig, ip)\n",
    "    print(f\"[KNN] k={k:2d}: overlap mean={ov.mean():.3f} p50={np.median(ov):.3f} | \"\n",
    "          f\"jaccard mean={jc.mean():.3f} p50={np.median(jc):.3f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2) RECALL@k AND NEARMISS@m METRICS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Recall and NearMiss Metrics ---\")\n",
    "\n",
    "recall_10, recall_10_per_point = recall_at_k(gt_coords_np, coords_pred, k=10)\n",
    "print(f\"[RECALL@10] mean={recall_10:.4f} p50={np.median(recall_10_per_point):.4f}\")\n",
    "\n",
    "nearmiss_20, nearmiss_20_per_point = nearmiss_at_m(gt_coords_np, coords_pred, k_base=10, m=20)\n",
    "print(f\"[NEARMISS@20] (k_base=10) mean={nearmiss_20:.4f} p50={np.median(nearmiss_20_per_point):.4f}\")\n",
    "\n",
    "nearmiss_50, nearmiss_50_per_point = nearmiss_at_m(gt_coords_np, coords_pred, k_base=10, m=50)\n",
    "print(f\"[NEARMISS@50] (k_base=10) mean={nearmiss_50:.4f} p50={np.median(nearmiss_50_per_point):.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3) LOCAL SPEARMAN (distance ordering within local neighborhood)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Local Spearman Correlation ---\")\n",
    "\n",
    "r20 = kth_neighbor_radius(gt_coords_np, 20)\n",
    "R = np.median(r20)\n",
    "print(f\"[LOCAL-RADIUS] R = median GT d(20) = {R:.6f}\")\n",
    "\n",
    "rho_local = local_spearman_within_radius(gt_coords_np, coords_pred, R)\n",
    "good = np.isfinite(rho_local)\n",
    "print(f\"[LOCAL-SPEARMAN] finite_frac={good.mean():.2%} \"\n",
    "      f\"mean={np.nanmean(rho_local):.3f} p50={np.nanmedian(rho_local):.3f} \"\n",
    "      f\"p10={np.nanpercentile(rho_local,10):.3f} p90={np.nanpercentile(rho_local,90):.3f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 4) SOFT WEIGHTED JACCARD (partial credit for near-misses)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Soft Weighted Jaccard ---\")\n",
    "\n",
    "sj20, tau20 = soft_weighted_jaccard(gt_coords_np, coords_pred, k=20, tau=None)\n",
    "sj50, tau50 = soft_weighted_jaccard(gt_coords_np, coords_pred, k=50, tau=None)\n",
    "print(f\"[SOFT-JACCARD] k=20 tau={tau20:.6f}: mean={sj20.mean():.3f} p50={np.median(sj20):.3f}\")\n",
    "print(f\"[SOFT-JACCARD] k=50 tau={tau50:.6f}: mean={sj50.mean():.3f} p50={np.median(sj50):.3f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ov_10 = knn_overlap_frac(idx_gt_10, idx_pr_10).mean()\n",
    "jc_10 = knn_jaccard(idx_gt_10, idx_pr_10).mean()\n",
    "\n",
    "print(f\"\\n  Global Metrics:\")\n",
    "print(f\"    EDM Pearson:      {pearson_corr:.4f}\")\n",
    "print(f\"    EDM Spearman:     {spearman_corr:.4f}\")\n",
    "\n",
    "print(f\"\\n  Local Metrics (Hard):\")\n",
    "print(f\"    kNN@10 overlap:   {ov_10:.4f}\")\n",
    "print(f\"    kNN@10 Jaccard:   {jc_10:.4f}\")\n",
    "print(f\"    Recall@10:        {recall_10:.4f}\")\n",
    "\n",
    "print(f\"\\n  Local Metrics (Tolerant):\")\n",
    "print(f\"    NearMiss@20:      {nearmiss_20:.4f}\")\n",
    "print(f\"    NearMiss@50:      {nearmiss_50:.4f}\")\n",
    "\n",
    "print(f\"\\n  Local Metrics (Stable):\")\n",
    "print(f\"    Local Spearman:   {np.nanmean(rho_local):.4f}\")\n",
    "print(f\"    Soft Jaccard@20:  {sj20.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "from core_models_et_p3 import GEMSModel, infer_anchor_train_from_checkpoint\n",
    "import utils_et as uet\n",
    "\n",
    "# ===================================================================\n",
    "# PATHS AND CONFIG\n",
    "# ===================================================================\n",
    "\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output_anchored\"\n",
    "output_dir_old = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA AND MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "adata_sc = sc.read_h5ad(f\"{output_dir_old}/scadata_with_gems_20260112_023604.h5ad\")\n",
    "\n",
    "if hasattr(adata_sc, 'raw') and adata_sc.raw is not None:\n",
    "    sc_expr = torch.tensor(adata_sc.raw.X.toarray() if hasattr(adata_sc.raw.X, 'toarray') else adata_sc.raw.X, dtype=torch.float32)\n",
    "else:\n",
    "    sc_expr = torch.tensor(adata_sc.X.toarray() if hasattr(adata_sc.X, 'toarray') else adata_sc.X, dtype=torch.float32)\n",
    "\n",
    "# ===================================================================\n",
    "# LOAD AND NORMALIZE GT COORDS\n",
    "# ===================================================================\n",
    "gt_coords_raw = adata_sc.obsm['spatial_gt']\n",
    "n_cells, n_genes = sc_expr.shape\n",
    "print(f\"✓ Loaded SC data: {n_cells} cells × {n_genes} genes\")\n",
    "print(f\"✓ Ground truth coords (raw): {gt_coords_raw.shape}\")\n",
    "\n",
    "gt_coords_tensor = torch.tensor(gt_coords_raw, dtype=torch.float32, device=device)\n",
    "slide_ids = torch.zeros(gt_coords_tensor.shape[0], dtype=torch.long, device=device)\n",
    "\n",
    "gt_coords_norm, gt_mu, gt_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    gt_coords_tensor, slide_ids\n",
    ")\n",
    "\n",
    "gt_coords_np = gt_coords_norm.cpu().numpy()\n",
    "\n",
    "print(f\"✓ GT coords normalized: scale={gt_scale[0].item():.4f}\")\n",
    "print(f\"✓ GT coords RMS: {gt_coords_norm.pow(2).mean().sqrt().item():.4f}\")\n",
    "print(f\"✓ GT coords range: X=[{gt_coords_np[:,0].min():.3f}, {gt_coords_np[:,0].max():.3f}], \"\n",
    "      f\"Y=[{gt_coords_np[:,1].min():.3f}, {gt_coords_np[:,1].max():.3f}]\")\n",
    "\n",
    "# ===================================================================\n",
    "# AUTO-DETECT ANCHOR MODE FROM CHECKPOINT BEFORE MODEL INIT\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AUTO-DETECTING ANCHOR MODE FROM CHECKPOINT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "\n",
    "# Detect anchor mode (base_h_dim=128 for [512,256,128] encoder)\n",
    "base_h_dim = 128  # Last dimension of n_embedding\n",
    "anchor_train_detected = infer_anchor_train_from_checkpoint(checkpoint, base_h_dim)\n",
    "\n",
    "print(f\"✓ Detected anchor_train={anchor_train_detected}\")\n",
    "\n",
    "# ===================================================================\n",
    "# INITIALIZE MODEL WITH CORRECT ANCHOR MODE\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INITIALIZING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=128,\n",
    "    dist_bins=24,\n",
    "    device=device,\n",
    "    anchor_train=anchor_train_detected,  # USE DETECTED VALUE!\n",
    ")\n",
    "\n",
    "print(f\"✓ Model initialized with anchor_train={model.anchor_train}\")\n",
    "print(f\"✓ context_encoder.input_dim={model.context_encoder.input_dim}\")\n",
    "\n",
    "# ===================================================================\n",
    "# LOAD CHECKPOINT WEIGHTS\n",
    "# ===================================================================\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "# Restore EDM parameters\n",
    "if 'sigma_data' in checkpoint:\n",
    "    model.sigma_data = checkpoint['sigma_data']\n",
    "if 'sigma_min' in checkpoint:\n",
    "    model.sigma_min = checkpoint['sigma_min']\n",
    "if 'sigma_max' in checkpoint:\n",
    "    model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.generator.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "print(f\"✓ Model loaded and set to eval mode\")\n",
    "print(f\"✓ sigma_data={getattr(model, 'sigma_data', 'N/A')}\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# COMPUTE CORAL TRANSFORMATION\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPUTING CORAL TRANSFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Load ST data (you need this to compute ST context distribution)\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "# Get common genes\n",
    "common = sorted(list(set(adata_sc.var_names) & set(stadata.var_names)))\n",
    "X_st = stadata[:, common].X\n",
    "if hasattr(X_st, \"toarray\"): \n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, _, _ = uet.canonicalize_st_coords_per_slide(st_coords_raw, slide_ids)\n",
    "\n",
    "print(f\"✓ Loaded ST data: {st_expr.shape[0]} spots × {st_expr.shape[1]} genes\")\n",
    "\n",
    "# 2. Prepare ST gene expression dict\n",
    "st_gene_expr_dict = {0: st_expr.cpu()}\n",
    "\n",
    "# 3. Load Stage B targets (needed for compute_coral_params_from_st)\n",
    "# This should be in your checkpoint or output directory\n",
    "targets_path = f\"{output_dir}/stageB_targets/targets_dict.pt\"\n",
    "if os.path.exists(targets_path):\n",
    "    model.targets_dict = torch.load(targets_path, map_location='cpu')\n",
    "    print(f\"✓ Loaded targets_dict from {targets_path}\")\n",
    "else:\n",
    "    print(f\"⚠️  Targets not found at {targets_path}\")\n",
    "    print(\"   Running Stage B precomputation...\")\n",
    "    slides_dict = {0: (st_coords, st_expr)}\n",
    "    model.train_stageB(slides=slides_dict, outdir=f\"{output_dir}/stageB_targets\")\n",
    "    print(\"✓ Stage B complete\")\n",
    "\n",
    "# 4. Compute CORAL parameters\n",
    "print(\"\\n--- Computing ST context distribution ---\")\n",
    "model.compute_coral_params_from_st(\n",
    "    st_gene_expr_dict=st_gene_expr_dict,\n",
    "    n_samples=2000,\n",
    "    n_min=96,\n",
    "    n_max=384,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Building CORAL transformation ---\")\n",
    "model.build_coral_transform(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_samples=2000,\n",
    "    n_min=96,\n",
    "    n_max=384,\n",
    "    shrink=0.01,\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "print(\"✓ CORAL transformation ready!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# RUN INFERENCE (NEW GLOBAL SCALE VERSION)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING INFERENCE (GLOBAL SCALE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ADD THIS after loading checkpoint and before calling inference\n",
    "if 'sigma_data' in checkpoint:\n",
    "    model.sigma_data = checkpoint['sigma_data']\n",
    "if 'sigma_min' in checkpoint:\n",
    "    model.sigma_min = checkpoint['sigma_min']\n",
    "if 'sigma_max' in checkpoint:\n",
    "    model.sigma_max = checkpoint['sigma_max']\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     results = model.infer_sc_patchwise(\n",
    "#         sc_gene_expr=sc_expr,\n",
    "#         n_timesteps_sample=600,\n",
    "#         return_coords=True,\n",
    "#         patch_size=192,\n",
    "#         coverage_per_cell=8.0,\n",
    "#         n_align_iters=15,\n",
    "#         eta=0.0,\n",
    "#         guidance_scale=2.0,\n",
    "#         gt_coords=gt_coords_norm,\n",
    "#         debug_knn=True,\n",
    "#         debug_max_patches=15,\n",
    "#         debug_k_list=(10, 20),\n",
    "#         pool_mult=2.0,\n",
    "#         stochastic_tau=0.8,\n",
    "#         tau_mode=\"adaptive_kth\",\n",
    "#         ensure_connected=True,\n",
    "#         local_refine=False,\n",
    "#         anchor_sampling_mode=\"edm_anchor_local\",  # NEW PARAMETER\n",
    "#         commit_frac=1.0,\n",
    "#         seq_align_dim=32,\n",
    "#     )\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = model.infer_sc_patchwise(\n",
    "        sc_gene_expr=sc_expr,\n",
    "        n_timesteps_sample=500,\n",
    "        return_coords=True,\n",
    "        patch_size=192,\n",
    "        coverage_per_cell=6.0,\n",
    "        n_align_iters=15,\n",
    "        eta=0.0,\n",
    "        guidance_scale=2.0,\n",
    "        gt_coords=gt_coords_norm,\n",
    "        debug_knn=True,\n",
    "        debug_max_patches=15,\n",
    "        debug_k_list=(10, 20),\n",
    "        pool_mult=2.0,\n",
    "        stochastic_tau=0.8,\n",
    "        tau_mode=\"adaptive_kth\",\n",
    "        ensure_connected=True,\n",
    "        local_refine=False,\n",
    "        # ========== NEW PARAMETERS ==========\n",
    "        inference_mode=\"anchored\",  # NEW: Use this instead of anchor_sampling_mode\n",
    "        anchor_sampling_mode=\"align_vote_only\",  # Still specify the specific anchored method\n",
    "        commit_frac=0.6,\n",
    "        seq_align_dim=2,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "D_edm_pred = results['D_edm'].cpu().numpy()\n",
    "coords_pred = results['coords_canon'].cpu().numpy()\n",
    "print(f\"\\n✓ Inference complete!\")\n",
    "print(f\"  Predicted EDM: {D_edm_pred.shape}\")\n",
    "print(f\"  Predicted coords: {coords_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SINGLE-PATCH INFERENCE EXPERIMENT (NO STITCHING)\n",
    "# Set-size sweep: 96, 192, 256, 384, 838\n",
    "# Metrics: kNN@10, kNN@20, Pearson, Spearman (local patch only)\n",
    "# ===================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SINGLE-PATCH QUALITY EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ===================================================================\n",
    "\n",
    "def compute_knn_overlap(coords_gt, coords_pred, k=10):\n",
    "    \"\"\"Compute k-NN overlap fraction.\"\"\"\n",
    "    n = coords_gt.shape[0]\n",
    "    \n",
    "    # GT kNN\n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_gt)\n",
    "    _, idx_gt = nbrs_gt.kneighbors(coords_gt)\n",
    "    idx_gt = idx_gt[:, 1:]  # Exclude self\n",
    "    \n",
    "    # Pred kNN\n",
    "    nbrs_pred = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_pred)\n",
    "    _, idx_pred = nbrs_pred.kneighbors(coords_pred)\n",
    "    idx_pred = idx_pred[:, 1:]  # Exclude self\n",
    "    \n",
    "    # Compute overlap per cell\n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        gt_set = set(idx_gt[i])\n",
    "        pred_set = set(idx_pred[i])\n",
    "        overlap = len(gt_set & pred_set) / k\n",
    "        overlaps.append(overlap)\n",
    "    \n",
    "    return np.mean(overlaps)\n",
    "\n",
    "\n",
    "def compute_edm_correlations(coords_gt, coords_pred):\n",
    "    \"\"\"Compute Pearson and Spearman correlations of EDM distances.\"\"\"\n",
    "    # Compute EDMs\n",
    "    edm_gt = squareform(pdist(coords_gt, 'euclidean'))\n",
    "    edm_pred = squareform(pdist(coords_pred, 'euclidean'))\n",
    "    \n",
    "    # Extract upper triangle\n",
    "    n = coords_gt.shape[0]\n",
    "    triu_idx = np.triu_indices(n, k=1)\n",
    "    dist_gt = edm_gt[triu_idx]\n",
    "    dist_pred = edm_pred[triu_idx]\n",
    "    \n",
    "    # Scale alignment (median normalization)\n",
    "    scale = np.median(dist_gt) / np.median(dist_pred)\n",
    "    dist_pred_scaled = dist_pred * scale\n",
    "    \n",
    "    # Correlations\n",
    "    pearson_corr, _ = pearsonr(dist_gt, dist_pred_scaled)\n",
    "    spearman_corr, _ = spearmanr(dist_gt, dist_pred_scaled)\n",
    "    \n",
    "    return pearson_corr, spearman_corr\n",
    "\n",
    "\n",
    "def run_single_patch_inference(model, sc_expr, gt_coords_norm, subset_idx, n_timesteps=600, guidance_scale=2.0):\n",
    "    \"\"\"\n",
    "    Run inference on a single patch (subset of cells).\n",
    "    Returns predicted coordinates.\n",
    "    \"\"\"\n",
    "    # Extract subset\n",
    "    sc_expr_subset = sc_expr[subset_idx]\n",
    "    gt_coords_subset = gt_coords_norm[subset_idx]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode genes\n",
    "        H_latent = model.encoder(sc_expr_subset.to(device))  # (m, D_latent)\n",
    "        \n",
    "        # Create mask (all cells in patch)\n",
    "        m = sc_expr_subset.shape[0]\n",
    "        mask = torch.ones(m, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Build context (apply CORAL if available)\n",
    "        Z = H_latent\n",
    "        if hasattr(model, 'coral_params') and model.coral_params is not None:\n",
    "            H_ctx = GEMSModel.apply_coral_transform(\n",
    "                H_latent.unsqueeze(0),  # (1, m, D_latent)\n",
    "                mu_sc=model.coral_params['mu_sc'],\n",
    "                A=model.coral_params['A'],\n",
    "                B=model.coral_params['B'],\n",
    "                mu_st=model.coral_params['mu_st']\n",
    "            ).squeeze(0)\n",
    "        else:\n",
    "            H_ctx = H_latent\n",
    "        \n",
    "        # Add anchor channel if needed\n",
    "        if model.anchor_train:\n",
    "            anchor_channel = torch.zeros(m, 1, device=device)\n",
    "            Z_ctx = torch.cat([H_ctx, anchor_channel], dim=-1)  # (m, D_latent+1)\n",
    "        else:\n",
    "            Z_ctx = H_ctx\n",
    "        \n",
    "        # Encode context\n",
    "        H_context = model.context_encoder(Z_ctx.unsqueeze(0), mask.unsqueeze(0))  # (1, m, c_dim)\n",
    "        \n",
    "        # Generate initial proposal\n",
    "        V_gen = model.generator(H_context, mask.unsqueeze(0)).squeeze(0)  # (m, D_latent)\n",
    "        \n",
    "        # EDM sampling\n",
    "        sigma_data = getattr(model, 'sigma_data', 1.0)\n",
    "        sigma_min = getattr(model, 'sigma_min', 0.02)\n",
    "        sigma_max = getattr(model, 'sigma_max', 3.0)\n",
    "        \n",
    "        import utils_et as uet\n",
    "        sigmas = uet.edm_sigma_schedule(n_timesteps, sigma_min, sigma_max, rho=7.0, device=device)\n",
    "        \n",
    "        # Initialize with noise\n",
    "        V_t = V_gen + sigmas[0] * torch.randn_like(V_gen)\n",
    "        \n",
    "        # Denoising loop (Euler method)\n",
    "        for i in range(len(sigmas) - 1):\n",
    "            sigma = sigmas[i]\n",
    "            sigma_next = sigmas[i + 1]\n",
    "            \n",
    "            # Score network prediction\n",
    "            x0_c = model.score_net.forward_edm(\n",
    "                V_t.unsqueeze(0), \n",
    "                sigma.view(1), \n",
    "                H_context, \n",
    "                mask.unsqueeze(0), \n",
    "                sigma_data\n",
    "            ).squeeze(0)\n",
    "            \n",
    "            # CFG if guidance > 1\n",
    "            if guidance_scale != 1.0:\n",
    "                H_null = torch.zeros_like(H_context)\n",
    "                x0_u = model.score_net.forward_edm(\n",
    "                    V_t.unsqueeze(0), \n",
    "                    sigma.view(1), \n",
    "                    H_null, \n",
    "                    mask.unsqueeze(0), \n",
    "                    sigma_data\n",
    "                ).squeeze(0)\n",
    "                \n",
    "                x0 = x0_u + guidance_scale * (x0_c - x0_u)\n",
    "            else:\n",
    "                x0 = x0_c\n",
    "            \n",
    "            # Euler step\n",
    "            d = (V_t - x0) / sigma.clamp_min(1e-8)\n",
    "            V_t = V_t + (sigma_next - sigma) * d\n",
    "        \n",
    "        # Final coordinates\n",
    "        coords_pred = V_t.cpu().numpy()\n",
    "        \n",
    "        # Center and PCA to 2D\n",
    "        coords_pred_centered = coords_pred - coords_pred.mean(axis=0, keepdims=True)\n",
    "        if coords_pred.shape[1] > 2:\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca = PCA(n_components=2)\n",
    "            coords_pred_2d = pca.fit_transform(coords_pred_centered)\n",
    "        else:\n",
    "            coords_pred_2d = coords_pred_centered\n",
    "    \n",
    "    return coords_pred_2d\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# EXPERIMENT: SWEEP OVER PATCH SIZES\n",
    "# ===================================================================\n",
    "\n",
    "patch_sizes = [96, 192, 256, 384, 838]\n",
    "n_repeats_per_size = {\n",
    "    96: 10,\n",
    "    192: 10,\n",
    "    256: 10,\n",
    "    384: 6,\n",
    "    838: 1  # Full dataset, run once\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for patch_size in patch_sizes:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PATCH SIZE: {patch_size}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    n_repeats = n_repeats_per_size[patch_size]\n",
    "    \n",
    "    metrics_this_size = {\n",
    "        'knn10': [],\n",
    "        'knn20': [],\n",
    "        'pearson': [],\n",
    "        'spearman': []\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_repeats):\n",
    "        # Sample random subset\n",
    "        if patch_size == 838:\n",
    "            subset_idx = np.arange(838)\n",
    "        else:\n",
    "            subset_idx = np.random.choice(838, size=patch_size, replace=False)\n",
    "        \n",
    "        # Get GT coords for subset\n",
    "        gt_coords_subset = gt_coords_np[subset_idx]\n",
    "        \n",
    "        # Run inference\n",
    "        print(f\"  Run {rep+1}/{n_repeats}: sampling {patch_size} cells...\", end=' ')\n",
    "        \n",
    "        coords_pred = run_single_patch_inference(\n",
    "            model=model,\n",
    "            sc_expr=sc_expr,\n",
    "            gt_coords_norm=gt_coords_norm,\n",
    "            subset_idx=subset_idx,\n",
    "            n_timesteps=500,\n",
    "            guidance_scale=2.0\n",
    "        )\n",
    "        \n",
    "        # Compute metrics\n",
    "        knn10 = compute_knn_overlap(gt_coords_subset, coords_pred, k=10)\n",
    "        knn20 = compute_knn_overlap(gt_coords_subset, coords_pred, k=20)\n",
    "        pearson, spearman = compute_edm_correlations(gt_coords_subset, coords_pred)\n",
    "        \n",
    "        metrics_this_size['knn10'].append(knn10)\n",
    "        metrics_this_size['knn20'].append(knn20)\n",
    "        metrics_this_size['pearson'].append(pearson)\n",
    "        metrics_this_size['spearman'].append(spearman)\n",
    "        \n",
    "        print(f\"kNN@10={knn10:.3f} kNN@20={knn20:.3f} Pearson={pearson:.3f} Spearman={spearman:.3f}\")\n",
    "    \n",
    "    # Average across repeats\n",
    "    results.append({\n",
    "        'patch_size': patch_size,\n",
    "        'n_repeats': n_repeats,\n",
    "        'knn10_mean': np.mean(metrics_this_size['knn10']),\n",
    "        'knn10_std': np.std(metrics_this_size['knn10']),\n",
    "        'knn20_mean': np.mean(metrics_this_size['knn20']),\n",
    "        'knn20_std': np.std(metrics_this_size['knn20']),\n",
    "        'pearson_mean': np.mean(metrics_this_size['pearson']),\n",
    "        'pearson_std': np.std(metrics_this_size['pearson']),\n",
    "        'spearman_mean': np.mean(metrics_this_size['spearman']),\n",
    "        'spearman_std': np.std(metrics_this_size['spearman']),\n",
    "    })\n",
    "\n",
    "# ===================================================================\n",
    "# PRINT SUMMARY TABLE\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: SINGLE-PATCH QUALITY vs PATCH SIZE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Export to CSV\n",
    "df_results.to_csv(f\"{output_dir}/single_patch_quality_sweep.csv\", index=False)\n",
    "print(f\"\\n✓ Results saved to {output_dir}/single_patch_quality_sweep.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# k-NN PRESERVATION ANALYSIS\n",
    "# ===================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def compute_knn_preservation(coords_gt, coords_pred, k=10):\n",
    "    \"\"\"\n",
    "    Compute k-nearest neighbor preservation.\n",
    "    Returns average number of preserved neighbors per cell.\n",
    "    \"\"\"\n",
    "    n = coords_gt.shape[0]\n",
    "\n",
    "    # Build k-NN for ground truth\n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_gt)\n",
    "    _, indices_gt = nbrs_gt.kneighbors(coords_gt)\n",
    "    indices_gt = indices_gt[:, 1:]  # Remove self\n",
    "\n",
    "    # Build k-NN for predicted\n",
    "    nbrs_pred = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords_pred)\n",
    "    _, indices_pred = nbrs_pred.kneighbors(coords_pred)\n",
    "    indices_pred = indices_pred[:, 1:]  # Remove self\n",
    "\n",
    "    # Compute overlap for each cell\n",
    "    overlaps = []\n",
    "    for i in range(n):\n",
    "        gt_neighbors = set(indices_gt[i])\n",
    "        pred_neighbors = set(indices_pred[i])\n",
    "        overlap = len(gt_neighbors.intersection(pred_neighbors))\n",
    "        overlaps.append(overlap)\n",
    "\n",
    "    return np.mean(overlaps), overlaps\n",
    "\n",
    "#Compute for k=10 and k=20\n",
    "\n",
    "knn_k10, overlaps_k10 = compute_knn_preservation(gt_coords_np, coords_pred, k=10)\n",
    "knn_k20, overlaps_k20 = compute_knn_preservation(gt_coords_np, coords_pred, k=20)\n",
    "\n",
    "print(f\"\\nk-NN Preservation Results:\")\n",
    "print(f\" k=10: {knn_k10:.2f} / 10 ({knn_k10/10*100:.1f}% neighbors preserved)\")\n",
    "print(f\" k=20: {knn_k20:.2f} / 20 ({knn_k20/20*100:.1f}% neighbors preserved)\")\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\" EDM Pearson: {pearson_corr:.4f} (global distances)\")\n",
    "print(f\" EDM Spearman: {spearman_corr:.4f} (global distances)\")\n",
    "print(f\" k-NN@10: {knn_k10/10:.4f} (local neighborhoods)\")\n",
    "print(f\" k-NN@20: {knn_k20/20:.4f} (local neighborhoods)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if knn_k10/10 < 0.3:\n",
    "    print(\"\\n⚠️ SEVERE local scrambling detected!\")\n",
    "    print(\" < 30% of neighbors preserved - local structure heavily disrupted\")\n",
    "elif knn_k10/10 < 0.5:\n",
    "    print(\"\\n⚠️ MODERATE local scrambling detected\")\n",
    "    print(\" 30-50% of neighbors preserved - significant local disruption\")\n",
    "elif knn_k10/10 < 0.7:\n",
    "    print(\"\\n✓ MILD local scrambling\")\n",
    "    print(\" 50-70% of neighbors preserved - some local structure retained\")\n",
    "else:\n",
    "    print(\"\\n✓✓ GOOD local preservation\")\n",
    "    print(\" > 70% of neighbors preserved - local structure mostly intact\")\n",
    "\n",
    "if pearson_corr > 0.6 and knn_k10/10 < 0.4:\n",
    "    print(\"\\n🔍 DIAGNOSIS: High EDM correlation but low k-NN preservation\")\n",
    "    print(\" → Global geometry preserved, but specific neighbor relationships lost\")\n",
    "    print(\" → Consistent with context-dependent coordinate generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# k-NN PRESERVATION ANALYSIS (ENHANCED)\n",
    "# ===================================================================\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION ANALYSIS (ENHANCED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# --- Helper functions ---\n",
    "\n",
    "def knn_sets(coords, k):\n",
    "    \"\"\"Get k-NN indices for all points.\"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords)\n",
    "    _, idx = nbrs.kneighbors(coords)\n",
    "    return idx[:, 1:]  # (N, k) - remove self\n",
    "\n",
    "def knn_overlap_frac(idx_gt, idx_pr):\n",
    "    \"\"\"Compute per-point overlap fraction: |intersection| / k\"\"\"\n",
    "    n, k = idx_gt.shape\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        out[i] = len(set(idx_gt[i]).intersection(idx_pr[i])) / k\n",
    "    return out  # (N,)\n",
    "\n",
    "def knn_jaccard(idx_gt, idx_pr):\n",
    "    \"\"\"Compute per-point Jaccard: |intersection| / |union|\"\"\"\n",
    "    n, k = idx_gt.shape\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        a = set(idx_gt[i])\n",
    "        b = set(idx_pr[i])\n",
    "        out[i] = len(a & b) / max(1, len(a | b))\n",
    "    return out  # (N,)\n",
    "\n",
    "def kth_neighbor_radius(coords, k):\n",
    "    \"\"\"Get distance to k-th neighbor for all points.\"\"\"\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(coords)\n",
    "    d, _ = nbrs.kneighbors(coords)\n",
    "    return d[:, k]  # distance to k-th neighbor\n",
    "\n",
    "def local_spearman_within_radius(gt_coords, pr_coords, R, min_n=10, fallback_k=30):\n",
    "    \"\"\"Compute per-point local Spearman correlation within radius R.\"\"\"\n",
    "    nbrs_gt = NearestNeighbors(radius=R, algorithm='ball_tree').fit(gt_coords)\n",
    "    ind = nbrs_gt.radius_neighbors(gt_coords, return_distance=False)\n",
    "\n",
    "    # For fallback when radius gives too few neighbors\n",
    "    nbrs_gt_k = NearestNeighbors(n_neighbors=fallback_k+1, algorithm='ball_tree').fit(gt_coords)\n",
    "    _, idx_k = nbrs_gt_k.kneighbors(gt_coords)\n",
    "    idx_k = idx_k[:, 1:]\n",
    "\n",
    "    n = gt_coords.shape[0]\n",
    "    vals = np.full(n, np.nan, dtype=np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        neigh = ind[i]\n",
    "        neigh = neigh[neigh != i]\n",
    "        if neigh.shape[0] < min_n:\n",
    "            neigh = idx_k[i]  # fallback to kNN set\n",
    "\n",
    "        d_gt = np.linalg.norm(gt_coords[neigh] - gt_coords[i], axis=1)\n",
    "        d_pr = np.linalg.norm(pr_coords[neigh] - pr_coords[i], axis=1)\n",
    "\n",
    "        # handle degenerate cases\n",
    "        if np.std(d_gt) < 1e-12 or np.std(d_pr) < 1e-12:\n",
    "            continue\n",
    "\n",
    "        vals[i] = spearmanr(d_gt, d_pr).correlation\n",
    "\n",
    "    return vals\n",
    "\n",
    "def soft_weighted_jaccard(gt_coords, pr_coords, k=20, tau=None):\n",
    "    \"\"\"Compute soft distance-weighted Jaccard (gives partial credit for near-misses).\"\"\"\n",
    "    nbrs_gt = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(gt_coords)\n",
    "    d_gt, idx_gt = nbrs_gt.kneighbors(gt_coords)\n",
    "    d_gt, idx_gt = d_gt[:, 1:], idx_gt[:, 1:]\n",
    "\n",
    "    nbrs_pr = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(pr_coords)\n",
    "    d_pr, idx_pr = nbrs_pr.kneighbors(pr_coords)\n",
    "    d_pr, idx_pr = d_pr[:, 1:], idx_pr[:, 1:]\n",
    "\n",
    "    # tau: data-driven default = median GT d(k)\n",
    "    if tau is None:\n",
    "        tau = np.median(d_gt[:, -1]) + 1e-12\n",
    "\n",
    "    n = gt_coords.shape[0]\n",
    "    out = np.empty(n, dtype=np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        wgt = {int(j): np.exp(-float(d)/tau) for j, d in zip(idx_gt[i], d_gt[i])}\n",
    "        wpr = {int(j): np.exp(-float(d)/tau) for j, d in zip(idx_pr[i], d_pr[i])}\n",
    "\n",
    "        keys = set(wgt.keys()) | set(wpr.keys())\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        for j in keys:\n",
    "            a = wgt.get(j, 0.0)\n",
    "            b = wpr.get(j, 0.0)\n",
    "            num += min(a, b)\n",
    "            den += max(a, b)\n",
    "        out[i] = num / max(1e-12, den)\n",
    "\n",
    "    return out, tau\n",
    "\n",
    "# ===================================================================\n",
    "# 1) HARD kNN OVERLAP + JACCARD (k=10, 20, 50)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Hard k-NN Metrics ---\")\n",
    "\n",
    "idx_gt_10 = knn_sets(gt_coords_np, 10)\n",
    "idx_pr_10 = knn_sets(coords_pred, 10)\n",
    "idx_gt_20 = knn_sets(gt_coords_np, 20)\n",
    "idx_pr_20 = knn_sets(coords_pred, 20)\n",
    "idx_gt_50 = knn_sets(gt_coords_np, 50)\n",
    "idx_pr_50 = knn_sets(coords_pred, 50)\n",
    "\n",
    "for k, ig, ip in [(10, idx_gt_10, idx_pr_10), (20, idx_gt_20, idx_pr_20), (50, idx_gt_50, idx_pr_50)]:\n",
    "    ov = knn_overlap_frac(ig, ip)\n",
    "    jc = knn_jaccard(ig, ip)\n",
    "    print(f\"[KNN] k={k:2d}: overlap mean={ov.mean():.3f} p50={np.median(ov):.3f} | \"\n",
    "          f\"jaccard mean={jc.mean():.3f} p50={np.median(jc):.3f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2) LOCAL SPEARMAN (distance ordering within local neighborhood)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Local Spearman Correlation ---\")\n",
    "\n",
    "r20 = kth_neighbor_radius(gt_coords_np, 20)\n",
    "R = np.median(r20)\n",
    "print(f\"[LOCAL-RADIUS] R = median GT d(20) = {R:.6f}\")\n",
    "\n",
    "rho_local = local_spearman_within_radius(gt_coords_np, coords_pred, R)\n",
    "good = np.isfinite(rho_local)\n",
    "print(f\"[LOCAL-SPEARMAN] finite_frac={good.mean():.2%} \"\n",
    "      f\"mean={np.nanmean(rho_local):.3f} p50={np.nanmedian(rho_local):.3f} \"\n",
    "      f\"p10={np.nanpercentile(rho_local,10):.3f} p90={np.nanpercentile(rho_local,90):.3f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 3) SOFT WEIGHTED JACCARD (partial credit for near-misses)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Soft Weighted Jaccard ---\")\n",
    "\n",
    "sj20, tau20 = soft_weighted_jaccard(gt_coords_np, coords_pred, k=20, tau=None)\n",
    "sj50, tau50 = soft_weighted_jaccard(gt_coords_np, coords_pred, k=50, tau=None)\n",
    "print(f\"[SOFT-JACCARD] k=20 tau={tau20:.6f}: mean={sj20.mean():.3f} p50={np.median(sj20):.3f}\")\n",
    "print(f\"[SOFT-JACCARD] k=50 tau={tau50:.6f}: mean={sj50.mean():.3f} p50={np.median(sj50):.3f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ov_10 = knn_overlap_frac(idx_gt_10, idx_pr_10).mean()\n",
    "jc_10 = knn_jaccard(idx_gt_10, idx_pr_10).mean()\n",
    "\n",
    "print(f\"\\n  Global Metrics:\")\n",
    "print(f\"    EDM Pearson:      {pearson_corr:.4f}\")\n",
    "print(f\"    EDM Spearman:     {spearman_corr:.4f}\")\n",
    "\n",
    "print(f\"\\n  Local Metrics (Hard):\")\n",
    "print(f\"    kNN@10 overlap:   {ov_10:.4f}\")\n",
    "print(f\"    kNN@10 Jaccard:   {jc_10:.4f}\")\n",
    "\n",
    "print(f\"\\n  Local Metrics (Stable):\")\n",
    "print(f\"    Local Spearman:   {np.nanmean(rho_local):.4f}\")\n",
    "print(f\"    Soft Jaccard@20:  {sj20.mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# INTERPRETATION\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Diagnosis based on multiple metrics\n",
    "if ov_10 < 0.3:\n",
    "    print(\"\\n⚠️ SEVERE local scrambling (kNN@10 overlap < 30%)\")\n",
    "elif ov_10 < 0.5:\n",
    "    print(\"\\n⚠️ MODERATE local scrambling (kNN@10 overlap 30-50%)\")\n",
    "elif ov_10 < 0.7:\n",
    "    print(\"\\n✓ MILD local scrambling (kNN@10 overlap 50-70%)\")\n",
    "else:\n",
    "    print(\"\\n✓✓ GOOD local preservation (kNN@10 overlap > 70%)\")\n",
    "\n",
    "# Check for near-ties scenario\n",
    "if ov_10 < 0.4 and sj20.mean() > ov_10 + 0.15:\n",
    "    print(\"\\n🔍 NEAR-TIES DETECTED:\")\n",
    "    print(f\"   Hard kNN@10 is low ({ov_10:.3f}) but Soft Jaccard is higher ({sj20.mean():.3f})\")\n",
    "    print(\"   → Neighbors are 'almost' correct but exact ranks unstable (GT has near-ties)\")\n",
    "\n",
    "# Check for local Spearman vs global\n",
    "if spearman_corr > 0.6 and np.nanmean(rho_local) < 0.4:\n",
    "    print(\"\\n🔍 LOCAL vs GLOBAL MISMATCH:\")\n",
    "    print(f\"   Global EDM Spearman is decent ({spearman_corr:.3f})\")\n",
    "    print(f\"   But Local Spearman is poor ({np.nanmean(rho_local):.3f})\")\n",
    "    print(\"   → Macro geometry OK, but fine local ordering disrupted\")\n",
    "\n",
    "# Check if kNN@50 >> kNN@10\n",
    "ov_50 = knn_overlap_frac(idx_gt_50, idx_pr_50).mean()\n",
    "if ov_50 > ov_10 + 0.2:\n",
    "    print(\"\\n🔍 kNN SCALE EFFECT:\")\n",
    "    print(f\"   kNN@10 overlap: {ov_10:.3f}\")\n",
    "    print(f\"   kNN@50 overlap: {ov_50:.3f}\")\n",
    "    print(\"   → Larger neighborhoods preserved better (consistent with near-ties at small k)\")\n",
    "\n",
    "if pearson_corr > 0.6 and ov_10 < 0.4:\n",
    "    print(\"\\n🔍 CLASSIC DIAGNOSIS:\")\n",
    "    print(\"   High EDM correlation + low kNN preservation\")\n",
    "    print(\"   → Global geometry preserved, specific neighbor relationships lost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_edm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# k-NN PRESERVATION FROM EDM (HIGH-DIMENSIONAL)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"k-NN PRESERVATION FROM EDM (NO 2D PROJECTION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Compute ground truth EDM\n",
    "D_edm_gt = squareform(pdist(gt_coords_np, metric='euclidean'))\n",
    "print(f\"✓ Ground truth EDM computed: {D_edm_gt.shape}\")\n",
    "\n",
    "# 2. k-NN preservation function using precomputed distances\n",
    "def knn_acc_from_dist(D_pred, D_gt, k=10):\n",
    "    \"\"\"\n",
    "    Compute k-NN preservation from distance matrices directly.\n",
    "    No 2D projection involved - uses full distance information.\n",
    "    \"\"\"\n",
    "    n = D_pred.shape[0]\n",
    "    \n",
    "    # Get k nearest neighbors from predicted distances\n",
    "    nn_pred_idx = np.argsort(D_pred, axis=1)[:, 1:k+1]  # skip self (index 0)\n",
    "    \n",
    "    # Get k nearest neighbors from ground truth distances\n",
    "    nn_gt_idx = np.argsort(D_gt, axis=1)[:, 1:k+1]\n",
    "    \n",
    "    # Compute overlap fraction for each cell\n",
    "    overlaps = np.array([\n",
    "        len(set(nn_pred_idx[i]) & set(nn_gt_idx[i])) / k \n",
    "        for i in range(n)\n",
    "    ])\n",
    "    \n",
    "    return overlaps.mean(), overlaps\n",
    "\n",
    "# 3. Compute k-NN from EDM\n",
    "knn_edm_k10, overlaps_edm_k10 = knn_acc_from_dist(D_edm_pred, D_edm_gt, k=10)\n",
    "knn_edm_k20, overlaps_edm_k20 = knn_acc_from_dist(D_edm_pred, D_edm_gt, k=20)\n",
    "\n",
    "print(f\"\\nk-NN Preservation from EDM (no projection):\")\n",
    "print(f\"  k=10: {knn_edm_k10*10:.2f} / 10  ({knn_edm_k10*100:.1f}% neighbors preserved)\")\n",
    "print(f\"  k=20: {knn_edm_k20*20:.2f} / 20  ({knn_edm_k20*100:.1f}% neighbors preserved)\")\n",
    "\n",
    "print(f\"\\n📊 COMPARISON: EDM-based vs 2D-coordinate-based k-NN:\")\n",
    "print(f\"  Method                  k=10        k=20\")\n",
    "print(f\"  {'─'*50}\")\n",
    "print(f\"  From 2D coords:         {knn_k10/10:.3f}      {knn_k20/20:.3f}\")\n",
    "print(f\"  From EDM (no proj):     {knn_edm_k10:.3f}      {knn_edm_k20:.3f}\")\n",
    "print(f\"  Difference:             {knn_edm_k10 - knn_k10/10:+.3f}      {knn_edm_k20 - knn_k20/20:+.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EDM vs 2D DIAGNOSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "improvement = knn_edm_k10 - knn_k10/10\n",
    "\n",
    "if improvement > 0.1:\n",
    "    print(\"\\n✓✓ EDM k-NN is MUCH better than 2D k-NN\")\n",
    "    print(\"   → 2D MDS projection is destroying local structure\")\n",
    "    print(\"   → Your model IS learning good neighborhoods\")\n",
    "    print(\"   → Problem is the visualization/embedding step, not the model\")\n",
    "elif improvement > 0.03:\n",
    "    print(\"\\n✓ EDM k-NN is somewhat better than 2D k-NN\")\n",
    "    print(\"   → Some information loss in 2D projection\")\n",
    "    print(\"   → Model performance better than 2D metrics suggest\")\n",
    "elif improvement > -0.03:\n",
    "    print(\"\\n⚠️ EDM and 2D k-NN are similar\")\n",
    "    print(\"   → 2D projection is reasonably faithful\")\n",
    "    print(\"   → Local scrambling issue is real, not projection artifact\")\n",
    "else:\n",
    "    print(\"\\n❌ EDM k-NN is WORSE than 2D k-NN\")\n",
    "    print(\"   → This shouldn't happen (2D can't add information)\")\n",
    "    print(\"   → Check for bugs in distance matrix computation\")\n",
    "\n",
    "# 4. Optional: Distribution of per-cell overlaps\n",
    "print(f\"\\nPer-cell k-NN@10 distribution (from EDM):\")\n",
    "print(f\"  Min:     {overlaps_edm_k10.min():.3f}\")\n",
    "print(f\"  25th %:  {np.percentile(overlaps_edm_k10, 25):.3f}\")\n",
    "print(f\"  Median:  {np.median(overlaps_edm_k10):.3f}\")\n",
    "print(f\"  75th %:  {np.percentile(overlaps_edm_k10, 75):.3f}\")\n",
    "print(f\"  Max:     {overlaps_edm_k10.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# LOCAL-ONLY DISTANCE CORRELATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOCAL-ONLY DISTANCE CORRELATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def local_pair_corr(D_pred, D_gt, k=20):\n",
    "    \"\"\"\n",
    "    Compute Pearson/Spearman only on k-NN edges from ground truth.\n",
    "    This isolates local distance preservation from global structure.\n",
    "    \"\"\"\n",
    "    N = D_gt.shape[0]\n",
    "    \n",
    "    # Get k nearest neighbors from GT distances\n",
    "    nn_gt_idx = np.argsort(D_gt, axis=1)[:, 1:k+1]  # exclude self\n",
    "    \n",
    "    # Extract (i,j) pairs for all k-NN edges\n",
    "    ii = np.repeat(np.arange(N), k)\n",
    "    jj = nn_gt_idx.reshape(-1)\n",
    "    \n",
    "    # Get predicted and GT distances for these pairs only\n",
    "    x_pred = D_pred[ii, jj]\n",
    "    y_gt = D_gt[ii, jj]\n",
    "    \n",
    "    return pearsonr(x_pred, y_gt)[0], spearmanr(x_pred, y_gt)[0]\n",
    "\n",
    "def quantile_corr(D_pred, D_gt, q=0.05):\n",
    "    \"\"\"\n",
    "    Compute correlation only on shortest q% of distances.\n",
    "    Another way to isolate local structure.\n",
    "    \"\"\"\n",
    "    N = D_gt.shape[0]\n",
    "    \n",
    "    # Get upper triangle (unique pairs)\n",
    "    tri = np.triu_indices(N, k=1)\n",
    "    x_pred = D_pred[tri]\n",
    "    y_gt = D_gt[tri]\n",
    "    \n",
    "    # Keep only shortest q% by GT distance\n",
    "    threshold = np.quantile(y_gt, q)\n",
    "    mask = y_gt <= threshold\n",
    "    \n",
    "    return pearsonr(x_pred[mask], y_gt[mask])[0], spearmanr(x_pred[mask], y_gt[mask])[0]\n",
    "\n",
    "# Compute local correlations\n",
    "print(\"\\n1. k-NN Edge Correlations (local neighborhoods only):\")\n",
    "local_p_k10, local_s_k10 = local_pair_corr(D_edm_pred, D_edm_gt, k=10)\n",
    "local_p_k20, local_s_k20 = local_pair_corr(D_edm_pred, D_edm_gt, k=20)\n",
    "\n",
    "print(f\"  k=10 edges: Pearson={local_p_k10:.4f}, Spearman={local_s_k10:.4f}\")\n",
    "print(f\"  k=20 edges: Pearson={local_p_k20:.4f}, Spearman={local_s_k20:.4f}\")\n",
    "\n",
    "# Compute quantile correlations\n",
    "print(\"\\n2. Shortest Distance Quantile Correlations:\")\n",
    "q5_p, q5_s = quantile_corr(D_edm_pred, D_edm_gt, q=0.05)\n",
    "q10_p, q10_s = quantile_corr(D_edm_pred, D_edm_gt, q=0.10)\n",
    "\n",
    "print(f\"  Shortest 5%:  Pearson={q5_p:.4f}, Spearman={q5_s:.4f}\")\n",
    "print(f\"  Shortest 10%: Pearson={q10_p:.4f}, Spearman={q10_s:.4f}\")\n",
    "\n",
    "# Compare with global correlations\n",
    "print(f\"\\n📊 GLOBAL vs LOCAL COMPARISON:\")\n",
    "print(f\"  Metric                          Pearson    Spearman\")\n",
    "print(f\"  {'─'*60}\")\n",
    "print(f\"  Global (all pairs):             {pearson_corr:.4f}     {spearman_corr:.4f}\")\n",
    "print(f\"  Local k=20 edges only:          {local_p_k20:.4f}     {local_s_k20:.4f}\")\n",
    "print(f\"  Shortest 5% distances only:     {q5_p:.4f}     {q5_s:.4f}\")\n",
    "print(f\"\\n  Δ (Local k20 - Global):         {local_p_k20 - pearson_corr:+.4f}     {local_s_k20 - spearman_corr:+.4f}\")\n",
    "print(f\"  Δ (Shortest 5% - Global):       {q5_p - pearson_corr:+.4f}     {q5_s - spearman_corr:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOCAL vs GLOBAL DIAGNOSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "local_boost = local_p_k20 - pearson_corr\n",
    "\n",
    "if local_boost > 0.1:\n",
    "    print(\"\\n✓✓ LOCAL correlations are MUCH better than global\")\n",
    "    print(\"   → Model is learning good local structure\")\n",
    "    print(\"   → Global metric is dominated by long-range pairs\")\n",
    "    print(\"   → Guidance IS working on neighborhoods\")\n",
    "elif local_boost > 0.03:\n",
    "    print(\"\\n✓ LOCAL correlations are moderately better than global\")\n",
    "    print(\"   → Some local structure learned\")\n",
    "    print(\"   → Global metric partially masking local improvements\")\n",
    "elif local_boost > -0.03:\n",
    "    print(\"\\n⚠️ LOCAL and GLOBAL correlations are similar\")\n",
    "    print(\"   → Model treats all scales equally\")\n",
    "    print(\"   → No specific local structure emphasis\")\n",
    "else:\n",
    "    print(\"\\n❌ LOCAL correlations are WORSE than global\")\n",
    "    print(\"   → Model better at long-range than short-range\")\n",
    "    print(\"   → Local scrambling confirmed at distance level\")\n",
    "\n",
    "# Actionable insight\n",
    "if local_boost > 0.05 and knn_edm_k10 < 0.4:\n",
    "    print(\"\\n🔍 INSIGHT: Good local DISTANCES but poor k-NN preservation\")\n",
    "    print(\"   → Distances are right but WHICH neighbors is wrong\")\n",
    "    print(\"   → Consider: triplet losses, contrastive neighbor losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PROCRUSTES ALIGNMENT + CELL TYPE VISUALIZATION (WITH HEATMAPS)\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCRUSTES ALIGNMENT + CELL TYPE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def canonicalize_unit_rms(X):\n",
    "    \"\"\"Center and scale to unit RMS\"\"\"\n",
    "    X_centered = X - X.mean(axis=0, keepdims=True)\n",
    "    rms = np.sqrt((X_centered ** 2).sum() / X_centered.size)\n",
    "    return X_centered / rms\n",
    "\n",
    "def procrustes_align(X, Y):\n",
    "    \"\"\"Align X to Y using Procrustes (allows rotation + reflection)\"\"\"\n",
    "    X_mean = X.mean(axis=0, keepdims=True)\n",
    "    Y_mean = Y.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    X_centered = X - X_mean\n",
    "    Y_centered = Y - Y_mean\n",
    "    \n",
    "    H = X_centered.T @ Y_centered\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = Vt.T @ U.T\n",
    "    \n",
    "    X_aligned = X_centered @ R + Y_mean\n",
    "    return X_aligned, R\n",
    "\n",
    "# Canonicalize both to unit RMS\n",
    "gt_coords_canon = canonicalize_unit_rms(gt_coords_np)\n",
    "coords_pred_canon = canonicalize_unit_rms(coords_pred)\n",
    "\n",
    "print(f\"\\n✓ Canonicalized coordinates to unit RMS\")\n",
    "print(f\"  GT RMS: {np.sqrt((gt_coords_canon ** 2).sum() / gt_coords_canon.size):.6f}\")\n",
    "print(f\"  Pred RMS: {np.sqrt((coords_pred_canon ** 2).sum() / coords_pred_canon.size):.6f}\")\n",
    "\n",
    "# Procrustes alignment\n",
    "coords_pred_aligned, R = procrustes_align(coords_pred_canon, gt_coords_canon)\n",
    "det_R = np.linalg.det(R)\n",
    "alignment_error = np.linalg.norm(gt_coords_canon - coords_pred_aligned, 'fro')\n",
    "\n",
    "print(f\"\\n✓ Procrustes alignment complete\")\n",
    "print(f\"  det(R) = {det_R:.4f} {'(reflection)' if det_R < 0 else '(rotation)'}\")\n",
    "print(f\"  Frobenius error: {alignment_error:.4f}\")\n",
    "print(f\"  Per-cell RMSE: {alignment_error / np.sqrt(n_cells):.4f}\")\n",
    "\n",
    "# Compute distance matrices from aligned coordinates\n",
    "D_gt_aligned = squareform(pdist(gt_coords_canon, 'euclidean'))\n",
    "D_pred_aligned = squareform(pdist(coords_pred_aligned, 'euclidean'))\n",
    "\n",
    "print(f\"\\n✓ Computed distance matrices from aligned coords\")\n",
    "print(f\"  GT distance matrix: {D_gt_aligned.shape}\")\n",
    "print(f\"  Pred distance matrix: {D_pred_aligned.shape}\")\n",
    "\n",
    "# Get cell types\n",
    "print(\"\\nAvailable columns in adata_sc.obs:\")\n",
    "print(list(adata_sc.obs.columns))\n",
    "\n",
    "cell_type_col = None\n",
    "for col in ['cell_type', 'celltype', 'cluster', 'annotation', 'cell_ontology_class', 'celltype_mapped_refined']:\n",
    "    if col in adata_sc.obs.columns:\n",
    "        cell_type_col = col\n",
    "        break\n",
    "\n",
    "if cell_type_col is None:\n",
    "    categorical_cols = adata_sc.obs.select_dtypes(include=['category', 'object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        cell_type_col = categorical_cols[0]\n",
    "    else:\n",
    "        adata_sc.obs['cell_type'] = 'Unknown'\n",
    "        cell_type_col = 'cell_type'\n",
    "\n",
    "print(f\"\\nUsing cell type column: '{cell_type_col}'\")\n",
    "cell_types = adata_sc.obs[cell_type_col].values\n",
    "\n",
    "unique_types = np.unique(cell_types)\n",
    "n_types = len(unique_types)\n",
    "\n",
    "print(f\"Found {n_types} unique cell types:\")\n",
    "for i, ct in enumerate(unique_types):\n",
    "    count = (cell_types == ct).sum()\n",
    "    print(f\"  {i+1}. {ct}: {count} cells\")\n",
    "\n",
    "# Colormap\n",
    "if n_types <= 10:\n",
    "    cmap = plt.cm.tab10\n",
    "elif n_types <= 20:\n",
    "    cmap = plt.cm.tab20\n",
    "else:\n",
    "    cmap = plt.cm.gist_ncar\n",
    "\n",
    "type_to_color = {ct: cmap(i / n_types) for i, ct in enumerate(unique_types)}\n",
    "\n",
    "# PLOT 1: Side-by-side cell type visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Ground Truth\n",
    "ax = axes[0]\n",
    "for ct in unique_types:\n",
    "    mask = cell_types == ct\n",
    "    ax.scatter(gt_coords_canon[mask, 0], gt_coords_canon[mask, 1], \n",
    "              c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Ground Truth - Cell Types', fontsize=14, fontweight='bold')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Predicted (Procrustes Aligned)\n",
    "ax = axes[1]\n",
    "for ct in unique_types:\n",
    "    mask = cell_types == ct\n",
    "    ax.scatter(coords_pred_aligned[mask, 0], coords_pred_aligned[mask, 1], \n",
    "              c=[type_to_color[ct]], label=ct, s=15, alpha=0.7, edgecolors='none')\n",
    "\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'Predicted (Aligned) - Frobenius Error: {alignment_error:.2f}', fontsize=14, fontweight='bold')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "if n_types <= 15:\n",
    "    handles, labels = axes[1].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center left', bbox_to_anchor=(1.0, 0.5), \n",
    "              fontsize=10, title='Cell Type', title_fontsize=12, frameon=True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 1])\n",
    "celltype_path = os.path.join(output_dir, f'patchwise_celltype_aligned_{timestamp}.png')\n",
    "# plt.savefig(celltype_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\n✓ Saved cell type visualization: {celltype_path}\")\n",
    "plt.show()\n",
    "\n",
    "# PLOT 2: Distance matrix heatmaps\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISTANCE MATRIX HEATMAP COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sample cells for visualization\n",
    "sample_size = min(600, n_cells)\n",
    "sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "print(f\"\\nCreating distance matrix heatmaps with {sample_size} sampled cells...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "fig.suptitle('Distance Matrix Comparison (from aligned coordinates)', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Ground Truth Distance Matrix\n",
    "im1 = axes[0].imshow(D_gt_aligned[np.ix_(sample_indices, sample_indices)], \n",
    "                     cmap='viridis', aspect='auto')\n",
    "axes[0].set_title('Ground Truth Distance Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Cell Index (Sampled)', fontsize=12)\n",
    "axes[0].set_ylabel('Cell Index (Sampled)', fontsize=12)\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04, label='Euclidean Distance')\n",
    "\n",
    "# Predicted Distance Matrix\n",
    "im2 = axes[1].imshow(D_pred_aligned[np.ix_(sample_indices, sample_indices)], \n",
    "                     cmap='viridis', aspect='auto')\n",
    "axes[1].set_title('Predicted Distance Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Cell Index (Sampled)', fontsize=12)\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04, label='Euclidean Distance')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "distmat_path = os.path.join(output_dir, f'patchwise_distmat_heatmap_{timestamp}.png')\n",
    "# plt.savefig(distmat_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved distance matrix heatmap: {distmat_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: correlation on canonicalized distance matrices\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "D_gt_flat = squareform(pdist(gt_coords_canon, 'euclidean'))\n",
    "D_pred_flat = squareform(pdist(coords_pred_canon, 'euclidean'))\n",
    "triu_idx = np.triu_indices(len(D_gt_flat), k=1)\n",
    "pearson_r = pearsonr(D_gt_flat[triu_idx], D_pred_flat[triu_idx])[0]\n",
    "spearman_r = spearmanr(D_gt_flat[triu_idx], D_pred_flat[triu_idx])[0]\n",
    "print(f\"\\n  Distance correlation (canonicalized):\")\n",
    "print(f\"    Pearson:  {pearson_r:.4f}\")\n",
    "print(f\"    Spearman: {spearman_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load ST1 training coordinates\n",
    "st_meta = pd.read_csv('/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv', index_col=0)\n",
    "coords = st_meta[['coord_x', 'coord_y']].values\n",
    "\n",
    "# Compute geometry metrics\n",
    "coords_centered = coords - coords.mean(axis=0)\n",
    "cov = np.cov(coords_centered.T)\n",
    "eigvals = np.linalg.eigvalsh(cov)\n",
    "eigvals = np.sort(eigvals)[::-1]\n",
    "\n",
    "# Effective dimensionality (participation ratio)\n",
    "dim_eff = (eigvals.sum() ** 2) / (eigvals ** 2).sum()\n",
    "\n",
    "# Anisotropy ratio\n",
    "aniso = eigvals[0] / (eigvals[1] + 1e-8)\n",
    "\n",
    "print(f\"Mouse Brain ST1 Training Data:\")\n",
    "print(f\"  N cells: {coords.shape[0]}\")\n",
    "print(f\"  Effective dimensionality: {dim_eff:.4f}\")\n",
    "print(f\"  Anisotropy ratio: {aniso:.4f}\")\n",
    "print(f\"  Eigenvalues: {eigvals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EIGENVALUE ANISOTROPY ANALYSIS FOR GEMS INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GEMS INFERENCE: 2D GEOMETRY VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ANALYZE GEMS PREDICTED COORDINATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Analyzing GEMS Predicted Coordinates ===\\n\")\n",
    "\n",
    "gems_coords = coords_pred\n",
    "\n",
    "# Get GEMS coordinates (already loaded as gems_coords)\n",
    "coords_gems = gems_coords.numpy() if torch.is_tensor(gems_coords) else gems_coords\n",
    "\n",
    "# Compute anisotropy for GEMS\n",
    "X = coords_gems.astype(float)\n",
    "Xc = X - X.mean(axis=0, keepdims=True)\n",
    "\n",
    "cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "eigvals_gems, eigvecs_gems = np.linalg.eigh(cov)\n",
    "eigvals_gems = eigvals_gems[::-1]\n",
    "\n",
    "lam1_gems, lam2_gems = eigvals_gems\n",
    "ratio_gems = lam1_gems / (lam2_gems + 1e-12)\n",
    "\n",
    "print(f\"GEMS Predicted Coordinates ({coords_gems.shape[0]} cells):\")\n",
    "print(f\"  λ1 = {lam1_gems:.4f},  λ2 = {lam2_gems:.4f}\")\n",
    "print(f\"  λ1/λ2 = {ratio_gems:.2f}\")\n",
    "\n",
    "if ratio_gems < 5:\n",
    "    interpretation_gems = \"→ GENUINELY 2D ✓\"\n",
    "elif ratio_gems < 20:\n",
    "    interpretation_gems = \"→ Anisotropic but still 2D-ish\"\n",
    "else:\n",
    "    interpretation_gems = \"→ EFFECTIVELY 1D (very elongated) ✗\"\n",
    "\n",
    "print(f\"  {interpretation_gems}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ANALYZE ST MINISETS FOR COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== Analyzing ST Mini-Subsets for Comparison ===\\n\")\n",
    "\n",
    "ratios_st = []\n",
    "eigenvalues_st = []\n",
    "\n",
    "for i, data in enumerate(miniset_data):\n",
    "    D = data['D_edm']\n",
    "    \n",
    "    # Reconstruct coordinates from EDM using classical MDS\n",
    "    n = D.shape[0]\n",
    "    Jn = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = -0.5 * (Jn @ (D ** 2) @ Jn)\n",
    "    \n",
    "    eigvals_full, eigvecs_full = np.linalg.eigh(B)\n",
    "    eigvals_full = eigvals_full[::-1]\n",
    "    eigvecs_full = eigvecs_full[:, ::-1]\n",
    "    \n",
    "    coords_patch = eigvecs_full[:, :2] @ np.diag(np.sqrt(np.maximum(eigvals_full[:2], 0)))\n",
    "    \n",
    "    # Analyze 2D variance\n",
    "    X = coords_patch.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals_2d, _ = np.linalg.eigh(cov)\n",
    "    eigvals_2d = eigvals_2d[::-1]\n",
    "    \n",
    "    lam1, lam2 = eigvals_2d\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    \n",
    "    ratios_st.append(ratio)\n",
    "    eigenvalues_st.append((lam1, lam2))\n",
    "\n",
    "ratios_st = np.array(ratios_st)\n",
    "eigenvalues_st = np.array(eigenvalues_st)\n",
    "\n",
    "print(f\"ST Mini-Subsets Statistics:\")\n",
    "print(f\"  λ1/λ2 - Median: {np.median(ratios_st):.2f}\")\n",
    "print(f\"  λ1/λ2 - Mean:   {ratios_st.mean():.2f}\")\n",
    "print(f\"  λ1/λ2 - Range:  [{ratios_st.min():.2f}, {ratios_st.max():.2f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. COMPARISON VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Creating Comparison Visualizations ===\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Histogram comparison - ST vs GEMS\n",
    "ax = axes[0, 0]\n",
    "ax.hist(ratios_st, bins=30, alpha=0.6, edgecolor='black', color='steelblue', \n",
    "        label=f'ST Minisets (n={len(ratios_st)})')\n",
    "ax.axvline(ratio_gems, color='red', linestyle='--', linewidth=3, \n",
    "           label=f'GEMS: {ratio_gems:.2f}')\n",
    "ax.axvline(np.median(ratios_st), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'ST Median: {np.median(ratios_st):.2f}')\n",
    "ax.axvline(5, color='g', linestyle=':', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axvline(20, color='orange', linestyle=':', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_xlabel('λ1/λ2 (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Anisotropy Comparison: ST Minisets vs GEMS', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Eigenvalue scatter - ST minisets\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(eigenvalues_st[:, 1], eigenvalues_st[:, 0], \n",
    "                    c=ratios_st, cmap='viridis', alpha=0.7, s=80, \n",
    "                    edgecolors='black', linewidth=1, label='ST Minisets')\n",
    "\n",
    "# Add GEMS point\n",
    "ax.scatter(lam2_gems, lam1_gems, c='red', s=300, marker='*', \n",
    "          edgecolors='darkred', linewidth=2, label='GEMS', zorder=5)\n",
    "\n",
    "# Diagonal line\n",
    "min_val = min(eigenvalues_st[:, 1].min(), lam2_gems)\n",
    "max_val = max(eigenvalues_st[:, 0].max(), lam1_gems)\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "        linewidth=2, label='λ1 = λ2', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('λ2 (Smaller Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('λ1 (Larger Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Eigenvalue Scatter Plot', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('λ1/λ2', fontsize=11)\n",
    "\n",
    "# Plot 3: Coordinate scatter - GEMS\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(coords_gems[:, 0], coords_gems[:, 1], alpha=0.5, s=10, \n",
    "          c='red', edgecolors='none')\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'GEMS Predicted Coordinates\\nλ1/λ2 = {ratio_gems:.2f}', \n",
    "             fontsize=14, fontweight='bold', color='red')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 4: Box plot comparison\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Combine ST ratios with GEMS ratio for box plot\n",
    "data_for_plot = [ratios_st, [ratio_gems]]\n",
    "labels = ['ST Minisets\\n(Ground Truth)', 'GEMS\\n(Predicted)']\n",
    "\n",
    "bp = ax.boxplot(data_for_plot, labels=labels, patch_artist=True,\n",
    "                showmeans=True, meanline=True, widths=0.6)\n",
    "\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "bp['boxes'][1].set_alpha(0.7)\n",
    "\n",
    "ax.axhline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axhline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_ylabel('λ1/λ2 (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Anisotropy Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "gems_anisotropy_path = os.path.join(output_dir, f'gems_anisotropy_analysis_{timestamp}.png')\n",
    "plt.savefig(gems_anisotropy_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved GEMS anisotropy analysis: {gems_anisotropy_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DETAILED COMPARISON TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED COMPARISON: ST MINISETS vs GEMS INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'ST Minisets':<20} {'GEMS Inference':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Number of samples':<30} {len(ratios_st):<20} {coords_gems.shape[0]:<20}\")\n",
    "print(f\"{'λ1 (larger eigenvalue)':<30} {eigenvalues_st[:, 0].mean():.4f} ± {eigenvalues_st[:, 0].std():.4f}   {lam1_gems:.4f}\")\n",
    "print(f\"{'λ2 (smaller eigenvalue)':<30} {eigenvalues_st[:, 1].mean():.4f} ± {eigenvalues_st[:, 1].std():.4f}   {lam2_gems:.4f}\")\n",
    "print(f\"{'λ1/λ2 ratio (median)':<30} {np.median(ratios_st):.2f}              {ratio_gems:.2f}\")\n",
    "print(f\"{'λ1/λ2 ratio (mean)':<30} {ratios_st.mean():.2f}              {ratio_gems:.2f}\")\n",
    "print(f\"{'λ1/λ2 ratio (range)':<30} [{ratios_st.min():.2f}, {ratios_st.max():.2f}]     {ratio_gems:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "st_genuinely_2d_pct = (ratios_st < 5).sum() / len(ratios_st) * 100\n",
    "\n",
    "print(f\"\\nST Mini-Subsets (Ground Truth):\")\n",
    "print(f\"  {st_genuinely_2d_pct:.1f}% are genuinely 2D (λ1/λ2 < 5)\")\n",
    "print(f\"  Median anisotropy: {np.median(ratios_st):.2f}\")\n",
    "if np.median(ratios_st) < 5:\n",
    "    print(f\"  → GENUINELY 2D ✓\")\n",
    "\n",
    "print(f\"\\nGEMS Predicted Coordinates:\")\n",
    "if ratio_gems < 5:\n",
    "    print(f\"  ✓ GENUINELY 2D (λ1/λ2 = {ratio_gems:.2f})\")\n",
    "    print(f\"  → GEMS successfully preserves 2D spatial structure\")\n",
    "elif ratio_gems < 20:\n",
    "    print(f\"  ⚠ Anisotropic but still 2D-ish (λ1/λ2 = {ratio_gems:.2f})\")\n",
    "    print(f\"  → GEMS produces elongated but 2D structures\")\n",
    "else:\n",
    "    print(f\"  ✗ EFFECTIVELY 1D (λ1/λ2 = {ratio_gems:.2f})\")\n",
    "    print(f\"  → WARNING: GEMS collapsed to 1D structure\")\n",
    "\n",
    "# Comparison\n",
    "if ratio_gems < 5 and np.median(ratios_st) < 5:\n",
    "    print(f\"\\n✓ EXCELLENT: Both ST and GEMS are genuinely 2D\")\n",
    "elif abs(ratio_gems - np.median(ratios_st)) < 3:\n",
    "    print(f\"\\n✓ GOOD: GEMS anisotropy ({ratio_gems:.2f}) is similar to ST ({np.median(ratios_st):.2f})\")\n",
    "else:\n",
    "    print(f\"\\n⚠ WARNING: Large anisotropy difference between GEMS ({ratio_gems:.2f}) and ST ({np.median(ratios_st):.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GEMS GEOMETRY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PATCHWISE INFERENCE - TESTING INIT-ONLY (NO PROCRUSTES ALIGNMENT)\n",
    "# ===================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.insert(0, '/home/ehtesamul/sc_st/model')\n",
    "from core_models_et_p3 import GEMSModel\n",
    "import utils_et as uet\n",
    "\n",
    "# ===================================================================\n",
    "# PATHS AND CONFIG\n",
    "# ===================================================================\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "checkpoint_path = f\"{output_dir}/phase2_sc_finetuned_checkpoint.pt\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA AND MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load data\n",
    "adata_sc = sc.read_h5ad(f\"{output_dir}/scadata_with_gems_20251129_205637.h5ad\")\n",
    "\n",
    "if hasattr(adata_sc, 'raw') and adata_sc.raw is not None:\n",
    "    sc_expr = torch.tensor(adata_sc.raw.X.toarray() if hasattr(adata_sc.raw.X, 'toarray') else adata_sc.raw.X, dtype=torch.float32)\n",
    "else:\n",
    "    sc_expr = torch.tensor(adata_sc.X.toarray() if hasattr(adata_sc.X, 'toarray') else adata_sc.X, dtype=torch.float32)\n",
    "\n",
    "gt_coords = adata_sc.obsm['spatial_gt']\n",
    "n_cells, n_genes = sc_expr.shape\n",
    "print(f\"✓ Loaded SC data: {n_cells} cells × {n_genes} genes\")\n",
    "print(f\"✓ Ground truth coords: {gt_coords.shape}\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "print(f\"✓ Loaded checkpoint\")\n",
    "\n",
    "# Initialize model\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    dist_bins=24,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.generator.eval()\n",
    "model.score_net.eval()\n",
    "print(f\"✓ Model loaded and set to eval mode\")\n",
    "\n",
    "# ===================================================================\n",
    "# RUN INFERENCE WITH DIFFERENT CONFIGS\n",
    "# ===================================================================\n",
    "\n",
    "configs = [\n",
    "    {\"patch_size\": 838, \"n_align_iters\": 1, \"name\": \"Single Patch (baseline)\"},\n",
    "    {\"patch_size\": 256, \"n_align_iters\": 0, \"name\": \"patch_size=256, INIT ONLY (no Procrustes)\"},\n",
    "    {\"patch_size\": 256, \"n_align_iters\": 10, \"name\": \"patch_size=256, WITH Procrustes\"},\n",
    "    {\"patch_size\": 384, \"n_align_iters\": 0, \"name\": \"patch_size=384, INIT ONLY (no Procrustes)\"},\n",
    "    {\"patch_size\": 384, \"n_align_iters\": 10, \"name\": \"patch_size=384, WITH Procrustes\"},\n",
    "]\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "for config in configs:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"RUNNING: {config['name']}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model.infer_sc_patchwise(\n",
    "            sc_gene_expr=sc_expr,\n",
    "            n_timesteps_sample=500,\n",
    "            return_coords=True,\n",
    "            patch_size=config['patch_size'],\n",
    "            coverage_per_cell=6.0,\n",
    "            n_align_iters=config['n_align_iters'],\n",
    "            eta=0.0,\n",
    "            guidance_scale=2.0,\n",
    "            sigma_min=0.01,\n",
    "            sigma_max=3.0,\n",
    "        )\n",
    "    \n",
    "    D_edm_pred = results['D_edm'].cpu().numpy()\n",
    "    coords_pred = results['coords_canon'].cpu().numpy()\n",
    "    \n",
    "    print(f\"\\n✓ Inference complete!\")\n",
    "    print(f\"  Predicted EDM: {D_edm_pred.shape}\")\n",
    "    print(f\"  Predicted coords: {coords_pred.shape}\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    gt_edm = squareform(pdist(gt_coords, 'euclidean'))\n",
    "    triu_indices = np.triu_indices(n_cells, k=1)\n",
    "    gt_distances = gt_edm[triu_indices]\n",
    "    pred_distances = D_edm_pred[triu_indices]\n",
    "    \n",
    "    scale = np.median(gt_distances) / np.median(pred_distances)\n",
    "    pred_distances_scaled = pred_distances * scale\n",
    "    \n",
    "    pearson_corr, _ = pearsonr(gt_distances, pred_distances_scaled)\n",
    "    spearman_corr, _ = spearmanr(gt_distances, pred_distances_scaled)\n",
    "    \n",
    "    print(f\"\\nPearson Correlation:  {pearson_corr:.4f}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    print(f\"Scale factor: {scale:.4f}\")\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'config': config['name'],\n",
    "        'patch_size': config['patch_size'],\n",
    "        'n_align_iters': config['n_align_iters'],\n",
    "        'pearson': pearson_corr,\n",
    "        'spearman': spearman_corr,\n",
    "        'scale': scale,\n",
    "        'coords': coords_pred,\n",
    "        'edm': D_edm_pred\n",
    "    })\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY TABLE\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Config':<50} {'Pearson':>10} {'Spearman':>10} {'Scale':>12}\")\n",
    "print(\"-\"*84)\n",
    "for r in results_comparison:\n",
    "    print(f\"{r['config']:<50} {r['pearson']:>10.4f} {r['spearman']:>10.4f} {r['scale']:>12.2f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPARISON PLOTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Plot coordinates for each config\n",
    "n_configs = len(results_comparison)\n",
    "fig, axes = plt.subplots(2, n_configs, figsize=(5*n_configs, 10))\n",
    "\n",
    "for i, r in enumerate(results_comparison):\n",
    "    # Ground truth\n",
    "    axes[0, i].scatter(gt_coords[:, 0], gt_coords[:, 1], s=3, alpha=0.6, c='blue')\n",
    "    axes[0, i].set_title(f'Ground Truth\\n(ref for all)', fontsize=10, weight='bold')\n",
    "    axes[0, i].set_aspect('equal')\n",
    "    \n",
    "    # Predicted\n",
    "    axes[1, i].scatter(r['coords'][:, 0], r['coords'][:, 1], s=3, alpha=0.6, c='red')\n",
    "    axes[1, i].set_title(f\"{r['config']}\\nρ={r['spearman']:.3f}\", fontsize=10, weight='bold')\n",
    "    axes[1, i].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('patchwise_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of correlations\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(results_comparison))\n",
    "width = 0.35\n",
    "\n",
    "pearson_vals = [r['pearson'] for r in results_comparison]\n",
    "spearman_vals = [r['spearman'] for r in results_comparison]\n",
    "\n",
    "ax.bar(x - width/2, pearson_vals, width, label='Pearson', alpha=0.8)\n",
    "ax.bar(x + width/2, spearman_vals, width, label='Spearman', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('Correlation', fontsize=12)\n",
    "ax.set_title('EDM Correlation: Init-Only vs Full Procrustes Alignment', fontsize=14, weight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([r['config'] for r in results_comparison], rotation=15, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Ground truth: 8-sided polygon (octagon)\n",
    "n_cells = 8\n",
    "angles = np.linspace(0, 2*np.pi, n_cells, endpoint=False)\n",
    "X_true = np.column_stack([np.cos(angles), np.sin(angles)])\n",
    "\n",
    "# Define 3 overlapping patches\n",
    "patches = [\n",
    "    np.array([0, 1, 2, 3, 4]),\n",
    "    np.array([3, 4, 5, 6, 7]),\n",
    "    np.array([6, 7, 0, 1, 2])\n",
    "]\n",
    "patch_centers = [2, 5, 7]\n",
    "\n",
    "# REALISTIC: Generate patches with SAME scale but random rotation/translation\n",
    "# This mimics what your diffusion model does\n",
    "def realistic_transform(X, rotation_deg=None, translation=None):\n",
    "    \"\"\"Only rotate and translate, NO SCALING (like diffusion model)\"\"\"\n",
    "    if rotation_deg is None:\n",
    "        rotation_deg = np.random.uniform(0, 360)\n",
    "    if translation is None:\n",
    "        translation = np.random.randn(2) * 0.5\n",
    "    \n",
    "    theta = np.radians(rotation_deg)\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "                  [np.sin(theta), np.cos(theta)]])\n",
    "    return X @ R.T + translation\n",
    "\n",
    "# Create transformed local coordinates (centered, no scale change)\n",
    "local_coords = []\n",
    "for patch_idx in patches:\n",
    "    X_patch = X_true[patch_idx].copy()\n",
    "    X_patch -= X_patch.mean(axis=0)  # Center\n",
    "    X_transformed = realistic_transform(X_patch)  # Only rotate + translate\n",
    "    X_transformed -= X_transformed.mean(axis=0)  # Re-center\n",
    "    local_coords.append(X_transformed)\n",
    "\n",
    "def compute_weights(patch_idx, center_cell_global_idx):\n",
    "    n_points = len(patch_idx)\n",
    "    center_pos = np.where(patch_idx == center_cell_global_idx)[0]\n",
    "    \n",
    "    if len(center_pos) > 0:\n",
    "        center_pos = center_pos[0]\n",
    "    else:\n",
    "        center_pos = n_points // 2\n",
    "    \n",
    "    positions = np.arange(n_points)\n",
    "    distances = np.abs(positions - center_pos)\n",
    "    weights = np.exp(-distances**2 / (2 * (n_points/4)**2))\n",
    "    return weights\n",
    "\n",
    "def procrustes_error(X, X_true):\n",
    "    X_c = X - X.mean(axis=0)\n",
    "    X_true_c = X_true - X_true.mean(axis=0)\n",
    "    C = X_true_c.T @ X_c\n",
    "    U, S, Vt = svd(C)\n",
    "    R = U @ Vt\n",
    "    if np.linalg.det(R) < 0:\n",
    "        U[:, -1] *= -1\n",
    "        R = U @ Vt\n",
    "    s = S.sum() / (X_c**2).sum()\n",
    "    X_aligned = s * (X_c @ R.T)\n",
    "    rmse = np.sqrt(((X_aligned - X_true_c)**2).sum() / len(X))\n",
    "    return rmse\n",
    "\n",
    "def distance_matrix_error(X, X_true):\n",
    "    D = cdist(X, X)\n",
    "    D_true = cdist(X_true, X_true)\n",
    "    return np.sqrt(((D - D_true)**2).mean())\n",
    "\n",
    "# Run TWO experiments in parallel\n",
    "def run_alignment(use_global_scale):\n",
    "    \"\"\"\n",
    "    use_global_scale=True: Use same scale for all patches (correct)\n",
    "    use_global_scale=False: Use per-patch scales (your current code)\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    X_global = np.zeros_like(X_true)\n",
    "    X_global[patches[0]] = local_coords[0]\n",
    "    for i, patch_idx in enumerate(patches[1:], 1):\n",
    "        mask = ~np.isin(patch_idx, patches[0])\n",
    "        X_global[patch_idx[mask]] = local_coords[i][mask]\n",
    "    X_global -= X_global.mean(axis=0)\n",
    "    \n",
    "    history = [X_global.copy()]\n",
    "    rmse_hist = [procrustes_error(X_global, X_true)]\n",
    "    dm_hist = [distance_matrix_error(X_global, X_true)]\n",
    "    \n",
    "    for iter_idx in range(10):\n",
    "        # Step A: Compute rotations\n",
    "        R_list = []\n",
    "        numerators = []\n",
    "        denominators = []\n",
    "        mu_X_list = []\n",
    "        mu_V_list = []\n",
    "        \n",
    "        for patch_idx, V_local, center_cell in zip(patches, local_coords, patch_centers):\n",
    "            X_patch = X_global[patch_idx]\n",
    "            weights = compute_weights(patch_idx, center_cell)\n",
    "            \n",
    "            w_sum = weights.sum()\n",
    "            mu_X = (weights[:, None] * X_patch).sum(axis=0) / w_sum\n",
    "            mu_V = (weights[:, None] * V_local).sum(axis=0) / w_sum\n",
    "            \n",
    "            X_centered = X_patch - mu_X\n",
    "            V_centered = V_local - mu_V\n",
    "            \n",
    "            C = (X_centered.T * weights) @ V_centered\n",
    "            \n",
    "            U, S, Vt = svd(C)\n",
    "            R = U @ Vt\n",
    "            if np.linalg.det(R) < 0:\n",
    "                U[:, -1] *= -1\n",
    "                R = U @ Vt\n",
    "            \n",
    "            numerators.append(S.sum())\n",
    "            denominators.append((weights[:, None] * V_centered**2).sum())\n",
    "            \n",
    "            R_list.append(R)\n",
    "            mu_X_list.append(mu_X)\n",
    "            mu_V_list.append(mu_V)\n",
    "        \n",
    "        # Compute scale(s)\n",
    "        if use_global_scale:\n",
    "            # GLOBAL SCALE: shared across all patches\n",
    "            s_global = sum(numerators) / sum(denominators)\n",
    "            s_list = [s_global] * len(patches)\n",
    "        else:\n",
    "            # PER-PATCH SCALE: each patch gets its own\n",
    "            s_list = [num / (denom + 1e-8) for num, denom in zip(numerators, denominators)]\n",
    "        \n",
    "        # Compute translations\n",
    "        t_list = []\n",
    "        for R, mu_X, mu_V, s, V_local in zip(R_list, mu_X_list, mu_V_list, s_list, local_coords):\n",
    "            t = mu_X - s * (mu_V @ R.T)\n",
    "            t_list.append(t)\n",
    "        \n",
    "        # Step B: Update global coordinates\n",
    "        X_new = np.zeros_like(X_global)\n",
    "        W_total = np.zeros(n_cells)\n",
    "        \n",
    "        for s, R, t, patch_idx, V_local, center_cell in zip(\n",
    "            s_list, R_list, t_list, patches, local_coords, patch_centers\n",
    "        ):\n",
    "            X_transformed = s * (V_local @ R.T) + t\n",
    "            weights = compute_weights(patch_idx, center_cell)\n",
    "            \n",
    "            for i, cell_i in enumerate(patch_idx):\n",
    "                X_new[cell_i] += weights[i] * X_transformed[i]\n",
    "                W_total[cell_i] += weights[i]\n",
    "        \n",
    "        X_new /= W_total[:, None]\n",
    "        X_new -= X_new.mean(axis=0)\n",
    "        X_global = X_new\n",
    "        \n",
    "        history.append(X_global.copy())\n",
    "        rmse_hist.append(procrustes_error(X_global, X_true))\n",
    "        dm_hist.append(distance_matrix_error(X_global, X_true))\n",
    "    \n",
    "    return history, rmse_hist, dm_hist, s_list\n",
    "\n",
    "# Run both methods\n",
    "history_global, rmse_global, dm_global, s_global = run_alignment(use_global_scale=True)\n",
    "history_perPatch, rmse_perPatch, dm_perPatch, s_perPatch = run_alignment(use_global_scale=False)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(3, 4, figsize=(24, 18))\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "def plot_octagon(ax, X, title):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='black', s=150, zorder=3, edgecolors='white', linewidths=2)\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        next_j = (j + 1) % n_cells\n",
    "        ax.plot([X[j, 0], X[next_j, 0]], [X[j, 1], X[next_j, 1]], \n",
    "                'k-', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        ax.text(X[j, 0], X[j, 1], str(j), fontsize=14, \n",
    "                ha='center', va='center', color='white', weight='bold')\n",
    "    \n",
    "    for patch_idx, color in zip(patches, colors):\n",
    "        X_patch = X[patch_idx]\n",
    "        ax.fill(X_patch[:, 0], X_patch[:, 1], alpha=0.15, color=color, edgecolor=color, linewidth=2)\n",
    "    \n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=16, weight='bold', pad=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Row 0: Ground truth\n",
    "plot_octagon(axes[0, 0], X_true, 'Ground Truth')\n",
    "plot_octagon(axes[0, 1], X_true, 'Ground Truth')\n",
    "axes[0, 2].axis('off')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Row 1: Global scale method\n",
    "plot_octagon(axes[1, 0], history_global[0], 'Global Scale: Iter 0')\n",
    "plot_octagon(axes[1, 1], history_global[5], 'Global Scale: Iter 5')\n",
    "plot_octagon(axes[1, 2], history_global[10], 'Global Scale: Iter 10')\n",
    "\n",
    "ax = axes[1, 3]\n",
    "ax.plot(rmse_global, 'o-', color='blue', linewidth=2, label='RMSE')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(dm_global, 's-', color='red', linewidth=2, label='DM Error')\n",
    "ax.set_xlabel('Iteration', fontsize=12, weight='bold')\n",
    "ax.set_ylabel('RMSE', fontsize=12, color='blue')\n",
    "ax2.set_ylabel('DM Error', fontsize=12, color='red')\n",
    "ax.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('Global Scale Convergence', fontsize=14, weight='bold')\n",
    "\n",
    "# Row 2: Per-patch scale method\n",
    "plot_octagon(axes[2, 0], history_perPatch[0], 'Per-Patch Scale: Iter 0')\n",
    "plot_octagon(axes[2, 1], history_perPatch[5], 'Per-Patch Scale: Iter 5')\n",
    "plot_octagon(axes[2, 2], history_perPatch[10], 'Per-Patch Scale: Iter 10')\n",
    "\n",
    "ax = axes[2, 3]\n",
    "ax.plot(rmse_perPatch, 'o-', color='blue', linewidth=2, label='RMSE')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(dm_perPatch, 's-', color='red', linewidth=2, label='DM Error')\n",
    "ax.set_xlabel('Iteration', fontsize=12, weight='bold')\n",
    "ax.set_ylabel('RMSE', fontsize=12, color='blue')\n",
    "ax2.set_ylabel('DM Error', fontsize=12, color='red')\n",
    "ax.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_title('Per-Patch Scale Convergence', fontsize=14, weight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGLOBAL SCALE METHOD:\")\n",
    "print(f\"  Final RMSE: {rmse_global[-1]:.6f}\")\n",
    "print(f\"  Final DM Error: {dm_global[-1]:.6f}\")\n",
    "print(f\"  All patches use scale: {s_global[0]:.4f}\")\n",
    "\n",
    "print(f\"\\nPER-PATCH SCALE METHOD:\")\n",
    "print(f\"  Final RMSE: {rmse_perPatch[-1]:.6f}\")\n",
    "print(f\"  Final DM Error: {dm_perPatch[-1]:.6f}\")\n",
    "print(f\"  Patch scales: {[f'{s:.4f}' for s in s_perPatch]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Ground truth: 8-sided polygon (octagon)\n",
    "n_cells = 8\n",
    "angles = np.linspace(0, 2*np.pi, n_cells, endpoint=False)\n",
    "X_true = np.column_stack([np.cos(angles), np.sin(angles)])\n",
    "\n",
    "# Define 3 overlapping patches WITH CENTER CELLS\n",
    "patches = [\n",
    "    np.array([0, 1, 2, 3, 4]),\n",
    "    np.array([3, 4, 5, 6, 7]),\n",
    "    np.array([6, 7, 0, 1, 2])\n",
    "]\n",
    "\n",
    "# Store which cell is the \"center\" of each patch (middle of the array)\n",
    "patch_centers = [2, 5, 7]  # The middle cell in each patch\n",
    "\n",
    "# Generate patch-local coordinates with random transforms\n",
    "def random_transform(X, rotation_deg=None, scale=None, translation=None):\n",
    "    if rotation_deg is None:\n",
    "        rotation_deg = np.random.uniform(0, 360)\n",
    "    if scale is None:\n",
    "        scale = np.random.uniform(0.8, 1.2)\n",
    "    if translation is None:\n",
    "        translation = np.random.randn(2) * 0.5\n",
    "    \n",
    "    theta = np.radians(rotation_deg)\n",
    "    R = np.array([[np.cos(theta), -np.sin(theta)], \n",
    "                  [np.sin(theta), np.cos(theta)]])\n",
    "    return scale * (X @ R.T) + translation\n",
    "\n",
    "# Create transformed local coordinates for each patch\n",
    "local_coords = []\n",
    "for patch_idx in patches:\n",
    "    X_patch = X_true[patch_idx].copy()\n",
    "    X_patch -= X_patch.mean(axis=0)\n",
    "    X_transformed = random_transform(X_patch)\n",
    "    X_transformed -= X_transformed.mean(axis=0)\n",
    "    local_coords.append(X_transformed)\n",
    "\n",
    "# Initialize global coordinates from first patch\n",
    "X_global = np.zeros_like(X_true)\n",
    "X_global[patches[0]] = local_coords[0]\n",
    "for i, patch_idx in enumerate(patches[1:], 1):\n",
    "    mask = ~np.isin(patch_idx, patches[0])\n",
    "    X_global[patch_idx[mask]] = local_coords[i][mask]\n",
    "X_global -= X_global.mean(axis=0)\n",
    "\n",
    "# Centrality weights based on INDEX distance from center cell\n",
    "def compute_weights(patch_idx, center_cell_global_idx):\n",
    "    \"\"\"\n",
    "    Compute weights based on position in patch array.\n",
    "    The center_cell should get weight 1.0, edges get lower weights.\n",
    "    \"\"\"\n",
    "    n_points = len(patch_idx)\n",
    "    # Find position of center cell in this patch\n",
    "    center_pos = np.where(patch_idx == center_cell_global_idx)[0]\n",
    "    \n",
    "    if len(center_pos) > 0:\n",
    "        center_pos = center_pos[0]\n",
    "    else:\n",
    "        # Fallback: use middle of array\n",
    "        center_pos = n_points // 2\n",
    "    \n",
    "    # Distance in index space from center position\n",
    "    positions = np.arange(n_points)\n",
    "    distances = np.abs(positions - center_pos)\n",
    "    \n",
    "    # Exponential decay\n",
    "    weights = np.exp(-distances**2 / (2 * (n_points/4)**2))\n",
    "    return weights\n",
    "\n",
    "n_iters = 10\n",
    "history = [X_global.copy()]\n",
    "\n",
    "# Compute metrics\n",
    "def procrustes_error(X, X_true):\n",
    "    X_c = X - X.mean(axis=0)\n",
    "    X_true_c = X_true - X_true.mean(axis=0)\n",
    "    \n",
    "    C = X_true_c.T @ X_c\n",
    "    U, S, Vt = svd(C)\n",
    "    R = U @ Vt\n",
    "    if np.linalg.det(R) < 0:\n",
    "        U[:, -1] *= -1\n",
    "        R = U @ Vt\n",
    "    \n",
    "    s = S.sum() / (X_c**2).sum()\n",
    "    X_aligned = s * (X_c @ R.T)\n",
    "    \n",
    "    rmse = np.sqrt(((X_aligned - X_true_c)**2).sum() / len(X))\n",
    "    return rmse\n",
    "\n",
    "def distance_matrix_error(X, X_true):\n",
    "    from scipy.spatial.distance import cdist\n",
    "    D = cdist(X, X)\n",
    "    D_true = cdist(X_true, X_true)\n",
    "    return np.sqrt(((D - D_true)**2).mean())\n",
    "\n",
    "rmse_history = [procrustes_error(X_global, X_true)]\n",
    "dm_error_history = [distance_matrix_error(X_global, X_true)]\n",
    "\n",
    "for iter_idx in range(n_iters):\n",
    "    # Step A: Align each patch via Procrustes with centrality weighting\n",
    "    transforms = []\n",
    "    for patch_idx, V_local, center_cell in zip(patches, local_coords, patch_centers):\n",
    "        X_patch = X_global[patch_idx]\n",
    "        weights = compute_weights(patch_idx, center_cell)\n",
    "        \n",
    "        # Weighted centroids\n",
    "        w_sum = weights.sum()\n",
    "        mu_X = (weights[:, None] * X_patch).sum(axis=0) / w_sum\n",
    "        mu_V = (weights[:, None] * V_local).sum(axis=0) / w_sum\n",
    "        \n",
    "        # Center\n",
    "        X_centered = X_patch - mu_X\n",
    "        V_centered = V_local - mu_V\n",
    "        \n",
    "        # Weighted covariance\n",
    "        C = (X_centered.T * weights) @ V_centered\n",
    "        \n",
    "        # SVD for rotation\n",
    "        U, S, Vt = svd(C)\n",
    "        R = U @ Vt\n",
    "        if np.linalg.det(R) < 0:\n",
    "            U[:, -1] *= -1\n",
    "            R = U @ Vt\n",
    "        \n",
    "        # Scale\n",
    "        numerator = S.sum()\n",
    "        denominator = (weights[:, None] * V_centered**2).sum()\n",
    "        s = numerator / denominator if denominator > 0 else 1.0\n",
    "        \n",
    "        # Translation\n",
    "        t = mu_X - s * (mu_V @ R.T)\n",
    "        \n",
    "        transforms.append((s, R, t))\n",
    "    \n",
    "    # Step B: Update global coordinates via centrality-weighted averaging\n",
    "    X_new = np.zeros_like(X_global)\n",
    "    W_total = np.zeros(n_cells)\n",
    "    \n",
    "    for (s, R, t), patch_idx, V_local, center_cell in zip(transforms, patches, local_coords, patch_centers):\n",
    "        X_transformed = s * (V_local @ R.T) + t\n",
    "        weights = compute_weights(patch_idx, center_cell)\n",
    "        \n",
    "        for i, cell_i in enumerate(patch_idx):\n",
    "            X_new[cell_i] += weights[i] * X_transformed[i]\n",
    "            W_total[cell_i] += weights[i]\n",
    "    \n",
    "    X_new /= W_total[:, None]\n",
    "    X_new -= X_new.mean(axis=0)\n",
    "    X_global = X_new\n",
    "    history.append(X_global.copy())\n",
    "    \n",
    "    # Compute errors\n",
    "    rmse_history.append(procrustes_error(X_global, X_true))\n",
    "    dm_error_history.append(distance_matrix_error(X_global, X_true))\n",
    "\n",
    "# Setup figure\n",
    "fig, axes = plt.subplots(6, 4, figsize=(24, 30))\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "def plot_octagon(ax, X, title, show_patches=True):\n",
    "    ax.scatter(X[:, 0], X[:, 1], c='black', s=200, zorder=3, edgecolors='white', linewidths=2)\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        next_j = (j + 1) % n_cells\n",
    "        ax.plot([X[j, 0], X[next_j, 0]], [X[j, 1], X[next_j, 1]], \n",
    "                'k-', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        ax.text(X[j, 0], X[j, 1], str(j), fontsize=16, \n",
    "                ha='center', va='center', color='white', weight='bold')\n",
    "    \n",
    "    if show_patches:\n",
    "        for patch_idx, color in zip(patches, colors):\n",
    "            X_patch = X[patch_idx]\n",
    "            ax.fill(X_patch[:, 0], X_patch[:, 1], alpha=0.15, color=color, edgecolor=color, linewidth=2)\n",
    "    \n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(title, fontsize=18, weight='bold', pad=10)\n",
    "    ax.grid(True, alpha=0.3, linewidth=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "def plot_error_metrics(ax, iter_idx, rmse_hist, dm_hist):\n",
    "    ax.clear()\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    iters = np.arange(len(rmse_hist[:iter_idx+1]))\n",
    "    \n",
    "    line1 = ax.plot(iters, rmse_hist[:iter_idx+1], 'o-', color='blue', linewidth=3, \n",
    "                    markersize=8, label='RMSE (Procrustes)')\n",
    "    line2 = ax2.plot(iters, dm_hist[:iter_idx+1], 's-', color='red', linewidth=3, \n",
    "                     markersize=8, label='Distance Matrix Error')\n",
    "    \n",
    "    ax.set_xlabel('Iteration', fontsize=14, weight='bold')\n",
    "    ax.set_ylabel('RMSE', fontsize=14, weight='bold', color='blue')\n",
    "    ax2.set_ylabel('Distance Matrix Error', fontsize=14, weight='bold', color='red')\n",
    "    \n",
    "    ax.tick_params(axis='y', labelcolor='blue', labelsize=12)\n",
    "    ax2.tick_params(axis='y', labelcolor='red', labelsize=12)\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    \n",
    "    ax.grid(True, alpha=0.3, linewidth=1)\n",
    "    ax.set_title(f'Convergence Metrics (Iter {iter_idx})', fontsize=16, weight='bold', pad=10)\n",
    "    \n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper right', fontsize=12, framealpha=0.9)\n",
    "\n",
    "# Row 0\n",
    "plot_octagon(axes[0, 0], X_true, 'Ground Truth', show_patches=True)\n",
    "plot_octagon(axes[0, 1], history[0], 'Iteration 0', show_patches=True)\n",
    "plot_octagon(axes[0, 2], X_true, 'Ground Truth', show_patches=True)\n",
    "plot_error_metrics(axes[0, 3], 0, rmse_history, dm_error_history)\n",
    "\n",
    "# Rows 1-5\n",
    "for row in range(1, 6):\n",
    "    for col in range(2):\n",
    "        iter_idx = (row - 1) * 2 + col + 1\n",
    "        if iter_idx < len(history):\n",
    "            plot_octagon(axes[row, col], history[iter_idx], f'Iteration {iter_idx}', show_patches=True)\n",
    "        else:\n",
    "            axes[row, col].axis('off')\n",
    "    \n",
    "    iter_idx = (row - 1) * 2 + 1\n",
    "    if iter_idx < len(history):\n",
    "        plot_octagon(axes[row, 2], X_true, 'Ground Truth', show_patches=True)\n",
    "        plot_error_metrics(axes[row, 3], iter_idx, rmse_history, dm_error_history)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from core_models_et_p1 import STSetDataset, collate_minisets\n",
    "import utils_et as uet\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251129_205637\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ST DATA\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "st_ct     = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_celltype_et.csv'\n",
    "\n",
    "print(\"\\nLoading ST1 (training ST data)...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "st_ct_df = pd.read_csv(st_ct, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "stadata.obs['celltype_mapped_refined'] = st_ct_df.idxmax(axis=1).values\n",
    "\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD GEMS INFERENCE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n=== Loading GEMS Inference Results (timestamp: {timestamp}) ===\")\n",
    "\n",
    "processed_path = os.path.join(output_dir, f\"sc_inference_processed_{timestamp}.pt\")\n",
    "gems_results = torch.load(processed_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "gems_coords = gems_results['coords_canon']\n",
    "gems_D_edm = gems_results['D_edm']\n",
    "\n",
    "print(f\"GEMS results loaded:\")\n",
    "print(f\"  - Coordinates shape: {gems_coords.shape}\")\n",
    "print(f\"  - EDM shape: {gems_D_edm.shape}\")\n",
    "print(f\"  - Number of cells: {gems_results['n_cells']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE ST MINI-SUBSETS (NO MODEL NEEDED)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Generating 5 ST Mini-Subsets ===\")\n",
    "\n",
    "n_min = 96\n",
    "n_max = 384\n",
    "num_minisets = 8\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "miniset_data = []\n",
    "\n",
    "for i in range(num_minisets):\n",
    "    # Sample random subset (same logic as STSetDataset)\n",
    "    n_total = st_coords.shape[0]\n",
    "    n = np.random.randint(n_min, min(n_max + 1, n_total))\n",
    "    \n",
    "    # Random indices\n",
    "    indices = torch.randperm(n_total, device=device)[:n]\n",
    "    \n",
    "    # Get coordinates for this miniset\n",
    "    miniset_coords = st_coords[indices]\n",
    "    \n",
    "    # Compute ground truth EDM\n",
    "    D_gt = torch.cdist(miniset_coords, miniset_coords).cpu().numpy()\n",
    "    \n",
    "    print(f\"Miniset {i+1}: {n} points\")\n",
    "    \n",
    "    miniset_data.append({\n",
    "        'index': i,\n",
    "        'n_points': n,\n",
    "        'D_edm': D_gt\n",
    "    })\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: EDM HEATMAPS (2 rows x 3 cols)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Creating EDM Heatmap Visualizations ===\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 5 ST minisets\n",
    "for i, data in enumerate(miniset_data):\n",
    "    ax = axes[i]\n",
    "    D = data['D_edm']\n",
    "    \n",
    "    im = ax.imshow(D, cmap='viridis', aspect='auto')\n",
    "    ax.set_title(f'ST Miniset {i+1}\\n({data[\"n_points\"]} points)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Cell Index', fontsize=11)\n",
    "    ax.set_ylabel('Cell Index', fontsize=11)\n",
    "    \n",
    "    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label('Distance', fontsize=10)\n",
    "\n",
    "# Plot GEMS inference result\n",
    "ax = axes[5]\n",
    "D_gems = gems_D_edm.numpy() if torch.is_tensor(gems_D_edm) else gems_D_edm\n",
    "\n",
    "# Subsample if too large\n",
    "max_viz_size = 1000\n",
    "if D_gems.shape[0] > max_viz_size:\n",
    "    idx_sample = np.random.choice(D_gems.shape[0], max_viz_size, replace=False)\n",
    "    idx_sample = np.sort(idx_sample)\n",
    "    D_gems_viz = D_gems[np.ix_(idx_sample, idx_sample)]\n",
    "    title_suffix = f'\\n(showing {max_viz_size}/{D_gems.shape[0]} cells)'\n",
    "else:\n",
    "    D_gems_viz = D_gems\n",
    "    title_suffix = f'\\n({D_gems.shape[0]} cells)'\n",
    "\n",
    "im = ax.imshow(D_gems_viz, cmap='viridis', aspect='auto')\n",
    "ax.set_title(f'GEMS Inference Result{title_suffix}', \n",
    "             fontsize=14, fontweight='bold', color='red')\n",
    "ax.set_xlabel('Cell Index (Sampled)', fontsize=11)\n",
    "ax.set_ylabel('Cell Index (Sampled)', fontsize=11)\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Distance', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "edm_heatmap_path = os.path.join(output_dir, f'comparison_edm_heatmaps_{timestamp}.png')\n",
    "# plt.savefig(edm_heatmap_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved EDM heatmaps: {edm_heatmap_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: DISTANCE DISTRIBUTIONS (2 rows x 3 cols)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Creating Distance Distribution Visualizations ===\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(21, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot 5 ST minisets\n",
    "for i, data in enumerate(miniset_data):\n",
    "    ax = axes[i]\n",
    "    D = data['D_edm']\n",
    "    \n",
    "    upper_tri_idx = np.triu_indices_from(D, k=1)\n",
    "    distances = D[upper_tri_idx]\n",
    "    \n",
    "    ax.hist(distances, bins=50, alpha=0.7, edgecolor='black', color='steelblue', label='ST Miniset')\n",
    "    ax.set_title(f'ST Miniset {i+1}\\n({data[\"n_points\"]} points)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Distance', fontsize=11)\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    \n",
    "    mean_dist = distances.mean()\n",
    "    median_dist = np.median(distances)\n",
    "    ax.axvline(mean_dist, color='r', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {mean_dist:.2f}')\n",
    "    ax.axvline(median_dist, color='g', linestyle='--', linewidth=2, \n",
    "               label=f'Median: {median_dist:.2f}')\n",
    "    \n",
    "    ax.legend(fontsize=9, loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot GEMS inference result\n",
    "ax = axes[8]\n",
    "upper_tri_idx = np.triu_indices_from(D_gems, k=1)\n",
    "distances_gems = D_gems[upper_tri_idx]\n",
    "\n",
    "ax.hist(distances_gems, bins=100, alpha=0.7, edgecolor='black', \n",
    "        color='red', label='GEMS Inference')\n",
    "ax.set_title(f'GEMS Inference Result\\n({D_gems.shape[0]} cells)', \n",
    "             fontsize=14, fontweight='bold', color='red')\n",
    "ax.set_xlabel('Distance', fontsize=11)\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "\n",
    "mean_gems = distances_gems.mean()\n",
    "median_gems = np.median(distances_gems)\n",
    "ax.axvline(mean_gems, color='darkred', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {mean_gems:.2f}')\n",
    "ax.axvline(median_gems, color='darkgreen', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {median_gems:.2f}')\n",
    "\n",
    "ax.legend(fontsize=9, loc='upper right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "dist_hist_path = os.path.join(output_dir, f'comparison_distance_distributions_{timestamp}.png')\n",
    "# plt.savefig(dist_hist_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved distance distributions: {dist_hist_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nST Minisets (Ground Truth):\")\n",
    "for i, data in enumerate(miniset_data):\n",
    "    D = data['D_edm']\n",
    "    upper_tri_idx = np.triu_indices_from(D, k=1)\n",
    "    distances = D[upper_tri_idx]\n",
    "    \n",
    "    print(f\"\\n  Miniset {i+1} ({data['n_points']} points):\")\n",
    "    print(f\"    Mean distance:   {distances.mean():.4f}\")\n",
    "    print(f\"    Median distance: {np.median(distances):.4f}\")\n",
    "    print(f\"    Std distance:    {distances.std():.4f}\")\n",
    "    print(f\"    Min distance:    {distances.min():.4f}\")\n",
    "    print(f\"    Max distance:    {distances.max():.4f}\")\n",
    "\n",
    "print(f\"\\nGEMS Inference Result ({D_gems.shape[0]} cells):\")\n",
    "print(f\"  Mean distance:   {distances_gems.mean():.4f}\")\n",
    "print(f\"  Median distance: {np.median(distances_gems):.4f}\")\n",
    "print(f\"  Std distance:    {distances_gems.std():.4f}\")\n",
    "print(f\"  Min distance:    {distances_gems.min():.4f}\")\n",
    "print(f\"  Max distance:    {distances_gems.max():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"All visualizations saved to: {output_dir}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EIGENVALUE ANISOTROPY ANALYSIS FOR GEMS INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GEMS INFERENCE: 2D GEOMETRY VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ANALYZE GEMS PREDICTED COORDINATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Analyzing GEMS Predicted Coordinates ===\\n\")\n",
    "\n",
    "# Get GEMS coordinates (already loaded as gems_coords)\n",
    "coords_gems = gems_coords.numpy() if torch.is_tensor(gems_coords) else gems_coords\n",
    "\n",
    "# Compute anisotropy for GEMS\n",
    "X = coords_gems.astype(float)\n",
    "Xc = X - X.mean(axis=0, keepdims=True)\n",
    "\n",
    "cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "eigvals_gems, eigvecs_gems = np.linalg.eigh(cov)\n",
    "eigvals_gems = eigvals_gems[::-1]\n",
    "\n",
    "lam1_gems, lam2_gems = eigvals_gems\n",
    "ratio_gems = lam1_gems / (lam2_gems + 1e-12)\n",
    "\n",
    "print(f\"GEMS Predicted Coordinates ({coords_gems.shape[0]} cells):\")\n",
    "print(f\"  λ1 = {lam1_gems:.4f},  λ2 = {lam2_gems:.4f}\")\n",
    "print(f\"  λ1/λ2 = {ratio_gems:.2f}\")\n",
    "\n",
    "if ratio_gems < 5:\n",
    "    interpretation_gems = \"→ GENUINELY 2D ✓\"\n",
    "elif ratio_gems < 20:\n",
    "    interpretation_gems = \"→ Anisotropic but still 2D-ish\"\n",
    "else:\n",
    "    interpretation_gems = \"→ EFFECTIVELY 1D (very elongated) ✗\"\n",
    "\n",
    "print(f\"  {interpretation_gems}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ANALYZE ST MINISETS FOR COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=== Analyzing ST Mini-Subsets for Comparison ===\\n\")\n",
    "\n",
    "ratios_st = []\n",
    "eigenvalues_st = []\n",
    "\n",
    "for i, data in enumerate(miniset_data):\n",
    "    D = data['D_edm']\n",
    "    \n",
    "    # Reconstruct coordinates from EDM using classical MDS\n",
    "    n = D.shape[0]\n",
    "    Jn = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = -0.5 * (Jn @ (D ** 2) @ Jn)\n",
    "    \n",
    "    eigvals_full, eigvecs_full = np.linalg.eigh(B)\n",
    "    eigvals_full = eigvals_full[::-1]\n",
    "    eigvecs_full = eigvecs_full[:, ::-1]\n",
    "    \n",
    "    coords_patch = eigvecs_full[:, :2] @ np.diag(np.sqrt(np.maximum(eigvals_full[:2], 0)))\n",
    "    \n",
    "    # Analyze 2D variance\n",
    "    X = coords_patch.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals_2d, _ = np.linalg.eigh(cov)\n",
    "    eigvals_2d = eigvals_2d[::-1]\n",
    "    \n",
    "    lam1, lam2 = eigvals_2d\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    \n",
    "    ratios_st.append(ratio)\n",
    "    eigenvalues_st.append((lam1, lam2))\n",
    "\n",
    "ratios_st = np.array(ratios_st)\n",
    "eigenvalues_st = np.array(eigenvalues_st)\n",
    "\n",
    "print(f\"ST Mini-Subsets Statistics:\")\n",
    "print(f\"  λ1/λ2 - Median: {np.median(ratios_st):.2f}\")\n",
    "print(f\"  λ1/λ2 - Mean:   {ratios_st.mean():.2f}\")\n",
    "print(f\"  λ1/λ2 - Range:  [{ratios_st.min():.2f}, {ratios_st.max():.2f}]\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. COMPARISON VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Creating Comparison Visualizations ===\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Histogram comparison - ST vs GEMS\n",
    "ax = axes[0, 0]\n",
    "ax.hist(ratios_st, bins=30, alpha=0.6, edgecolor='black', color='steelblue', \n",
    "        label=f'ST Minisets (n={len(ratios_st)})')\n",
    "ax.axvline(ratio_gems, color='red', linestyle='--', linewidth=3, \n",
    "           label=f'GEMS: {ratio_gems:.2f}')\n",
    "ax.axvline(np.median(ratios_st), color='blue', linestyle='--', linewidth=2, \n",
    "           label=f'ST Median: {np.median(ratios_st):.2f}')\n",
    "ax.axvline(5, color='g', linestyle=':', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axvline(20, color='orange', linestyle=':', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_xlabel('λ1/λ2 (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Anisotropy Comparison: ST Minisets vs GEMS', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Eigenvalue scatter - ST minisets\n",
    "ax = axes[0, 1]\n",
    "scatter = ax.scatter(eigenvalues_st[:, 1], eigenvalues_st[:, 0], \n",
    "                    c=ratios_st, cmap='viridis', alpha=0.7, s=80, \n",
    "                    edgecolors='black', linewidth=1, label='ST Minisets')\n",
    "\n",
    "# Add GEMS point\n",
    "ax.scatter(lam2_gems, lam1_gems, c='red', s=300, marker='*', \n",
    "          edgecolors='darkred', linewidth=2, label='GEMS', zorder=5)\n",
    "\n",
    "# Diagonal line\n",
    "min_val = min(eigenvalues_st[:, 1].min(), lam2_gems)\n",
    "max_val = max(eigenvalues_st[:, 0].max(), lam1_gems)\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "        linewidth=2, label='λ1 = λ2', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('λ2 (Smaller Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('λ1 (Larger Eigenvalue)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Eigenvalue Scatter Plot', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('λ1/λ2', fontsize=11)\n",
    "\n",
    "# Plot 3: Coordinate scatter - GEMS\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(coords_gems[:, 0], coords_gems[:, 1], alpha=0.5, s=10, \n",
    "          c='red', edgecolors='none')\n",
    "ax.set_xlabel('Dimension 1', fontsize=13, fontweight='bold')\n",
    "ax.set_ylabel('Dimension 2', fontsize=13, fontweight='bold')\n",
    "ax.set_title(f'GEMS Predicted Coordinates\\nλ1/λ2 = {ratio_gems:.2f}', \n",
    "             fontsize=14, fontweight='bold', color='red')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 4: Box plot comparison\n",
    "ax = axes[1, 1]\n",
    "\n",
    "# Combine ST ratios with GEMS ratio for box plot\n",
    "data_for_plot = [ratios_st, [ratio_gems]]\n",
    "labels = ['ST Minisets\\n(Ground Truth)', 'GEMS\\n(Predicted)']\n",
    "\n",
    "bp = ax.boxplot(data_for_plot, labels=labels, patch_artist=True,\n",
    "                showmeans=True, meanline=True, widths=0.6)\n",
    "\n",
    "bp['boxes'][0].set_facecolor('steelblue')\n",
    "bp['boxes'][0].set_alpha(0.7)\n",
    "bp['boxes'][1].set_facecolor('red')\n",
    "bp['boxes'][1].set_alpha(0.7)\n",
    "\n",
    "ax.axhline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axhline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_ylabel('λ1/λ2 (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Anisotropy Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "gems_anisotropy_path = os.path.join(output_dir, f'gems_anisotropy_analysis_{timestamp}.png')\n",
    "plt.savefig(gems_anisotropy_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved GEMS anisotropy analysis: {gems_anisotropy_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DETAILED COMPARISON TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED COMPARISON: ST MINISETS vs GEMS INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'ST Minisets':<20} {'GEMS Inference':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Number of samples':<30} {len(ratios_st):<20} {coords_gems.shape[0]:<20}\")\n",
    "print(f\"{'λ1 (larger eigenvalue)':<30} {eigenvalues_st[:, 0].mean():.4f} ± {eigenvalues_st[:, 0].std():.4f}   {lam1_gems:.4f}\")\n",
    "print(f\"{'λ2 (smaller eigenvalue)':<30} {eigenvalues_st[:, 1].mean():.4f} ± {eigenvalues_st[:, 1].std():.4f}   {lam2_gems:.4f}\")\n",
    "print(f\"{'λ1/λ2 ratio (median)':<30} {np.median(ratios_st):.2f}              {ratio_gems:.2f}\")\n",
    "print(f\"{'λ1/λ2 ratio (mean)':<30} {ratios_st.mean():.2f}              {ratio_gems:.2f}\")\n",
    "print(f\"{'λ1/λ2 ratio (range)':<30} [{ratios_st.min():.2f}, {ratios_st.max():.2f}]     {ratio_gems:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "st_genuinely_2d_pct = (ratios_st < 5).sum() / len(ratios_st) * 100\n",
    "\n",
    "print(f\"\\nST Mini-Subsets (Ground Truth):\")\n",
    "print(f\"  {st_genuinely_2d_pct:.1f}% are genuinely 2D (λ1/λ2 < 5)\")\n",
    "print(f\"  Median anisotropy: {np.median(ratios_st):.2f}\")\n",
    "if np.median(ratios_st) < 5:\n",
    "    print(f\"  → GENUINELY 2D ✓\")\n",
    "\n",
    "print(f\"\\nGEMS Predicted Coordinates:\")\n",
    "if ratio_gems < 5:\n",
    "    print(f\"  ✓ GENUINELY 2D (λ1/λ2 = {ratio_gems:.2f})\")\n",
    "    print(f\"  → GEMS successfully preserves 2D spatial structure\")\n",
    "elif ratio_gems < 20:\n",
    "    print(f\"  ⚠ Anisotropic but still 2D-ish (λ1/λ2 = {ratio_gems:.2f})\")\n",
    "    print(f\"  → GEMS produces elongated but 2D structures\")\n",
    "else:\n",
    "    print(f\"  ✗ EFFECTIVELY 1D (λ1/λ2 = {ratio_gems:.2f})\")\n",
    "    print(f\"  → WARNING: GEMS collapsed to 1D structure\")\n",
    "\n",
    "# Comparison\n",
    "if ratio_gems < 5 and np.median(ratios_st) < 5:\n",
    "    print(f\"\\n✓ EXCELLENT: Both ST and GEMS are genuinely 2D\")\n",
    "elif abs(ratio_gems - np.median(ratios_st)) < 3:\n",
    "    print(f\"\\n✓ GOOD: GEMS anisotropy ({ratio_gems:.2f}) is similar to ST ({np.median(ratios_st):.2f})\")\n",
    "else:\n",
    "    print(f\"\\n⚠ WARNING: Large anisotropy difference between GEMS ({ratio_gems:.2f}) and ST ({np.median(ratios_st):.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GEMS GEOMETRY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# ANISOTROPY CORRECTION DIAGNOSTIC\n",
    "# ===================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANISOTROPY CORRECTION DIAGNOSTIC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: ANALYZE CURRENT PREDICTION ANISOTROPY\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n=== Current Prediction Analysis ===\")\n",
    "\n",
    "coords_centered = coords_pred - coords_pred.mean(axis=0, keepdims=True)\n",
    "\n",
    "# PCA on predicted coordinates\n",
    "cov_pred = coords_centered.T @ coords_centered / (coords_centered.shape[0] - 1)\n",
    "eigvals_pred, eigvecs_pred = np.linalg.eigh(cov_pred)\n",
    "eigvals_pred = eigvals_pred[::-1]\n",
    "eigvecs_pred = eigvecs_pred[:, ::-1]\n",
    "\n",
    "lam1_pred, lam2_pred = eigvals_pred\n",
    "r_cur = lam1_pred / (lam2_pred + 1e-12)\n",
    "\n",
    "print(f\"Current Prediction:\")\n",
    "print(f\"  λ1 = {lam1_pred:.4f},  λ2 = {lam2_pred:.4f}\")\n",
    "print(f\"  λ1/λ2 = {r_cur:.2f}\")\n",
    "\n",
    "if r_cur < 5:\n",
    "    print(f\"  → GENUINELY 2D ✓\")\n",
    "elif r_cur < 20:\n",
    "    print(f\"  → Anisotropic but still 2D-ish\")\n",
    "else:\n",
    "    print(f\"  → EFFECTIVELY 1D (very elongated) ✗\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: COMPUTE CORRECTION FACTOR\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n=== Computing Anisotropy Correction ===\")\n",
    "\n",
    "# Target ratio from ST minisets (from your previous analysis)\n",
    "r_tgt = 2.2  \n",
    "\n",
    "print(f\"Target anisotropy ratio (from ST): {r_tgt:.2f}\")\n",
    "print(f\"Current anisotropy ratio: {r_cur:.2f}\")\n",
    "print(f\"Correction needed: {r_cur/r_tgt:.2f}x\")\n",
    "\n",
    "# Compute scaling factors\n",
    "s = np.sqrt(r_cur / r_tgt)\n",
    "\n",
    "print(f\"\\nScaling factors:\")\n",
    "print(f\"  First PC (compress):  1/{s:.4f} = {1/s:.4f}\")\n",
    "print(f\"  Second PC (expand):   {s:.4f}\")\n",
    "\n",
    "# Create scaling matrix\n",
    "S = np.diag([1/s, s])\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: APPLY CORRECTION\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n=== Applying Correction ===\")\n",
    "\n",
    "# Transform: center → rotate to PCs → scale → rotate back\n",
    "coords_balanced = coords_centered @ eigvecs_pred @ S @ eigvecs_pred.T\n",
    "\n",
    "# Verify new anisotropy\n",
    "cov_balanced = coords_balanced.T @ coords_balanced / (coords_balanced.shape[0] - 1)\n",
    "eigvals_balanced, _ = np.linalg.eigh(cov_balanced)\n",
    "eigvals_balanced = eigvals_balanced[::-1]\n",
    "\n",
    "lam1_balanced, lam2_balanced = eigvals_balanced\n",
    "r_balanced = lam1_balanced / (lam2_balanced + 1e-12)\n",
    "\n",
    "print(f\"\\nCorrected Coordinates:\")\n",
    "print(f\"  λ1 = {lam1_balanced:.4f},  λ2 = {lam2_balanced:.4f}\")\n",
    "print(f\"  λ1/λ2 = {r_balanced:.2f}\")\n",
    "print(f\"  → Target was {r_tgt:.2f}, achieved {r_balanced:.2f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: RECOMPUTE METRICS\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n=== Recomputing Metrics ===\")\n",
    "\n",
    "# Compute balanced EDM\n",
    "D_edm_balanced = squareform(pdist(coords_balanced, 'euclidean'))\n",
    "\n",
    "# Extract upper triangle\n",
    "triu_indices = np.triu_indices(n_cells, k=1)\n",
    "gt_distances = gt_edm[triu_indices]\n",
    "pred_distances_original = D_edm_pred[triu_indices]\n",
    "pred_distances_balanced = D_edm_balanced[triu_indices]\n",
    "\n",
    "# Scale alignment for both\n",
    "scale_original = np.median(gt_distances) / np.median(pred_distances_original)\n",
    "scale_balanced = np.median(gt_distances) / np.median(pred_distances_balanced)\n",
    "\n",
    "pred_distances_original_scaled = pred_distances_original * scale_original\n",
    "pred_distances_balanced_scaled = pred_distances_balanced * scale_balanced\n",
    "\n",
    "# Correlations\n",
    "pearson_original, _ = pearsonr(gt_distances, pred_distances_original_scaled)\n",
    "spearman_original, _ = spearmanr(gt_distances, pred_distances_original_scaled)\n",
    "\n",
    "pearson_balanced, _ = pearsonr(gt_distances, pred_distances_balanced_scaled)\n",
    "spearman_balanced, _ = spearmanr(gt_distances, pred_distances_balanced_scaled)\n",
    "\n",
    "print(f\"\\nOriginal Prediction:\")\n",
    "print(f\"  Pearson:  {pearson_original:.4f}\")\n",
    "print(f\"  Spearman: {spearman_original:.4f}\")\n",
    "\n",
    "print(f\"\\nAnisotropy-Corrected Prediction:\")\n",
    "print(f\"  Pearson:  {pearson_balanced:.4f}\")\n",
    "print(f\"  Spearman: {spearman_balanced:.4f}\")\n",
    "\n",
    "delta_pearson = pearson_balanced - pearson_original\n",
    "delta_spearman = spearman_balanced - spearman_original\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "print(f\"  Δ Pearson:  {delta_pearson:+.4f}\")\n",
    "print(f\"  Δ Spearman: {delta_spearman:+.4f}\")\n",
    "\n",
    "if abs(delta_spearman) < 0.02:\n",
    "    print(f\"\\n→ Spearman barely changed - anisotropy was NOT the main issue\")\n",
    "else:\n",
    "    print(f\"\\n→ Significant Spearman change - anisotropy correction helps!\")\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATION 1: COORDINATE COMPARISON\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n=== Creating Visualizations ===\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "# Ground truth\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, c='blue')\n",
    "axes[0].set_title('Ground Truth Coordinates', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y', fontsize=12)\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Original prediction\n",
    "axes[1].scatter(coords_pred[:, 0], coords_pred[:, 1], s=5, alpha=0.6, c='red')\n",
    "axes[1].set_title(f'Original Prediction\\nλ1/λ2 = {r_cur:.2f}', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Y', fontsize=12)\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Corrected prediction\n",
    "axes[2].scatter(coords_balanced[:, 0], coords_balanced[:, 1], s=5, alpha=0.6, c='green')\n",
    "axes[2].set_title(f'Anisotropy-Corrected\\nλ1/λ2 = {r_balanced:.2f} (target: {r_tgt:.2f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('X', fontsize=12)\n",
    "axes[2].set_ylabel('Y', fontsize=12)\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATION 2: DISTANCE SCATTER COMPARISON\n",
    "# ===================================================================\n",
    "\n",
    "sample_size = 50000\n",
    "sample_idx = np.random.choice(len(gt_distances), min(sample_size, len(gt_distances)), replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Original\n",
    "axes[0].scatter(gt_distances[sample_idx], pred_distances_original_scaled[sample_idx], \n",
    "               alpha=0.2, s=5, c='red')\n",
    "axes[0].set_title(f'Original Prediction\\nSpearman ρ = {spearman_original:.4f}', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Ground Truth Distance', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Distance (scaled)', fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims_0 = [min(axes[0].get_xlim()[0], axes[0].get_ylim()[0]), \n",
    "          max(axes[0].get_xlim()[1], axes[0].get_ylim()[1])]\n",
    "axes[0].plot(lims_0, lims_0, 'k--', alpha=0.75, linewidth=2, label='Ideal')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "axes[0].legend(fontsize=11)\n",
    "\n",
    "# Corrected\n",
    "axes[1].scatter(gt_distances[sample_idx], pred_distances_balanced_scaled[sample_idx], \n",
    "               alpha=0.2, s=5, c='green')\n",
    "axes[1].set_title(f'Anisotropy-Corrected\\nSpearman ρ = {spearman_balanced:.4f} (Δ{delta_spearman:+.4f})', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Ground Truth Distance', fontsize=12)\n",
    "axes[1].set_ylabel('Predicted Distance (scaled)', fontsize=12)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims_1 = [min(axes[1].get_xlim()[0], axes[1].get_ylim()[0]), \n",
    "          max(axes[1].get_xlim()[1], axes[1].get_ylim()[1])]\n",
    "axes[1].plot(lims_1, lims_1, 'k--', alpha=0.75, linewidth=2, label='Ideal')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "axes[1].legend(fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATION 3: DISTANCE DISTRIBUTIONS\n",
    "# ===================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original\n",
    "axes[0].hist(gt_distances, bins=100, alpha=0.5, color='blue', \n",
    "            label='Ground Truth', density=True, edgecolor='black', linewidth=0.5)\n",
    "axes[0].hist(pred_distances_original_scaled, bins=100, alpha=0.5, color='red', \n",
    "            label='Original Prediction', density=True, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title('Original Prediction', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Distance', fontsize=12)\n",
    "axes[0].set_ylabel('Density', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Corrected\n",
    "axes[1].hist(gt_distances, bins=100, alpha=0.5, color='blue', \n",
    "            label='Ground Truth', density=True, edgecolor='black', linewidth=0.5)\n",
    "axes[1].hist(pred_distances_balanced_scaled, bins=100, alpha=0.5, color='green', \n",
    "            label='Anisotropy-Corrected', density=True, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_title('Anisotropy-Corrected', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Distance', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# VISUALIZATION 4: EIGENVALUE COMPARISON\n",
    "# ===================================================================\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Ground truth\n",
    "gt_coords_centered = gt_coords - gt_coords.mean(axis=0, keepdims=True)\n",
    "cov_gt = gt_coords_centered.T @ gt_coords_centered / (gt_coords_centered.shape[0] - 1)\n",
    "eigvals_gt, _ = np.linalg.eigh(cov_gt)\n",
    "eigvals_gt = eigvals_gt[::-1]\n",
    "lam1_gt, lam2_gt = eigvals_gt\n",
    "r_gt = lam1_gt / (lam2_gt + 1e-12)\n",
    "\n",
    "# Plot\n",
    "methods = ['Ground Truth', 'Original\\nPrediction', 'Anisotropy\\nCorrected']\n",
    "ratios = [r_gt, r_cur, r_balanced]\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "bars = ax.bar(methods, ratios, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "ax.axhline(5, color='darkgreen', linestyle='--', linewidth=2, alpha=0.7, label='2D threshold (5)')\n",
    "ax.axhline(20, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='1D threshold (20)')\n",
    "ax.axhline(r_tgt, color='purple', linestyle=':', linewidth=2, alpha=0.7, label=f'ST target ({r_tgt:.2f})')\n",
    "\n",
    "ax.set_ylabel('λ1/λ2 (Anisotropy Ratio)', fontsize=13, fontweight='bold')\n",
    "ax.set_title('Anisotropy Comparison', fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, ratio in zip(bars, ratios):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{ratio:.2f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# SUMMARY\n",
    "# ===================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nAnisotropy Ratios (λ1/λ2):\")\n",
    "print(f\"  Ground Truth:           {r_gt:.2f}\")\n",
    "print(f\"  Original Prediction:    {r_cur:.2f}\")\n",
    "print(f\"  Corrected Prediction:   {r_balanced:.2f}\")\n",
    "print(f\"  ST Miniset Target:      {r_tgt:.2f}\")\n",
    "\n",
    "print(f\"\\nSpearman Correlation:\")\n",
    "print(f\"  Original:   {spearman_original:.4f}\")\n",
    "print(f\"  Corrected:  {spearman_balanced:.4f}\")\n",
    "print(f\"  Change:     {delta_spearman:+.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CONCLUSION\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if abs(delta_spearman) < 0.02:\n",
    "    print(\"\\n✓ Spearman correlation barely changed after correction\")\n",
    "    print(\"→ The main issue is NOT just global anisotropy\")\n",
    "    print(\"→ You likely need to retrain with anisotropy regularization\")\n",
    "    print(\"→ But the visual improvement suggests anisotropy IS part of the problem\")\n",
    "elif delta_spearman > 0.02:\n",
    "    print(\"\\n✓ Spearman correlation IMPROVED significantly\")\n",
    "    print(\"→ Anisotropy correction helps!\")\n",
    "    print(\"→ Training with anisotropy regularization should improve results\")\n",
    "else:\n",
    "    print(\"\\n⚠ Spearman correlation DECREASED\")\n",
    "    print(\"→ Anisotropy correction made things worse (unexpected)\")\n",
    "    print(\"→ The elongation might be capturing real structure\")\n",
    "\n",
    "if r_cur > 10:\n",
    "    print(f\"\\n⚠ Original prediction is very elongated (λ1/λ2 = {r_cur:.2f})\")\n",
    "    print(\"→ This is much more anisotropic than ST minisets\")\n",
    "    print(\"→ STRONGLY RECOMMEND adding anisotropy regularization to training\")\n",
    "elif r_cur > 5:\n",
    "    print(f\"\\n⚠ Original prediction is somewhat elongated (λ1/λ2 = {r_cur:.2f})\")\n",
    "    print(\"→ Moderately more anisotropic than ST minisets\")\n",
    "    print(\"→ Consider adding anisotropy regularization\")\n",
    "else:\n",
    "    print(f\"\\n✓ Original prediction is already well-balanced (λ1/λ2 = {r_cur:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# EIGENVALUE ANISOTROPY ANALYSIS FOR ST MINI-SUBSETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ST MINI-SUBSETS: 2D GEOMETRY VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. ANALYZE THE 5 MINI-SUBSETS WE ALREADY GENERATED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Individual Mini-Subset Analysis ===\\n\")\n",
    "\n",
    "ratios_individual = []\n",
    "eigenvalues_list = []\n",
    "\n",
    "for i, data in enumerate(miniset_data):\n",
    "    D = data['D_edm']\n",
    "    \n",
    "    # Reconstruct coordinates from EDM using classical MDS\n",
    "    n = D.shape[0]\n",
    "    Jn = np.eye(n) - np.ones((n, n)) / n\n",
    "    B = -0.5 * (Jn @ (D ** 2) @ Jn)\n",
    "    \n",
    "    eigvals_full, eigvecs_full = np.linalg.eigh(B)\n",
    "    eigvals_full = eigvals_full[::-1]\n",
    "    eigvecs_full = eigvecs_full[:, ::-1]\n",
    "    \n",
    "    # Take top 2 eigenvalues for 2D reconstruction\n",
    "    coords_patch = eigvecs_full[:, :2] @ np.diag(np.sqrt(np.maximum(eigvals_full[:2], 0)))\n",
    "    \n",
    "    # Now analyze 2D variance of these coordinates\n",
    "    X = coords_patch.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals_2d, eigvecs_2d = np.linalg.eigh(cov)\n",
    "    eigvals_2d = eigvals_2d[::-1]\n",
    "    \n",
    "    lam1, lam2 = eigvals_2d\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    \n",
    "    ratios_individual.append(ratio)\n",
    "    eigenvalues_list.append((lam1, lam2))\n",
    "    \n",
    "    print(f\"Miniset {i+1} ({data['n_points']} points):\")\n",
    "    print(f\"  λ1 = {lam1:.4f},  λ2 = {lam2:.4f}\")\n",
    "    print(f\"  λ1/λ2 = {ratio:.2f}\")\n",
    "    \n",
    "    if ratio < 5:\n",
    "        interpretation = \"→ GENUINELY 2D ✓\"\n",
    "    elif ratio < 20:\n",
    "        interpretation = \"→ Anisotropic but still 2D-ish\"\n",
    "    else:\n",
    "        interpretation = \"→ EFFECTIVELY 1D (very elongated) ✗\"\n",
    "    \n",
    "    print(f\"  {interpretation}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. GENERATE MANY MORE ST MINI-SUBSETS FOR STATISTICAL ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Statistical Analysis Over 200 Random ST Mini-Subsets ===\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "num_samples = 200\n",
    "ratios_stats = []\n",
    "eigenvalues_stats = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Sample random subset\n",
    "    n_total = st_coords.shape[0]\n",
    "    n = np.random.randint(n_min, min(n_max + 1, n_total))\n",
    "    \n",
    "    indices = torch.randperm(n_total, device=device)[:n]\n",
    "    miniset_coords = st_coords[indices].cpu().numpy()\n",
    "    \n",
    "    # Analyze 2D variance directly from coordinates\n",
    "    X = miniset_coords.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals, _ = np.linalg.eigh(cov)\n",
    "    eigvals = eigvals[::-1]\n",
    "    \n",
    "    lam1, lam2 = eigvals\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    \n",
    "    ratios_stats.append(ratio)\n",
    "    eigenvalues_stats.append((lam1, lam2))\n",
    "\n",
    "ratios_stats = np.array(ratios_stats)\n",
    "eigenvalues_stats = np.array(eigenvalues_stats)\n",
    "\n",
    "print(f\"Number of patches analyzed: {len(ratios_stats)}\")\n",
    "print(f\"\\nλ1/λ2 Anisotropy Ratio Statistics:\")\n",
    "print(f\"  Min:        {ratios_stats.min():.2f}\")\n",
    "print(f\"  25th %ile:  {np.percentile(ratios_stats, 25):.2f}\")\n",
    "print(f\"  Median:     {np.median(ratios_stats):.2f}\")\n",
    "print(f\"  75th %ile:  {np.percentile(ratios_stats, 75):.2f}\")\n",
    "print(f\"  95th %ile:  {np.percentile(ratios_stats, 95):.2f}\")\n",
    "print(f\"  Max:        {ratios_stats.max():.2f}\")\n",
    "print(f\"  Mean:       {ratios_stats.mean():.2f}\")\n",
    "print(f\"  Std:        {ratios_stats.std():.2f}\")\n",
    "\n",
    "print(f\"\\nEigenvalue Statistics:\")\n",
    "print(f\"  λ1 - Mean: {eigenvalues_stats[:, 0].mean():.4f}, Std: {eigenvalues_stats[:, 0].std():.4f}\")\n",
    "print(f\"  λ2 - Mean: {eigenvalues_stats[:, 1].mean():.4f}, Std: {eigenvalues_stats[:, 1].std():.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "median_ratio = np.median(ratios_stats)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"INTERPRETATION:\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if median_ratio < 5:\n",
    "    print(\"✓ ST minisets are GENUINELY 2D\")\n",
    "    print(\"  → Good for training 2D spatial reconstruction\")\n",
    "elif median_ratio < 20:\n",
    "    print(\"⚠ ST minisets are ANISOTROPIC but still 2D-ish\")\n",
    "    print(\"  → Some elongation present, but dimensionality is 2D\")\n",
    "else:\n",
    "    print(\"✗ ST minisets are EFFECTIVELY 1D (very elongated)\")\n",
    "    print(\"  → Warning: training on curved 1D strips, not 2D patches\")\n",
    "\n",
    "# Count how many are genuinely 2D\n",
    "genuinely_2d = (ratios_stats < 5).sum()\n",
    "anisotropic_2d = ((ratios_stats >= 5) & (ratios_stats < 20)).sum()\n",
    "effectively_1d = (ratios_stats >= 20).sum()\n",
    "\n",
    "print(f\"\\nDistribution:\")\n",
    "print(f\"  Genuinely 2D (λ1/λ2 < 5):      {genuinely_2d}/{num_samples} ({100*genuinely_2d/num_samples:.1f}%)\")\n",
    "print(f\"  Anisotropic 2D (5 ≤ λ1/λ2 < 20): {anisotropic_2d}/{num_samples} ({100*anisotropic_2d/num_samples:.1f}%)\")\n",
    "print(f\"  Effectively 1D (λ1/λ2 ≥ 20):    {effectively_1d}/{num_samples} ({100*effectively_1d/num_samples:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n=== Creating Visualizations ===\\n\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: Histogram of λ1/λ2 ratios\n",
    "ax = axes[0, 0]\n",
    "ax.hist(ratios_stats, bins=50, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "ax.axvline(np.median(ratios_stats), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {np.median(ratios_stats):.2f}')\n",
    "ax.axvline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axvline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "ax.set_xlabel('λ1/λ2 (Anisotropy Ratio)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution of Eigenvalue Ratios\\n(ST Mini-Subsets)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Log-scale histogram\n",
    "ax = axes[0, 1]\n",
    "ax.hist(np.log10(ratios_stats), bins=50, alpha=0.7, edgecolor='black', color='coral')\n",
    "ax.axvline(np.log10(np.median(ratios_stats)), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Median: {np.median(ratios_stats):.2f}')\n",
    "ax.axvline(np.log10(5), color='g', linestyle='--', linewidth=2, alpha=0.5, label='log₁₀(5)')\n",
    "ax.axvline(np.log10(20), color='orange', linestyle='--', linewidth=2, alpha=0.5, label='log₁₀(20)')\n",
    "ax.set_xlabel('log₁₀(λ1/λ2)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Log-Scale Distribution\\n(ST Mini-Subsets)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Scatter plot of λ1 vs λ2\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(eigenvalues_stats[:, 1], eigenvalues_stats[:, 0], \n",
    "                    c=ratios_stats, cmap='viridis', alpha=0.6, s=30)\n",
    "ax.plot([eigenvalues_stats[:, 1].min(), eigenvalues_stats[:, 1].max()],\n",
    "        [eigenvalues_stats[:, 1].min(), eigenvalues_stats[:, 1].max()],\n",
    "        'r--', linewidth=2, label='λ1 = λ2 (isotropic)')\n",
    "ax.set_xlabel('λ2 (Smaller Eigenvalue)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('λ1 (Larger Eigenvalue)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Eigenvalue Scatter Plot\\n(Color = λ1/λ2)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('λ1/λ2', fontsize=10)\n",
    "\n",
    "# Plot 4: CDF of ratios\n",
    "ax = axes[1, 1]\n",
    "sorted_ratios = np.sort(ratios_stats)\n",
    "cdf = np.arange(1, len(sorted_ratios) + 1) / len(sorted_ratios)\n",
    "ax.plot(sorted_ratios, cdf, linewidth=2, color='steelblue')\n",
    "ax.axvline(5, color='g', linestyle='--', linewidth=2, alpha=0.7, \n",
    "           label=f'2D threshold (5): {(ratios_stats < 5).sum()/num_samples*100:.1f}%')\n",
    "ax.axvline(20, color='orange', linestyle='--', linewidth=2, alpha=0.7, \n",
    "           label=f'1D threshold (20): {(ratios_stats >= 20).sum()/num_samples*100:.1f}%')\n",
    "ax.axhline(0.5, color='r', linestyle='--', linewidth=1, alpha=0.5, label='Median')\n",
    "ax.set_xlabel('λ1/λ2 (Anisotropy Ratio)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Cumulative Distribution Function\\n(ST Mini-Subsets)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(left=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "anisotropy_plot_path = os.path.join(output_dir, f'st_minisets_anisotropy_analysis_{timestamp}.png')\n",
    "# plt.savefig(anisotropy_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved anisotropy analysis plot: {anisotropy_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VISUALIZE EXAMPLE MINI-SUBSETS WITH DIFFERENT ANISOTROPIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Visualizing Example Mini-Subsets by Anisotropy ===\\n\")\n",
    "\n",
    "# Find examples of different anisotropy levels\n",
    "low_aniso_idx = np.where(ratios_stats < 3)[0]\n",
    "med_aniso_idx = np.where((ratios_stats >= 5) & (ratios_stats < 10))[0]\n",
    "high_aniso_idx = np.where(ratios_stats >= 20)[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "examples = [\n",
    "    (low_aniso_idx, \"Low Anisotropy (λ1/λ2 < 3)\", 0),\n",
    "    (med_aniso_idx, \"Medium Anisotropy (5 ≤ λ1/λ2 < 10)\", 1),\n",
    "    (high_aniso_idx, \"High Anisotropy (λ1/λ2 ≥ 20)\", 2)\n",
    "]\n",
    "\n",
    "for idx_array, title, ax_idx in examples:\n",
    "    if len(idx_array) > 0:\n",
    "        example_idx = idx_array[np.random.randint(len(idx_array))]\n",
    "        \n",
    "        # Generate this specific miniset again\n",
    "        np.random.seed(42 + example_idx)\n",
    "        torch.manual_seed(42 + example_idx)\n",
    "        \n",
    "        n = np.random.randint(n_min, min(n_max + 1, st_coords.shape[0]))\n",
    "        indices = torch.randperm(st_coords.shape[0], device=device)[:n]\n",
    "        miniset_coords = st_coords[indices].cpu().numpy()\n",
    "        \n",
    "        ratio = ratios_stats[example_idx]\n",
    "        \n",
    "        ax = axes[ax_idx]\n",
    "        ax.scatter(miniset_coords[:, 0], miniset_coords[:, 1], \n",
    "                  alpha=0.6, s=20, c='steelblue', edgecolors='black', linewidth=0.5)\n",
    "        ax.set_xlabel('Coordinate 1', fontsize=11)\n",
    "        ax.set_ylabel('Coordinate 2', fontsize=11)\n",
    "        ax.set_title(f'{title}\\nλ1/λ2 = {ratio:.2f}', fontsize=12, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_aspect('equal')\n",
    "    else:\n",
    "        ax = axes[ax_idx]\n",
    "        ax.text(0.5, 0.5, f'No examples found\\nfor {title}', \n",
    "               ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "examples_plot_path = os.path.join(output_dir, f'st_minisets_anisotropy_examples_{timestamp}.png')\n",
    "# plt.savefig(examples_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved example minisets plot: {examples_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANISOTROPY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# EFFECT OF CANONICALIZATION ON ST MINI-SUBSET GEOMETRY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CANONICALIZATION IMPACT ON 2D GEOMETRY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FOR DIFFERENT CANONICALIZATION METHODS\n",
    "# ============================================================================\n",
    "\n",
    "def center_only(coords):\n",
    "    \"\"\"Center coordinates to zero mean.\"\"\"\n",
    "    coords_centered = coords - coords.mean(axis=0, keepdims=True)\n",
    "    return coords_centered\n",
    "\n",
    "def center_and_scale_rms(coords):\n",
    "    \"\"\"Center and scale to unit RMS distance from origin.\"\"\"\n",
    "    coords_centered = coords - coords.mean(axis=0, keepdims=True)\n",
    "    rms = np.sqrt((coords_centered ** 2).sum() / coords_centered.shape[0])\n",
    "    coords_scaled = coords_centered / (rms + 1e-12)\n",
    "    return coords_scaled\n",
    "\n",
    "def center_and_scale_bbox(coords):\n",
    "    \"\"\"Center and scale to unit bounding box.\"\"\"\n",
    "    coords_centered = coords - coords.mean(axis=0, keepdims=True)\n",
    "    bbox_size = coords_centered.max(axis=0) - coords_centered.min(axis=0)\n",
    "    max_dim = bbox_size.max()\n",
    "    coords_scaled = coords_centered / (max_dim + 1e-12)\n",
    "    return coords_scaled\n",
    "\n",
    "def full_canonicalize(coords):\n",
    "    \"\"\"Full canonicalization: center, scale, rotate to principal axes.\"\"\"\n",
    "    # Center\n",
    "    coords_centered = coords - coords.mean(axis=0, keepdims=True)\n",
    "    \n",
    "    # Scale to unit RMS\n",
    "    rms = np.sqrt((coords_centered ** 2).sum() / coords_centered.shape[0])\n",
    "    coords_scaled = coords_centered / (rms + 1e-12)\n",
    "    \n",
    "    # Rotate to principal axes\n",
    "    cov = coords_scaled.T @ coords_scaled / (coords_scaled.shape[0] - 1)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    eigvecs = eigvecs[:, ::-1]  # Descending order\n",
    "    coords_rotated = coords_scaled @ eigvecs\n",
    "    \n",
    "    # Flip to positive quadrant\n",
    "    for d in range(coords_rotated.shape[1]):\n",
    "        if coords_rotated[:, d].sum() < 0:\n",
    "            coords_rotated[:, d] *= -1\n",
    "    \n",
    "    return coords_rotated\n",
    "\n",
    "def compute_anisotropy(coords):\n",
    "    \"\"\"Compute eigenvalue ratio λ1/λ2.\"\"\"\n",
    "    X = coords.astype(float)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)\n",
    "    cov = Xc.T @ Xc / (Xc.shape[0] - 1)\n",
    "    eigvals, _ = np.linalg.eigh(cov)\n",
    "    eigvals = eigvals[::-1]\n",
    "    lam1, lam2 = eigvals\n",
    "    ratio = lam1 / (lam2 + 1e-12)\n",
    "    return ratio, lam1, lam2\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE TEST DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Generating Test Dataset ===\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "num_samples = 200\n",
    "\n",
    "# Store results for each canonicalization method\n",
    "results = {\n",
    "    'raw': {'ratios': [], 'eigenvalues': []},\n",
    "    'center_only': {'ratios': [], 'eigenvalues': []},\n",
    "    'center_rms': {'ratios': [], 'eigenvalues': []},\n",
    "    'center_bbox': {'ratios': [], 'eigenvalues': []},\n",
    "    'full_canon': {'ratios': [], 'eigenvalues': []}\n",
    "}\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Sample random subset\n",
    "    n_total = st_coords.shape[0]\n",
    "    n = np.random.randint(n_min, min(n_max + 1, n_total))\n",
    "    \n",
    "    indices = torch.randperm(n_total, device=device)[:n]\n",
    "    coords_raw = st_coords[indices].cpu().numpy()\n",
    "    \n",
    "    # Test each canonicalization method\n",
    "    methods = {\n",
    "        'raw': coords_raw,\n",
    "        'center_only': center_only(coords_raw),\n",
    "        'center_rms': center_and_scale_rms(coords_raw),\n",
    "        'center_bbox': center_and_scale_bbox(coords_raw),\n",
    "        'full_canon': full_canonicalize(coords_raw)\n",
    "    }\n",
    "    \n",
    "    for method_name, coords_transformed in methods.items():\n",
    "        ratio, lam1, lam2 = compute_anisotropy(coords_transformed)\n",
    "        results[method_name]['ratios'].append(ratio)\n",
    "        results[method_name]['eigenvalues'].append((lam1, lam2))\n",
    "\n",
    "# Convert to arrays\n",
    "for method_name in results:\n",
    "    results[method_name]['ratios'] = np.array(results[method_name]['ratios'])\n",
    "    results[method_name]['eigenvalues'] = np.array(results[method_name]['eigenvalues'])\n",
    "\n",
    "# ============================================================================\n",
    "# STATISTICAL COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Number of patches analyzed: {num_samples}\\n\")\n",
    "print(\"=\"*70)\n",
    "print(\"ANISOTROPY STATISTICS (λ1/λ2) BY CANONICALIZATION METHOD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "method_labels = {\n",
    "    'raw': 'Raw Coordinates',\n",
    "    'center_only': 'Center Only',\n",
    "    'center_rms': 'Center + Unit RMS',\n",
    "    'center_bbox': 'Center + Unit BBox',\n",
    "    'full_canon': 'Full Canonicalization'\n",
    "}\n",
    "\n",
    "for method_name, label in method_labels.items():\n",
    "    ratios = results[method_name]['ratios']\n",
    "    eigenvalues = results[method_name]['eigenvalues']\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  λ1/λ2 Ratio:\")\n",
    "    print(f\"    Min:       {ratios.min():.2f}\")\n",
    "    print(f\"    Median:    {np.median(ratios):.2f}\")\n",
    "    print(f\"    Mean:      {ratios.mean():.2f}\")\n",
    "    print(f\"    Max:       {ratios.max():.2f}\")\n",
    "    print(f\"    Std:       {ratios.std():.2f}\")\n",
    "    \n",
    "    genuinely_2d = (ratios < 5).sum()\n",
    "    anisotropic_2d = ((ratios >= 5) & (ratios < 20)).sum()\n",
    "    effectively_1d = (ratios >= 20).sum()\n",
    "    \n",
    "    print(f\"  Distribution:\")\n",
    "    print(f\"    Genuinely 2D (<5):      {genuinely_2d}/{num_samples} ({100*genuinely_2d/num_samples:.1f}%)\")\n",
    "    print(f\"    Anisotropic (5-20):     {anisotropic_2d}/{num_samples} ({100*anisotropic_2d/num_samples:.1f}%)\")\n",
    "    print(f\"    Effectively 1D (≥20):   {effectively_1d}/{num_samples} ({100*effectively_1d/num_samples:.1f}%)\")\n",
    "    \n",
    "    print(f\"  Eigenvalues:\")\n",
    "    print(f\"    λ1 Mean: {eigenvalues[:, 0].mean():.4f}, Std: {eigenvalues[:, 0].std():.4f}\")\n",
    "    print(f\"    λ2 Mean: {eigenvalues[:, 1].mean():.4f}, Std: {eigenvalues[:, 1].std():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION: COMPARISON OF METHODS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Plot 1: Histogram comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['steelblue', 'coral', 'seagreen', 'purple', 'crimson']\n",
    "\n",
    "for idx, (method_name, label) in enumerate(method_labels.items()):\n",
    "    ax = axes[idx]\n",
    "    ratios = results[method_name]['ratios']\n",
    "    \n",
    "    ax.hist(ratios, bins=50, alpha=0.7, edgecolor='black', color=colors[idx])\n",
    "    ax.axvline(np.median(ratios), color='r', linestyle='--', linewidth=2, \n",
    "               label=f'Median: {np.median(ratios):.2f}')\n",
    "    ax.axvline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold')\n",
    "    ax.axvline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold')\n",
    "    \n",
    "    ax.set_xlabel('λ1/λ2', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Count', fontsize=11)\n",
    "    ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_xlim(0, min(100, ratios.max() + 5))\n",
    "\n",
    "# Hide the 6th subplot\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_hist_path = os.path.join(output_dir, f'canonicalization_comparison_histograms_{timestamp}.png')\n",
    "# plt.savefig(comparison_hist_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved comparison histograms: {comparison_hist_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "data_for_boxplot = [results[method]['ratios'] for method in method_labels.keys()]\n",
    "labels_short = ['Raw', 'Center\\nOnly', 'Center +\\nUnit RMS', 'Center +\\nUnit BBox', 'Full\\nCanon']\n",
    "\n",
    "bp = ax.boxplot(data_for_boxplot, labels=labels_short, patch_artist=True,\n",
    "                showmeans=True, meanline=True)\n",
    "\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.axhline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axhline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_ylabel('λ1/λ2 (Anisotropy Ratio)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Canonicalization Method', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Effect of Canonicalization on Anisotropy\\n(200 ST Mini-Subsets)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "boxplot_path = os.path.join(output_dir, f'canonicalization_boxplot_{timestamp}.png')\n",
    "# plt.savefig(boxplot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved boxplot comparison: {boxplot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: CDF comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "\n",
    "for method_name, label, color in zip(method_labels.keys(), method_labels.values(), colors):\n",
    "    ratios = results[method_name]['ratios']\n",
    "    sorted_ratios = np.sort(ratios)\n",
    "    cdf = np.arange(1, len(sorted_ratios) + 1) / len(sorted_ratios)\n",
    "    ax.plot(sorted_ratios, cdf, linewidth=2.5, label=label, color=color, alpha=0.8)\n",
    "\n",
    "ax.axvline(5, color='g', linestyle='--', linewidth=2, alpha=0.5, label='2D threshold (5)')\n",
    "ax.axvline(20, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='1D threshold (20)')\n",
    "\n",
    "ax.set_xlabel('λ1/λ2 (Anisotropy Ratio)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Cumulative Distribution Comparison\\n(Effect of Canonicalization)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.legend(fontsize=11, loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "cdf_path = os.path.join(output_dir, f'canonicalization_cdf_{timestamp}.png')\n",
    "# plt.savefig(cdf_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved CDF comparison: {cdf_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Eigenvalue scatter comparison (2x3 grid)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (method_name, label) in enumerate(method_labels.items()):\n",
    "    ax = axes[idx]\n",
    "    eigenvalues = results[method_name]['eigenvalues']\n",
    "    ratios = results[method_name]['ratios']\n",
    "    \n",
    "    scatter = ax.scatter(eigenvalues[:, 1], eigenvalues[:, 0], \n",
    "                        c=ratios, cmap='viridis', alpha=0.6, s=30, vmin=0, vmax=20)\n",
    "    \n",
    "    # Add diagonal line (isotropic reference)\n",
    "    min_val = min(eigenvalues[:, 1].min(), eigenvalues[:, 0].min())\n",
    "    max_val = max(eigenvalues[:, 1].max(), eigenvalues[:, 0].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', \n",
    "            linewidth=2, label='λ1 = λ2', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('λ2 (Smaller)', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('λ1 (Larger)', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if idx == 1:  # Add colorbar to middle plot\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('λ1/λ2', fontsize=10)\n",
    "\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "scatter_path = os.path.join(output_dir, f'canonicalization_eigenvalue_scatter_{timestamp}.png')\n",
    "# plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved eigenvalue scatter plots: {scatter_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# VISUAL EXAMPLE: SAME MINISET WITH DIFFERENT CANONICALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n=== Creating Visual Example ===\\n\")\n",
    "\n",
    "# Generate one example miniset\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "n = np.random.randint(n_min, min(n_max + 1, st_coords.shape[0]))\n",
    "indices = torch.randperm(st_coords.shape[0], device=device)[:n]\n",
    "coords_example_raw = st_coords[indices].cpu().numpy()\n",
    "\n",
    "example_coords = {\n",
    "    'Raw': coords_example_raw,\n",
    "    'Center Only': center_only(coords_example_raw),\n",
    "    'Center + RMS': center_and_scale_rms(coords_example_raw),\n",
    "    'Center + BBox': center_and_scale_bbox(coords_example_raw),\n",
    "    'Full Canon': full_canonicalize(coords_example_raw)\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (label, coords) in enumerate(example_coords.items()):\n",
    "    ax = axes[idx]\n",
    "    ratio, lam1, lam2 = compute_anisotropy(coords)\n",
    "    \n",
    "    ax.scatter(coords[:, 0], coords[:, 1], alpha=0.6, s=30, \n",
    "              c='steelblue', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Dimension 1', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Dimension 2', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{label}\\nλ1/λ2 = {ratio:.2f}, λ1={lam1:.3f}, λ2={lam2:.3f}', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "example_path = os.path.join(output_dir, f'canonicalization_visual_example_{timestamp}.png')\n",
    "# plt.savefig(example_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved visual example: {example_path}\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CANONICALIZATION IMPACT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for method_name, label in method_labels.items():\n",
    "    ratios = results[method_name]['ratios']\n",
    "    median_ratio = np.median(ratios)\n",
    "    pct_2d = (ratios < 5).sum() / num_samples * 100\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Median λ1/λ2: {median_ratio:.2f}\")\n",
    "    print(f\"  % Genuinely 2D: {pct_2d:.1f}%\")\n",
    "    \n",
    "    if median_ratio < 5:\n",
    "        print(f\"  → ✓ Remains GENUINELY 2D\")\n",
    "    elif median_ratio < 20:\n",
    "        print(f\"  → ⚠ Becomes more anisotropic but still 2D-ish\")\n",
    "    else:\n",
    "        print(f\"  → ✗ Becomes EFFECTIVELY 1D\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils_et as uet\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD RAW ST DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CANONICALIZATION IMPACT ON RAW ST DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "print(\"\\nLoading ST1 data...\")\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "\n",
    "# Get raw coordinates\n",
    "st_coords_raw = torch.tensor(st_meta_df[['coord_x', 'coord_y']].values, \n",
    "                             dtype=torch.float32)\n",
    "\n",
    "print(f\"Loaded {st_coords_raw.shape[0]} spots\")\n",
    "\n",
    "# ============================================================================\n",
    "# APPLY CANONICALIZATION (EXACTLY AS IN run_mouse_brain_2.py)\n",
    "# ============================================================================\n",
    "\n",
    "slide_ids = torch.zeros(st_coords_raw.shape[0], dtype=torch.long)\n",
    "\n",
    "st_coords_canon, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"\\nCanonicalization parameters:\")\n",
    "print(f\"  Mean (μ):        {st_mu[0].numpy()}\")\n",
    "print(f\"  Scale factor:    {st_scale[0].item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPUTE STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_stats(coords, label):\n",
    "    \"\"\"Compute comprehensive statistics for coordinates.\"\"\"\n",
    "    coords_np = coords.numpy() if torch.is_tensor(coords) else coords\n",
    "    \n",
    "    # Basic stats\n",
    "    mean = coords_np.mean(axis=0)\n",
    "    std = coords_np.std(axis=0)\n",
    "    min_vals = coords_np.min(axis=0)\n",
    "    max_vals = coords_np.max(axis=0)\n",
    "    \n",
    "    # RMS radius (after centering)\n",
    "    centered = coords_np - mean\n",
    "    rms = np.sqrt((centered ** 2).sum(axis=1).mean())\n",
    "    \n",
    "    # Pairwise distances\n",
    "    from scipy.spatial.distance import pdist\n",
    "    distances = pdist(coords_np)\n",
    "    \n",
    "    # Eigenvalue analysis\n",
    "    cov = np.cov(coords_np.T)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    eigvals = eigvals[::-1]\n",
    "    \n",
    "    print(f\"\\n{label}:\")\n",
    "    print(f\"  Shape: {coords_np.shape}\")\n",
    "    print(f\"  Mean: [{mean[0]:.4f}, {mean[1]:.4f}]\")\n",
    "    print(f\"  Std:  [{std[0]:.4f}, {std[1]:.4f}]\")\n",
    "    print(f\"  Min:  [{min_vals[0]:.4f}, {min_vals[1]:.4f}]\")\n",
    "    print(f\"  Max:  [{max_vals[0]:.4f}, {max_vals[1]:.4f}]\")\n",
    "    print(f\"  RMS radius (from mean): {rms:.4f}\")\n",
    "    print(f\"  Pairwise distances:\")\n",
    "    print(f\"    Mean:   {distances.mean():.4f}\")\n",
    "    print(f\"    Median: {np.median(distances):.4f}\")\n",
    "    print(f\"    p90:    {np.percentile(distances, 90):.4f}\")\n",
    "    print(f\"  Eigenvalues: λ1={eigvals[0]:.4f}, λ2={eigvals[1]:.4f}\")\n",
    "    print(f\"  Anisotropy (λ1/λ2): {eigvals[0]/(eigvals[1]+1e-12):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'min': min_vals,\n",
    "        'max': max_vals,\n",
    "        'rms': rms,\n",
    "        'distances': distances,\n",
    "        'eigvals': eigvals,\n",
    "        'eigvecs': eigvecs\n",
    "    }\n",
    "\n",
    "stats_raw = compute_stats(st_coords_raw, \"RAW COORDINATES\")\n",
    "stats_canon = compute_stats(st_coords_canon, \"CANONICALIZED COORDINATES\")\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CREATING VISUALIZATIONS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# ROW 1: SCATTER PLOTS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot 1: Raw coordinates\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.scatter(st_coords_raw[:, 0], st_coords_raw[:, 1], \n",
    "           alpha=0.5, s=10, c='steelblue', edgecolors='none')\n",
    "ax1.axhline(stats_raw['mean'][1], color='r', linestyle='--', linewidth=2, alpha=0.7, label='Mean Y')\n",
    "ax1.axvline(stats_raw['mean'][0], color='r', linestyle='--', linewidth=2, alpha=0.7, label='Mean X')\n",
    "ax1.set_xlabel('X (pixels)', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Y (pixels)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title(f'Raw Coordinates\\nMean: [{stats_raw[\"mean\"][0]:.1f}, {stats_raw[\"mean\"][1]:.1f}]', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 2: Canonicalized coordinates\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.scatter(st_coords_canon[:, 0], st_coords_canon[:, 1], \n",
    "           alpha=0.5, s=10, c='coral', edgecolors='none')\n",
    "ax2.axhline(0, color='r', linestyle='--', linewidth=2, alpha=0.7, label='Mean Y = 0')\n",
    "ax2.axvline(0, color='r', linestyle='--', linewidth=2, alpha=0.7, label='Mean X = 0')\n",
    "\n",
    "# Add RMS circle\n",
    "circle = plt.Circle((0, 0), 1.0, color='g', fill=False, \n",
    "                    linewidth=2, linestyle='--', alpha=0.7, label=f'RMS = 1.0')\n",
    "ax2.add_patch(circle)\n",
    "\n",
    "ax2.set_xlabel('X (canonicalized)', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Y (canonicalized)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title(f'Canonicalized Coordinates\\nRMS: {stats_canon[\"rms\"]:.4f}', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 3: Overlay comparison\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "# Normalize raw coords for visual comparison (same scale as canonical)\n",
    "raw_centered = st_coords_raw - st_coords_raw.mean(dim=0)\n",
    "raw_normalized = raw_centered / stats_raw['rms']\n",
    "ax3.scatter(raw_normalized[:, 0], raw_normalized[:, 1], \n",
    "           alpha=0.3, s=8, c='steelblue', edgecolors='none', label='Raw (normalized)')\n",
    "ax3.scatter(st_coords_canon[:, 0], st_coords_canon[:, 1], \n",
    "           alpha=0.3, s=8, c='coral', edgecolors='none', label='Canonical')\n",
    "ax3.set_xlabel('X (normalized scale)', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Y (normalized scale)', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Overlay: Raw vs Canonical\\n(both normalized to same scale)', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 4: Scale comparison bar chart\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "metrics = ['Mean X', 'Mean Y', 'RMS Radius', 'Max Distance']\n",
    "raw_vals = [abs(stats_raw['mean'][0]), abs(stats_raw['mean'][1]), \n",
    "           stats_raw['rms'], stats_raw['distances'].max()]\n",
    "canon_vals = [abs(stats_canon['mean'][0]), abs(stats_canon['mean'][1]), \n",
    "             stats_canon['rms'], stats_canon['distances'].max()]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax4.bar(x - width/2, raw_vals, width, label='Raw', color='steelblue', alpha=0.8)\n",
    "bars2 = ax4.bar(x + width/2, canon_vals, width, label='Canonical', color='coral', alpha=0.8)\n",
    "\n",
    "ax4.set_ylabel('Value', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Metric Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(metrics, rotation=45, ha='right', fontsize=9)\n",
    "ax4.legend(fontsize=10)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# ============================================================================\n",
    "# ROW 2: DISTANCE DISTRIBUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot 5: Raw distance distribution\n",
    "ax5 = fig.add_subplot(gs[1, 0])\n",
    "ax5.hist(stats_raw['distances'], bins=100, alpha=0.7, \n",
    "        edgecolor='black', color='steelblue')\n",
    "ax5.axvline(stats_raw['distances'].mean(), color='r', linestyle='--', \n",
    "           linewidth=2, label=f'Mean: {stats_raw[\"distances\"].mean():.1f}')\n",
    "ax5.axvline(np.median(stats_raw['distances']), color='g', linestyle='--', \n",
    "           linewidth=2, label=f'Median: {np.median(stats_raw[\"distances\"]):.1f}')\n",
    "ax5.set_xlabel('Pairwise Distance', fontsize=11, fontweight='bold')\n",
    "ax5.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Raw: Distance Distribution', fontsize=12, fontweight='bold')\n",
    "ax5.legend(fontsize=9)\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Canonical distance distribution\n",
    "ax6 = fig.add_subplot(gs[1, 1])\n",
    "ax6.hist(stats_canon['distances'], bins=100, alpha=0.7, \n",
    "        edgecolor='black', color='coral')\n",
    "ax6.axvline(stats_canon['distances'].mean(), color='r', linestyle='--', \n",
    "           linewidth=2, label=f'Mean: {stats_canon[\"distances\"].mean():.4f}')\n",
    "ax6.axvline(np.median(stats_canon['distances']), color='g', linestyle='--', \n",
    "           linewidth=2, label=f'Median: {np.median(stats_canon[\"distances\"]):.4f}')\n",
    "ax6.set_xlabel('Pairwise Distance', fontsize=11, fontweight='bold')\n",
    "ax6.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Canonical: Distance Distribution', fontsize=12, fontweight='bold')\n",
    "ax6.legend(fontsize=9)\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 7: Distance ratio histogram\n",
    "ax7 = fig.add_subplot(gs[1, 2])\n",
    "# Compute distance scaling factor\n",
    "scale_factor = stats_raw['rms']\n",
    "expected_ratio = scale_factor\n",
    "actual_ratio = stats_raw['distances'].mean() / stats_canon['distances'].mean()\n",
    "\n",
    "ax7.bar(['Scale Factor\\n(RMS)', 'Distance Ratio\\n(Mean)'], \n",
    "       [scale_factor, actual_ratio], \n",
    "       color=['steelblue', 'coral'], alpha=0.8, edgecolor='black')\n",
    "ax7.set_ylabel('Value', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Scaling Consistency Check', fontsize=12, fontweight='bold')\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (label, val) in enumerate(zip(['Scale Factor', 'Distance Ratio'], \n",
    "                                     [scale_factor, actual_ratio])):\n",
    "    ax7.text(i, val, f'{val:.4f}', ha='center', va='bottom', \n",
    "            fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot 8: CDF comparison\n",
    "ax8 = fig.add_subplot(gs[1, 3])\n",
    "raw_sorted = np.sort(stats_raw['distances'])\n",
    "canon_sorted = np.sort(stats_canon['distances'])\n",
    "raw_cdf = np.arange(1, len(raw_sorted) + 1) / len(raw_sorted)\n",
    "canon_cdf = np.arange(1, len(canon_sorted) + 1) / len(canon_sorted)\n",
    "\n",
    "ax8.plot(raw_sorted, raw_cdf, linewidth=2, color='steelblue', label='Raw', alpha=0.8)\n",
    "ax8.plot(canon_sorted, canon_cdf, linewidth=2, color='coral', label='Canonical', alpha=0.8)\n",
    "ax8.set_xlabel('Pairwise Distance', fontsize=11, fontweight='bold')\n",
    "ax8.set_ylabel('Cumulative Probability', fontsize=11, fontweight='bold')\n",
    "ax8.set_title('Distance CDF Comparison', fontsize=12, fontweight='bold')\n",
    "ax8.legend(fontsize=10)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# ============================================================================\n",
    "# ROW 3: EIGENVALUE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot 9: Eigenvalue scatter (raw)\n",
    "ax9 = fig.add_subplot(gs[2, 0])\n",
    "raw_centered_np = (st_coords_raw - st_coords_raw.mean(dim=0)).numpy()\n",
    "ax9.scatter(raw_centered_np[:, 0], raw_centered_np[:, 1], \n",
    "           alpha=0.4, s=10, c='steelblue', edgecolors='none')\n",
    "\n",
    "# Draw principal axes\n",
    "eigvecs_raw = stats_raw['eigvecs']\n",
    "origin = np.array([0, 0])\n",
    "for i in range(2):\n",
    "    direction = eigvecs_raw[:, -(i+1)] * np.sqrt(stats_raw['eigvals'][i]) * 3\n",
    "    ax9.arrow(origin[0], origin[1], direction[0], direction[1], \n",
    "             head_width=20, head_length=30, fc='red', ec='red', linewidth=2, alpha=0.7)\n",
    "    ax9.text(direction[0], direction[1], f'λ{i+1}={stats_raw[\"eigvals\"][i]:.1f}', \n",
    "            fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "ax9.set_xlabel('X (centered)', fontsize=11, fontweight='bold')\n",
    "ax9.set_ylabel('Y (centered)', fontsize=11, fontweight='bold')\n",
    "ax9.set_title(f'Raw: Principal Axes\\nλ1/λ2 = {stats_raw[\"eigvals\"][0]/stats_raw[\"eigvals\"][1]:.2f}', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax9.grid(True, alpha=0.3)\n",
    "ax9.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 10: Eigenvalue scatter (canonical)\n",
    "ax10 = fig.add_subplot(gs[2, 1])\n",
    "canon_np = st_coords_canon.numpy()\n",
    "ax10.scatter(canon_np[:, 0], canon_np[:, 1], \n",
    "            alpha=0.4, s=10, c='coral', edgecolors='none')\n",
    "\n",
    "# Draw principal axes\n",
    "eigvecs_canon = stats_canon['eigvecs']\n",
    "for i in range(2):\n",
    "    direction = eigvecs_canon[:, -(i+1)] * np.sqrt(stats_canon['eigvals'][i]) * 3\n",
    "    ax10.arrow(0, 0, direction[0], direction[1], \n",
    "              head_width=0.1, head_length=0.15, fc='red', ec='red', linewidth=2, alpha=0.7)\n",
    "    ax10.text(direction[0], direction[1], f'λ{i+1}={stats_canon[\"eigvals\"][i]:.3f}', \n",
    "             fontsize=10, fontweight='bold', color='red')\n",
    "\n",
    "ax10.set_xlabel('X (canonicalized)', fontsize=11, fontweight='bold')\n",
    "ax10.set_ylabel('Y (canonicalized)', fontsize=11, fontweight='bold')\n",
    "ax10.set_title(f'Canonical: Principal Axes\\nλ1/λ2 = {stats_canon[\"eigvals\"][0]/stats_canon[\"eigvals\"][1]:.2f}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax10.grid(True, alpha=0.3)\n",
    "ax10.set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Plot 11: Eigenvalue comparison\n",
    "ax11 = fig.add_subplot(gs[2, 2])\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax11.bar(x - width/2, stats_raw['eigvals'], width, \n",
    "                label='Raw', color='steelblue', alpha=0.8)\n",
    "bars2 = ax11.bar(x + width/2, stats_canon['eigvals'], width, \n",
    "                label='Canonical', color='coral', alpha=0.8)\n",
    "\n",
    "ax11.set_ylabel('Eigenvalue', fontsize=11, fontweight='bold')\n",
    "ax11.set_title('Eigenvalue Comparison', fontsize=12, fontweight='bold')\n",
    "ax11.set_xticks(x)\n",
    "ax11.set_xticklabels(['λ1', 'λ2'], fontsize=11)\n",
    "ax11.legend(fontsize=10)\n",
    "ax11.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax11.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 12: Summary text\n",
    "ax12 = fig.add_subplot(gs[2, 3])\n",
    "ax12.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "CANONICALIZATION SUMMARY\n",
    "\n",
    "Transformation Applied:\n",
    "  1. Center: X - μ\n",
    "  2. Scale: (X - μ) / RMS\n",
    "\n",
    "Parameters:\n",
    "  μ = [{st_mu[0, 0].item():.2f}, {st_mu[0, 1].item():.2f}]\n",
    "  RMS = {st_scale[0].item():.4f}\n",
    "\n",
    "Impact:\n",
    "  • Mean: {stats_raw['mean']} → {stats_canon['mean']}\n",
    "  • RMS: {stats_raw['rms']:.4f} → {stats_canon['rms']:.4f}\n",
    "  • Distances scaled by: 1/{st_scale[0].item():.4f}\n",
    "  \n",
    "Geometry Preserved:\n",
    "  • Anisotropy ratio unchanged:\n",
    "    Raw: {stats_raw['eigvals'][0]/stats_raw['eigvals'][1]:.2f}\n",
    "    Canon: {stats_canon['eigvals'][0]/stats_canon['eigvals'][1]:.2f}\n",
    "  \n",
    "  • Shape identical (similarity transform)\n",
    "  • Only scale changed\n",
    "\"\"\"\n",
    "\n",
    "ax12.text(0.1, 0.5, summary_text, fontsize=11, verticalalignment='center',\n",
    "         fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('Impact of canonicalize_st_coords_per_slide() on Raw ST Data', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "output_path = os.path.join(output_dir, f'canonicalization_impact_{timestamp}.png')\n",
    "# plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved visualization: {output_path}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CANONICALIZATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPLETE NOTEBOOK: SINGLE PATCH INFERENCE + FULL EVALUATION\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251128_100055\"\n",
    "checkpoint_path = f\"{output_dir}/phase2_sc_finetuned_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SINGLE PATCH INFERENCE (Diagnostic Mode)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "# Extract SC gene expression\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "print(f\"Ground truth coords shape: {scadata.obsm['spatial_gt'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL AND CHECKPOINT\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model and Checkpoint ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded checkpoint from: {checkpoint_path}\")\n",
    "print(f\"  Epochs trained: {checkpoint.get('epochs_finetune', 'N/A')}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: SINGLE PATCH INFERENCE (DIAGNOSTIC MODE)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Single Patch Inference ---\")\n",
    "print(f\"Config: patch_size={n_cells}, coverage_per_cell=1.0\")\n",
    "print(\"This runs ONE patch with ALL cells (no stitching)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "results = model.infer_sc_patchwise(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_timesteps_sample=600,\n",
    "    sigma_min=0.01,\n",
    "    sigma_max=7.0,\n",
    "    patch_size=n_cells,          # SINGLE PATCH MODE\n",
    "    coverage_per_cell=1.0,       # NO OVERLAP\n",
    "    n_align_iters=1,             # IRRELEVANT (only 1 patch)\n",
    "    eta=0.0,\n",
    "    guidance_scale=5.0,\n",
    "    return_coords=True,\n",
    "    debug_flag=True,\n",
    "    debug_every=10,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Inference complete\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: EXTRACT RAW EDM (NO PROJECTION, NO RESCALING)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Computing Raw EDM (No Post-Processing) ---\")\n",
    "\n",
    "# Extract canonicalized coordinates\n",
    "coords_canon = results['coords_canon'].cpu().numpy()\n",
    "\n",
    "# Compute RAW EDM directly from coordinates (NO edm_project, NO rescaling)\n",
    "gems_edm = cdist(coords_canon, coords_canon, metric='euclidean')\n",
    "\n",
    "print(f\"Raw EDM shape: {gems_edm.shape}\")\n",
    "print(f\"Raw EDM stats:\")\n",
    "print(f\"  Min: {gems_edm[gems_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gems_edm[gems_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gems_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gems_edm[gems_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: COMPUTE GROUND TRUTH EDM\n",
    "# ===================================================================\n",
    "print(\"\\n--- Calculating Ground Truth EDM ---\")\n",
    "\n",
    "gt_coords = scadata.obsm['spatial_gt']\n",
    "gt_edm = squareform(pdist(gt_coords, 'euclidean'))\n",
    "\n",
    "print(f\"Ground Truth EDM shape: {gt_edm.shape}\")\n",
    "print(f\"Ground Truth EDM stats:\")\n",
    "print(f\"  Min: {gt_edm[gt_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gt_edm[gt_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gt_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gt_edm[gt_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: NORMALIZE FOR COMPARISON\n",
    "# ===================================================================\n",
    "def normalize_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    return (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "gems_edm_norm = normalize_matrix(gems_edm)\n",
    "gt_edm_norm = normalize_matrix(gt_edm)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 7: QUANTITATIVE COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTITATIVE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract upper triangle (excluding diagonal)\n",
    "triu_indices = np.triu_indices(n_cells, k=1)\n",
    "gt_distances_flat = gt_edm[triu_indices]\n",
    "gems_distances_flat = gems_edm[triu_indices]\n",
    "\n",
    "# Scale alignment (median matching)\n",
    "scale = np.median(gt_distances_flat) / np.median(gems_distances_flat)\n",
    "gems_distances_flat_scaled = gems_distances_flat * scale\n",
    "\n",
    "print(f\"\\nScale factor (median matching): {scale:.4f}\")\n",
    "\n",
    "# Calculate correlations\n",
    "pearson_corr, _ = pearsonr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "spearman_corr, _ = spearmanr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "\n",
    "print(f\"\\nPearson Correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 8: VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "# --- PLOT 1: Side-by-Side Heatmaps ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('EDM Comparison: Ground Truth vs. GEMS (Single Patch, Raw EDM)', \n",
    "             fontsize=18, fontweight='bold')\n",
    "\n",
    "sample_size = min(838, n_cells)\n",
    "sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "im1 = axes[0].imshow(gt_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[0].set_title('Ground Truth EDM (Normalized)', fontsize=14)\n",
    "axes[0].set_xlabel('Cell Index (Sampled)')\n",
    "axes[0].set_ylabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im2 = axes[1].imshow(gems_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[1].set_title('GEMS Predicted EDM (Normalized)', fontsize=14)\n",
    "axes[1].set_xlabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 2: Distribution of Distances ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(gt_distances_flat, color=\"blue\", label='Ground Truth Distances', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "sns.histplot(gems_distances_flat_scaled, color=\"red\", label='GEMS Distances (Scaled)', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "ax.set_title('Distribution of Pairwise Distances (Single Patch Mode)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Distance', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 3: Scatter Plot of Distances ---\n",
    "sample_size_scatter = min(50000, len(gt_distances_flat))\n",
    "sample_indices_scatter = np.random.choice(len(gt_distances_flat), sample_size_scatter, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(\n",
    "    gt_distances_flat[sample_indices_scatter],\n",
    "    gems_distances_flat_scaled[sample_indices_scatter],\n",
    "    alpha=0.2, s=5, color='steelblue'\n",
    ")\n",
    "ax.set_title(f'GEMS vs. Ground Truth Distances (Single Patch)\\nSpearman ρ = {spearman_corr:.4f}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Ground Truth Pairwise Distance', fontsize=12)\n",
    "ax.set_ylabel('GEMS Pairwise Distance (Scaled)', fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims = [\n",
    "    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "    max(ax.get_xlim()[1], ax.get_ylim()[1]),\n",
    "]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.75, linewidth=2, zorder=0, label='Ideal Correlation')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 4: Coordinate Comparison ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Spatial Coordinates: Ground Truth vs. GEMS (Single Patch)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, color='blue')\n",
    "axes[0].set_title('Ground Truth Coordinates', fontsize=14)\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(coords_canon[:, 0], coords_canon[:, 1], s=5, alpha=0.6, color='red')\n",
    "axes[1].set_title('GEMS Predicted Coordinates', fontsize=14)\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Y', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 5: Distance Error Distribution ---\n",
    "distance_errors = np.abs(gt_distances_flat - gems_distances_flat_scaled)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(distance_errors, bins=100, kde=True, ax=ax, color='purple')\n",
    "ax.set_title('Distance Prediction Error Distribution', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Absolute Error |GT - GEMS|', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.axvline(np.median(distance_errors), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Median Error: {np.median(distance_errors):.4f}')\n",
    "ax.axvline(np.mean(distance_errors), color='g', linestyle='--', linewidth=2, \n",
    "           label=f'Mean Error: {np.mean(distance_errors):.4f}')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 9: SAVE RESULTS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Saving Results ---\")\n",
    "\n",
    "new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_suffix = f\"single_patch_{new_timestamp}\"\n",
    "\n",
    "results_processed = {\n",
    "    'D_edm': gems_edm,  # RAW EDM (no projection, no rescaling)\n",
    "    'coords': results['coords'].cpu().numpy(),\n",
    "    'coords_canon': coords_canon,\n",
    "    'n_cells': n_cells,\n",
    "    'timestamp': new_timestamp,\n",
    "    'mode': 'single_patch_no_projection',\n",
    "    'scale_factor': scale,\n",
    "    'pearson_corr': pearson_corr,\n",
    "    'spearman_corr': spearman_corr,\n",
    "    'model_config': {\n",
    "        'n_genes': n_genes,\n",
    "        'D_latent': 32,\n",
    "        'c_dim': 256,\n",
    "    }\n",
    "}\n",
    "\n",
    "processed_path = os.path.join(output_dir, f\"sc_inference_processed_{output_suffix}.pt\")\n",
    "# torch.save(results_processed, processed_path)\n",
    "# print(f\"✓ Saved: {processed_path}\")\n",
    "\n",
    "scadata.obsm['X_gems'] = coords_canon\n",
    "adata_path = os.path.join(output_dir, f\"scadata_with_gems_{output_suffix}.h5ad\")\n",
    "scadata.write_h5ad(adata_path)\n",
    "print(f\"✓ Saved: {adata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SINGLE PATCH DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"  Mode: Single patch (patch_size={n_cells})\")\n",
    "print(f\"  EDM: Raw (no projection, no rescaling)\")\n",
    "print(f\"  Pearson: {pearson_corr:.4f}\")\n",
    "print(f\"  Spearman: {spearman_corr:.4f}\")\n",
    "print(f\"  Scale factor: {scale:.4f}\")\n",
    "print(f\"  Output timestamp: {output_suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils_et as uet  # Ensure this is in your python path\n",
    "\n",
    "# 1. Load the Raw ST Data (Exact paths from your code)\n",
    "print(\"Loading ST Data...\")\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "# Load coords\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "raw_coords = st_meta_df[['coord_x', 'coord_y']].values\n",
    "st_coords_tensor = torch.tensor(raw_coords, dtype=torch.float32)\n",
    "\n",
    "# 2. Apply the EXACT normalization used in run_mouse_brain_2.py\n",
    "print(\"Applying Global RMS Normalization...\")\n",
    "# Dummy slide IDs (all 0) since you have single slide logic in the snippets\n",
    "slide_ids = torch.zeros(st_coords_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "# This is the function called in line 165 of run_mouse_brain_2.py\n",
    "norm_coords, mu, scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_tensor, slide_ids\n",
    ")\n",
    "\n",
    "norm_coords = norm_coords.numpy()\n",
    "print(f\"Normalization Scale Factor used: {scale[0].item():.4f}\")\n",
    "\n",
    "# 3. Calculate Statistics\n",
    "radii = np.sqrt(np.sum(norm_coords**2, axis=1))\n",
    "points_outside = np.sum(radii > 1.0)\n",
    "pct_outside = (points_outside / len(radii)) * 100\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Points: {len(radii)}\")\n",
    "print(f\"Points outside Unit Circle (Radius > 1.0): {points_outside}\")\n",
    "print(f\"Percentage outside: {pct_outside:.2f}%\")\n",
    "print(f\"Max Radius: {radii.max():.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot A: The Normalized Geometry\n",
    "axes[0].scatter(norm_coords[:, 0], norm_coords[:, 1], s=5, alpha=0.6, c='steelblue', label='ST Cells')\n",
    "# Draw the Unit Circle\n",
    "circle = plt.Circle((0, 0), 1.0, color='red', fill=False, linestyle='--', linewidth=2, label='Unit RMS Circle')\n",
    "axes[0].add_patch(circle)\n",
    "axes[0].set_title(f\"Normalized ST Data\\n({pct_outside:.1f}% points outside red circle)\", fontsize=14)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot B: Histogram of Radii\n",
    "sns.histplot(radii, bins=50, ax=axes[1], kde=True, color='purple')\n",
    "axes[1].axvline(1.0, color='red', linestyle='--', linewidth=2, label='Radius = 1.0')\n",
    "axes[1].set_title(\"Distribution of Radii from Center\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Distance from Center\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPLETE NOTEBOOK: ST-ONLY MODEL (PHASE 1) - SINGLE PATCH INFERENCE\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251128_100055\"\n",
    "\n",
    "# USE PHASE 1 CHECKPOINT (ST-ONLY, BEFORE SC FINE-TUNING)\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ST-ONLY MODEL INFERENCE (Phase 1, Single Patch)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "# Extract SC gene expression\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "print(f\"Ground truth coords shape: {scadata.obsm['spatial_gt'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL AND ST-ONLY CHECKPOINT (PHASE 1)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model and ST-Only Checkpoint (Phase 1) ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded ST-ONLY checkpoint from: {checkpoint_path}\")\n",
    "print(f\"  Best ST epoch: {checkpoint.get('E_ST_best', 'N/A')}\")\n",
    "print(f\"  This model was trained ONLY on ST data (NO SC fine-tuning)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: SINGLE PATCH INFERENCE (DIAGNOSTIC MODE)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Single Patch Inference (ST-Only Model) ---\")\n",
    "print(f\"Config: patch_size={n_cells}, coverage_per_cell=1.0, n_align_iters=1\")\n",
    "print(\"This runs ONE patch with ALL cells (no stitching)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "results = model.infer_sc_patchwise(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_timesteps_sample=600,\n",
    "    sigma_min=0.01,\n",
    "    sigma_max=7.0,\n",
    "    patch_size=n_cells,          # SINGLE PATCH MODE\n",
    "    coverage_per_cell=1.0,       # NO OVERLAP\n",
    "    n_align_iters=1,             # NO STITCHING (only 1 patch)\n",
    "    eta=0.0,\n",
    "    guidance_scale=5.0,\n",
    "    return_coords=True,\n",
    "    debug_flag=True,\n",
    "    debug_every=10,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Inference complete\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: EXTRACT RAW EDM (NO PROJECTION, NO RESCALING)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Computing Raw EDM (No Post-Processing) ---\")\n",
    "\n",
    "# Extract canonicalized coordinates\n",
    "coords_canon = results['coords_canon'].cpu().numpy()\n",
    "\n",
    "# Compute RAW EDM directly from coordinates (NO edm_project, NO rescaling)\n",
    "gems_edm = cdist(coords_canon, coords_canon, metric='euclidean')\n",
    "\n",
    "print(f\"Raw EDM shape: {gems_edm.shape}\")\n",
    "print(f\"Raw EDM stats:\")\n",
    "print(f\"  Min: {gems_edm[gems_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gems_edm[gems_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gems_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gems_edm[gems_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: COMPUTE GROUND TRUTH EDM\n",
    "# ===================================================================\n",
    "print(\"\\n--- Calculating Ground Truth EDM ---\")\n",
    "\n",
    "gt_coords = scadata.obsm['spatial_gt']\n",
    "gt_edm = squareform(pdist(gt_coords, 'euclidean'))\n",
    "\n",
    "print(f\"Ground Truth EDM shape: {gt_edm.shape}\")\n",
    "print(f\"Ground Truth EDM stats:\")\n",
    "print(f\"  Min: {gt_edm[gt_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gt_edm[gt_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gt_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gt_edm[gt_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: NORMALIZE FOR COMPARISON\n",
    "# ===================================================================\n",
    "def normalize_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    return (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "gems_edm_norm = normalize_matrix(gems_edm)\n",
    "gt_edm_norm = normalize_matrix(gt_edm)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 7: QUANTITATIVE COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTITATIVE COMPARISON (ST-ONLY MODEL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract upper triangle (excluding diagonal)\n",
    "triu_indices = np.triu_indices(n_cells, k=1)\n",
    "gt_distances_flat = gt_edm[triu_indices]\n",
    "gems_distances_flat = gems_edm[triu_indices]\n",
    "\n",
    "# Scale alignment (median matching)\n",
    "scale = np.median(gt_distances_flat) / np.median(gems_distances_flat)\n",
    "gems_distances_flat_scaled = gems_distances_flat * scale\n",
    "\n",
    "print(f\"\\nScale factor (median matching): {scale:.4f}\")\n",
    "\n",
    "# Calculate correlations\n",
    "pearson_corr, _ = pearsonr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "spearman_corr, _ = spearmanr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "\n",
    "print(f\"\\nPearson Correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 8: VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "# --- PLOT 1: Side-by-Side Heatmaps ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('EDM Comparison: Ground Truth vs. GEMS (ST-Only Model, Single Patch)', \n",
    "             fontsize=18, fontweight='bold')\n",
    "\n",
    "sample_size = min(838, n_cells)\n",
    "sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "im1 = axes[0].imshow(gt_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[0].set_title('Ground Truth EDM (Normalized)', fontsize=14)\n",
    "axes[0].set_xlabel('Cell Index (Sampled)')\n",
    "axes[0].set_ylabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im2 = axes[1].imshow(gems_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[1].set_title('GEMS Predicted EDM (ST-Only, Normalized)', fontsize=14)\n",
    "axes[1].set_xlabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 2: Distribution of Distances ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(gt_distances_flat, color=\"blue\", label='Ground Truth Distances', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "sns.histplot(gems_distances_flat_scaled, color=\"orange\", label='GEMS Distances (ST-Only, Scaled)', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "ax.set_title('Distribution of Pairwise Distances (ST-Only Model)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Distance', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 3: Scatter Plot of Distances ---\n",
    "sample_size_scatter = min(50000, len(gt_distances_flat))\n",
    "sample_indices_scatter = np.random.choice(len(gt_distances_flat), sample_size_scatter, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(\n",
    "    gt_distances_flat[sample_indices_scatter],\n",
    "    gems_distances_flat_scaled[sample_indices_scatter],\n",
    "    alpha=0.2, s=5, color='orange'\n",
    ")\n",
    "ax.set_title(f'GEMS vs. Ground Truth Distances (ST-Only Model)\\nSpearman ρ = {spearman_corr:.4f}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Ground Truth Pairwise Distance', fontsize=12)\n",
    "ax.set_ylabel('GEMS Pairwise Distance (Scaled)', fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims = [\n",
    "    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "    max(ax.get_xlim()[1], ax.get_ylim()[1]),\n",
    "]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.75, linewidth=2, zorder=0, label='Ideal Correlation')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 4: Coordinate Comparison ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Spatial Coordinates: Ground Truth vs. GEMS (ST-Only Model)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, color='blue')\n",
    "axes[0].set_title('Ground Truth Coordinates', fontsize=14)\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(coords_canon[:, 0], coords_canon[:, 1], s=5, alpha=0.6, color='orange')\n",
    "axes[1].set_title('GEMS Predicted Coordinates (ST-Only)', fontsize=14)\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Y', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 5: Distance Error Distribution ---\n",
    "distance_errors = np.abs(gt_distances_flat - gems_distances_flat_scaled)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(distance_errors, bins=100, kde=True, ax=ax, color='orange')\n",
    "ax.set_title('Distance Prediction Error Distribution (ST-Only Model)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Absolute Error |GT - GEMS|', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.axvline(np.median(distance_errors), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Median Error: {np.median(distance_errors):.4f}')\n",
    "ax.axvline(np.mean(distance_errors), color='g', linestyle='--', linewidth=2, \n",
    "           label=f'Mean Error: {np.mean(distance_errors):.4f}')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 9: SAVE RESULTS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Saving Results ---\")\n",
    "\n",
    "new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_suffix = f\"st_only_single_patch_{new_timestamp}\"\n",
    "\n",
    "results_processed = {\n",
    "    'D_edm': gems_edm,  # RAW EDM (no projection, no rescaling)\n",
    "    'coords': results['coords'].cpu().numpy(),\n",
    "    'coords_canon': coords_canon,\n",
    "    'n_cells': n_cells,\n",
    "    'timestamp': new_timestamp,\n",
    "    'mode': 'st_only_single_patch_no_projection',\n",
    "    'scale_factor': scale,\n",
    "    'pearson_corr': pearson_corr,\n",
    "    'spearman_corr': spearman_corr,\n",
    "    'model_config': {\n",
    "        'n_genes': n_genes,\n",
    "        'D_latent': 32,\n",
    "        'c_dim': 256,\n",
    "        'phase': 'ST-only (Phase 1)',\n",
    "    }\n",
    "}\n",
    "\n",
    "processed_path = os.path.join(output_dir, f\"sc_inference_processed_{output_suffix}.pt\")\n",
    "# torch.save(results_processed, processed_path)\n",
    "# print(f\"✓ Saved: {processed_path}\")\n",
    "\n",
    "scadata.obsm['X_gems_st_only'] = coords_canon\n",
    "adata_path = os.path.join(output_dir, f\"scadata_with_gems_{output_suffix}.h5ad\")\n",
    "scadata.write_h5ad(adata_path)\n",
    "print(f\"✓ Saved: {adata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ST-ONLY MODEL DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"  Model: ST-Only (Phase 1, BEFORE SC fine-tuning)\")\n",
    "print(f\"  Mode: Single patch (patch_size={n_cells})\")\n",
    "print(f\"  EDM: Raw (no projection, no rescaling)\")\n",
    "print(f\"  Pearson: {pearson_corr:.4f}\")\n",
    "print(f\"  Spearman: {spearman_corr:.4f}\")\n",
    "print(f\"  Scale factor: {scale:.4f}\")\n",
    "print(f\"  Output timestamp: {output_suffix}\")\n",
    "print(\"\\nThis tells you if ring collapse happens during:\")\n",
    "print(\"  - ST-only training (Phase 1) → if you see ring now\")\n",
    "print(\"  - SC fine-tuning (Phase 2) → if you saw ring only with fine-tuned model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TIMESTEP-BY-TIMESTEP DIFFUSION VISUALIZATION\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251128_100055\"\n",
    "checkpoint_path = f\"{output_dir}/phase2_sc_finetuned_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIFFUSION TIMESTEP VISUALIZATION (Single Patch)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "import utils_et as uet\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded checkpoint\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: INLINE DIFFUSION SAMPLER WITH TIMESTEP CAPTURE\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Diffusion with Timestep Capture ---\")\n",
    "\n",
    "device = 'cuda'\n",
    "n_timesteps_sample = 600\n",
    "sigma_min = 0.01\n",
    "sigma_max = 7.0\n",
    "guidance_scale = 2.0\n",
    "D_latent = 32\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "print(f\"Config: n_timesteps={n_timesteps_sample}, guidance_scale={guidance_scale}\")\n",
    "print(f\"        sigma_min={sigma_min}, sigma_max={sigma_max}\")\n",
    "\n",
    "# Encode all SC cells\n",
    "print(\"\\n[1/4] Encoding SC cells...\")\n",
    "with torch.no_grad():\n",
    "    Z_all = model.encoder(sc_expr.to(device))  # (n_cells, hidden_dim)\n",
    "    \n",
    "# Prepare context\n",
    "print(\"[2/4] Computing context...\")\n",
    "Z_batch = Z_all.unsqueeze(0)  # (1, n_cells, hidden_dim)\n",
    "mask = torch.ones(1, n_cells, dtype=torch.bool, device=device)\n",
    "H = model.context_encoder(Z_batch, mask)  # (1, n_cells, c_dim)\n",
    "\n",
    "# Sigma schedule\n",
    "sigmas = torch.exp(torch.linspace(\n",
    "    torch.log(torch.tensor(sigma_max, device=device)),\n",
    "    torch.log(torch.tensor(sigma_min, device=device)),\n",
    "    n_timesteps_sample,\n",
    "    device=device,\n",
    "))\n",
    "\n",
    "# Initialize noise\n",
    "print(\"[3/4] Running reverse diffusion...\")\n",
    "V_t = torch.randn(1, n_cells, D_latent, device=device) * sigmas[0]\n",
    "\n",
    "# Timesteps to save\n",
    "save_timesteps = [0, 100, 200, 300, 400, 500, 599]\n",
    "saved_samples = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t_idx in range(n_timesteps_sample):\n",
    "        sigma_t = sigmas[t_idx]\n",
    "        t_norm = torch.tensor([[t_idx / float(n_timesteps_sample - 1)]], device=device)\n",
    "        \n",
    "        # CFG sampling\n",
    "        H_null = torch.zeros_like(H)\n",
    "        eps_uncond = model.score_net(V_t, t_norm, H_null, mask)\n",
    "        eps_cond = model.score_net(V_t, t_norm, H, mask)\n",
    "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "        \n",
    "        # Update\n",
    "        if t_idx < n_timesteps_sample - 1:\n",
    "            sigma_next = sigmas[t_idx + 1]\n",
    "            V_0_pred = V_t - sigma_t * eps\n",
    "            V_t = V_0_pred + (sigma_next / sigma_t) * (V_t - V_0_pred)\n",
    "        else:\n",
    "            V_t = V_t - sigma_t * eps\n",
    "        \n",
    "        # Save at specific timesteps\n",
    "        if t_idx in save_timesteps:\n",
    "            # Canonicalize the current sample\n",
    "            V_canon = uet.canonicalize_coords(V_t.squeeze(0))\n",
    "            saved_samples[t_idx] = V_canon.cpu().numpy()\n",
    "            print(f\"  Saved timestep {t_idx}/{n_timesteps_sample-1}\")\n",
    "\n",
    "# Final sample\n",
    "V_final = V_t.squeeze(0)  # (n_cells, D_latent)\n",
    "V_final_canon = uet.canonicalize_coords(V_final)\n",
    "coords_final = V_final_canon.cpu().numpy()\n",
    "\n",
    "print(\"[4/4] Complete!\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: CONVERT TO 2D COORDINATES VIA MDS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Converting to 2D coordinates ---\")\n",
    "\n",
    "def latent_to_2d(V_latent):\n",
    "    \"\"\"Convert D_latent dimensional coordinates to 2D via MDS\"\"\"\n",
    "    n = V_latent.shape[0]\n",
    "    V_tensor = torch.tensor(V_latent, dtype=torch.float32)\n",
    "    \n",
    "    # Compute EDM from latent coordinates\n",
    "    D = torch.cdist(V_tensor, V_tensor)\n",
    "    \n",
    "    # Classical MDS\n",
    "    Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "    B = -0.5 * (Jn @ (D**2) @ Jn)\n",
    "    \n",
    "    # Extract 2D coordinates\n",
    "    coords_2d = uet.classical_mds(B, d_out=2).numpy()\n",
    "    coords_2d = uet.canonicalize_coords(torch.tensor(coords_2d)).numpy()\n",
    "    \n",
    "    return coords_2d\n",
    "\n",
    "coords_at_timesteps = {}\n",
    "for t_idx, V in saved_samples.items():\n",
    "    coords_at_timesteps[t_idx] = latent_to_2d(V)\n",
    "    print(f\"  Converted timestep {t_idx} to 2D\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: VISUALIZE DIFFUSION EVOLUTION\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "# Ground truth for reference\n",
    "gt_coords = scadata.obsm['spatial_gt']\n",
    "\n",
    "# Plot grid: GT + all saved timesteps\n",
    "n_plots = len(save_timesteps) + 1\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot ground truth\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, c='blue')\n",
    "axes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot diffusion timesteps\n",
    "for idx, t_idx in enumerate(save_timesteps):\n",
    "    ax = axes[idx + 1]\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    \n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=5, alpha=0.6, c='red')\n",
    "    ax.set_title(f'Timestep {t_idx}/{n_timesteps_sample-1}\\n(σ={sigmas[t_idx]:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_plots, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Diffusion Evolution (guidance_scale={guidance_scale}, n_timesteps={n_timesteps_sample})', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# ADDITIONAL PLOT: SIDE-BY-SIDE EVOLUTION\n",
    "# ===================================================================\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Top row: early timesteps\n",
    "for idx, t_idx in enumerate([0, 100, 200, 300]):\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    axes[0, idx].scatter(coords[:, 0], coords[:, 1], s=5, alpha=0.6, c='red')\n",
    "    axes[0, idx].set_title(f't={t_idx} (σ={sigmas[t_idx]:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[0, idx].set_aspect('equal')\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom row: late timesteps\n",
    "for idx, t_idx in enumerate([400, 500, 599]):\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    axes[1, idx].scatter(coords[:, 0], coords[:, 1], s=5, alpha=0.6, c='red')\n",
    "    axes[1, idx].set_title(f't={t_idx} (σ={sigmas[t_idx]:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[1, idx].set_aspect('equal')\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Ground truth in last position\n",
    "axes[1, 3].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, c='blue')\n",
    "axes[1, 3].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "axes[1, 3].set_aspect('equal')\n",
    "axes[1, 3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Diffusion Denoising Trajectory', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: QUANTIFY STRUCTURE COLLAPSE\n",
    "# ===================================================================\n",
    "print(\"\\n--- Analyzing Structure Collapse ---\")\n",
    "\n",
    "def compute_pca_variance_ratio(coords):\n",
    "    \"\"\"Compute variance explained by first 2 PCA components\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(coords)\n",
    "    return pca.explained_variance_ratio_\n",
    "\n",
    "def compute_circularity(coords):\n",
    "    \"\"\"Compute circularity score (higher = more ring-like)\"\"\"\n",
    "    center = coords.mean(axis=0)\n",
    "    radii = np.linalg.norm(coords - center, axis=1)\n",
    "    return 1.0 - (radii.std() / radii.mean())\n",
    "\n",
    "print(\"\\n{:<10} {:<15} {:<15} {:<15}\".format(\"Timestep\", \"PCA-1 Var\", \"PCA-2 Var\", \"Circularity\"))\n",
    "print(\"-\"*60)\n",
    "\n",
    "for t_idx in save_timesteps:\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    var_ratios = compute_pca_variance_ratio(coords)\n",
    "    circ = compute_circularity(coords)\n",
    "    print(f\"{t_idx:<10} {var_ratios[0]:<15.4f} {var_ratios[1]:<15.4f} {circ:<15.4f}\")\n",
    "\n",
    "# Ground truth\n",
    "gt_var_ratios = compute_pca_variance_ratio(gt_coords)\n",
    "gt_circ = compute_circularity(gt_coords)\n",
    "print(f\"{'GT':<10} {gt_var_ratios[0]:<15.4f} {gt_var_ratios[1]:<15.4f} {gt_circ:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIMESTEP ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "from core_models_et_p1 import STSetDataset, collate_minisets\n",
    "import utils_et as uet\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA (from run_mouse_brain_2.py)\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "st_ct     = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_celltype_et.csv'\n",
    "\n",
    "print(\"Loading ST1 (training ST data)...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "st_ct_df = pd.read_csv(st_ct, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "stadata.obs['celltype_mapped_refined'] = st_ct_df.idxmax(axis=1).values\n",
    "stadata.obsm['celltype_proportions'] = st_ct_df.values\n",
    "\n",
    "print(f\"ST1 loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# Extract expression and coordinates\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "# Apply per-slide canonicalization (same as training)\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST coords canonicalized: scale={st_scale[0].item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TRAINED ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "outdir = '/home/ehtesamul/sc_st/model/gems_mousebrain_output'\n",
    "checkpoint_path = os.path.join(outdir, 'ab_init.pt')\n",
    "\n",
    "n_genes = stadata.shape[1]\n",
    "\n",
    "# Create model with same config as run_mouse_brain_2.py\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=str(device),\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading checkpoint from: {checkpoint_path}\")\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "model.encoder.load_state_dict(ckpt['encoder'])\n",
    "model.encoder.eval()\n",
    "\n",
    "print(\"Encoder loaded and frozen.\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN STAGE B (takes ~3 seconds)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Running Stage B ===\")\n",
    "slides_dict = {0: (st_coords, st_expr)}\n",
    "model.train_stageB(\n",
    "    slides=slides_dict,\n",
    "    outdir='temp_stageB_cache'\n",
    ")\n",
    "\n",
    "print(\"Stage B complete. targets_dict populated.\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE SUPERVISED REGRESSION HEAD\n",
    "# ============================================================================\n",
    "\n",
    "# class SupervisedEDMHead(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Simple supervised head that predicts EDM from encoder embeddings.\n",
    "    \n",
    "#     Architecture:\n",
    "#     Z (from encoder) -> MLP -> upper triangular EDM prediction\n",
    "#     \"\"\"\n",
    "#     def __init__(self, h_dim: int, hidden_dim: int = 256):\n",
    "#         super().__init__()\n",
    "#         self.h_dim = h_dim\n",
    "        \n",
    "#         # MLP to predict pairwise distances\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(h_dim * 2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, Z: torch.Tensor, mask: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             Z: (batch, n, h_dim) encoder embeddings\n",
    "#             mask: (batch, n) validity mask\n",
    "            \n",
    "#         Returns:\n",
    "#             D_pred: (batch, n, n) predicted distance matrix\n",
    "#         \"\"\"\n",
    "#         batch, n, h = Z.shape\n",
    "        \n",
    "#         # Create pairwise concatenations\n",
    "#         Z_i = Z.unsqueeze(2).expand(-1, -1, n, -1)  # (batch, n, n, h)\n",
    "#         Z_j = Z.unsqueeze(1).expand(-1, n, -1, -1)  # (batch, n, n, h)\n",
    "#         Z_pairs = torch.cat([Z_i, Z_j], dim=-1)     # (batch, n, n, 2h)\n",
    "        \n",
    "#         # Predict distances\n",
    "#         D_pred = self.mlp(Z_pairs).squeeze(-1)      # (batch, n, n)\n",
    "#         D_pred = torch.relu(D_pred)                  # Ensure non-negative\n",
    "        \n",
    "#         # Symmetrize\n",
    "#         D_pred = (D_pred + D_pred.transpose(-1, -2)) / 2.0\n",
    "        \n",
    "#         # Zero out diagonal\n",
    "#         diag_mask = torch.eye(n, device=Z.device).unsqueeze(0).bool()\n",
    "#         D_pred = D_pred.masked_fill(diag_mask, 0.0)\n",
    "        \n",
    "#         # Apply validity mask\n",
    "#         valid_mask = mask.unsqueeze(-1) & mask.unsqueeze(-2)\n",
    "#         D_pred = D_pred * valid_mask.float()\n",
    "        \n",
    "#         return D_pred\n",
    "    \n",
    "class SupervisedCoordHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple supervised head that predicts 2D coordinates from encoder embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, h_dim: int, hidden_dim: int = 256, D_out: int = 2):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(h_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, D_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, Z: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Z: (batch, n, h_dim) encoder embeddings\n",
    "            mask: (batch, n) validity mask\n",
    "            \n",
    "        Returns:\n",
    "            coords: (batch, n, 2) predicted coordinates\n",
    "        \"\"\"\n",
    "        coords = self.mlp(Z)                    # (batch, n, 2)\n",
    "        coords = coords * mask.unsqueeze(-1)    # zero out padded entries\n",
    "        return coords\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATASET AND DATALOADER\n",
    "# ============================================================================\n",
    "\n",
    "# Create ST miniset dataset (same as Stage C training)\n",
    "st_gene_expr_dict_cpu = {0: st_expr.cpu()}\n",
    "\n",
    "st_dataset = STSetDataset(\n",
    "    targets_dict=model.targets_dict,\n",
    "    encoder=model.encoder,\n",
    "    st_gene_expr_dict=st_gene_expr_dict_cpu,\n",
    "    n_min=64,\n",
    "    n_max=384,\n",
    "    D_latent=model.D_latent,\n",
    "    num_samples=4000,  # Same as run_mouse_brain_2.py\n",
    "    knn_k=12,\n",
    "    device=device,\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "st_loader = DataLoader(\n",
    "    st_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_minisets\n",
    ")\n",
    "\n",
    "print(f\"ST dataset created: {len(st_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE SUPERVISED HEAD\n",
    "# ============================================================================\n",
    "\n",
    "# h_dim = model.encoder.fc_list[-1].out_features  # Get encoder output dim\n",
    "# Get encoder output dimension by doing a forward pass\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, n_genes, device=device)\n",
    "    h_dim = model.encoder(dummy_input).shape[-1]\n",
    "# supervised_head = SupervisedEDMHead(h_dim=h_dim, hidden_dim=256).to(device)\n",
    "\n",
    "supervised_head = SupervisedCoordHead(h_dim=h_dim, hidden_dim=256).to(device)\n",
    "\n",
    "optimizer = optim.Adam(supervised_head.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "print(f\"\\nSupervised head initialized: h_dim={h_dim}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "num_epochs = 50\n",
    "loss_history = []\n",
    "\n",
    "print(\"\\n=== Training Supervised Baseline ===\\n\")\n",
    "\n",
    "supervised_head.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(st_loader):\n",
    "        # Move batch to device\n",
    "        Z = batch['Z_set'].to(device)              # (batch, n, h)\n",
    "        mask = batch['mask'].to(device)            # (batch, n)\n",
    "        D_target = batch['D_target'].to(device)    # (batch, n, n)\n",
    "        \n",
    "        # # Forward pass\n",
    "        # D_pred = supervised_head(Z, mask)\n",
    "        \n",
    "        # # Loss: MSE on valid EDM entries\n",
    "        # valid_mask = mask.unsqueeze(-1) & mask.unsqueeze(-2)\n",
    "        # loss = ((D_pred - D_target) ** 2 * valid_mask.float()).sum() / valid_mask.float().sum()\n",
    "        \n",
    "        # Forward pass\n",
    "        coords_pred = supervised_head(Z, mask)  # (batch, n, 2)\n",
    "\n",
    "        # Compute EDM from predicted coords\n",
    "        D_pred = torch.cdist(coords_pred, coords_pred)  # (batch, n, n)\n",
    "\n",
    "        # Loss: MSE on valid EDM entries\n",
    "        valid_mask = mask.unsqueeze(-1) & mask.unsqueeze(-2)\n",
    "        loss = ((D_pred - D_target) ** 2 * valid_mask.float()).sum() / valid_mask.float().sum()\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n=== Training Complete ===\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE: SAMPLE A FEW MINISETS AND CHECK GEOMETRY\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_head.eval()\n",
    "\n",
    "print(\"=== Evaluating Supervised Baseline ===\\n\")\n",
    "\n",
    "num_eval_samples = 5\n",
    "eval_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_iter = iter(st_loader)\n",
    "    \n",
    "    for i in range(num_eval_samples):\n",
    "        batch = next(eval_iter)\n",
    "        \n",
    "        Z = batch['Z_set'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        D_target = batch['D_target'].to(device)\n",
    "        \n",
    "        # Predict coordinates directly\n",
    "        coords_pred = supervised_head(Z, mask)\n",
    "        \n",
    "        # Take first sample in batch\n",
    "        b = 0\n",
    "        m = mask[b]\n",
    "        n_valid = m.sum().item()\n",
    "        \n",
    "        coords_pred_sample = coords_pred[b, m].cpu()\n",
    "        D_target_sample = D_target[b, m][:, m].cpu()\n",
    "        \n",
    "        # Compute MDS from target EDM (ground truth)\n",
    "        n = D_target_sample.shape[0]\n",
    "        Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "        B_target = -0.5 * (Jn @ (D_target_sample ** 2) @ Jn)\n",
    "        coords_target = uet.classical_mds(B_target, d_out=2)\n",
    "        \n",
    "        # Canonicalize both\n",
    "        coords_pred_canon = uet.canonicalize_coords(coords_pred_sample)\n",
    "        coords_target_canon = uet.canonicalize_coords(coords_target)\n",
    "        \n",
    "        # Compute correlation\n",
    "        corr_x = np.corrcoef(coords_pred_canon[:, 0].numpy(), coords_target_canon[:, 0].numpy())[0, 1]\n",
    "        corr_y = np.corrcoef(coords_pred_canon[:, 1].numpy(), coords_target_canon[:, 1].numpy())[0, 1]\n",
    "        avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "        \n",
    "        # EDM correlation\n",
    "        D_pred_sample = torch.cdist(coords_pred_sample.unsqueeze(0), coords_pred_sample.unsqueeze(0)).squeeze(0)\n",
    "        edm_corr = np.corrcoef(\n",
    "            D_pred_sample.flatten().numpy(),\n",
    "            D_target_sample.flatten().numpy()\n",
    "        )[0, 1]\n",
    "        \n",
    "        eval_results.append({\n",
    "            'sample': i,\n",
    "            'n_points': n_valid,\n",
    "            'corr_x': corr_x,\n",
    "            'corr_y': corr_y,\n",
    "            'avg_corr': avg_corr,\n",
    "            'edm_corr': edm_corr,\n",
    "            'coords_pred': coords_pred_canon.numpy(),\n",
    "            'coords_target': coords_target_canon.numpy()\n",
    "        })\n",
    "        \n",
    "        print(f\"Sample {i}: n={n_valid:3d} | EDM_corr={edm_corr:.4f} | \"\n",
    "              f\"Coord_corr: x={corr_x:.4f}, y={corr_y:.4f}, avg={avg_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLOT GROUND TRUTH VS PREDICTED COORDINATES\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, num_eval_samples, figsize=(4*num_eval_samples, 8))\n",
    "\n",
    "for i, res in enumerate(eval_results):\n",
    "    # Predicted coordinates\n",
    "    axes[0, i].scatter(res['coords_pred'][:, 0], res['coords_pred'][:, 1], \n",
    "                      s=10, alpha=0.6, c='blue')\n",
    "    axes[0, i].set_title(f\"Sample {i}: Predicted\\ncorr={res['avg_corr']:.3f}\")\n",
    "    axes[0, i].set_aspect('equal')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ground truth coordinates\n",
    "    axes[1, i].scatter(res['coords_target'][:, 0], res['coords_target'][:, 1],\n",
    "                      s=10, alpha=0.6, c='red')\n",
    "    axes[1, i].set_title(f\"Ground Truth\")\n",
    "    axes[1, i].set_aspect('equal')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('supervised_baseline_coords_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPLETE NOTEBOOK: ST-ONLY MODEL (PHASE 1) - SINGLE PATCH INFERENCE\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "# timestamp = \"20251125_105556\"\n",
    "timestamp = \"20251125_105556\"\n",
    "\n",
    "\n",
    "# USE PHASE 1 CHECKPOINT (ST-ONLY, BEFORE SC FINE-TUNING)\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ST-ONLY MODEL INFERENCE (Phase 1, Single Patch)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "# Extract SC gene expression\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "print(f\"Ground truth coords shape: {scadata.obsm['spatial_gt'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL AND ST-ONLY CHECKPOINT (PHASE 1)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model and ST-Only Checkpoint (Phase 1) ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded ST-ONLY checkpoint from: {checkpoint_path}\")\n",
    "print(f\"  Best ST epoch: {checkpoint.get('E_ST_best', 'N/A')}\")\n",
    "print(f\"  This model was trained ONLY on ST data (NO SC fine-tuning)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: SINGLE PATCH INFERENCE (DIAGNOSTIC MODE)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Single Patch Inference (ST-Only Model) ---\")\n",
    "print(f\"Config: patch_size={n_cells}, coverage_per_cell=1.0, n_align_iters=1\")\n",
    "print(\"This runs ONE patch with ALL cells (no stitching)\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIFFUSION INFERENCE ON ST MINISETS - COMPLETE CODE\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "from core_models_et_p1 import STSetDataset, collate_minisets\n",
    "import utils_et as uet\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = '/home/ehtesamul/sc_st/model/gems_mousebrain_output'\n",
    "checkpoint_path = os.path.join(output_dir, 'phase1_st_checkpoint.pt')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFUSION MODEL INFERENCE ON ST MINISETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ST DATA\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "st_ct     = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_celltype_et.csv'\n",
    "\n",
    "print(\"\\nLoading ST1 data...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "st_ct_df = pd.read_csv(st_ct, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "stadata.obs['celltype_mapped_refined'] = st_ct_df.idxmax(axis=1).values\n",
    "\n",
    "print(f\"ST1 loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# Extract and canonicalize\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST coords canonicalized: scale={st_scale[0].item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL AND PHASE 1 CHECKPOINT\n",
    "# ============================================================================\n",
    "\n",
    "n_genes = stadata.shape[1]\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=str(device),\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded Phase 1 ST-only checkpoint\")\n",
    "print(f\"  Best ST epoch: {checkpoint.get('E_ST_best', 'N/A')}\")\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "# ============================================================================\n",
    "# RUN STAGE B TO GET TARGETS_DICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Running Stage B ===\")\n",
    "slides_dict = {0: (st_coords, st_expr)}\n",
    "model.train_stageB(\n",
    "    slides=slides_dict,\n",
    "    outdir='temp_stageB_cache'\n",
    ")\n",
    "print(\"Stage B complete.\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE ST MINISET DATASET\n",
    "# ============================================================================\n",
    "\n",
    "st_gene_expr_dict_cpu = {0: st_expr.cpu()}\n",
    "\n",
    "st_dataset = STSetDataset(\n",
    "    targets_dict=model.targets_dict,\n",
    "    encoder=model.encoder,\n",
    "    st_gene_expr_dict=st_gene_expr_dict_cpu,\n",
    "    n_min=64,\n",
    "    n_max=384,\n",
    "    D_latent=model.D_latent,\n",
    "    num_samples=4000,\n",
    "    knn_k=12,\n",
    "    device=device,\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "st_loader = DataLoader(\n",
    "    st_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_minisets\n",
    ")\n",
    "\n",
    "print(f\"ST dataset created: {len(st_dataset)} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN DIFFUSION INFERENCE ON ST MINISETS\n",
    "# ============================================================================\n",
    "\n",
    "num_eval_samples = 5\n",
    "diffusion_results = []\n",
    "\n",
    "print(\"\\n--- Running diffusion inference on ST minisets ---\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_iter = iter(st_loader)\n",
    "    \n",
    "    for i in range(num_eval_samples):\n",
    "        batch = next(eval_iter)\n",
    "        \n",
    "        mask = batch['mask'].to(device)\n",
    "        D_target = batch['D_target'].to(device)\n",
    "        \n",
    "        # Take first sample in batch\n",
    "        b = 0\n",
    "        m = mask[b]\n",
    "        n_valid = m.sum().item()\n",
    "        \n",
    "        # Get indices for this miniset\n",
    "        indices = batch['overlap_info']['indices'][b]\n",
    "        valid_indices = indices[m].cpu()\n",
    "        \n",
    "        # Get gene expression for these specific ST spots\n",
    "        miniset_expr = st_expr.cpu()[valid_indices]\n",
    "        \n",
    "        print(f\"Sample {i}: Running diffusion inference on {n_valid} points...\")\n",
    "        \n",
    "        # Run patchwise inference with single patch (no stitching)\n",
    "        inf_results = model.infer_sc_patchwise(\n",
    "            sc_gene_expr=miniset_expr,\n",
    "            n_timesteps_sample=300,\n",
    "            sigma_min=0.01,\n",
    "            sigma_max=7.0,\n",
    "            patch_size=n_valid,          # Single patch = all points\n",
    "            coverage_per_cell=1.0,       # No overlap\n",
    "            n_align_iters=1,             # No stitching\n",
    "            eta=0.0,\n",
    "            guidance_scale=6.0,\n",
    "            return_coords=True,\n",
    "            debug_flag=False,\n",
    "            debug_every=10,\n",
    "        )\n",
    "        \n",
    "        # Extract predicted coordinates\n",
    "        coords_diffusion = inf_results['coords_canon']\n",
    "        \n",
    "        # Get ground truth coordinates from target EDM\n",
    "        D_target_sample = D_target[b, m][:, m].cpu()\n",
    "        n = D_target_sample.shape[0]\n",
    "        Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "        B_target = -0.5 * (Jn @ (D_target_sample ** 2) @ Jn)\n",
    "        coords_target = uet.classical_mds(B_target, d_out=2)\n",
    "        coords_target_canon = uet.canonicalize_coords(coords_target)\n",
    "        \n",
    "        # Compute correlations\n",
    "        corr_x = np.corrcoef(coords_diffusion[:, 0].numpy(), coords_target_canon[:, 0].numpy())[0, 1]\n",
    "        corr_y = np.corrcoef(coords_diffusion[:, 1].numpy(), coords_target_canon[:, 1].numpy())[0, 1]\n",
    "        avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "        \n",
    "        # EDM correlation\n",
    "        D_diffusion = torch.cdist(coords_diffusion.unsqueeze(0), coords_diffusion.unsqueeze(0)).squeeze(0)\n",
    "        edm_corr = np.corrcoef(\n",
    "            D_diffusion.flatten().numpy(),\n",
    "            D_target_sample.flatten().numpy()\n",
    "        )[0, 1]\n",
    "        \n",
    "        diffusion_results.append({\n",
    "            'sample': i,\n",
    "            'n_points': n_valid,\n",
    "            'corr_x': corr_x,\n",
    "            'corr_y': corr_y,\n",
    "            'avg_corr': avg_corr,\n",
    "            'edm_corr': edm_corr,\n",
    "            'coords_diffusion': coords_diffusion.numpy(),\n",
    "            'coords_target': coords_target_canon.numpy()\n",
    "        })\n",
    "        \n",
    "        print(f\"  EDM_corr={edm_corr:.4f} | Coord_corr: x={corr_x:.4f}, y={corr_y:.4f}, avg={avg_corr:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFUSION INFERENCE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PRINT COMPARISON (assuming eval_results from supervised baseline exists)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nDiffusion Model (Phase 1 ST-only) Results:\")\n",
    "print(f\"  Average EDM correlation:   {np.mean([r['edm_corr'] for r in diffusion_results]):.4f}\")\n",
    "print(f\"  Average Coord correlation: {np.mean([r['avg_corr'] for r in diffusion_results]):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT: DIFFUSION vs GROUND TRUTH\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, num_eval_samples, figsize=(4*num_eval_samples, 8))\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    # Diffusion prediction\n",
    "    axes[0, i].scatter(diffusion_results[i]['coords_diffusion'][:, 0],\n",
    "                      diffusion_results[i]['coords_diffusion'][:, 1],\n",
    "                      s=10, alpha=0.6, c='green')\n",
    "    axes[0, i].set_title(f\"Sample {i}: Diffusion\\ncorr={diffusion_results[i]['avg_corr']:.3f}\")\n",
    "    axes[0, i].set_aspect('equal')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[1, i].scatter(diffusion_results[i]['coords_target'][:, 0],\n",
    "                      diffusion_results[i]['coords_target'][:, 1],\n",
    "                      s=10, alpha=0.6, c='red')\n",
    "    axes[1, i].set_title(f\"Ground Truth\")\n",
    "    axes[1, i].set_aspect('equal')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('diffusion_vs_groundtruth.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Plot saved: diffusion_vs_groundtruth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIFFUSION INFERENCE ON ST MINISETS - FIXED\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "import utils_et as uet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = '/home/ehtesamul/sc_st/model/gems_mousebrain_output'\n",
    "# checkpoint_path = os.path.join(output_dir, 'phase2_sc_finetuned_checkpoint.pt')\n",
    "checkpoint_path = os.path.join(output_dir, 'phase2_sc_finetuned_checkpoint.pt')\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFUSION MODEL INFERENCE ON ST MINISETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ST DATA\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "print(\"\\nLoading ST1 data...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "n_genes = stadata.shape[1]\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=str(device),\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded Phase 1 checkpoint (best epoch: {checkpoint.get('E_ST_best', 'N/A')})\")\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLE ST MINISETS AND RUN DIFFUSION INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "num_eval_samples = 10\n",
    "diffusion_results = []\n",
    "\n",
    "print(\"\\n--- Running diffusion inference on ST minisets ---\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    # Sample random miniset (same logic as STSetDataset)\n",
    "    n_min, n_max = 192, 384\n",
    "    n_total = st_coords.shape[0]\n",
    "    \n",
    "    # Random subset size\n",
    "    n = np.random.randint(n_min, min(n_max + 1, n_total))\n",
    "    \n",
    "    # Random indices\n",
    "    indices = torch.randperm(n_total)[:n]\n",
    "    \n",
    "    # Get gene expression and coords for this miniset\n",
    "    miniset_expr = st_expr[indices].cpu()\n",
    "    miniset_coords = st_coords[indices].cpu()\n",
    "    \n",
    "    # Compute ground truth EDM\n",
    "    D_target = torch.cdist(miniset_coords, miniset_coords)\n",
    "    \n",
    "    print(f\"Sample {i}: Running diffusion on {n} points...\")\n",
    "    \n",
    "    # Run inference with single patch (no stitching)\n",
    "    with torch.no_grad():\n",
    "        inf_results = model.infer_sc_patchwise(\n",
    "            sc_gene_expr=miniset_expr,\n",
    "            n_timesteps_sample=300,\n",
    "            sigma_min=0.01,\n",
    "            sigma_max=7.0,\n",
    "            patch_size=n,            # Single patch\n",
    "            coverage_per_cell=1.0,   # No overlap\n",
    "            n_align_iters=1,         # No alignment\n",
    "            eta=0.0,\n",
    "            guidance_scale=6.0,\n",
    "            return_coords=True,\n",
    "            debug_flag=False,\n",
    "        )\n",
    "    \n",
    "    coords_diffusion = inf_results['coords_canon']\n",
    "    \n",
    "    # Ground truth coords via MDS\n",
    "    Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "    B_target = -0.5 * (Jn @ (D_target**2) @ Jn)\n",
    "    coords_target = uet.classical_mds(B_target, d_out=2)\n",
    "    coords_target_canon = uet.canonicalize_coords(coords_target)\n",
    "    \n",
    "    # Compute correlations\n",
    "    corr_x = np.corrcoef(coords_diffusion[:, 0].numpy(), coords_target_canon[:, 0].numpy())[0, 1]\n",
    "    corr_y = np.corrcoef(coords_diffusion[:, 1].numpy(), coords_target_canon[:, 1].numpy())[0, 1]\n",
    "    avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "    \n",
    "    # EDM correlation\n",
    "    D_diffusion = torch.cdist(coords_diffusion.unsqueeze(0), coords_diffusion.unsqueeze(0)).squeeze(0)\n",
    "    edm_corr = np.corrcoef(\n",
    "        D_diffusion.flatten().numpy(),\n",
    "        D_target.flatten().numpy()\n",
    "    )[0, 1]\n",
    "    \n",
    "    diffusion_results.append({\n",
    "        'sample': i,\n",
    "        'n_points': n,\n",
    "        'corr_x': corr_x,\n",
    "        'corr_y': corr_y,\n",
    "        'avg_corr': avg_corr,\n",
    "        'edm_corr': edm_corr,\n",
    "        'coords_diffusion': coords_diffusion.numpy(),\n",
    "        'coords_target': coords_target_canon.numpy()\n",
    "    })\n",
    "    \n",
    "    print(f\"  EDM_corr={edm_corr:.4f} | Coord: x={corr_x:.4f}, y={corr_y:.4f}, avg={avg_corr:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDiffusion Results (avg over {num_eval_samples} samples):\")\n",
    "print(f\"  EDM correlation:   {np.mean([r['edm_corr'] for r in diffusion_results]):.4f}\")\n",
    "print(f\"  Coord correlation: {np.mean([r['avg_corr'] for r in diffusion_results]):.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT - 3 COLUMNS MAX PER ROW\n",
    "# ============================================================================\n",
    "\n",
    "n_cols = min(3, num_eval_samples)\n",
    "n_rows = int(np.ceil(num_eval_samples / n_cols)) * 2  # *2 for diffusion + GT rows\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "# Handle single row case\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if n_cols == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    row_pair = (i // n_cols) * 2  # Which pair of rows (diffusion + GT)\n",
    "    col = i % n_cols\n",
    "    \n",
    "    # Diffusion prediction\n",
    "    ax_diff = axes[row_pair, col]\n",
    "    ax_diff.scatter(diffusion_results[i]['coords_diffusion'][:, 0],\n",
    "                   diffusion_results[i]['coords_diffusion'][:, 1],\n",
    "                   s=10, alpha=0.6, c='green')\n",
    "    ax_diff.set_title(f\"Sample {i}: Diffusion\\n\"\n",
    "                     f\"Coord: {diffusion_results[i]['avg_corr']:.3f} | \"\n",
    "                     f\"EDM: {diffusion_results[i]['edm_corr']:.3f}\",\n",
    "                     fontsize=10)\n",
    "    ax_diff.set_aspect('equal')\n",
    "    ax_diff.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ground truth\n",
    "    ax_gt = axes[row_pair + 1, col]\n",
    "    ax_gt.scatter(diffusion_results[i]['coords_target'][:, 0],\n",
    "                 diffusion_results[i]['coords_target'][:, 1],\n",
    "                 s=10, alpha=0.6, c='red')\n",
    "    ax_gt.set_title(f\"Ground Truth (n={diffusion_results[i]['n_points']})\",\n",
    "                   fontsize=10)\n",
    "    ax_gt.set_aspect('equal')\n",
    "    ax_gt.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_eval_samples, n_rows // 2 * n_cols):\n",
    "    row_pair = (i // n_cols) * 2\n",
    "    col = i % n_cols\n",
    "    axes[row_pair, col].axis('off')\n",
    "    axes[row_pair + 1, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SMART OUTLIER REMOVAL - DISTANCE-BASED METHOD\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER REMOVAL - DISTANCE FROM MEDIAN CENTER\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRationale:\")\n",
    "print(\"- Diffusion occasionally samples points in low-probability tail regions\")\n",
    "print(\"- Ground truth tissue has consistent density (filled region)\")\n",
    "print(\"- Outliers are scattered points FAR from main cluster\")\n",
    "print(\"- Method: Keep only points within 90th percentile distance from median center\")\n",
    "print(\"- Why median? Robust to outliers (unlike mean)\")\n",
    "print(\"- Why 90th percentile? Keeps main distribution, removes extreme tail\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def remove_outliers_distance_percentile(coords, coords_target, percentile=90):\n",
    "    \"\"\"\n",
    "    Remove outliers based on distance from median center.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Find median center (robust to outliers)\n",
    "    2. Compute distance of each point from center\n",
    "    3. Keep only points within `percentile` of distances\n",
    "    4. Filter both predicted and target coords to match\n",
    "    \n",
    "    Args:\n",
    "        coords: (n, 2) predicted coordinates\n",
    "        coords_target: (n, 2) target coordinates\n",
    "        percentile: keep points within this percentile (90 = remove top 10%)\n",
    "    \n",
    "    Returns:\n",
    "        coords_clean, coords_target_clean, inlier_mask\n",
    "    \"\"\"\n",
    "    # Use MEDIAN center (robust to outliers, unlike mean)\n",
    "    center = np.median(coords, axis=0)\n",
    "    \n",
    "    # Distance from center for each point\n",
    "    dists = np.linalg.norm(coords - center, axis=1)\n",
    "    \n",
    "    # Threshold: keep only points within percentile\n",
    "    threshold = np.percentile(dists, percentile)\n",
    "    \n",
    "    # Inlier mask\n",
    "    inlier_mask = dists <= threshold\n",
    "    \n",
    "    # Filter both predicted and target\n",
    "    coords_clean = coords[inlier_mask]\n",
    "    coords_target_clean = coords_target[inlier_mask]\n",
    "    \n",
    "    return coords_clean, coords_target_clean, inlier_mask, threshold\n",
    "\n",
    "# ============================================================================\n",
    "# CLEAN EACH SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "diffusion_results_clean = []\n",
    "\n",
    "for i, res in enumerate(diffusion_results):\n",
    "    coords_pred = res['coords_diffusion']\n",
    "    coords_gt = res['coords_target']\n",
    "    n_orig = len(coords_pred)\n",
    "    \n",
    "    # Remove outliers\n",
    "    coords_clean, coords_gt_clean, mask, thresh = remove_outliers_distance_percentile(\n",
    "        coords_pred, coords_gt, percentile=90\n",
    "    )\n",
    "    \n",
    "    n_kept = len(coords_clean)\n",
    "    n_removed = n_orig - n_kept\n",
    "    pct_removed = 100 * n_removed / n_orig\n",
    "    \n",
    "    # Recanonialize after filtering\n",
    "    coords_clean_t = torch.from_numpy(coords_clean).float()\n",
    "    coords_gt_t = torch.from_numpy(coords_gt_clean).float()\n",
    "    \n",
    "    coords_clean_canon = uet.canonicalize_coords(coords_clean_t).numpy()\n",
    "    coords_gt_canon = uet.canonicalize_coords(coords_gt_t).numpy()\n",
    "    \n",
    "    # Recompute correlations\n",
    "    corr_x_before = res['corr_x']\n",
    "    corr_y_before = res['corr_y']\n",
    "    avg_corr_before = res['avg_corr']\n",
    "    edm_corr_before = res['edm_corr']\n",
    "    \n",
    "    corr_x = np.corrcoef(coords_clean_canon[:, 0], coords_gt_canon[:, 0])[0, 1]\n",
    "    corr_y = np.corrcoef(coords_clean_canon[:, 1], coords_gt_canon[:, 1])[0, 1]\n",
    "    avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "    \n",
    "    # EDM correlation\n",
    "    D_clean = torch.cdist(\n",
    "        torch.from_numpy(coords_clean_canon).unsqueeze(0).float(),\n",
    "        torch.from_numpy(coords_clean_canon).unsqueeze(0).float()\n",
    "    ).squeeze(0)\n",
    "    D_gt = torch.cdist(\n",
    "        torch.from_numpy(coords_gt_canon).unsqueeze(0).float(),\n",
    "        torch.from_numpy(coords_gt_canon).unsqueeze(0).float()\n",
    "    ).squeeze(0)\n",
    "    \n",
    "    edm_corr = np.corrcoef(D_clean.flatten().numpy(), D_gt.flatten().numpy())[0, 1]\n",
    "    \n",
    "    # Store\n",
    "    diffusion_results_clean.append({\n",
    "        'sample': i,\n",
    "        'n_points': n_kept,\n",
    "        'n_removed': n_removed,\n",
    "        'pct_removed': pct_removed,\n",
    "        'corr_x': corr_x,\n",
    "        'corr_y': corr_y,\n",
    "        'avg_corr': avg_corr,\n",
    "        'edm_corr': edm_corr,\n",
    "        'coords_diffusion': coords_clean_canon,\n",
    "        'coords_target': coords_gt_canon,\n",
    "        'threshold': thresh\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Sample {i}: removed {n_removed}/{n_orig} outliers ({pct_removed:.1f}%), \"\n",
    "          f\"threshold={thresh:.3f}\")\n",
    "    print(f\"  Before: Coord={avg_corr_before:.3f}, EDM={edm_corr_before:.3f}\")\n",
    "    print(f\"  After:  Coord={avg_corr:.3f} (Δ={avg_corr-avg_corr_before:+.3f}), \"\n",
    "          f\"EDM={edm_corr:.3f} (Δ={edm_corr-edm_corr_before:+.3f})\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY: BEFORE vs AFTER OUTLIER REMOVAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "avg_coord_before = np.mean([r['avg_corr'] for r in diffusion_results])\n",
    "avg_edm_before = np.mean([r['edm_corr'] for r in diffusion_results])\n",
    "\n",
    "avg_coord_after = np.mean([r['avg_corr'] for r in diffusion_results_clean])\n",
    "avg_edm_after = np.mean([r['edm_corr'] for r in diffusion_results_clean])\n",
    "\n",
    "avg_pct_removed = np.mean([r['pct_removed'] for r in diffusion_results_clean])\n",
    "\n",
    "print(f\"Average Coordinate Correlation:\")\n",
    "print(f\"  Before: {avg_coord_before:.4f}\")\n",
    "print(f\"  After:  {avg_coord_after:.4f} (Δ={avg_coord_after-avg_coord_before:+.4f})\")\n",
    "\n",
    "print(f\"\\nAverage EDM Correlation:\")\n",
    "print(f\"  Before: {avg_edm_before:.4f}\")\n",
    "print(f\"  After:  {avg_edm_after:.4f} (Δ={avg_edm_after-avg_edm_before:+.4f})\")\n",
    "\n",
    "print(f\"\\nAverage outliers removed: {avg_pct_removed:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*80)\n",
    "if avg_edm_after - avg_edm_before > 0.1:\n",
    "    print(\"✓ EDM correlation IMPROVED significantly after outlier removal\")\n",
    "    print(\"  → Confirms outliers were corrupting distance metrics\")\n",
    "    print(\"  → Main cluster has better geometric structure than raw output\")\n",
    "elif avg_edm_after - avg_edm_before > 0:\n",
    "    print(\"✓ EDM correlation improved slightly\")\n",
    "    print(\"  → Outliers had some negative effect on distances\")\n",
    "else:\n",
    "    print(\"⚠ EDM correlation unchanged or decreased\")\n",
    "    print(\"  → Problem is not just outliers, geometry of main cluster needs work\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLE PLOT: PREDICTED vs GROUND TRUTH (AFTER OUTLIER REMOVAL)\n",
    "# ============================================================================\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(num_eval_samples / n_cols)) * 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if n_cols == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    row_pair = (i // n_cols) * 2\n",
    "    col = i % n_cols\n",
    "    \n",
    "    # Predicted (cleaned)\n",
    "    ax_pred = axes[row_pair, col]\n",
    "    ax_pred.scatter(diffusion_results_clean[i]['coords_diffusion'][:, 0],\n",
    "                   diffusion_results_clean[i]['coords_diffusion'][:, 1],\n",
    "                   s=10, alpha=0.7, c='#2ecc71', edgecolors='none')\n",
    "    ax_pred.set_title(f\"Sample {i}: Predicted\\n\"\n",
    "                     f\"Coord: {diffusion_results_clean[i]['avg_corr']:.3f} | \"\n",
    "                     f\"EDM: {diffusion_results_clean[i]['edm_corr']:.3f}\",\n",
    "                     fontsize=10)\n",
    "    ax_pred.set_aspect('equal')\n",
    "    ax_pred.grid(True, alpha=0.2)\n",
    "    \n",
    "    # Ground Truth\n",
    "    ax_gt = axes[row_pair + 1, col]\n",
    "    ax_gt.scatter(diffusion_results_clean[i]['coords_target'][:, 0],\n",
    "                 diffusion_results_clean[i]['coords_target'][:, 1],\n",
    "                 s=10, alpha=0.7, c='#e74c3c', edgecolors='none')\n",
    "    ax_gt.set_title(f\"Ground Truth (n={diffusion_results_clean[i]['n_points']})\",\n",
    "                   fontsize=10)\n",
    "    ax_gt.set_aspect('equal')\n",
    "    ax_gt.grid(True, alpha=0.2)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_eval_samples, n_rows // 2 * n_cols):\n",
    "    row_pair = (i // n_cols) * 2\n",
    "    col = i % n_cols\n",
    "    axes[row_pair, col].axis('off')\n",
    "    axes[row_pair + 1, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('cleaned_results.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# print(\"✓ Saved plot: outlier_removal_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
