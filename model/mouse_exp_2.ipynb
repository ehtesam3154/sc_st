{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPLETE NOTEBOOK: SINGLE PATCH INFERENCE + FULL EVALUATION\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251128_100055\"\n",
    "checkpoint_path = f\"{output_dir}/phase2_sc_finetuned_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SINGLE PATCH INFERENCE (Diagnostic Mode)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "# Extract SC gene expression\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "print(f\"Ground truth coords shape: {scadata.obsm['spatial_gt'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL AND CHECKPOINT\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model and Checkpoint ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded checkpoint from: {checkpoint_path}\")\n",
    "print(f\"  Epochs trained: {checkpoint.get('epochs_finetune', 'N/A')}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: SINGLE PATCH INFERENCE (DIAGNOSTIC MODE)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Single Patch Inference ---\")\n",
    "print(f\"Config: patch_size={n_cells}, coverage_per_cell=1.0\")\n",
    "print(\"This runs ONE patch with ALL cells (no stitching)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "results = model.infer_sc_patchwise(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_timesteps_sample=600,\n",
    "    sigma_min=0.01,\n",
    "    sigma_max=7.0,\n",
    "    patch_size=n_cells,          # SINGLE PATCH MODE\n",
    "    coverage_per_cell=1.0,       # NO OVERLAP\n",
    "    n_align_iters=1,             # IRRELEVANT (only 1 patch)\n",
    "    eta=0.0,\n",
    "    guidance_scale=5.0,\n",
    "    return_coords=True,\n",
    "    debug_flag=True,\n",
    "    debug_every=10,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Inference complete\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: EXTRACT RAW EDM (NO PROJECTION, NO RESCALING)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Computing Raw EDM (No Post-Processing) ---\")\n",
    "\n",
    "# Extract canonicalized coordinates\n",
    "coords_canon = results['coords_canon'].cpu().numpy()\n",
    "\n",
    "# Compute RAW EDM directly from coordinates (NO edm_project, NO rescaling)\n",
    "gems_edm = cdist(coords_canon, coords_canon, metric='euclidean')\n",
    "\n",
    "print(f\"Raw EDM shape: {gems_edm.shape}\")\n",
    "print(f\"Raw EDM stats:\")\n",
    "print(f\"  Min: {gems_edm[gems_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gems_edm[gems_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gems_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gems_edm[gems_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: COMPUTE GROUND TRUTH EDM\n",
    "# ===================================================================\n",
    "print(\"\\n--- Calculating Ground Truth EDM ---\")\n",
    "\n",
    "gt_coords = scadata.obsm['spatial_gt']\n",
    "gt_edm = squareform(pdist(gt_coords, 'euclidean'))\n",
    "\n",
    "print(f\"Ground Truth EDM shape: {gt_edm.shape}\")\n",
    "print(f\"Ground Truth EDM stats:\")\n",
    "print(f\"  Min: {gt_edm[gt_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gt_edm[gt_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gt_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gt_edm[gt_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: NORMALIZE FOR COMPARISON\n",
    "# ===================================================================\n",
    "def normalize_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    return (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "gems_edm_norm = normalize_matrix(gems_edm)\n",
    "gt_edm_norm = normalize_matrix(gt_edm)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 7: QUANTITATIVE COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTITATIVE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract upper triangle (excluding diagonal)\n",
    "triu_indices = np.triu_indices(n_cells, k=1)\n",
    "gt_distances_flat = gt_edm[triu_indices]\n",
    "gems_distances_flat = gems_edm[triu_indices]\n",
    "\n",
    "# Scale alignment (median matching)\n",
    "scale = np.median(gt_distances_flat) / np.median(gems_distances_flat)\n",
    "gems_distances_flat_scaled = gems_distances_flat * scale\n",
    "\n",
    "print(f\"\\nScale factor (median matching): {scale:.4f}\")\n",
    "\n",
    "# Calculate correlations\n",
    "pearson_corr, _ = pearsonr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "spearman_corr, _ = spearmanr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "\n",
    "print(f\"\\nPearson Correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 8: VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "# --- PLOT 1: Side-by-Side Heatmaps ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('EDM Comparison: Ground Truth vs. GEMS (Single Patch, Raw EDM)', \n",
    "             fontsize=18, fontweight='bold')\n",
    "\n",
    "sample_size = min(838, n_cells)\n",
    "sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "im1 = axes[0].imshow(gt_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[0].set_title('Ground Truth EDM (Normalized)', fontsize=14)\n",
    "axes[0].set_xlabel('Cell Index (Sampled)')\n",
    "axes[0].set_ylabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im2 = axes[1].imshow(gems_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[1].set_title('GEMS Predicted EDM (Normalized)', fontsize=14)\n",
    "axes[1].set_xlabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 2: Distribution of Distances ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(gt_distances_flat, color=\"blue\", label='Ground Truth Distances', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "sns.histplot(gems_distances_flat_scaled, color=\"red\", label='GEMS Distances (Scaled)', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "ax.set_title('Distribution of Pairwise Distances (Single Patch Mode)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Distance', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 3: Scatter Plot of Distances ---\n",
    "sample_size_scatter = min(50000, len(gt_distances_flat))\n",
    "sample_indices_scatter = np.random.choice(len(gt_distances_flat), sample_size_scatter, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(\n",
    "    gt_distances_flat[sample_indices_scatter],\n",
    "    gems_distances_flat_scaled[sample_indices_scatter],\n",
    "    alpha=0.2, s=5, color='steelblue'\n",
    ")\n",
    "ax.set_title(f'GEMS vs. Ground Truth Distances (Single Patch)\\nSpearman ρ = {spearman_corr:.4f}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Ground Truth Pairwise Distance', fontsize=12)\n",
    "ax.set_ylabel('GEMS Pairwise Distance (Scaled)', fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims = [\n",
    "    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "    max(ax.get_xlim()[1], ax.get_ylim()[1]),\n",
    "]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.75, linewidth=2, zorder=0, label='Ideal Correlation')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 4: Coordinate Comparison ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Spatial Coordinates: Ground Truth vs. GEMS (Single Patch)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, color='blue')\n",
    "axes[0].set_title('Ground Truth Coordinates', fontsize=14)\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(coords_canon[:, 0], coords_canon[:, 1], s=5, alpha=0.6, color='red')\n",
    "axes[1].set_title('GEMS Predicted Coordinates', fontsize=14)\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Y', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 5: Distance Error Distribution ---\n",
    "distance_errors = np.abs(gt_distances_flat - gems_distances_flat_scaled)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(distance_errors, bins=100, kde=True, ax=ax, color='purple')\n",
    "ax.set_title('Distance Prediction Error Distribution', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Absolute Error |GT - GEMS|', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.axvline(np.median(distance_errors), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Median Error: {np.median(distance_errors):.4f}')\n",
    "ax.axvline(np.mean(distance_errors), color='g', linestyle='--', linewidth=2, \n",
    "           label=f'Mean Error: {np.mean(distance_errors):.4f}')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 9: SAVE RESULTS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Saving Results ---\")\n",
    "\n",
    "new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_suffix = f\"single_patch_{new_timestamp}\"\n",
    "\n",
    "results_processed = {\n",
    "    'D_edm': gems_edm,  # RAW EDM (no projection, no rescaling)\n",
    "    'coords': results['coords'].cpu().numpy(),\n",
    "    'coords_canon': coords_canon,\n",
    "    'n_cells': n_cells,\n",
    "    'timestamp': new_timestamp,\n",
    "    'mode': 'single_patch_no_projection',\n",
    "    'scale_factor': scale,\n",
    "    'pearson_corr': pearson_corr,\n",
    "    'spearman_corr': spearman_corr,\n",
    "    'model_config': {\n",
    "        'n_genes': n_genes,\n",
    "        'D_latent': 32,\n",
    "        'c_dim': 256,\n",
    "    }\n",
    "}\n",
    "\n",
    "processed_path = os.path.join(output_dir, f\"sc_inference_processed_{output_suffix}.pt\")\n",
    "# torch.save(results_processed, processed_path)\n",
    "# print(f\"✓ Saved: {processed_path}\")\n",
    "\n",
    "scadata.obsm['X_gems'] = coords_canon\n",
    "adata_path = os.path.join(output_dir, f\"scadata_with_gems_{output_suffix}.h5ad\")\n",
    "scadata.write_h5ad(adata_path)\n",
    "print(f\"✓ Saved: {adata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SINGLE PATCH DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"  Mode: Single patch (patch_size={n_cells})\")\n",
    "print(f\"  EDM: Raw (no projection, no rescaling)\")\n",
    "print(f\"  Pearson: {pearson_corr:.4f}\")\n",
    "print(f\"  Spearman: {spearman_corr:.4f}\")\n",
    "print(f\"  Scale factor: {scale:.4f}\")\n",
    "print(f\"  Output timestamp: {output_suffix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import utils_et as uet  # Ensure this is in your python path\n",
    "\n",
    "# 1. Load the Raw ST Data (Exact paths from your code)\n",
    "print(\"Loading ST Data...\")\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "# Load coords\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "raw_coords = st_meta_df[['coord_x', 'coord_y']].values\n",
    "st_coords_tensor = torch.tensor(raw_coords, dtype=torch.float32)\n",
    "\n",
    "# 2. Apply the EXACT normalization used in run_mouse_brain_2.py\n",
    "print(\"Applying Global RMS Normalization...\")\n",
    "# Dummy slide IDs (all 0) since you have single slide logic in the snippets\n",
    "slide_ids = torch.zeros(st_coords_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "# This is the function called in line 165 of run_mouse_brain_2.py\n",
    "norm_coords, mu, scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_tensor, slide_ids\n",
    ")\n",
    "\n",
    "norm_coords = norm_coords.numpy()\n",
    "print(f\"Normalization Scale Factor used: {scale[0].item():.4f}\")\n",
    "\n",
    "# 3. Calculate Statistics\n",
    "radii = np.sqrt(np.sum(norm_coords**2, axis=1))\n",
    "points_outside = np.sum(radii > 1.0)\n",
    "pct_outside = (points_outside / len(radii)) * 100\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Points: {len(radii)}\")\n",
    "print(f\"Points outside Unit Circle (Radius > 1.0): {points_outside}\")\n",
    "print(f\"Percentage outside: {pct_outside:.2f}%\")\n",
    "print(f\"Max Radius: {radii.max():.4f}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# 4. Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot A: The Normalized Geometry\n",
    "axes[0].scatter(norm_coords[:, 0], norm_coords[:, 1], s=5, alpha=0.6, c='steelblue', label='ST Cells')\n",
    "# Draw the Unit Circle\n",
    "circle = plt.Circle((0, 0), 1.0, color='red', fill=False, linestyle='--', linewidth=2, label='Unit RMS Circle')\n",
    "axes[0].add_patch(circle)\n",
    "axes[0].set_title(f\"Normalized ST Data\\n({pct_outside:.1f}% points outside red circle)\", fontsize=14)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot B: Histogram of Radii\n",
    "sns.histplot(radii, bins=50, ax=axes[1], kde=True, color='purple')\n",
    "axes[1].axvline(1.0, color='red', linestyle='--', linewidth=2, label='Radius = 1.0')\n",
    "axes[1].set_title(\"Distribution of Radii from Center\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Distance from Center\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPLETE NOTEBOOK: ST-ONLY MODEL (PHASE 1) - SINGLE PATCH INFERENCE\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251128_100055\"\n",
    "\n",
    "# USE PHASE 1 CHECKPOINT (ST-ONLY, BEFORE SC FINE-TUNING)\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ST-ONLY MODEL INFERENCE (Phase 1, Single Patch)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "# Extract SC gene expression\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "print(f\"Ground truth coords shape: {scadata.obsm['spatial_gt'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL AND ST-ONLY CHECKPOINT (PHASE 1)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model and ST-Only Checkpoint (Phase 1) ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded ST-ONLY checkpoint from: {checkpoint_path}\")\n",
    "print(f\"  Best ST epoch: {checkpoint.get('E_ST_best', 'N/A')}\")\n",
    "print(f\"  This model was trained ONLY on ST data (NO SC fine-tuning)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: SINGLE PATCH INFERENCE (DIAGNOSTIC MODE)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Single Patch Inference (ST-Only Model) ---\")\n",
    "print(f\"Config: patch_size={n_cells}, coverage_per_cell=1.0, n_align_iters=1\")\n",
    "print(\"This runs ONE patch with ALL cells (no stitching)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "results = model.infer_sc_patchwise(\n",
    "    sc_gene_expr=sc_expr,\n",
    "    n_timesteps_sample=600,\n",
    "    sigma_min=0.01,\n",
    "    sigma_max=7.0,\n",
    "    patch_size=n_cells,          # SINGLE PATCH MODE\n",
    "    coverage_per_cell=1.0,       # NO OVERLAP\n",
    "    n_align_iters=1,             # NO STITCHING (only 1 patch)\n",
    "    eta=0.0,\n",
    "    guidance_scale=5.0,\n",
    "    return_coords=True,\n",
    "    debug_flag=True,\n",
    "    debug_every=10,\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Inference complete\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: EXTRACT RAW EDM (NO PROJECTION, NO RESCALING)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Computing Raw EDM (No Post-Processing) ---\")\n",
    "\n",
    "# Extract canonicalized coordinates\n",
    "coords_canon = results['coords_canon'].cpu().numpy()\n",
    "\n",
    "# Compute RAW EDM directly from coordinates (NO edm_project, NO rescaling)\n",
    "gems_edm = cdist(coords_canon, coords_canon, metric='euclidean')\n",
    "\n",
    "print(f\"Raw EDM shape: {gems_edm.shape}\")\n",
    "print(f\"Raw EDM stats:\")\n",
    "print(f\"  Min: {gems_edm[gems_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gems_edm[gems_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gems_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gems_edm[gems_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: COMPUTE GROUND TRUTH EDM\n",
    "# ===================================================================\n",
    "print(\"\\n--- Calculating Ground Truth EDM ---\")\n",
    "\n",
    "gt_coords = scadata.obsm['spatial_gt']\n",
    "gt_edm = squareform(pdist(gt_coords, 'euclidean'))\n",
    "\n",
    "print(f\"Ground Truth EDM shape: {gt_edm.shape}\")\n",
    "print(f\"Ground Truth EDM stats:\")\n",
    "print(f\"  Min: {gt_edm[gt_edm > 0].min():.4f}\")\n",
    "print(f\"  Median: {np.median(gt_edm[gt_edm > 0]):.4f}\")\n",
    "print(f\"  Max: {gt_edm.max():.4f}\")\n",
    "print(f\"  Mean: {gt_edm[gt_edm > 0].mean():.4f}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: NORMALIZE FOR COMPARISON\n",
    "# ===================================================================\n",
    "def normalize_matrix(matrix):\n",
    "    min_val = matrix.min()\n",
    "    max_val = matrix.max()\n",
    "    return (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "gems_edm_norm = normalize_matrix(gems_edm)\n",
    "gt_edm_norm = normalize_matrix(gt_edm)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 7: QUANTITATIVE COMPARISON\n",
    "# ===================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTITATIVE COMPARISON (ST-ONLY MODEL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract upper triangle (excluding diagonal)\n",
    "triu_indices = np.triu_indices(n_cells, k=1)\n",
    "gt_distances_flat = gt_edm[triu_indices]\n",
    "gems_distances_flat = gems_edm[triu_indices]\n",
    "\n",
    "# Scale alignment (median matching)\n",
    "scale = np.median(gt_distances_flat) / np.median(gems_distances_flat)\n",
    "gems_distances_flat_scaled = gems_distances_flat * scale\n",
    "\n",
    "print(f\"\\nScale factor (median matching): {scale:.4f}\")\n",
    "\n",
    "# Calculate correlations\n",
    "pearson_corr, _ = pearsonr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "spearman_corr, _ = spearmanr(gt_distances_flat, gems_distances_flat_scaled)\n",
    "\n",
    "print(f\"\\nPearson Correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 8: VISUALIZATIONS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "# --- PLOT 1: Side-by-Side Heatmaps ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "fig.suptitle('EDM Comparison: Ground Truth vs. GEMS (ST-Only Model, Single Patch)', \n",
    "             fontsize=18, fontweight='bold')\n",
    "\n",
    "sample_size = min(838, n_cells)\n",
    "sample_indices = np.random.choice(n_cells, sample_size, replace=False)\n",
    "sample_indices = np.sort(sample_indices)\n",
    "\n",
    "im1 = axes[0].imshow(gt_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[0].set_title('Ground Truth EDM (Normalized)', fontsize=14)\n",
    "axes[0].set_xlabel('Cell Index (Sampled)')\n",
    "axes[0].set_ylabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im2 = axes[1].imshow(gems_edm_norm[np.ix_(sample_indices, sample_indices)], cmap='viridis')\n",
    "axes[1].set_title('GEMS Predicted EDM (ST-Only, Normalized)', fontsize=14)\n",
    "axes[1].set_xlabel('Cell Index (Sampled)')\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 2: Distribution of Distances ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(gt_distances_flat, color=\"blue\", label='Ground Truth Distances', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "sns.histplot(gems_distances_flat_scaled, color=\"orange\", label='GEMS Distances (ST-Only, Scaled)', \n",
    "             ax=ax, stat='density', bins=100, alpha=0.6)\n",
    "ax.set_title('Distribution of Pairwise Distances (ST-Only Model)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Distance', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 3: Scatter Plot of Distances ---\n",
    "sample_size_scatter = min(50000, len(gt_distances_flat))\n",
    "sample_indices_scatter = np.random.choice(len(gt_distances_flat), sample_size_scatter, replace=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.scatter(\n",
    "    gt_distances_flat[sample_indices_scatter],\n",
    "    gems_distances_flat_scaled[sample_indices_scatter],\n",
    "    alpha=0.2, s=5, color='orange'\n",
    ")\n",
    "ax.set_title(f'GEMS vs. Ground Truth Distances (ST-Only Model)\\nSpearman ρ = {spearman_corr:.4f}', \n",
    "             fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Ground Truth Pairwise Distance', fontsize=12)\n",
    "ax.set_ylabel('GEMS Pairwise Distance (Scaled)', fontsize=12)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "lims = [\n",
    "    min(ax.get_xlim()[0], ax.get_ylim()[0]),\n",
    "    max(ax.get_xlim()[1], ax.get_ylim()[1]),\n",
    "]\n",
    "ax.plot(lims, lims, 'r--', alpha=0.75, linewidth=2, zorder=0, label='Ideal Correlation')\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 4: Coordinate Comparison ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "fig.suptitle('Spatial Coordinates: Ground Truth vs. GEMS (ST-Only Model)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, color='blue')\n",
    "axes[0].set_title('Ground Truth Coordinates', fontsize=14)\n",
    "axes[0].set_xlabel('X', fontsize=12)\n",
    "axes[0].set_ylabel('Y', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(coords_canon[:, 0], coords_canon[:, 1], s=5, alpha=0.6, color='orange')\n",
    "axes[1].set_title('GEMS Predicted Coordinates (ST-Only)', fontsize=14)\n",
    "axes[1].set_xlabel('X', fontsize=12)\n",
    "axes[1].set_ylabel('Y', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# --- PLOT 5: Distance Error Distribution ---\n",
    "distance_errors = np.abs(gt_distances_flat - gems_distances_flat_scaled)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(distance_errors, bins=100, kde=True, ax=ax, color='orange')\n",
    "ax.set_title('Distance Prediction Error Distribution (ST-Only Model)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Absolute Error |GT - GEMS|', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.axvline(np.median(distance_errors), color='r', linestyle='--', linewidth=2, \n",
    "           label=f'Median Error: {np.median(distance_errors):.4f}')\n",
    "ax.axvline(np.mean(distance_errors), color='g', linestyle='--', linewidth=2, \n",
    "           label=f'Mean Error: {np.mean(distance_errors):.4f}')\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 9: SAVE RESULTS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Saving Results ---\")\n",
    "\n",
    "new_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_suffix = f\"st_only_single_patch_{new_timestamp}\"\n",
    "\n",
    "results_processed = {\n",
    "    'D_edm': gems_edm,  # RAW EDM (no projection, no rescaling)\n",
    "    'coords': results['coords'].cpu().numpy(),\n",
    "    'coords_canon': coords_canon,\n",
    "    'n_cells': n_cells,\n",
    "    'timestamp': new_timestamp,\n",
    "    'mode': 'st_only_single_patch_no_projection',\n",
    "    'scale_factor': scale,\n",
    "    'pearson_corr': pearson_corr,\n",
    "    'spearman_corr': spearman_corr,\n",
    "    'model_config': {\n",
    "        'n_genes': n_genes,\n",
    "        'D_latent': 32,\n",
    "        'c_dim': 256,\n",
    "        'phase': 'ST-only (Phase 1)',\n",
    "    }\n",
    "}\n",
    "\n",
    "processed_path = os.path.join(output_dir, f\"sc_inference_processed_{output_suffix}.pt\")\n",
    "# torch.save(results_processed, processed_path)\n",
    "# print(f\"✓ Saved: {processed_path}\")\n",
    "\n",
    "scadata.obsm['X_gems_st_only'] = coords_canon\n",
    "adata_path = os.path.join(output_dir, f\"scadata_with_gems_{output_suffix}.h5ad\")\n",
    "scadata.write_h5ad(adata_path)\n",
    "print(f\"✓ Saved: {adata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ST-ONLY MODEL DIAGNOSTIC COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"  Model: ST-Only (Phase 1, BEFORE SC fine-tuning)\")\n",
    "print(f\"  Mode: Single patch (patch_size={n_cells})\")\n",
    "print(f\"  EDM: Raw (no projection, no rescaling)\")\n",
    "print(f\"  Pearson: {pearson_corr:.4f}\")\n",
    "print(f\"  Spearman: {spearman_corr:.4f}\")\n",
    "print(f\"  Scale factor: {scale:.4f}\")\n",
    "print(f\"  Output timestamp: {output_suffix}\")\n",
    "print(\"\\nThis tells you if ring collapse happens during:\")\n",
    "print(\"  - ST-only training (Phase 1) → if you see ring now\")\n",
    "print(\"  - SC fine-tuning (Phase 2) → if you saw ring only with fine-tuned model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# TIMESTEP-BY-TIMESTEP DIFFUSION VISUALIZATION\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "timestamp = \"20251128_100055\"\n",
    "checkpoint_path = f\"{output_dir}/phase2_sc_finetuned_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DIFFUSION TIMESTEP VISUALIZATION (Single Patch)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "import utils_et as uet\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded checkpoint\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: INLINE DIFFUSION SAMPLER WITH TIMESTEP CAPTURE\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Diffusion with Timestep Capture ---\")\n",
    "\n",
    "device = 'cuda'\n",
    "n_timesteps_sample = 600\n",
    "sigma_min = 0.01\n",
    "sigma_max = 7.0\n",
    "guidance_scale = 2.0\n",
    "D_latent = 32\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "print(f\"Config: n_timesteps={n_timesteps_sample}, guidance_scale={guidance_scale}\")\n",
    "print(f\"        sigma_min={sigma_min}, sigma_max={sigma_max}\")\n",
    "\n",
    "# Encode all SC cells\n",
    "print(\"\\n[1/4] Encoding SC cells...\")\n",
    "with torch.no_grad():\n",
    "    Z_all = model.encoder(sc_expr.to(device))  # (n_cells, hidden_dim)\n",
    "    \n",
    "# Prepare context\n",
    "print(\"[2/4] Computing context...\")\n",
    "Z_batch = Z_all.unsqueeze(0)  # (1, n_cells, hidden_dim)\n",
    "mask = torch.ones(1, n_cells, dtype=torch.bool, device=device)\n",
    "H = model.context_encoder(Z_batch, mask)  # (1, n_cells, c_dim)\n",
    "\n",
    "# Sigma schedule\n",
    "sigmas = torch.exp(torch.linspace(\n",
    "    torch.log(torch.tensor(sigma_max, device=device)),\n",
    "    torch.log(torch.tensor(sigma_min, device=device)),\n",
    "    n_timesteps_sample,\n",
    "    device=device,\n",
    "))\n",
    "\n",
    "# Initialize noise\n",
    "print(\"[3/4] Running reverse diffusion...\")\n",
    "V_t = torch.randn(1, n_cells, D_latent, device=device) * sigmas[0]\n",
    "\n",
    "# Timesteps to save\n",
    "save_timesteps = [0, 100, 200, 300, 400, 500, 599]\n",
    "saved_samples = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t_idx in range(n_timesteps_sample):\n",
    "        sigma_t = sigmas[t_idx]\n",
    "        t_norm = torch.tensor([[t_idx / float(n_timesteps_sample - 1)]], device=device)\n",
    "        \n",
    "        # CFG sampling\n",
    "        H_null = torch.zeros_like(H)\n",
    "        eps_uncond = model.score_net(V_t, t_norm, H_null, mask)\n",
    "        eps_cond = model.score_net(V_t, t_norm, H, mask)\n",
    "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "        \n",
    "        # Update\n",
    "        if t_idx < n_timesteps_sample - 1:\n",
    "            sigma_next = sigmas[t_idx + 1]\n",
    "            V_0_pred = V_t - sigma_t * eps\n",
    "            V_t = V_0_pred + (sigma_next / sigma_t) * (V_t - V_0_pred)\n",
    "        else:\n",
    "            V_t = V_t - sigma_t * eps\n",
    "        \n",
    "        # Save at specific timesteps\n",
    "        if t_idx in save_timesteps:\n",
    "            # Canonicalize the current sample\n",
    "            V_canon = uet.canonicalize_coords(V_t.squeeze(0))\n",
    "            saved_samples[t_idx] = V_canon.cpu().numpy()\n",
    "            print(f\"  Saved timestep {t_idx}/{n_timesteps_sample-1}\")\n",
    "\n",
    "# Final sample\n",
    "V_final = V_t.squeeze(0)  # (n_cells, D_latent)\n",
    "V_final_canon = uet.canonicalize_coords(V_final)\n",
    "coords_final = V_final_canon.cpu().numpy()\n",
    "\n",
    "print(\"[4/4] Complete!\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 4: CONVERT TO 2D COORDINATES VIA MDS\n",
    "# ===================================================================\n",
    "print(\"\\n--- Converting to 2D coordinates ---\")\n",
    "\n",
    "def latent_to_2d(V_latent):\n",
    "    \"\"\"Convert D_latent dimensional coordinates to 2D via MDS\"\"\"\n",
    "    n = V_latent.shape[0]\n",
    "    V_tensor = torch.tensor(V_latent, dtype=torch.float32)\n",
    "    \n",
    "    # Compute EDM from latent coordinates\n",
    "    D = torch.cdist(V_tensor, V_tensor)\n",
    "    \n",
    "    # Classical MDS\n",
    "    Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "    B = -0.5 * (Jn @ (D**2) @ Jn)\n",
    "    \n",
    "    # Extract 2D coordinates\n",
    "    coords_2d = uet.classical_mds(B, d_out=2).numpy()\n",
    "    coords_2d = uet.canonicalize_coords(torch.tensor(coords_2d)).numpy()\n",
    "    \n",
    "    return coords_2d\n",
    "\n",
    "coords_at_timesteps = {}\n",
    "for t_idx, V in saved_samples.items():\n",
    "    coords_at_timesteps[t_idx] = latent_to_2d(V)\n",
    "    print(f\"  Converted timestep {t_idx} to 2D\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 5: VISUALIZE DIFFUSION EVOLUTION\n",
    "# ===================================================================\n",
    "print(\"\\n--- Generating Visualizations ---\")\n",
    "\n",
    "# Ground truth for reference\n",
    "gt_coords = scadata.obsm['spatial_gt']\n",
    "\n",
    "# Plot grid: GT + all saved timesteps\n",
    "n_plots = len(save_timesteps) + 1\n",
    "n_cols = 4\n",
    "n_rows = int(np.ceil(n_plots / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot ground truth\n",
    "axes[0].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, c='blue')\n",
    "axes[0].set_title('Ground Truth', fontsize=14, fontweight='bold')\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot diffusion timesteps\n",
    "for idx, t_idx in enumerate(save_timesteps):\n",
    "    ax = axes[idx + 1]\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    \n",
    "    ax.scatter(coords[:, 0], coords[:, 1], s=5, alpha=0.6, c='red')\n",
    "    ax.set_title(f'Timestep {t_idx}/{n_timesteps_sample-1}\\n(σ={sigmas[t_idx]:.4f})', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_plots, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Diffusion Evolution (guidance_scale={guidance_scale}, n_timesteps={n_timesteps_sample})', \n",
    "             fontsize=18, fontweight='bold', y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# ADDITIONAL PLOT: SIDE-BY-SIDE EVOLUTION\n",
    "# ===================================================================\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "# Top row: early timesteps\n",
    "for idx, t_idx in enumerate([0, 100, 200, 300]):\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    axes[0, idx].scatter(coords[:, 0], coords[:, 1], s=5, alpha=0.6, c='red')\n",
    "    axes[0, idx].set_title(f't={t_idx} (σ={sigmas[t_idx]:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[0, idx].set_aspect('equal')\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom row: late timesteps\n",
    "for idx, t_idx in enumerate([400, 500, 599]):\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    axes[1, idx].scatter(coords[:, 0], coords[:, 1], s=5, alpha=0.6, c='red')\n",
    "    axes[1, idx].set_title(f't={t_idx} (σ={sigmas[t_idx]:.3f})', fontsize=12, fontweight='bold')\n",
    "    axes[1, idx].set_aspect('equal')\n",
    "    axes[1, idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Ground truth in last position\n",
    "axes[1, 3].scatter(gt_coords[:, 0], gt_coords[:, 1], s=5, alpha=0.6, c='blue')\n",
    "axes[1, 3].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "axes[1, 3].set_aspect('equal')\n",
    "axes[1, 3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Diffusion Denoising Trajectory', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 6: QUANTIFY STRUCTURE COLLAPSE\n",
    "# ===================================================================\n",
    "print(\"\\n--- Analyzing Structure Collapse ---\")\n",
    "\n",
    "def compute_pca_variance_ratio(coords):\n",
    "    \"\"\"Compute variance explained by first 2 PCA components\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(coords)\n",
    "    return pca.explained_variance_ratio_\n",
    "\n",
    "def compute_circularity(coords):\n",
    "    \"\"\"Compute circularity score (higher = more ring-like)\"\"\"\n",
    "    center = coords.mean(axis=0)\n",
    "    radii = np.linalg.norm(coords - center, axis=1)\n",
    "    return 1.0 - (radii.std() / radii.mean())\n",
    "\n",
    "print(\"\\n{:<10} {:<15} {:<15} {:<15}\".format(\"Timestep\", \"PCA-1 Var\", \"PCA-2 Var\", \"Circularity\"))\n",
    "print(\"-\"*60)\n",
    "\n",
    "for t_idx in save_timesteps:\n",
    "    coords = coords_at_timesteps[t_idx]\n",
    "    var_ratios = compute_pca_variance_ratio(coords)\n",
    "    circ = compute_circularity(coords)\n",
    "    print(f\"{t_idx:<10} {var_ratios[0]:<15.4f} {var_ratios[1]:<15.4f} {circ:<15.4f}\")\n",
    "\n",
    "# Ground truth\n",
    "gt_var_ratios = compute_pca_variance_ratio(gt_coords)\n",
    "gt_circ = compute_circularity(gt_coords)\n",
    "print(f\"{'GT':<10} {gt_var_ratios[0]:<15.4f} {gt_var_ratios[1]:<15.4f} {gt_circ:<15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TIMESTEP ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "from core_models_et_p1 import STSetDataset, collate_minisets\n",
    "import utils_et as uet\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA (from run_mouse_brain_2.py)\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "st_ct     = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_celltype_et.csv'\n",
    "\n",
    "print(\"Loading ST1 (training ST data)...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "st_ct_df = pd.read_csv(st_ct, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "stadata.obs['celltype_mapped_refined'] = st_ct_df.idxmax(axis=1).values\n",
    "stadata.obsm['celltype_proportions'] = st_ct_df.values\n",
    "\n",
    "print(f\"ST1 loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# Extract expression and coordinates\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "# Apply per-slide canonicalization (same as training)\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST coords canonicalized: scale={st_scale[0].item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD TRAINED ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "outdir = '/home/ehtesamul/sc_st/model/gems_mousebrain_output'\n",
    "checkpoint_path = os.path.join(outdir, 'ab_init.pt')\n",
    "\n",
    "n_genes = stadata.shape[1]\n",
    "\n",
    "# Create model with same config as run_mouse_brain_2.py\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=str(device),\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading checkpoint from: {checkpoint_path}\")\n",
    "ckpt = torch.load(checkpoint_path, map_location=device)\n",
    "model.encoder.load_state_dict(ckpt['encoder'])\n",
    "model.encoder.eval()\n",
    "\n",
    "print(\"Encoder loaded and frozen.\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN STAGE B (takes ~3 seconds)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Running Stage B ===\")\n",
    "slides_dict = {0: (st_coords, st_expr)}\n",
    "model.train_stageB(\n",
    "    slides=slides_dict,\n",
    "    outdir='temp_stageB_cache'\n",
    ")\n",
    "\n",
    "print(\"Stage B complete. targets_dict populated.\")\n",
    "\n",
    "# ============================================================================\n",
    "# DEFINE SUPERVISED REGRESSION HEAD\n",
    "# ============================================================================\n",
    "\n",
    "# class SupervisedEDMHead(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Simple supervised head that predicts EDM from encoder embeddings.\n",
    "    \n",
    "#     Architecture:\n",
    "#     Z (from encoder) -> MLP -> upper triangular EDM prediction\n",
    "#     \"\"\"\n",
    "#     def __init__(self, h_dim: int, hidden_dim: int = 256):\n",
    "#         super().__init__()\n",
    "#         self.h_dim = h_dim\n",
    "        \n",
    "#         # MLP to predict pairwise distances\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(h_dim * 2, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, Z: torch.Tensor, mask: torch.Tensor):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             Z: (batch, n, h_dim) encoder embeddings\n",
    "#             mask: (batch, n) validity mask\n",
    "            \n",
    "#         Returns:\n",
    "#             D_pred: (batch, n, n) predicted distance matrix\n",
    "#         \"\"\"\n",
    "#         batch, n, h = Z.shape\n",
    "        \n",
    "#         # Create pairwise concatenations\n",
    "#         Z_i = Z.unsqueeze(2).expand(-1, -1, n, -1)  # (batch, n, n, h)\n",
    "#         Z_j = Z.unsqueeze(1).expand(-1, n, -1, -1)  # (batch, n, n, h)\n",
    "#         Z_pairs = torch.cat([Z_i, Z_j], dim=-1)     # (batch, n, n, 2h)\n",
    "        \n",
    "#         # Predict distances\n",
    "#         D_pred = self.mlp(Z_pairs).squeeze(-1)      # (batch, n, n)\n",
    "#         D_pred = torch.relu(D_pred)                  # Ensure non-negative\n",
    "        \n",
    "#         # Symmetrize\n",
    "#         D_pred = (D_pred + D_pred.transpose(-1, -2)) / 2.0\n",
    "        \n",
    "#         # Zero out diagonal\n",
    "#         diag_mask = torch.eye(n, device=Z.device).unsqueeze(0).bool()\n",
    "#         D_pred = D_pred.masked_fill(diag_mask, 0.0)\n",
    "        \n",
    "#         # Apply validity mask\n",
    "#         valid_mask = mask.unsqueeze(-1) & mask.unsqueeze(-2)\n",
    "#         D_pred = D_pred * valid_mask.float()\n",
    "        \n",
    "#         return D_pred\n",
    "    \n",
    "class SupervisedCoordHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple supervised head that predicts 2D coordinates from encoder embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, h_dim: int, hidden_dim: int = 256, D_out: int = 2):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(h_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, D_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, Z: torch.Tensor, mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Z: (batch, n, h_dim) encoder embeddings\n",
    "            mask: (batch, n) validity mask\n",
    "            \n",
    "        Returns:\n",
    "            coords: (batch, n, 2) predicted coordinates\n",
    "        \"\"\"\n",
    "        coords = self.mlp(Z)                    # (batch, n, 2)\n",
    "        coords = coords * mask.unsqueeze(-1)    # zero out padded entries\n",
    "        return coords\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATASET AND DATALOADER\n",
    "# ============================================================================\n",
    "\n",
    "# Create ST miniset dataset (same as Stage C training)\n",
    "st_gene_expr_dict_cpu = {0: st_expr.cpu()}\n",
    "\n",
    "st_dataset = STSetDataset(\n",
    "    targets_dict=model.targets_dict,\n",
    "    encoder=model.encoder,\n",
    "    st_gene_expr_dict=st_gene_expr_dict_cpu,\n",
    "    n_min=64,\n",
    "    n_max=384,\n",
    "    D_latent=model.D_latent,\n",
    "    num_samples=4000,  # Same as run_mouse_brain_2.py\n",
    "    knn_k=12,\n",
    "    device=device,\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "st_loader = DataLoader(\n",
    "    st_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_minisets\n",
    ")\n",
    "\n",
    "print(f\"ST dataset created: {len(st_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INITIALIZE SUPERVISED HEAD\n",
    "# ============================================================================\n",
    "\n",
    "# h_dim = model.encoder.fc_list[-1].out_features  # Get encoder output dim\n",
    "# Get encoder output dimension by doing a forward pass\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, n_genes, device=device)\n",
    "    h_dim = model.encoder(dummy_input).shape[-1]\n",
    "# supervised_head = SupervisedEDMHead(h_dim=h_dim, hidden_dim=256).to(device)\n",
    "\n",
    "supervised_head = SupervisedCoordHead(h_dim=h_dim, hidden_dim=256).to(device)\n",
    "\n",
    "optimizer = optim.Adam(supervised_head.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "\n",
    "print(f\"\\nSupervised head initialized: h_dim={h_dim}\")\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "num_epochs = 50\n",
    "loss_history = []\n",
    "\n",
    "print(\"\\n=== Training Supervised Baseline ===\\n\")\n",
    "\n",
    "supervised_head.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for batch_idx, batch in enumerate(st_loader):\n",
    "        # Move batch to device\n",
    "        Z = batch['Z_set'].to(device)              # (batch, n, h)\n",
    "        mask = batch['mask'].to(device)            # (batch, n)\n",
    "        D_target = batch['D_target'].to(device)    # (batch, n, n)\n",
    "        \n",
    "        # # Forward pass\n",
    "        # D_pred = supervised_head(Z, mask)\n",
    "        \n",
    "        # # Loss: MSE on valid EDM entries\n",
    "        # valid_mask = mask.unsqueeze(-1) & mask.unsqueeze(-2)\n",
    "        # loss = ((D_pred - D_target) ** 2 * valid_mask.float()).sum() / valid_mask.float().sum()\n",
    "        \n",
    "        # Forward pass\n",
    "        coords_pred = supervised_head(Z, mask)  # (batch, n, 2)\n",
    "\n",
    "        # Compute EDM from predicted coords\n",
    "        D_pred = torch.cdist(coords_pred, coords_pred)  # (batch, n, n)\n",
    "\n",
    "        # Loss: MSE on valid EDM entries\n",
    "        valid_mask = mask.unsqueeze(-1) & mask.unsqueeze(-2)\n",
    "        loss = ((D_pred - D_target) ** 2 * valid_mask.float()).sum() / valid_mask.float().sum()\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    loss_history.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"\\n=== Training Complete ===\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATE: SAMPLE A FEW MINISETS AND CHECK GEOMETRY\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_head.eval()\n",
    "\n",
    "print(\"=== Evaluating Supervised Baseline ===\\n\")\n",
    "\n",
    "num_eval_samples = 5\n",
    "eval_results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_iter = iter(st_loader)\n",
    "    \n",
    "    for i in range(num_eval_samples):\n",
    "        batch = next(eval_iter)\n",
    "        \n",
    "        Z = batch['Z_set'].to(device)\n",
    "        mask = batch['mask'].to(device)\n",
    "        D_target = batch['D_target'].to(device)\n",
    "        \n",
    "        # Predict coordinates directly\n",
    "        coords_pred = supervised_head(Z, mask)\n",
    "        \n",
    "        # Take first sample in batch\n",
    "        b = 0\n",
    "        m = mask[b]\n",
    "        n_valid = m.sum().item()\n",
    "        \n",
    "        coords_pred_sample = coords_pred[b, m].cpu()\n",
    "        D_target_sample = D_target[b, m][:, m].cpu()\n",
    "        \n",
    "        # Compute MDS from target EDM (ground truth)\n",
    "        n = D_target_sample.shape[0]\n",
    "        Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "        B_target = -0.5 * (Jn @ (D_target_sample ** 2) @ Jn)\n",
    "        coords_target = uet.classical_mds(B_target, d_out=2)\n",
    "        \n",
    "        # Canonicalize both\n",
    "        coords_pred_canon = uet.canonicalize_coords(coords_pred_sample)\n",
    "        coords_target_canon = uet.canonicalize_coords(coords_target)\n",
    "        \n",
    "        # Compute correlation\n",
    "        corr_x = np.corrcoef(coords_pred_canon[:, 0].numpy(), coords_target_canon[:, 0].numpy())[0, 1]\n",
    "        corr_y = np.corrcoef(coords_pred_canon[:, 1].numpy(), coords_target_canon[:, 1].numpy())[0, 1]\n",
    "        avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "        \n",
    "        # EDM correlation\n",
    "        D_pred_sample = torch.cdist(coords_pred_sample.unsqueeze(0), coords_pred_sample.unsqueeze(0)).squeeze(0)\n",
    "        edm_corr = np.corrcoef(\n",
    "            D_pred_sample.flatten().numpy(),\n",
    "            D_target_sample.flatten().numpy()\n",
    "        )[0, 1]\n",
    "        \n",
    "        eval_results.append({\n",
    "            'sample': i,\n",
    "            'n_points': n_valid,\n",
    "            'corr_x': corr_x,\n",
    "            'corr_y': corr_y,\n",
    "            'avg_corr': avg_corr,\n",
    "            'edm_corr': edm_corr,\n",
    "            'coords_pred': coords_pred_canon.numpy(),\n",
    "            'coords_target': coords_target_canon.numpy()\n",
    "        })\n",
    "        \n",
    "        print(f\"Sample {i}: n={n_valid:3d} | EDM_corr={edm_corr:.4f} | \"\n",
    "              f\"Coord_corr: x={corr_x:.4f}, y={corr_y:.4f}, avg={avg_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLOT GROUND TRUTH VS PREDICTED COORDINATES\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, num_eval_samples, figsize=(4*num_eval_samples, 8))\n",
    "\n",
    "for i, res in enumerate(eval_results):\n",
    "    # Predicted coordinates\n",
    "    axes[0, i].scatter(res['coords_pred'][:, 0], res['coords_pred'][:, 1], \n",
    "                      s=10, alpha=0.6, c='blue')\n",
    "    axes[0, i].set_title(f\"Sample {i}: Predicted\\ncorr={res['avg_corr']:.3f}\")\n",
    "    axes[0, i].set_aspect('equal')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ground truth coordinates\n",
    "    axes[1, i].scatter(res['coords_target'][:, 0], res['coords_target'][:, 1],\n",
    "                      s=10, alpha=0.6, c='red')\n",
    "    axes[1, i].set_title(f\"Ground Truth\")\n",
    "    axes[1, i].set_aspect('equal')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('supervised_baseline_coords_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# COMPLETE NOTEBOOK: ST-ONLY MODEL (PHASE 1) - SINGLE PATCH INFERENCE\n",
    "# ===================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "output_dir = \"/home/ehtesamul/sc_st/model/gems_mousebrain_output\"\n",
    "# timestamp = \"20251125_105556\"\n",
    "timestamp = \"20251125_105556\"\n",
    "\n",
    "\n",
    "# USE PHASE 1 CHECKPOINT (ST-ONLY, BEFORE SC FINE-TUNING)\n",
    "checkpoint_path = f\"{output_dir}/phase1_st_checkpoint.pt\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ST-ONLY MODEL INFERENCE (Phase 1, Single Patch)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 1: LOAD TEST DATA\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Test Data ---\")\n",
    "\n",
    "from run_mouse_brain_2 import load_mouse_data\n",
    "scadata, stadata = load_mouse_data()\n",
    "\n",
    "# Extract SC gene expression\n",
    "common = sorted(list(set(scadata.var_names) & set(stadata.var_names)))\n",
    "X_sc = scadata[:, common].X\n",
    "if hasattr(X_sc, \"toarray\"):\n",
    "    X_sc = X_sc.toarray()\n",
    "sc_expr = torch.tensor(X_sc, dtype=torch.float32)\n",
    "\n",
    "n_cells = sc_expr.shape[0]\n",
    "n_genes = sc_expr.shape[1]\n",
    "\n",
    "print(f\"Loaded SC data: {n_cells} cells, {n_genes} genes\")\n",
    "print(f\"Ground truth coords shape: {scadata.obsm['spatial_gt'].shape}\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 2: LOAD MODEL AND ST-ONLY CHECKPOINT (PHASE 1)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Loading Model and ST-Only Checkpoint (Phase 1) ---\")\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device='cuda',\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded ST-ONLY checkpoint from: {checkpoint_path}\")\n",
    "print(f\"  Best ST epoch: {checkpoint.get('E_ST_best', 'N/A')}\")\n",
    "print(f\"  This model was trained ONLY on ST data (NO SC fine-tuning)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STEP 3: SINGLE PATCH INFERENCE (DIAGNOSTIC MODE)\n",
    "# ===================================================================\n",
    "print(\"\\n--- Running Single Patch Inference (ST-Only Model) ---\")\n",
    "print(f\"Config: patch_size={n_cells}, coverage_per_cell=1.0, n_align_iters=1\")\n",
    "print(\"This runs ONE patch with ALL cells (no stitching)\")\n",
    "print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIFFUSION INFERENCE ON ST MINISETS - COMPLETE CODE\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "from core_models_et_p1 import STSetDataset, collate_minisets\n",
    "import utils_et as uet\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = '/home/ehtesamul/sc_st/model/gems_mousebrain_output'\n",
    "checkpoint_path = os.path.join(output_dir, 'phase1_st_checkpoint.pt')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFUSION MODEL INFERENCE ON ST MINISETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ST DATA\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "st_ct     = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_celltype_et.csv'\n",
    "\n",
    "print(\"\\nLoading ST1 data...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "st_ct_df = pd.read_csv(st_ct, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "stadata.obs['celltype_mapped_refined'] = st_ct_df.idxmax(axis=1).values\n",
    "\n",
    "print(f\"ST1 loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# Extract and canonicalize\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST coords canonicalized: scale={st_scale[0].item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL AND PHASE 1 CHECKPOINT\n",
    "# ============================================================================\n",
    "\n",
    "n_genes = stadata.shape[1]\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=str(device),\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded Phase 1 ST-only checkpoint\")\n",
    "print(f\"  Best ST epoch: {checkpoint.get('E_ST_best', 'N/A')}\")\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "# ============================================================================\n",
    "# RUN STAGE B TO GET TARGETS_DICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n=== Running Stage B ===\")\n",
    "slides_dict = {0: (st_coords, st_expr)}\n",
    "model.train_stageB(\n",
    "    slides=slides_dict,\n",
    "    outdir='temp_stageB_cache'\n",
    ")\n",
    "print(\"Stage B complete.\")\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE ST MINISET DATASET\n",
    "# ============================================================================\n",
    "\n",
    "st_gene_expr_dict_cpu = {0: st_expr.cpu()}\n",
    "\n",
    "st_dataset = STSetDataset(\n",
    "    targets_dict=model.targets_dict,\n",
    "    encoder=model.encoder,\n",
    "    st_gene_expr_dict=st_gene_expr_dict_cpu,\n",
    "    n_min=64,\n",
    "    n_max=384,\n",
    "    D_latent=model.D_latent,\n",
    "    num_samples=4000,\n",
    "    knn_k=12,\n",
    "    device=device,\n",
    "    landmarks_L=16\n",
    ")\n",
    "\n",
    "st_loader = DataLoader(\n",
    "    st_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_minisets\n",
    ")\n",
    "\n",
    "print(f\"ST dataset created: {len(st_dataset)} samples\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN DIFFUSION INFERENCE ON ST MINISETS\n",
    "# ============================================================================\n",
    "\n",
    "num_eval_samples = 5\n",
    "diffusion_results = []\n",
    "\n",
    "print(\"\\n--- Running diffusion inference on ST minisets ---\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_iter = iter(st_loader)\n",
    "    \n",
    "    for i in range(num_eval_samples):\n",
    "        batch = next(eval_iter)\n",
    "        \n",
    "        mask = batch['mask'].to(device)\n",
    "        D_target = batch['D_target'].to(device)\n",
    "        \n",
    "        # Take first sample in batch\n",
    "        b = 0\n",
    "        m = mask[b]\n",
    "        n_valid = m.sum().item()\n",
    "        \n",
    "        # Get indices for this miniset\n",
    "        indices = batch['overlap_info']['indices'][b]\n",
    "        valid_indices = indices[m].cpu()\n",
    "        \n",
    "        # Get gene expression for these specific ST spots\n",
    "        miniset_expr = st_expr.cpu()[valid_indices]\n",
    "        \n",
    "        print(f\"Sample {i}: Running diffusion inference on {n_valid} points...\")\n",
    "        \n",
    "        # Run patchwise inference with single patch (no stitching)\n",
    "        inf_results = model.infer_sc_patchwise(\n",
    "            sc_gene_expr=miniset_expr,\n",
    "            n_timesteps_sample=300,\n",
    "            sigma_min=0.01,\n",
    "            sigma_max=7.0,\n",
    "            patch_size=n_valid,          # Single patch = all points\n",
    "            coverage_per_cell=1.0,       # No overlap\n",
    "            n_align_iters=1,             # No stitching\n",
    "            eta=0.0,\n",
    "            guidance_scale=6.0,\n",
    "            return_coords=True,\n",
    "            debug_flag=False,\n",
    "            debug_every=10,\n",
    "        )\n",
    "        \n",
    "        # Extract predicted coordinates\n",
    "        coords_diffusion = inf_results['coords_canon']\n",
    "        \n",
    "        # Get ground truth coordinates from target EDM\n",
    "        D_target_sample = D_target[b, m][:, m].cpu()\n",
    "        n = D_target_sample.shape[0]\n",
    "        Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "        B_target = -0.5 * (Jn @ (D_target_sample ** 2) @ Jn)\n",
    "        coords_target = uet.classical_mds(B_target, d_out=2)\n",
    "        coords_target_canon = uet.canonicalize_coords(coords_target)\n",
    "        \n",
    "        # Compute correlations\n",
    "        corr_x = np.corrcoef(coords_diffusion[:, 0].numpy(), coords_target_canon[:, 0].numpy())[0, 1]\n",
    "        corr_y = np.corrcoef(coords_diffusion[:, 1].numpy(), coords_target_canon[:, 1].numpy())[0, 1]\n",
    "        avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "        \n",
    "        # EDM correlation\n",
    "        D_diffusion = torch.cdist(coords_diffusion.unsqueeze(0), coords_diffusion.unsqueeze(0)).squeeze(0)\n",
    "        edm_corr = np.corrcoef(\n",
    "            D_diffusion.flatten().numpy(),\n",
    "            D_target_sample.flatten().numpy()\n",
    "        )[0, 1]\n",
    "        \n",
    "        diffusion_results.append({\n",
    "            'sample': i,\n",
    "            'n_points': n_valid,\n",
    "            'corr_x': corr_x,\n",
    "            'corr_y': corr_y,\n",
    "            'avg_corr': avg_corr,\n",
    "            'edm_corr': edm_corr,\n",
    "            'coords_diffusion': coords_diffusion.numpy(),\n",
    "            'coords_target': coords_target_canon.numpy()\n",
    "        })\n",
    "        \n",
    "        print(f\"  EDM_corr={edm_corr:.4f} | Coord_corr: x={corr_x:.4f}, y={corr_y:.4f}, avg={avg_corr:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFUSION INFERENCE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PRINT COMPARISON (assuming eval_results from supervised baseline exists)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nDiffusion Model (Phase 1 ST-only) Results:\")\n",
    "print(f\"  Average EDM correlation:   {np.mean([r['edm_corr'] for r in diffusion_results]):.4f}\")\n",
    "print(f\"  Average Coord correlation: {np.mean([r['avg_corr'] for r in diffusion_results]):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT: DIFFUSION vs GROUND TRUTH\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, num_eval_samples, figsize=(4*num_eval_samples, 8))\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    # Diffusion prediction\n",
    "    axes[0, i].scatter(diffusion_results[i]['coords_diffusion'][:, 0],\n",
    "                      diffusion_results[i]['coords_diffusion'][:, 1],\n",
    "                      s=10, alpha=0.6, c='green')\n",
    "    axes[0, i].set_title(f\"Sample {i}: Diffusion\\ncorr={diffusion_results[i]['avg_corr']:.3f}\")\n",
    "    axes[0, i].set_aspect('equal')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[1, i].scatter(diffusion_results[i]['coords_target'][:, 0],\n",
    "                      diffusion_results[i]['coords_target'][:, 1],\n",
    "                      s=10, alpha=0.6, c='red')\n",
    "    axes[1, i].set_title(f\"Ground Truth\")\n",
    "    axes[1, i].set_aspect('equal')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('diffusion_vs_groundtruth.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Plot saved: diffusion_vs_groundtruth.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIFFUSION INFERENCE ON ST MINISETS - FIXED\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from core_models_et_p3 import GEMSModel\n",
    "import utils_et as uet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = '/home/ehtesamul/sc_st/model/gems_mousebrain_output'\n",
    "checkpoint_path = os.path.join(output_dir, 'phase2_sc_finetuned_checkpoint.pt')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DIFFUSION MODEL INFERENCE ON ST MINISETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD ST DATA\n",
    "# ============================================================================\n",
    "\n",
    "st_counts = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_counts_et.csv'\n",
    "st_meta   = '/home/ehtesamul/sc_st/data/mousedata_2020/E1z2/simu_st1_metadata_et.csv'\n",
    "\n",
    "print(\"\\nLoading ST1 data...\")\n",
    "st_expr_df = pd.read_csv(st_counts, index_col=0)\n",
    "st_meta_df = pd.read_csv(st_meta, index_col=0)\n",
    "\n",
    "stadata = ad.AnnData(X=st_expr_df.values.T)\n",
    "stadata.obs_names = st_expr_df.columns\n",
    "stadata.var_names = st_expr_df.index\n",
    "stadata.obsm['spatial'] = st_meta_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "X_st = stadata.X\n",
    "if hasattr(X_st, \"toarray\"):\n",
    "    X_st = X_st.toarray()\n",
    "\n",
    "st_expr = torch.tensor(X_st, dtype=torch.float32, device=device)\n",
    "st_coords_raw = torch.tensor(stadata.obsm['spatial'], dtype=torch.float32, device=device)\n",
    "\n",
    "slide_ids = torch.zeros(st_expr.shape[0], dtype=torch.long, device=device)\n",
    "st_coords, st_mu, st_scale = uet.canonicalize_st_coords_per_slide(\n",
    "    st_coords_raw, slide_ids\n",
    ")\n",
    "\n",
    "print(f\"ST loaded: {stadata.shape[0]} spots, {stadata.shape[1]} genes\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "n_genes = stadata.shape[1]\n",
    "\n",
    "model = GEMSModel(\n",
    "    n_genes=n_genes,\n",
    "    n_embedding=[512, 256, 128],\n",
    "    D_latent=32,\n",
    "    c_dim=256,\n",
    "    n_heads=4,\n",
    "    isab_m=64,\n",
    "    device=str(device),\n",
    "    use_canonicalize=True,\n",
    "    use_dist_bias=True,\n",
    "    dist_bins=24,\n",
    "    dist_head_shared=True,\n",
    "    use_angle_features=True,\n",
    "    angle_bins=8,\n",
    "    knn_k=12,\n",
    "    self_conditioning=True,\n",
    "    sc_feat_mode='concat',\n",
    "    landmarks_L=16,\n",
    ")\n",
    "\n",
    "print(f\"\\nLoading checkpoint: {checkpoint_path}\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.context_encoder.load_state_dict(checkpoint['context_encoder'])\n",
    "model.generator.load_state_dict(checkpoint['generator'])\n",
    "model.score_net.load_state_dict(checkpoint['score_net'])\n",
    "\n",
    "print(f\"✓ Loaded Phase 1 checkpoint (best epoch: {checkpoint.get('E_ST_best', 'N/A')})\")\n",
    "\n",
    "model.encoder.eval()\n",
    "model.context_encoder.eval()\n",
    "model.score_net.eval()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLE ST MINISETS AND RUN DIFFUSION INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "num_eval_samples = 10\n",
    "diffusion_results = []\n",
    "\n",
    "print(\"\\n--- Running diffusion inference on ST minisets ---\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    # Sample random miniset (same logic as STSetDataset)\n",
    "    n_min, n_max = 192, 384\n",
    "    n_total = st_coords.shape[0]\n",
    "    \n",
    "    # Random subset size\n",
    "    n = np.random.randint(n_min, min(n_max + 1, n_total))\n",
    "    \n",
    "    # Random indices\n",
    "    indices = torch.randperm(n_total)[:n]\n",
    "    \n",
    "    # Get gene expression and coords for this miniset\n",
    "    miniset_expr = st_expr[indices].cpu()\n",
    "    miniset_coords = st_coords[indices].cpu()\n",
    "    \n",
    "    # Compute ground truth EDM\n",
    "    D_target = torch.cdist(miniset_coords, miniset_coords)\n",
    "    \n",
    "    print(f\"Sample {i}: Running diffusion on {n} points...\")\n",
    "    \n",
    "    # Run inference with single patch (no stitching)\n",
    "    with torch.no_grad():\n",
    "        inf_results = model.infer_sc_patchwise(\n",
    "            sc_gene_expr=miniset_expr,\n",
    "            n_timesteps_sample=300,\n",
    "            sigma_min=0.01,\n",
    "            sigma_max=7.0,\n",
    "            patch_size=n,            # Single patch\n",
    "            coverage_per_cell=1.0,   # No overlap\n",
    "            n_align_iters=1,         # No alignment\n",
    "            eta=0.0,\n",
    "            guidance_scale=6.0,\n",
    "            return_coords=True,\n",
    "            debug_flag=False,\n",
    "        )\n",
    "    \n",
    "    coords_diffusion = inf_results['coords_canon']\n",
    "    \n",
    "    # Ground truth coords via MDS\n",
    "    Jn = torch.eye(n) - torch.ones(n, n) / n\n",
    "    B_target = -0.5 * (Jn @ (D_target**2) @ Jn)\n",
    "    coords_target = uet.classical_mds(B_target, d_out=2)\n",
    "    coords_target_canon = uet.canonicalize_coords(coords_target)\n",
    "    \n",
    "    # Compute correlations\n",
    "    corr_x = np.corrcoef(coords_diffusion[:, 0].numpy(), coords_target_canon[:, 0].numpy())[0, 1]\n",
    "    corr_y = np.corrcoef(coords_diffusion[:, 1].numpy(), coords_target_canon[:, 1].numpy())[0, 1]\n",
    "    avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "    \n",
    "    # EDM correlation\n",
    "    D_diffusion = torch.cdist(coords_diffusion.unsqueeze(0), coords_diffusion.unsqueeze(0)).squeeze(0)\n",
    "    edm_corr = np.corrcoef(\n",
    "        D_diffusion.flatten().numpy(),\n",
    "        D_target.flatten().numpy()\n",
    "    )[0, 1]\n",
    "    \n",
    "    diffusion_results.append({\n",
    "        'sample': i,\n",
    "        'n_points': n,\n",
    "        'corr_x': corr_x,\n",
    "        'corr_y': corr_y,\n",
    "        'avg_corr': avg_corr,\n",
    "        'edm_corr': edm_corr,\n",
    "        'coords_diffusion': coords_diffusion.numpy(),\n",
    "        'coords_target': coords_target_canon.numpy()\n",
    "    })\n",
    "    \n",
    "    print(f\"  EDM_corr={edm_corr:.4f} | Coord: x={corr_x:.4f}, y={corr_y:.4f}, avg={avg_corr:.4f}\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDiffusion Results (avg over {num_eval_samples} samples):\")\n",
    "print(f\"  EDM correlation:   {np.mean([r['edm_corr'] for r in diffusion_results]):.4f}\")\n",
    "print(f\"  Coord correlation: {np.mean([r['avg_corr'] for r in diffusion_results]):.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# PLOT - 3 COLUMNS MAX PER ROW\n",
    "# ============================================================================\n",
    "\n",
    "n_cols = min(3, num_eval_samples)\n",
    "n_rows = int(np.ceil(num_eval_samples / n_cols)) * 2  # *2 for diffusion + GT rows\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "# Handle single row case\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if n_cols == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    row_pair = (i // n_cols) * 2  # Which pair of rows (diffusion + GT)\n",
    "    col = i % n_cols\n",
    "    \n",
    "    # Diffusion prediction\n",
    "    ax_diff = axes[row_pair, col]\n",
    "    ax_diff.scatter(diffusion_results[i]['coords_diffusion'][:, 0],\n",
    "                   diffusion_results[i]['coords_diffusion'][:, 1],\n",
    "                   s=10, alpha=0.6, c='green')\n",
    "    ax_diff.set_title(f\"Sample {i}: Diffusion\\n\"\n",
    "                     f\"Coord: {diffusion_results[i]['avg_corr']:.3f} | \"\n",
    "                     f\"EDM: {diffusion_results[i]['edm_corr']:.3f}\",\n",
    "                     fontsize=10)\n",
    "    ax_diff.set_aspect('equal')\n",
    "    ax_diff.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ground truth\n",
    "    ax_gt = axes[row_pair + 1, col]\n",
    "    ax_gt.scatter(diffusion_results[i]['coords_target'][:, 0],\n",
    "                 diffusion_results[i]['coords_target'][:, 1],\n",
    "                 s=10, alpha=0.6, c='red')\n",
    "    ax_gt.set_title(f\"Ground Truth (n={diffusion_results[i]['n_points']})\",\n",
    "                   fontsize=10)\n",
    "    ax_gt.set_aspect('equal')\n",
    "    ax_gt.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_eval_samples, n_rows // 2 * n_cols):\n",
    "    row_pair = (i // n_cols) * 2\n",
    "    col = i % n_cols\n",
    "    axes[row_pair, col].axis('off')\n",
    "    axes[row_pair + 1, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SMART OUTLIER REMOVAL - DISTANCE-BASED METHOD\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER REMOVAL - DISTANCE FROM MEDIAN CENTER\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRationale:\")\n",
    "print(\"- Diffusion occasionally samples points in low-probability tail regions\")\n",
    "print(\"- Ground truth tissue has consistent density (filled region)\")\n",
    "print(\"- Outliers are scattered points FAR from main cluster\")\n",
    "print(\"- Method: Keep only points within 90th percentile distance from median center\")\n",
    "print(\"- Why median? Robust to outliers (unlike mean)\")\n",
    "print(\"- Why 90th percentile? Keeps main distribution, removes extreme tail\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "def remove_outliers_distance_percentile(coords, coords_target, percentile=90):\n",
    "    \"\"\"\n",
    "    Remove outliers based on distance from median center.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Find median center (robust to outliers)\n",
    "    2. Compute distance of each point from center\n",
    "    3. Keep only points within `percentile` of distances\n",
    "    4. Filter both predicted and target coords to match\n",
    "    \n",
    "    Args:\n",
    "        coords: (n, 2) predicted coordinates\n",
    "        coords_target: (n, 2) target coordinates\n",
    "        percentile: keep points within this percentile (90 = remove top 10%)\n",
    "    \n",
    "    Returns:\n",
    "        coords_clean, coords_target_clean, inlier_mask\n",
    "    \"\"\"\n",
    "    # Use MEDIAN center (robust to outliers, unlike mean)\n",
    "    center = np.median(coords, axis=0)\n",
    "    \n",
    "    # Distance from center for each point\n",
    "    dists = np.linalg.norm(coords - center, axis=1)\n",
    "    \n",
    "    # Threshold: keep only points within percentile\n",
    "    threshold = np.percentile(dists, percentile)\n",
    "    \n",
    "    # Inlier mask\n",
    "    inlier_mask = dists <= threshold\n",
    "    \n",
    "    # Filter both predicted and target\n",
    "    coords_clean = coords[inlier_mask]\n",
    "    coords_target_clean = coords_target[inlier_mask]\n",
    "    \n",
    "    return coords_clean, coords_target_clean, inlier_mask, threshold\n",
    "\n",
    "# ============================================================================\n",
    "# CLEAN EACH SAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "diffusion_results_clean = []\n",
    "\n",
    "for i, res in enumerate(diffusion_results):\n",
    "    coords_pred = res['coords_diffusion']\n",
    "    coords_gt = res['coords_target']\n",
    "    n_orig = len(coords_pred)\n",
    "    \n",
    "    # Remove outliers\n",
    "    coords_clean, coords_gt_clean, mask, thresh = remove_outliers_distance_percentile(\n",
    "        coords_pred, coords_gt, percentile=90\n",
    "    )\n",
    "    \n",
    "    n_kept = len(coords_clean)\n",
    "    n_removed = n_orig - n_kept\n",
    "    pct_removed = 100 * n_removed / n_orig\n",
    "    \n",
    "    # Recanonialize after filtering\n",
    "    coords_clean_t = torch.from_numpy(coords_clean).float()\n",
    "    coords_gt_t = torch.from_numpy(coords_gt_clean).float()\n",
    "    \n",
    "    coords_clean_canon = uet.canonicalize_coords(coords_clean_t).numpy()\n",
    "    coords_gt_canon = uet.canonicalize_coords(coords_gt_t).numpy()\n",
    "    \n",
    "    # Recompute correlations\n",
    "    corr_x_before = res['corr_x']\n",
    "    corr_y_before = res['corr_y']\n",
    "    avg_corr_before = res['avg_corr']\n",
    "    edm_corr_before = res['edm_corr']\n",
    "    \n",
    "    corr_x = np.corrcoef(coords_clean_canon[:, 0], coords_gt_canon[:, 0])[0, 1]\n",
    "    corr_y = np.corrcoef(coords_clean_canon[:, 1], coords_gt_canon[:, 1])[0, 1]\n",
    "    avg_corr = (abs(corr_x) + abs(corr_y)) / 2.0\n",
    "    \n",
    "    # EDM correlation\n",
    "    D_clean = torch.cdist(\n",
    "        torch.from_numpy(coords_clean_canon).unsqueeze(0).float(),\n",
    "        torch.from_numpy(coords_clean_canon).unsqueeze(0).float()\n",
    "    ).squeeze(0)\n",
    "    D_gt = torch.cdist(\n",
    "        torch.from_numpy(coords_gt_canon).unsqueeze(0).float(),\n",
    "        torch.from_numpy(coords_gt_canon).unsqueeze(0).float()\n",
    "    ).squeeze(0)\n",
    "    \n",
    "    edm_corr = np.corrcoef(D_clean.flatten().numpy(), D_gt.flatten().numpy())[0, 1]\n",
    "    \n",
    "    # Store\n",
    "    diffusion_results_clean.append({\n",
    "        'sample': i,\n",
    "        'n_points': n_kept,\n",
    "        'n_removed': n_removed,\n",
    "        'pct_removed': pct_removed,\n",
    "        'corr_x': corr_x,\n",
    "        'corr_y': corr_y,\n",
    "        'avg_corr': avg_corr,\n",
    "        'edm_corr': edm_corr,\n",
    "        'coords_diffusion': coords_clean_canon,\n",
    "        'coords_target': coords_gt_canon,\n",
    "        'threshold': thresh\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Sample {i}: removed {n_removed}/{n_orig} outliers ({pct_removed:.1f}%), \"\n",
    "          f\"threshold={thresh:.3f}\")\n",
    "    print(f\"  Before: Coord={avg_corr_before:.3f}, EDM={edm_corr_before:.3f}\")\n",
    "    print(f\"  After:  Coord={avg_corr:.3f} (Δ={avg_corr-avg_corr_before:+.3f}), \"\n",
    "          f\"EDM={edm_corr:.3f} (Δ={edm_corr-edm_corr_before:+.3f})\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SUMMARY: BEFORE vs AFTER OUTLIER REMOVAL\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "avg_coord_before = np.mean([r['avg_corr'] for r in diffusion_results])\n",
    "avg_edm_before = np.mean([r['edm_corr'] for r in diffusion_results])\n",
    "\n",
    "avg_coord_after = np.mean([r['avg_corr'] for r in diffusion_results_clean])\n",
    "avg_edm_after = np.mean([r['edm_corr'] for r in diffusion_results_clean])\n",
    "\n",
    "avg_pct_removed = np.mean([r['pct_removed'] for r in diffusion_results_clean])\n",
    "\n",
    "print(f\"Average Coordinate Correlation:\")\n",
    "print(f\"  Before: {avg_coord_before:.4f}\")\n",
    "print(f\"  After:  {avg_coord_after:.4f} (Δ={avg_coord_after-avg_coord_before:+.4f})\")\n",
    "\n",
    "print(f\"\\nAverage EDM Correlation:\")\n",
    "print(f\"  Before: {avg_edm_before:.4f}\")\n",
    "print(f\"  After:  {avg_edm_after:.4f} (Δ={avg_edm_after-avg_edm_before:+.4f})\")\n",
    "\n",
    "print(f\"\\nAverage outliers removed: {avg_pct_removed:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*80)\n",
    "if avg_edm_after - avg_edm_before > 0.1:\n",
    "    print(\"✓ EDM correlation IMPROVED significantly after outlier removal\")\n",
    "    print(\"  → Confirms outliers were corrupting distance metrics\")\n",
    "    print(\"  → Main cluster has better geometric structure than raw output\")\n",
    "elif avg_edm_after - avg_edm_before > 0:\n",
    "    print(\"✓ EDM correlation improved slightly\")\n",
    "    print(\"  → Outliers had some negative effect on distances\")\n",
    "else:\n",
    "    print(\"⚠ EDM correlation unchanged or decreased\")\n",
    "    print(\"  → Problem is not just outliers, geometry of main cluster needs work\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLE PLOT: PREDICTED vs GROUND TRUTH (AFTER OUTLIER REMOVAL)\n",
    "# ============================================================================\n",
    "\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(num_eval_samples / n_cols)) * 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "if n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "if n_cols == 1:\n",
    "    axes = axes.reshape(-1, 1)\n",
    "\n",
    "for i in range(num_eval_samples):\n",
    "    row_pair = (i // n_cols) * 2\n",
    "    col = i % n_cols\n",
    "    \n",
    "    # Predicted (cleaned)\n",
    "    ax_pred = axes[row_pair, col]\n",
    "    ax_pred.scatter(diffusion_results_clean[i]['coords_diffusion'][:, 0],\n",
    "                   diffusion_results_clean[i]['coords_diffusion'][:, 1],\n",
    "                   s=10, alpha=0.7, c='#2ecc71', edgecolors='none')\n",
    "    ax_pred.set_title(f\"Sample {i}: Predicted\\n\"\n",
    "                     f\"Coord: {diffusion_results_clean[i]['avg_corr']:.3f} | \"\n",
    "                     f\"EDM: {diffusion_results_clean[i]['edm_corr']:.3f}\",\n",
    "                     fontsize=10)\n",
    "    ax_pred.set_aspect('equal')\n",
    "    ax_pred.grid(True, alpha=0.2)\n",
    "    \n",
    "    # Ground Truth\n",
    "    ax_gt = axes[row_pair + 1, col]\n",
    "    ax_gt.scatter(diffusion_results_clean[i]['coords_target'][:, 0],\n",
    "                 diffusion_results_clean[i]['coords_target'][:, 1],\n",
    "                 s=10, alpha=0.7, c='#e74c3c', edgecolors='none')\n",
    "    ax_gt.set_title(f\"Ground Truth (n={diffusion_results_clean[i]['n_points']})\",\n",
    "                   fontsize=10)\n",
    "    ax_gt.set_aspect('equal')\n",
    "    ax_gt.grid(True, alpha=0.2)\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_eval_samples, n_rows // 2 * n_cols):\n",
    "    row_pair = (i // n_cols) * 2\n",
    "    col = i % n_cols\n",
    "    axes[row_pair, col].axis('off')\n",
    "    axes[row_pair + 1, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('cleaned_results.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# print(\"✓ Saved plot: outlier_removal_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
