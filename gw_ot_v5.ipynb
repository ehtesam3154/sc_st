{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.special import expit\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# 1) Load your ST counts and coords\n",
    "st_raw = pd.read_csv('data/mousedata_2020/E1z2/simu_st_counts.csv', index_col=0).T\n",
    "coords = pd.read_csv('data/mousedata_2020/E1z2/simu_st_metadata.csv', index_col=0)[['coord_x','coord_y']].values\n",
    "\n",
    "# 2) Optionally normalize & log‐transform\n",
    "adata = sc.AnnData(st_raw)\n",
    "\n",
    "# 2) Highly‐variable gene selection with cell_ranger flavor\n",
    "adata_st = sc.AnnData(st_raw)\n",
    "X_umi = (adata.X if hasattr(adata, 'X') else st_raw.values).astype(np.float32).astype(np.float32)\n",
    "\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "X_log = adata.X.astype(np.float32)\n",
    "# sc.pp.highly_variable_genes(adata_st, n_top_genes=500, flavor='cell_ranger')\n",
    "# hvg_mask = adata_st.var['highly_variable'].values\n",
    "# X_all = (adata.X if hasattr(adata, 'X') else st_raw.values).astype(np.float32)[:, hvg_mask]\n",
    "\n",
    "\n",
    "class NB_VAE(nn.Module):\n",
    "    def __init__(self, n_input, n_latent = 10, hidden= 128, phi = 1.0):\n",
    "        super().__init__()\n",
    "        self.n_input = n_input\n",
    "        # self.phi = phi\n",
    "        self.log_phi = nn.Parameter(torch.zeros(n_input))\n",
    "        #encoder \n",
    "        self.enc1 = nn.Linear(n_input, hidden)\n",
    "        self.enc2 = nn.Linear(hidden, n_latent*2)\n",
    "        #decoder for rate r\n",
    "        self.dec_r1 = nn.Linear(n_latent, hidden)\n",
    "        self.dec_r2 = nn.Linear(hidden, n_input)\n",
    "        #decoder for logit p\n",
    "        self.dec_p1 = nn.Linear(n_latent, hidden)\n",
    "        self.dec_p2 = nn.Linear(hidden, n_input)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = F.dropout(x, p=0.1, training=self.training)\n",
    "        h = torch.tanh(self.enc1(h))\n",
    "        h = self.enc2(h)\n",
    "        mu, logvar = torch.chunk(h, 2, dim=1)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparam(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode_r(self, z):\n",
    "        h = torch.tanh(self.dec_r1(z))\n",
    "        return F.softplus(self.dec_r2(h)) + 1e-6\n",
    "    \n",
    "    def decode_p(self, z):\n",
    "        h = torch.tanh(self.dec_p1(z))\n",
    "        return self.dec_p2(h)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparam(mu, logvar)\n",
    "        r = self.decode_r(z)\n",
    "        p_logit = self.decode_p(z)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        return r, p_logit, mu, logvar, phi\n",
    "    \n",
    "#nb negative log likelihood + KL\n",
    "def nb_loss(x, r, p_logit, phi):\n",
    "    #x, r: [B, G], p_logit: logits\n",
    "    p = torch.sigmoid(p_logit)\n",
    "    phi_t = torch.tensor(phi, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    t1 = torch.lgamma(phi_t + x) - torch.lgamma(phi_t) - torch.lgamma(x+1)\n",
    "    t2 = phi_t * (torch.log(phi_t) - torch.log(phi_t + r))\n",
    "    t3 = x * (torch.log(r) - torch.log(phi_t +r))\n",
    "    ll = t1 + t2 + t3\n",
    "    return -ll.sum(dim=1).mean() #negative log-like over genes then mean over batch\n",
    "\n",
    "def kl_loss(mu, logvar):\n",
    "    return 0.5 * torch.sum(-logvar + logvar.exp() + mu.pow(2)-1, dim=1).mean()\n",
    "\n",
    "\n",
    "#train\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dataset = TensorDataset(torch.from_numpy(X_umi))\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = NB_VAE(n_input=X_umi.shape[1], n_latent=10).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_epochs = 400\n",
    "\n",
    "for ep in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (xb, ) in loader:\n",
    "        xb = xb.to(device)\n",
    "        r, p_logit, mu, logvar, phi = model(xb)\n",
    "        loss = nb_loss(xb, r, p_logit, phi) + kl_loss(mu, logvar) * min(1., ep/200)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    print(f'epoch {ep}/{n_epochs} avg_loss={(total_loss/len(dataset)):.4f}')\n",
    "\n",
    "#generate 30x synthetic spots + jittered coords\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_tensor = torch.from_numpy(X_umi).to(device)\n",
    "    r_all, p_logit_all, mu_all, logvar_all, _ = model(X_tensor)\n",
    "    r_all = r_all.cpu().numpy()\n",
    "    p_all = expit(p_logit_all.cpu().numpy())\n",
    "\n",
    "    z_orig = mu_all.cpu().numpy() #[n spots * latent dim]\n",
    "\n",
    "X_synth, coords_synth = [], []\n",
    "n_aug = 40\n",
    "sigma = 0.05\n",
    "\n",
    "# for i in range(X_all.shape[0]):\n",
    "#     for _ in range(n_aug):\n",
    "#         r_i = r_all[i:i+1]\n",
    "#         p_i = p_all[i:i+1]\n",
    "#         #nb sample\n",
    "#         r_param = r_i * (1 - p_i)/ (p_i + 1e-8)\n",
    "#         X_new = np.random.negative_binomial(r_param, p_i).astype(np.float32).squeeze()\n",
    "#         X_synth.append(X_new)\n",
    "#         #jitter coords\n",
    "#         coords_synth.append(coords[i] + np.random.randn(2) * sigma)\n",
    "\n",
    "# for i in range(X_all.shape[0]):\n",
    "#     for _ in range(n_aug):\n",
    "#         r_i = r_all[i]\n",
    "#         p_i = p_all[i]\n",
    "#         mu_i = r_i * (1-p_i)/p_i\n",
    "#         #draw lambda ~ Gamma(shape=r_i, scale = mu_i/r_i)\n",
    "#         lam = np.random.gamma(shape=r_i, scale=(mu_i/r_i))\n",
    "#         #then draw counts ~ Possion(lam)\n",
    "#         X_new = np.random.poisson(lam).astype(np.float32).squeeze()\n",
    "#         X_synth.append(X_new)\n",
    "#         coords_synth.append(coords[i] + np.random.randn(2)*sigma)\n",
    "\n",
    "orig_totals = X_umi.sum(axis=1)  # shape (N_real,)\n",
    "  \n",
    "# inside your augmentation loop, replacing the nb sample & jitter step:\n",
    "for i in range(X_umi.shape[0]):\n",
    "    # 1) draw an NB sample as before\n",
    "    r_i = r_all[i:i+1]\n",
    "    p_i = p_all[i:i+1]\n",
    "    r_param = r_i * (1 - p_i) / (p_i + 1e-8)\n",
    "    for _ in range(n_aug):\n",
    "        X_new = np.random.negative_binomial(r_param, p_i).astype(np.float32).squeeze()\n",
    "        X_synth.append(X_new)\n",
    "\n",
    "        # 4) jitter coords exactly as before\n",
    "        # coords_synth.append(coords[i] + np.random.randn(2) * sigma)\n",
    "\n",
    "X_synth = np.stack(X_synth)\n",
    "# coords_synth = np.stack(coords_synth)\n",
    "\n",
    "#re encode synth into latent and interpolate coords\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    Zs_t = torch.from_numpy(X_synth).to(device)\n",
    "    mu_syn, _ = model.encode(Zs_t)\n",
    "    z_syn = mu_syn.cpu().numpy()\n",
    "\n",
    "# #build knn on the og z-space\n",
    "# nbrs = NearestNeighbors(n_neighbors=5).fit(z_orig)\n",
    "# dists, idxs = nbrs.kneighbors(z_syn)\n",
    "# w = np.exp(-dists)\n",
    "# w/= w.sum(axis=1, keepdims=True)\n",
    "\n",
    "# #weighted avg of the 5 real coords\n",
    "# coords_synth = (w[:, :, None] * coords[idxs]).sum(axis=1)\n",
    "\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# 0) original real data\n",
    "real_coords = coords.copy()       # (N_real,2)\n",
    "real_z      = z_orig.copy()       # (N_real,latent_dim)\n",
    "real_X      = X_umi.copy()        # (N_real,genes)\n",
    "\n",
    "# 1) synthetic counts (your NB‐VAE + jitter) → X_synth, z_syn already exist\n",
    "#    shape X_synth=(N_synth,genes), z_syn=(N_synth,latent_dim)\n",
    "\n",
    "# 2) boost boundary: make extra coords and dummy z’s (we’ll re‐encode them, or you can skip)\n",
    "hull = ConvexHull(real_coords)\n",
    "boundary_idxs = hull.vertices\n",
    "center = real_coords.mean(0)\n",
    "\n",
    "extra_coords = []\n",
    "extra_X      = []\n",
    "for i in boundary_idxs:\n",
    "    base = real_coords[i]\n",
    "    out  = base - center\n",
    "    out /= np.linalg.norm(out)\n",
    "    for _ in range(20):\n",
    "        jittered = base + 0.01 * out * np.random.rand()\n",
    "        extra_coords.append(jittered)\n",
    "        # if you want to give them “counts” you could re‐sample from the NB‐VAE,\n",
    "        # or you can just copy the original X_umi[i]\n",
    "        extra_X.append(real_X[i])\n",
    "\n",
    "extra_coords = np.array(extra_coords)      # (N_extra,2)\n",
    "extra_X = np.array(extra_X)           # (N_extra,genes)\n",
    "\n",
    "# 3) join all synth pools\n",
    "all_X   = np.vstack([X_synth, extra_X])     # (N_synth+N_extra,genes)\n",
    "all_z   = np.vstack([z_syn, model.encode(torch.from_numpy(extra_X).to(device))[0].detach().cpu().numpy()]) \n",
    "# or simply re‐encode extra_coords if you prefer\n",
    "all_coords = np.vstack([extra_coords])           # we'll compute final below\n",
    "\n",
    "# 4) Dirichlet‐weighted KNN interpolation _once_ on the big pool\n",
    "nbrs = NearestNeighbors(n_neighbors=5).fit(real_z)  # only real z’s as reference\n",
    "d, idx = nbrs.kneighbors(all_z)                    # shape (N_big,5)\n",
    "\n",
    "alpha = 0.1\n",
    "w = np.random.dirichlet([alpha]*5, size=len(all_z)) # (N_big,5)\n",
    "\n",
    "coords_synth = (real_coords[idx] * w[:,:,None]).sum(1)  # (N_big,2)\n",
    "\n",
    "# 5) Radial expansion of both real and synthetic\n",
    "scale = 1.05\n",
    "c0 = real_coords.mean(0)\n",
    "\n",
    "real_coords   = (real_coords   - c0)*scale + c0\n",
    "coords_synth  = (coords_synth  - c0)*scale + c0\n",
    "\n",
    "# 6) final concat\n",
    "X_aug = np.vstack([real_X,   all_X])         # (N_real + N_big, genes)\n",
    "C_aug = np.vstack([real_coords, coords_synth])  # (N_real + N_big, 2)\n",
    "\n",
    "print(\"Original:\", X_umi.shape[0], \"Augmented total:\", X_aug.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata = pd.read_csv('./data/mousedata_2020/E1z2/simu_sc_counts.csv',index_col=0)\n",
    "scdata = scdata.T\n",
    "stdata = pd.read_csv('data/mousedata_2020/E1z2/simu_st_counts.csv',index_col=0)\n",
    "stdata = stdata.T\n",
    "stgtcelltype = pd.read_csv('./data/mousedata_2020/E1z2/simu_st_celltype.csv',index_col=0)\n",
    "spcoor = pd.read_csv('./data/mousedata_2020/E1z2/simu_st_metadata.csv',index_col=0)\n",
    "scmetadata = pd.read_csv('./data/mousedata_2020/E1z2/metadata.csv',index_col=0)\n",
    "\n",
    "adata = sc.AnnData(scdata,obs=scmetadata)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "scdata = pd.DataFrame(adata.X,index=adata.obs_names,columns=adata.var_names)\n",
    "stadata = sc.AnnData(stdata)\n",
    "sc.pp.normalize_total(stadata)\n",
    "sc.pp.log1p(stadata)\n",
    "stdata = pd.DataFrame(stadata.X,index=stadata.obs_names,columns=stadata.var_names)\n",
    "\n",
    "adata.obsm['spatial'] = scmetadata[['x_global','y_global']].values\n",
    "stadata.obsm['spatial'] = spcoor\n",
    "\n",
    "# Preprocess data (normalize, log transform)\n",
    "adata = sc.AnnData(scdata, obs=scmetadata)\n",
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)\n",
    "scdata_processed = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n",
    "X_sc = torch.tensor(scdata_processed.values, dtype=torch.float32)\n",
    "\n",
    "stadata = sc.AnnData(stdata)\n",
    "sc.pp.normalize_total(stadata)\n",
    "sc.pp.log1p(stadata)\n",
    "stdata_processed = pd.DataFrame(stadata.X, index=stadata.obs_names, columns=stadata.var_names)\n",
    "X_st = torch.tensor(stdata_processed.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# First, let's create an AnnData object to use with scanpy functions\n",
    "# We'll use the spatial coordinates and cell type information\n",
    "\n",
    "# Create DataFrame with spatial coordinates\n",
    "coord_df = pd.DataFrame({\n",
    "    'coord_x': spcoor['coord_x'],\n",
    "    'coord_y': spcoor['coord_y']\n",
    "})\n",
    "\n",
    "# Get cell type information from stgtcelltype\n",
    "# This is a binary matrix with spots as rows and cell types as columns\n",
    "cell_type_columns = stgtcelltype.columns\n",
    "\n",
    "# For each spot, find the dominant cell type (if any)\n",
    "# We'll create a new column in our DataFrame for this\n",
    "dominant_celltypes = []\n",
    "for i in range(stgtcelltype.shape[0]):\n",
    "    # Get the cell types present in this spot\n",
    "    cell_types_present = [col for col, val in zip(cell_type_columns, stgtcelltype.iloc[i]) if val > 0]\n",
    "    \n",
    "    # If multiple cell types, take the first one (or modify to use the most abundant)\n",
    "    dominant_celltype = cell_types_present[0] if cell_types_present else 'Unknown'\n",
    "    dominant_celltypes.append(dominant_celltype)\n",
    "\n",
    "# Add the dominant cell type to our DataFrame\n",
    "coord_df['celltype'] = dominant_celltypes\n",
    "\n",
    "# Create an AnnData object\n",
    "adata = anndata.AnnData(X=np.zeros((len(coord_df), 1)))  # Empty expression matrix\n",
    "adata.obs_names = [f\"spot_{i}\" for i in range(len(coord_df))]\n",
    "adata.obs['celltype'] = coord_df['celltype']\n",
    "\n",
    "# Add the spatial coordinates to the obsm attribute\n",
    "adata.obsm['spatial'] = coord_df[['coord_x', 'coord_y']].values\n",
    "\n",
    "# Method 2: Direct scatter plot with legend (alternative approach)\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Get unique cell types for coloring\n",
    "unique_celltypes = coord_df['celltype'].unique()\n",
    "\n",
    "# Create a custom colormap\n",
    "num_colors = len(unique_celltypes)\n",
    "palette = sns.color_palette('tab20', n_colors=num_colors)\n",
    "celltype_to_color = {celltype: palette[i] for i, celltype in enumerate(unique_celltypes)}\n",
    "\n",
    "# Plot each cell type with a different color\n",
    "for celltype in unique_celltypes:\n",
    "    subset = coord_df[coord_df['celltype'] == celltype]\n",
    "    plt.scatter(subset['coord_x'], subset['coord_y'], \n",
    "                color=celltype_to_color[celltype], \n",
    "                label=celltype, \n",
    "                alpha=0.8, \n",
    "                s=15)\n",
    "\n",
    "plt.title('Spatial Coordinates Colored by Cell Type', fontsize=16)\n",
    "plt.xlabel('X Coordinate', fontsize=12)\n",
    "plt.ylabel('Y Coordinate', fontsize=12)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 1) Your real data (already in memory)\n",
    "#    coords          : np.ndarray, shape (N_real, 2)\n",
    "#    z_orig          : np.ndarray, shape (N_real, latent_dim)\n",
    "#    dominant_celltypes : list or array of length N_real\n",
    "#    z_syn           : np.ndarray, shape (N_synth, latent_dim)\n",
    "#    X_synth         : your synthetic counts (not needed here for plotting)\n",
    "# ———————————————————————————————\n",
    "\n",
    "coords_real = coords.copy()\n",
    "feats_real  = z_orig.copy()\n",
    "labels_real = np.array(dominant_celltypes)\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 2)  Re-interpolate your synthetic latents → coords\n",
    "knn_feat = NearestNeighbors(n_neighbors=5).fit(feats_real)\n",
    "dists, idxs = knn_feat.kneighbors(z_syn)            # (N_synth,5)\n",
    "\n",
    "# spiky Dirichlet weights to hug single neighbors\n",
    "alpha = 0.3\n",
    "w = np.random.dirichlet([alpha]*5, size=len(z_syn))  # (N_synth,5)\n",
    "\n",
    "# coords_real[idxs] shape = (N_synth,5,2), multiply & sum to get (N_synth,2)\n",
    "coords_synth_knn = (coords_real[idxs] * w[:,:,None]).sum(axis=1)\n",
    "\n",
    "# label those synth points\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5).fit(feats_real, labels_real)\n",
    "labels_synth_knn = knn_clf.predict(z_syn)\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 3)  Build boundary‐boost points\n",
    "hull = ConvexHull(coords_real)\n",
    "center = coords_real.mean(axis=0)\n",
    "\n",
    "bidx = hull.vertices\n",
    "coords_boundary = []\n",
    "labels_boundary = []\n",
    "for i in bidx:\n",
    "    lab = labels_real[i]\n",
    "    base = coords_real[i]\n",
    "    direction = base - center\n",
    "    direction /= np.linalg.norm(direction)\n",
    "    for _ in range(3):  # pick 3 jittered extras per hull vertex\n",
    "        pt = base + 0.02 * direction * np.random.rand()\n",
    "        coords_boundary.append(pt)\n",
    "        labels_boundary.append(lab)\n",
    "\n",
    "coords_boundary = np.vstack(coords_boundary)      # (N_bound,2)\n",
    "labels_boundary = np.array(labels_boundary)       # (N_bound,)\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 4)  Radial expansion (optional, for more whitespace)\n",
    "all_coords = np.vstack([coords_real, coords_synth_knn, coords_boundary])\n",
    "c0 = all_coords.mean(axis=0)\n",
    "scale = 1.05\n",
    "all_coords = (all_coords - c0)*scale + c0\n",
    "\n",
    "# 5)  Stack labels in the same order\n",
    "all_labels = np.concatenate([\n",
    "    labels_real,            # (N_real,)\n",
    "    labels_synth_knn,       # (N_synth,)\n",
    "    labels_boundary         # (N_bound,)\n",
    "])\n",
    "\n",
    "# Sanity check\n",
    "assert all_coords.shape[0] == all_labels.shape[0], \\\n",
    "    f\"Coords ({all_coords.shape[0]}) vs labels ({all_labels.shape[0]})\"\n",
    "\n",
    "# ———————————————————————————————\n",
    "# 6)  Build DataFrame & plot\n",
    "df = pd.DataFrame(all_coords, columns=['coord_x','coord_y'])\n",
    "df['celltype'] = all_labels\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "uniq = df['celltype'].unique()\n",
    "palette = sns.color_palette('tab20', n_colors=len(uniq))\n",
    "colmap = {ct: palette[i] for i, ct in enumerate(uniq)}\n",
    "\n",
    "for ct in uniq:\n",
    "    sub = df[df['celltype']==ct]\n",
    "    plt.scatter(sub['coord_x'], sub['coord_y'],\n",
    "                color=colmap[ct], label=ct, s=10, alpha=0.6)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "plt.xlabel('X Coordinate'); plt.ylabel('Y Coordinate')\n",
    "plt.title('Real + Synthetic + Boundary spots')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot \n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_torch(X, k, mode='connectivity', metric = 'minkowski', p=2, device='cuda'):\n",
    "    '''construct knn graph with torch and gpu\n",
    "    args:\n",
    "        X: input data containing features (torch tensor)\n",
    "        k: number of neighbors for each data point\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric (now euclidean supported for gpu knn)\n",
    "        p: param for minkowski (not used if metric is euclidean)\n",
    "    \n",
    "    Returns:\n",
    "        knn graph as a pytorch sparse tensor (coo format) or dense tensor depending on mode     \n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'.\"\n",
    "    assert metric == 'euclidean', \"for gpu knn, only 'euclidean' metric is currently supported in this implementation\"\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "        mode_knn = 'connectivity'\n",
    "    else:\n",
    "        include_self = False\n",
    "        mode_knn = 'distance'\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=metric, algorithm='auto')\n",
    "\n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_cpu = X.cpu().numpy()\n",
    "    else:\n",
    "        X_cpu = X.numpy()\n",
    "\n",
    "    knn.fit(X_cpu)\n",
    "    knn_graph_cpu = kneighbors_graph(knn, k, mode=mode_knn, include_self=include_self, metric=metric) #scipy sparse matrix on cpu\n",
    "    knn_graph_coo = knn_graph_cpu.tocoo()\n",
    "\n",
    "    if mode == 'connectivity':\n",
    "        knn_graph = torch.sparse_coo_tensor(torch.LongTensor([knn_graph_coo.row, knn_graph_coo.col]),\n",
    "                                            torch.FloatTensor(knn_graph_coo.data),\n",
    "                                            size = knn_graph_coo.shape).to(device)\n",
    "    elif mode == 'distance':\n",
    "        knn_graph_dense = torch.tensor(knn_graph_cpu.toarray(), dtype=torch.float32, device=device) #move to gpu as dense tensor\n",
    "        knn_graph = knn_graph_dense\n",
    "    \n",
    "    return knn_graph\n",
    "    \n",
    "def distances_cal_torch(graph, type_aware=None, aware_power =2, device='cuda'):\n",
    "    '''\n",
    "    calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (pytorch sparse or dense tensor)\n",
    "        type_aware: not implemented in this torch version for simplicity\n",
    "        aware_power: same ^^\n",
    "        device (str): 'cpu' or 'cuda' device to use\n",
    "    Returns:\n",
    "        distance matrix as a torch tensor\n",
    "    '''\n",
    "\n",
    "    if isinstance(graph, torch.Tensor) and graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().to_dense().numpy())\n",
    "    elif isinstance(graph, torch.Tensor) and not graph.is_sparse:\n",
    "        graph_cpu_csr = csr_matrix(graph.cpu().numpy())\n",
    "    else:\n",
    "        graph_cpu_csr = csr_matrix(graph) #assume scipy sparse matrix if not torch tensor\n",
    "\n",
    "    shortestPath_cpu = dijkstra(csgraph = graph_cpu_csr, directed=False, return_predecessors=False) #dijkstra on cpu\n",
    "    shortestPath = torch.tensor(shortestPath_cpu, dtype=torch.float32, device=device)\n",
    "\n",
    "    # the_max = torch.nanmax(shortestPath[shortestPath != float('inf')])\n",
    "    # shortestPath[shortestPath > the_max] = the_max\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = torch.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    C_dis = shortestPath / the_max\n",
    "    C_dis -= torch.mean(C_dis)\n",
    "    return C_dis\n",
    "\n",
    "def calculate_D_sc_torch(X_sc, k_neighbors=10, graph_mode='connectivity', device='cpu'):\n",
    "    '''calculate distance matrix from graph using dijkstra's algo\n",
    "    args:\n",
    "        graph: knn graph (torch sparse or dense tensor)\n",
    "        type_aware: not implemented\n",
    "        aware_power: same ^^\n",
    "        \n",
    "    returns:\n",
    "        distanced matrix as torch tensor'''\n",
    "    \n",
    "    if not isinstance(X_sc, torch.Tensor):\n",
    "        raise TypeError('Input X_sc must be a pytorch tensor')\n",
    "    \n",
    "    if device == 'cuda' and torch.cuda.is_available():\n",
    "        X_sc = X_sc.cuda(device=device)\n",
    "    else:\n",
    "        X_sc = X_sc.cpu()\n",
    "        device= 'cpu'\n",
    "\n",
    "    print(f'using device: {device}')\n",
    "    print(f'constructing knn graph...')\n",
    "    # X_normalized = normalize(X_sc.cpu().numpy(), norm='l2') #normalize on cpu for sklearn knn\n",
    "    X_normalized = X_sc\n",
    "    X_normalized_torch = torch.tensor(X_normalized, dtype=torch.float32, device=device)\n",
    "\n",
    "    Xgraph = construct_graph_torch(X_normalized_torch, k=k_neighbors, mode=graph_mode, metric='euclidean', device=device)\n",
    "\n",
    "    print('calculating distances from graph....')\n",
    "    D_sc = distances_cal_torch(Xgraph, device=device)\n",
    "\n",
    "    print('D_sc calculation complete')\n",
    "    \n",
    "    return D_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph, NearestNeighbors\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from scipy.sparse import csr_matrix, issparse\n",
    "from sklearn.preprocessing import normalize\n",
    "import ot\n",
    "\n",
    "def construct_graph_spatial(location_array, k, mode='distance', metric='euclidean', p=2):\n",
    "    '''construct KNN graph based on spatial coordinates\n",
    "    args:\n",
    "        location_array: spatial coordinates of spots (n-spots * 2)\n",
    "        k: number of neighbors for each spot\n",
    "        mode: 'connectivity' or 'distance'\n",
    "        metric: distance metric for knn (p=2 is euclidean)\n",
    "        p: param for minkowski if connectivity\n",
    "        \n",
    "    returns:\n",
    "        scipy.sparse.csr_matrix: knn graph in csr format\n",
    "    '''\n",
    "\n",
    "    assert mode in ['connectivity', 'distance'], \"mode must be 'connectivity' or 'distance'\"\n",
    "    if mode == 'connectivity':\n",
    "        include_self = True\n",
    "    else:\n",
    "        include_self = False\n",
    "    \n",
    "    c_graph = kneighbors_graph(location_array, k, mode=mode, metric=metric, include_self=include_self, p=p)\n",
    "    return c_graph\n",
    "\n",
    "def distances_cal_spatial(graph, spot_ids=None, spot_types=None, aware_power=2):\n",
    "    '''calculate spatial distance matrix from knn graph\n",
    "    args:\n",
    "        graph (scipy.sparse.csr_matrix): knn graph\n",
    "        spot_ids (list, optional): list of spot ids corresponding to the rows/cols of the graph. required if type_aware is used\n",
    "        spot_types (pd.Series, optinal): pandas series of spot types for type aware distance adjustment. required if type_aware is used\n",
    "        aware_power (int): power for type-aware distance adjustment\n",
    "        \n",
    "    returns:\n",
    "        sptial distance matrix'''\n",
    "    shortestPath = dijkstra(csgraph = csr_matrix(graph), directed=False, return_predecessors=False)\n",
    "    shortestPath = np.nan_to_num(shortestPath, nan=np.inf) #handle potential inf valyes after dijkstra\n",
    "\n",
    "    if spot_types is not None and spot_ids is not None:\n",
    "        shortestPath_df = pd.DataFrame(shortestPath, index=spot_ids, columns=spot_ids)\n",
    "        shortestPath_df['id1'] = shortestPath_df.index\n",
    "        shortestPath_melted = shortestPath_df.melt(id_vars=['id1'], var_name='id2', value_name='value')\n",
    "\n",
    "        type_aware_df = pd.DataFrame({'spot': spot_ids, 'spot_type': spot_types}, index=spot_ids)\n",
    "        meta1 = type_aware_df.copy()\n",
    "        meta1.columns = ['id1', 'type1']\n",
    "        meta2 = type_aware_df.copy()\n",
    "        meta2.columns = ['id2', 'type2']\n",
    "\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta1, on='id1', how='left')\n",
    "        shortestPath_melted = pd.merge(shortestPath_melted, meta2, on='id2', how='left')\n",
    "\n",
    "        shortestPath_melted['same_type'] = shortestPath_melted['type1'] == shortestPath_melted['type2']\n",
    "        shortestPath_melted.loc[(~shortestPath_melted.smae_type), 'value'] = shortestPath_melted.loc[(~shortestPath_melted.same_type),\n",
    "                                                                                                     'value'] * aware_power\n",
    "        shortestPath_melted.drop(['type1', 'type2', 'same_type'], axis=1, inplace=True)\n",
    "        shortestPath_pivot = shortestPath_melted.pivot(index='id1', columns='id2', values='value')\n",
    "\n",
    "        order = spot_ids\n",
    "        shortestPath = shortestPath_pivot[order].loc[order].values\n",
    "    else:\n",
    "        shortestPath = np.asarray(shortestPath) #ensure it's a numpy array\n",
    "\n",
    "    #mask out infinite distances\n",
    "    mask = shortestPath != float('inf')\n",
    "    if mask.any():\n",
    "        the_max = np.max(shortestPath[mask])\n",
    "        shortestPath[~mask] = the_max #replace inf with max value\n",
    "    else:\n",
    "        the_max = 1.0 #fallback if all are inf (should not happen in connected graphs)\n",
    "\n",
    "    C_dis = shortestPath / the_max\n",
    "    C_dis -= np.mean(C_dis)\n",
    "\n",
    "    return C_dis\n",
    "\n",
    "def calculate_D_st_from_coords(spatial_coords, X_st=None, k_neighbors=10, graph_mode='distance', aware_st=False, \n",
    "                               spot_types=None, aware_power_st=2, spot_ids=None):\n",
    "    '''calculates the spatial distance matrix D_st for spatial transcriptomics data directly from coordinates and optional spot types\n",
    "    args:\n",
    "        spatial_coords: spatial coordinates of spots (n_spots * 2)\n",
    "        X_st: St gene expression data (not used for D_st calculation itself)\n",
    "        k_neighbors: number of neighbors for knn graph\n",
    "        graph_mode: 'connectivity or 'distance' for knn graph\n",
    "        aware_st: whether to use type-aware distance adjustment\n",
    "        spot_types: pandas series of spot types for type-aware adjustment\n",
    "        aware_power_st: power for type-aware distance adjustment\n",
    "        spot_ids: list or index of spot ids, required if spot_ids is provided\n",
    "        \n",
    "    returns:\n",
    "        np.ndarray: spatial disance matrix D_st'''\n",
    "    \n",
    "    if isinstance(spatial_coords, pd.DataFrame):\n",
    "        location_array = spatial_coords.values\n",
    "        if spot_ids is None:\n",
    "            spot_ids = spatial_coords.index.tolist() #use index of dataframe if available\n",
    "    elif isinstance(spatial_coords, np.ndarray):\n",
    "        location_array = spatial_coords\n",
    "        if spot_ids is None:\n",
    "            spot_ids = list(range(location_array.shape[0])) #generate default ids if not provided\n",
    "\n",
    "    else:\n",
    "        raise TypeError('spatial_coords must be a pandas dataframe or a numpy array')\n",
    "    \n",
    "    print(f'constructing {graph_mode} graph for ST data with k={k_neighbors}.....')\n",
    "    Xgraph_st = construct_graph_spatial(location_array, k=k_neighbors, mode=graph_mode)\n",
    "    \n",
    "    if aware_st:\n",
    "        if spot_types is None or spot_ids is None:\n",
    "            raise ValueError('spot_types and spot_ids must be provided when aware_st=True')\n",
    "        if not isinstance(spot_types, pd.Series):\n",
    "            spot_types = pd.Series(spot_types, idnex=spot_ids) \n",
    "        print('applying type aware distance adjustment for ST data')\n",
    "        print(f'aware power for ST: {aware_power_st}')\n",
    "    else:\n",
    "        spot_types = None \n",
    "\n",
    "    print(f'calculating spatial distances.....')\n",
    "    D_st = distances_cal_spatial(Xgraph_st, spot_ids=spot_ids, spot_types=spot_types, aware_power=aware_power_st)\n",
    "\n",
    "    print('D_st calculation complete')\n",
    "    return D_st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_gw_torch(X_sc, X_st, Y_st, alpha, k=100, G0=None, max_iter = 100, tol=1e-9, device='cuda', n_iter = 1):\n",
    "    n = X_sc.shape[0]\n",
    "    m = X_st.shape[0]\n",
    "\n",
    "    X_sc = X_sc.to(device)\n",
    "    X_st = X_st.to(device)\n",
    "\n",
    "    if not torch.is_tensor(Y_st):\n",
    "        Y_st_tensor = torch.tensor(Y_st, dtype=torch.float32, device=device)\n",
    "    else:\n",
    "        Y_st_tensor = Y_st.to(device, dtype=torch.float32)\n",
    "\n",
    "    #calculate distance matrices\n",
    "    print('calculating SC distances with knn-dijkstra.....')\n",
    "    D_sc = calculate_D_sc_torch(X_sc, k_neighbors=k, device=device)\n",
    "\n",
    "    print('Calculating ST distances.....')\n",
    "    D_st = calculate_D_st_from_coords(spatial_coords=Y_st, k_neighbors=15, graph_mode=\"distance\") # Using calculate_D_st_from_coords\n",
    "    D_st = torch.tensor(D_st, dtype=torch.float32, device=device) # Convert D_st to tensor and move to device\n",
    "\n",
    "    #get expression distance matrix\n",
    "    C_exp = torch.cdist(X_sc, X_st, p=2) #euclidean distance\n",
    "    C_exp = C_exp / (torch.max(C_exp) + 1e-16) #normalize\n",
    "\n",
    "    #ensure distance matries are C-contiguouse numpy arrays for POT\n",
    "    D_sc_np = D_sc.cpu().numpy()\n",
    "    D_st_np = D_st.cpu().numpy()\n",
    "    C_exp_np = C_exp.cpu().numpy()\n",
    "    D_sc_np = np.ascontiguousarray(D_sc_np)\n",
    "    D_st_np = np.ascontiguousarray(D_st_np)\n",
    "    C_exp_np = np.ascontiguousarray(C_exp_np)\n",
    "\n",
    "    #uniform distributions\n",
    "    p = ot.unif(n)\n",
    "    q = ot.unif(m)\n",
    "\n",
    "    #anneal the reg param over several steps\n",
    "    T_np = None\n",
    "    for i in range(n_iter):\n",
    "        #run fused gw with POT\n",
    "        T_np, log = ot.gromov.fused_gromov_wasserstein(\n",
    "            M=C_exp_np, C1=D_sc_np, C2=D_st_np,\n",
    "            p=p, q=q, loss_fun='square_loss',\n",
    "            alpha=alpha,\n",
    "            G0=T_np if T_np is not None else (G0.cpu().numpy() if G0 is not None else None),\n",
    "            log=True,\n",
    "            verbose=True,\n",
    "            max_iter = max_iter,\n",
    "            tol_abs=tol\n",
    "        )\n",
    "\n",
    "    fgw_dist = log['fgw_dist']\n",
    "\n",
    "    print(f'fgw distance: {fgw_dist}')\n",
    "\n",
    "    T = torch.tensor(T_np, dtype=torch.float32, device=device)\n",
    "\n",
    "    return T, D_sc, D_st, fgw_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "Y_st = spcoor.values\n",
    "# --- Run FGW using POT ---\n",
    "T, D_sc, D_st, fgw_dist = fused_gw_torch(\n",
    "    X_sc=X_sc, X_st=X_st, Y_st=Y_st,\n",
    "    alpha=0.3, # Example: balance expression and structure equally\n",
    "    k=300,      # k for SC graph\n",
    "    max_iter=200,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_st = D_st.to(device)\n",
    "D_induced = T @ D_st @ T.t()\n",
    "D_induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "class FeatureNet(nn.Module):\n",
    "    def __init__(self, n_genes, n_embedding=[512, 256, 128], dp=0):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(n_genes, n_embedding[0])\n",
    "        self.bn1 = nn.LayerNorm(n_embedding[0])\n",
    "        self.fc2 = nn.Linear(n_embedding[0], n_embedding[1])\n",
    "        self.bn2 = nn.LayerNorm(n_embedding[1])\n",
    "        self.fc3 = nn.Linear(n_embedding[1], n_embedding[2])\n",
    "        \n",
    "        self.dp = nn.Dropout(dp)\n",
    "        \n",
    "    def forward(self, x, isdp=False):\n",
    "        if isdp:\n",
    "            x = self.dp(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MMDLoss(nn.Module):\n",
    "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        super(MMDLoss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = fix_sigma\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul, kernel_num, fix_sigma):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
    "        L2_distance = ((total0-total1)**2).sum(2)\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]\n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]\n",
    "        tmp = 0\n",
    "        for x in kernel_val:\n",
    "            tmp += x\n",
    "        return tmp\n",
    "\n",
    "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
    "        loss = 0.0\n",
    "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd2(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "            loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    \"\"\"Sinusoidal embeddings for diffusion timesteps\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t * emb[None, :]\n",
    "        emb = torch.cat((torch.sin(emb), torch.cos(emb)), dim=-1)\n",
    "        if self.dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1, 0, 0))\n",
    "        return emb\n",
    "\n",
    "\n",
    "def calculate_distance_matrix(coords, sigma, device):\n",
    "    \"\"\"Calculate distance matrix with memory efficiency\"\"\"\n",
    "    # Clear memory first\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Move calculation to CPU\n",
    "    coords_cpu = coords.cpu().numpy()\n",
    "    dist_matrix = scipy.spatial.distance.cdist(coords_cpu, coords_cpu)\n",
    "    \n",
    "    # Apply Gaussian kernel on CPU\n",
    "    dist_matrix = np.exp(-dist_matrix**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "    \n",
    "    # Normalize rows\n",
    "    row_sums = dist_matrix.sum(axis=1, keepdims=True)\n",
    "    dist_matrix = dist_matrix / row_sums\n",
    "    \n",
    "    # Move back to device in smaller chunks if needed\n",
    "    return torch.tensor(dist_matrix, device=device, dtype=torch.float32)\n",
    "\n",
    "class STEMDiffusion:\n",
    "    def __init__(\n",
    "        self, \n",
    "        st_gene_expr,\n",
    "        st_coords,\n",
    "        D_st,\n",
    "        sc_gene_expr,\n",
    "        D_induced=None,\n",
    "        outf='./diffusion_output',\n",
    "        device='cuda',\n",
    "        n_genes=None,\n",
    "        n_embedding=[512, 256, 128],\n",
    "        hidden_dim=256,\n",
    "        dp=0.1,\n",
    "        n_timesteps=800,\n",
    "        beta_start=1e-4,\n",
    "        beta_end=0.02,\n",
    "        sigma=3.0,\n",
    "        alpha=0.8,\n",
    "        mmdbatch=1000,\n",
    "        batch_size=256\n",
    "    ):\n",
    "        self.device = torch.device(device)\n",
    "        \n",
    "        # Create output directory\n",
    "        self.outf = outf\n",
    "        if not os.path.exists(outf):\n",
    "            os.makedirs(outf)\n",
    "        \n",
    "        self.train_log = os.path.join(outf, 'train.log')\n",
    "        \n",
    "        # Store data\n",
    "        self.st_gene_expr = torch.tensor(st_gene_expr, dtype=torch.float32).to(self.device)\n",
    "        self.st_coords = torch.tensor(st_coords, dtype=torch.float32).to(self.device)\n",
    "        self.sc_gene_expr = torch.tensor(sc_gene_expr, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # Store distance matrices\n",
    "        self.D_st = torch.tensor(D_st, dtype=torch.float32).to(self.device)\n",
    "        if D_induced is not None:\n",
    "            self.D_induced = torch.tensor(D_induced, dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            self.D_induced = None\n",
    "        \n",
    "        # Normalize coordinates for diffusion model\n",
    "        coords_min = self.st_coords.min(dim=0)[0]\n",
    "        coords_max = self.st_coords.max(dim=0)[0]\n",
    "        coords_range = coords_max - coords_min\n",
    "        self.st_coords_norm = 2 * (self.st_coords - coords_min) / coords_range - 1\n",
    "        self.coords_min, self.coords_max = coords_min, coords_max\n",
    "        self.coords_range = coords_range\n",
    "        \n",
    "        # STEM parameters\n",
    "        self.n_genes = n_genes or st_gene_expr.shape[1]\n",
    "        self.sigma = sigma\n",
    "        self.alpha = alpha\n",
    "        self.mmdbatch = mmdbatch\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Initialize feature encoder (shared between ST and SC data)\n",
    "        self.netE = FeatureNet(self.n_genes, n_embedding=n_embedding, dp=dp).to(self.device)\n",
    "        \n",
    "        # Initialize diffusion model components\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embed = nn.Sequential(\n",
    "            SinusoidalEmbedding(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Coordinate encoder\n",
    "        self.coord_encoder = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Feature to hidden projection\n",
    "        self.feat_proj = nn.Sequential(\n",
    "            nn.Linear(n_embedding[-1], hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU()\n",
    "        ).to(self.device)\n",
    "\n",
    "        #coordinate head to get coords direct from gene expression\n",
    "        self.coord_head = nn.Sequential(\n",
    "            nn.Linear(n_embedding[-1], hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Main network blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.SiLU()\n",
    "            ).to(self.device) for _ in range(4)\n",
    "        ])\n",
    "        \n",
    "        # Output layer\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim//2, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Setup optimizers and losses\n",
    "        self.optimizer_E = torch.optim.AdamW(self.netE.parameters(), lr=0.002)\n",
    "        self.scheduler_E = lr_scheduler.StepLR(optimizer=self.optimizer_E, step_size=200, gamma=0.5)\n",
    "        \n",
    "        diffusion_params = list(self.time_embed.parameters()) + \\\n",
    "                           list(self.coord_encoder.parameters()) + \\\n",
    "                           list(self.feat_proj.parameters()) + \\\n",
    "                           list(self.blocks.parameters()) + \\\n",
    "                           list(self.final.parameters())\n",
    "        \n",
    "        self.optimizer_diff = torch.optim.AdamW(diffusion_params, lr=1e-4, weight_decay=1e-6)\n",
    "        self.scheduler_diff = lr_scheduler.CosineAnnealingLR(self.optimizer_diff, T_max=3000, eta_min=1e-6)\n",
    "        \n",
    "        self.mmd_fn = MMDLoss()\n",
    "        \n",
    "        # Setup noise schedule for diffusion\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.noise_schedule = self.get_noise_schedule(n_timesteps, beta_start, beta_end)\n",
    "        \n",
    "        # Tracking losses\n",
    "        self.loss_names = ['E', 'E_pred', 'E_circle', 'E_mmd', 'diffusion']\n",
    "    \n",
    "    def get_noise_schedule(self, timesteps=1000, beta1=1e-4, beta2=0.02):\n",
    "        \"\"\"Returns diffusion noise schedule parameters\"\"\"\n",
    "        # Linear schedule\n",
    "        betas = torch.linspace(beta1, beta2, timesteps, device=self.device)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "        sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
    "        \n",
    "        return {\n",
    "            'betas': betas,\n",
    "            'alphas': alphas,\n",
    "            'alphas_cumprod': alphas_cumprod,\n",
    "            'sqrt_alphas_cumprod': sqrt_alphas_cumprod,\n",
    "            'sqrt_one_minus_alphas_cumprod': sqrt_one_minus_alphas_cumprod\n",
    "        }\n",
    "    \n",
    "    def add_noise(self, x_0, t, noise_schedule):\n",
    "        \"\"\"Add noise to coordinates according to timestep t\"\"\"\n",
    "        noise = torch.randn_like(x_0)\n",
    "        sqrt_alphas_cumprod_t = noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "        \n",
    "        # Add noise according to schedule\n",
    "        x_t = sqrt_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        \n",
    "        return x_t, noise\n",
    "    \n",
    "    def forward_diffusion(self, coords, t, features):\n",
    "        \"\"\"Forward pass of diffusion model, predicting noise from noisy coordinates and conditioning\"\"\"\n",
    "        # Get feature embeddings\n",
    "        feat_emb = self.netE(features, isdp=False)\n",
    "        feat_proj = self.feat_proj(feat_emb)\n",
    "        \n",
    "        # Get time embedding\n",
    "        t_emb = self.time_embed(t)\n",
    "        \n",
    "        # Get coordinate embedding\n",
    "        coord_emb = self.coord_encoder(coords)\n",
    "        \n",
    "        # Combine all inputs\n",
    "        h = coord_emb + t_emb + feat_proj\n",
    "        \n",
    "        # Process through residual blocks\n",
    "        for block in self.blocks:\n",
    "            h = h + block(h)  # Residual connection\n",
    "        \n",
    "        # Predict noise\n",
    "        return self.final(h)\n",
    "    \n",
    "    def train_encoder(self, n_epochs=1000, ratio_start=0, ratio_end=1.0):\n",
    "        \"\"\"Train the STEM encoder to align ST and SC data\"\"\"\n",
    "        print(\"Training STEM encoder...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting STEM encoder training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, ratio_start={ratio_start}, ratio_end={ratio_end}\\n\")\n",
    "        \n",
    "        # Calculate spatial adjacency matrix\n",
    "        # if self.sigma == 0:\n",
    "        #     nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        # else:\n",
    "        #     nettrue = torch.tensor(scipy.spatial.distance.cdist(\n",
    "        #         self.st_coords.cpu().numpy(), \n",
    "        #         self.st_coords.cpu().numpy()\n",
    "        #     ), device=self.device).to(torch.float32)\n",
    "\n",
    "            # sigma = self.sigma\n",
    "            # nettrue = torch.exp(-nettrue**2/(2*sigma**2))/(np.sqrt(2*np.pi)*sigma)\n",
    "            # nettrue = F.normalize(nettrue, p=1, dim=1)\n",
    "\n",
    "        # Calculate spatial adjacency matrix\n",
    "        if self.sigma == 0:\n",
    "            nettrue = torch.eye(self.st_coords.shape[0], device=self.device)\n",
    "        else:\n",
    "            # Replace the original calculation with our memory-efficient version\n",
    "            nettrue = calculate_distance_matrix(self.st_coords, self.sigma, self.device)\n",
    "            \n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Schedule for circle loss weight\n",
    "            ratio = ratio_start + (ratio_end - ratio_start) * min(epoch / (n_epochs * 0.8), 1.0)\n",
    "            \n",
    "            # Forward pass ST data\n",
    "            e_seq_st = self.netE(self.st_gene_expr, True)\n",
    "            \n",
    "            # Sample from SC data due to large size\n",
    "            sc_idx = torch.randint(0, self.sc_gene_expr.shape[0], (min(self.batch_size, self.mmdbatch),), device=self.device)\n",
    "            sc_batch = self.sc_gene_expr[sc_idx]\n",
    "            e_seq_sc = self.netE(sc_batch, False)\n",
    "            \n",
    "            # Calculate losses\n",
    "            self.optimizer_E.zero_grad()\n",
    "            \n",
    "            # Prediction loss (equivalent to netpred in STEM)\n",
    "            netpred = e_seq_st.mm(e_seq_st.t())\n",
    "            loss_E_pred = F.cross_entropy(netpred, nettrue, reduction='mean')\n",
    "            \n",
    "            # Mapping matrices\n",
    "            st2sc = F.softmax(e_seq_st.mm(e_seq_sc.t()), dim=1)\n",
    "            sc2st = F.softmax(e_seq_sc.mm(e_seq_st.t()), dim=1)\n",
    "            \n",
    "            # Circle loss\n",
    "            st2st = torch.log(st2sc.mm(sc2st) + 1e-7)\n",
    "            loss_E_circle = F.kl_div(st2st, nettrue, reduction='none').sum(1).mean()\n",
    "            \n",
    "            # MMD loss\n",
    "            ranidx = torch.randint(0, e_seq_sc.shape[0], (min(self.mmdbatch, e_seq_sc.shape[0]),), device=self.device)\n",
    "            loss_E_mmd = self.mmd_fn(e_seq_st, e_seq_sc[ranidx])\n",
    "            \n",
    "            # Total loss\n",
    "            loss_E = loss_E_pred + self.alpha * loss_E_mmd + ratio * loss_E_circle\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss_E.backward()\n",
    "            self.optimizer_E.step()\n",
    "            self.scheduler_E.step()\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Encoder epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss_E: {loss_E.item():.6f}, \"\n",
    "                          f\"Loss_E_pred: {loss_E_pred.item():.6f}, \"\n",
    "                          f\"Loss_E_circle: {loss_E_circle.item():.6f}, \"\n",
    "                          f\"Loss_E_mmd: {loss_E_mmd.item():.6f}, \"\n",
    "                          f\"Ratio: {ratio:.4f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'netE_state_dict': self.netE.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_E.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_E.state_dict(),\n",
    "                    }, os.path.join(self.outf, f'encoder_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Save final encoder\n",
    "        torch.save({\n",
    "            'netE_state_dict': self.netE.state_dict(),\n",
    "        }, os.path.join(self.outf, 'final_encoder.pt'))\n",
    "        \n",
    "        print(\"Encoder training complete!\")\n",
    "    \n",
    "    def train_diffusion(self, n_epochs=2000, lambda_struct=10.0):\n",
    "        \"\"\"Train diffusion model using the trained encoder\"\"\"\n",
    "        print(\"Training diffusion model...\")\n",
    "        \n",
    "        # Log training start\n",
    "        with open(self.train_log, 'a') as f:\n",
    "            localtime = time.asctime(time.localtime(time.time()))\n",
    "            f.write(f\"{localtime} - Starting diffusion model training\\n\")\n",
    "            f.write(f\"n_epochs={n_epochs}, lambda_struct={lambda_struct}\\n\")\n",
    "        \n",
    "        # Freeze encoder during diffusion training\n",
    "        for param in self.netE.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Precompute adjacency matrix for structure loss\n",
    "        def compute_adjacency_matrix(distances, sigma=3.0):\n",
    "            weights = torch.exp(-(distances ** 2) / (2 * sigma * sigma))\n",
    "            # Zero out self-connections\n",
    "            weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "            # Normalize rows to sum to 1\n",
    "            row_sums = weights.sum(dim=1, keepdim=True)\n",
    "            row_sums = torch.clamp(row_sums, min=1e-10)\n",
    "            adjacency = weights / (row_sums + 1e-8)\n",
    "            return adjacency\n",
    "        \n",
    "        st_adj = compute_adjacency_matrix(self.D_st, sigma=self.sigma)\n",
    "        \n",
    "        # Keep track of best model\n",
    "        best_loss = float('inf')\n",
    "        best_state = None\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            # Sample batch of ST data\n",
    "            idx = torch.randperm(len(self.st_coords_norm))[:self.batch_size]\n",
    "            coords = self.st_coords_norm[idx]\n",
    "            features = self.st_gene_expr[idx]\n",
    "            sub_adj = st_adj[idx][:, idx]\n",
    "            sub_adj = sub_adj / (sub_adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "            \n",
    "            # Sample timesteps with emphasis on early and late stages\n",
    "            if np.random.random() < 0.3:\n",
    "                # Focus on early timesteps (high noise)\n",
    "                t = torch.randint(int(0.7 * self.n_timesteps), self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            elif np.random.random() < 0.6:\n",
    "                # Focus on late timesteps (low noise, more structure)\n",
    "                t = torch.randint(0, int(0.3 * self.n_timesteps), (self.batch_size,), device=self.device)\n",
    "            else:\n",
    "                # Random timesteps across the range\n",
    "                t = torch.randint(0, self.n_timesteps, (self.batch_size,), device=self.device)\n",
    "            \n",
    "            # Add noise to coordinates\n",
    "            noisy_coords, target_noise = self.add_noise(coords, t, self.noise_schedule)\n",
    "            \n",
    "            # Forward pass to predict noise\n",
    "            pred_noise = self.forward_diffusion(noisy_coords, t.unsqueeze(1).float() / self.n_timesteps, features)\n",
    "            \n",
    "            # Compute diffusion loss (noise prediction MSE)\n",
    "            diffusion_loss = F.mse_loss(pred_noise, target_noise)\n",
    "            \n",
    "            # Compute denoised coordinates for structure loss\n",
    "            sqrt_alphas_cumprod_t = self.noise_schedule['sqrt_alphas_cumprod'][t].view(-1, 1)\n",
    "            sqrt_one_minus_alphas_cumprod_t = self.noise_schedule['sqrt_one_minus_alphas_cumprod'][t].view(-1, 1)\n",
    "            pred_coords = (noisy_coords - sqrt_one_minus_alphas_cumprod_t * pred_noise) / sqrt_alphas_cumprod_t\n",
    "            \n",
    "            # Compute pairwise distances and adjacency for predicted coordinates\n",
    "            pred_distances = torch.cdist(pred_coords, pred_coords, p=2)\n",
    "            pred_adj = compute_adjacency_matrix(pred_distances, sigma=self.sigma)\n",
    "\n",
    "            # print(pred_adj.sum(dim=1))\n",
    "            # print(sub_adj.sum(dim=1))\n",
    "            \n",
    "            # Structure loss (KL divergence between adjacency matrices)\n",
    "            # Using KL divergence as you preferred\n",
    "            struct_loss = F.kl_div(\n",
    "                torch.log(pred_adj + 1e-10),\n",
    "                sub_adj,\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "            # Total loss\n",
    "            total_loss = diffusion_loss + lambda_struct * struct_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer_diff.zero_grad()\n",
    "            total_loss.backward()\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.time_embed.parameters()) + \n",
    "                list(self.coord_encoder.parameters()) + \n",
    "                list(self.feat_proj.parameters()) + \n",
    "                list(self.blocks.parameters()) + \n",
    "                list(self.final.parameters()) +\n",
    "                list(self.coord_head.parameters()),\n",
    "                1.0\n",
    "            )\n",
    "            self.optimizer_diff.step()\n",
    "            self.scheduler_diff.step()\n",
    "            \n",
    "            # Save best model\n",
    "            if total_loss.item() < best_loss:\n",
    "                best_loss = total_loss.item()\n",
    "                best_state = {\n",
    "                    'epoch': epoch,\n",
    "                    'time_embed': self.time_embed.state_dict(),\n",
    "                    'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                    'feat_proj': self.feat_proj.state_dict(),\n",
    "                    'blocks': [block.state_dict() for block in self.blocks],\n",
    "                    'final': self.final.state_dict(),\n",
    "                    'loss': best_loss\n",
    "                }\n",
    "                # Save best model\n",
    "                torch.save(best_state, os.path.join(self.outf, 'best_diffusion_model.pt'))\n",
    "            \n",
    "            # Log progress\n",
    "            if epoch % 100 == 0:\n",
    "                log_msg = (f\"Diffusion epoch {epoch}/{n_epochs}, \"\n",
    "                          f\"Loss: {total_loss.item():.6f}, \"\n",
    "                          f\"Diffusion Loss: {diffusion_loss.item():.6f}, \"\n",
    "                          f\"Structure Loss: {struct_loss.item():.6f}, \"\n",
    "                          f\"LR: {self.scheduler_diff.get_last_lr()[0]:.6f}\")\n",
    "                \n",
    "                print(log_msg)\n",
    "                with open(self.train_log, 'a') as f:\n",
    "                    f.write(log_msg + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 500 == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'time_embed': self.time_embed.state_dict(),\n",
    "                        'coord_encoder': self.coord_encoder.state_dict(),\n",
    "                        'feat_proj': self.feat_proj.state_dict(),\n",
    "                        'blocks': [block.state_dict() for block in self.blocks],\n",
    "                        'final': self.final.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer_diff.state_dict(),\n",
    "                        'scheduler_state_dict': self.scheduler_diff.state_dict(),\n",
    "                        'loss': total_loss.item()\n",
    "                    }, os.path.join(self.outf, f'diffusion_checkpoint_epoch_{epoch}.pt'))\n",
    "        \n",
    "        # Restore best model\n",
    "        if best_state:\n",
    "            self.time_embed.load_state_dict(best_state['time_embed'])\n",
    "            self.coord_encoder.load_state_dict(best_state['coord_encoder'])\n",
    "            self.feat_proj.load_state_dict(best_state['feat_proj'])\n",
    "            for i, block_state in enumerate(best_state['blocks']):\n",
    "                self.blocks[i].load_state_dict(block_state)\n",
    "            self.final.load_state_dict(best_state['final'])\n",
    "            print(f\"Restored best model from epoch {best_state['epoch']} with loss {best_state['loss']:.6f}\")\n",
    "        \n",
    "        print(\"Diffusion training complete!\")\n",
    "    \n",
    "    def train(self, encoder_epochs=1000, diffusion_epochs=2000, ratio_start=0, ratio_end=1.0, lambda_struct=10.0):\n",
    "        \"\"\"Combined training of encoder and diffusion model\"\"\"\n",
    "        # First train the encoder to align ST and SC\n",
    "        self.train_encoder(n_epochs=encoder_epochs, ratio_start=ratio_start, ratio_end=ratio_end)\n",
    "        \n",
    "        # Then train the diffusion model\n",
    "        self.train_diffusion(n_epochs=diffusion_epochs, lambda_struct=lambda_struct)\n",
    "    \n",
    "    def generate_st_coordinates_batched(self, batch_size=64, timesteps=None):\n",
    "        \"\"\"Generate ST coordinates in batches to avoid memory issues\"\"\"\n",
    "        print(\"Generating ST coordinates for evaluation in batches...\")\n",
    "        self.netE.eval()\n",
    "        \n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_spots = len(self.st_gene_expr)\n",
    "        n_batches = (n_spots + batch_size - 1) // batch_size\n",
    "        \n",
    "        all_coords = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = min((b + 1) * batch_size, n_spots)\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            # Get batch features\n",
    "            features = self.st_gene_expr[start_idx:end_idx]\n",
    "            \n",
    "            # Start from random noise\n",
    "            x = torch.randn(batch_size_actual, 2, device=self.device)\n",
    "            \n",
    "            # Gradually denoise\n",
    "            for t in tqdm(range(timesteps-1, -1, -1), \n",
    "                         desc=f\"Generating batch {b+1}/{n_batches}\",\n",
    "                         leave=(b == n_batches-1)):  # Only keep last progress bar\n",
    "                \n",
    "                # Create timestep tensor\n",
    "                time_tensor = torch.ones(batch_size_actual, 1, device=self.device) * t / timesteps\n",
    "                \n",
    "                # Predict noise\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                \n",
    "                # Get parameters for this timestep\n",
    "                alpha_t = self.noise_schedule['alphas'][t]\n",
    "                alpha_cumprod_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                beta_t = self.noise_schedule['betas'][t]\n",
    "                \n",
    "                # Apply noise (except for last step)\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = 0\n",
    "                \n",
    "                # Update sample with reverse diffusion step\n",
    "                x = (1 / torch.sqrt(alpha_t)) * (\n",
    "                    x - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * pred_noise\n",
    "                ) + torch.sqrt(beta_t) * noise\n",
    "            \n",
    "            # Store batch results\n",
    "            all_coords.append(x.detach().cpu())\n",
    "        \n",
    "        # Combine all batches\n",
    "        st_gen_coords_norm = torch.cat(all_coords, dim=0)\n",
    "        \n",
    "        # Denormalize coordinates\n",
    "        st_gen_coords = self.denormalize_coordinates(st_gen_coords_norm)\n",
    "        \n",
    "        print(\"Generation complete!\")\n",
    "        return st_gen_coords\n",
    "    \n",
    "    def sample_sc_coordinates_batched(self, batch_size=64, timesteps=None, use_structure_guidance=True):\n",
    "        \"\"\"Sample SC coordinates in batches to avoid memory issues\"\"\"\n",
    "        print(\"Sampling SC coordinates in batches...\")\n",
    "        self.netE.eval()\n",
    "        \n",
    "        timesteps = timesteps or self.n_timesteps\n",
    "        n_cells = len(self.sc_gene_expr)\n",
    "        n_batches = (n_cells + batch_size - 1) // batch_size\n",
    "        \n",
    "        all_coords = []\n",
    "        \n",
    "        for b in range(n_batches):\n",
    "            start_idx = b * batch_size\n",
    "            end_idx = min((b + 1) * batch_size, n_cells)\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            # Get batch features\n",
    "            features = self.sc_gene_expr[start_idx:end_idx]\n",
    "            \n",
    "            # Start from random noise\n",
    "            x = torch.randn(batch_size_actual, 2, device=self.device)\n",
    "            \n",
    "            # Get relevant subset of D_induced for structure guidance if available\n",
    "            if use_structure_guidance and self.D_induced is not None:\n",
    "                sub_D_induced = self.D_induced[start_idx:end_idx, start_idx:end_idx]\n",
    "                \n",
    "                # Compute adjacency matrix\n",
    "                weights = torch.exp(-(sub_D_induced ** 2) / (2 * self.sigma * self.sigma))\n",
    "                weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "                row_sums = weights.sum(dim=1, keepdim=True)\n",
    "                target_adj = weights / (row_sums + 1e-8)\n",
    "            else:\n",
    "                target_adj = None\n",
    "            \n",
    "            # Gradually denoise\n",
    "            for t in tqdm(range(timesteps-1, -1, -1), \n",
    "                         desc=f\"Sampling batch {b+1}/{n_batches}\",\n",
    "                         leave=(b == n_batches-1)):  # Only keep last progress bar\n",
    "                \n",
    "                # Create timestep tensor\n",
    "                time_tensor = torch.ones(batch_size_actual, 1, device=self.device) * t / timesteps\n",
    "                \n",
    "                # Predict noise\n",
    "                pred_noise = self.forward_diffusion(x, time_tensor, features)\n",
    "                \n",
    "                # Get parameters for this timestep\n",
    "                alpha_t = self.noise_schedule['alphas'][t]\n",
    "                alpha_cumprod_t = self.noise_schedule['alphas_cumprod'][t]\n",
    "                beta_t = self.noise_schedule['betas'][t]\n",
    "                \n",
    "                # Apply noise (except for last step)\n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = 0\n",
    "                \n",
    "                # Update sample with reverse diffusion step\n",
    "                x = (1 / torch.sqrt(alpha_t)) * (\n",
    "                    x - ((1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)) * pred_noise\n",
    "                ) + torch.sqrt(beta_t) * noise\n",
    "                \n",
    "                # Apply structure guidance in later steps if available\n",
    "                if use_structure_guidance and target_adj is not None and t < timesteps * 0.7 and t % 10 == 0:\n",
    "                    x = self.adjust_coordinates_to_match_structure(x, target_adj, t, timesteps)\n",
    "            \n",
    "            # Store batch results\n",
    "            all_coords.append(x.detach().cpu())\n",
    "        \n",
    "        # Combine all batches\n",
    "        sc_coords_norm = torch.cat(all_coords, dim=0)\n",
    "        \n",
    "        # Denormalize coordinates\n",
    "        sc_coords = self.denormalize_coordinates(sc_coords_norm)\n",
    "        \n",
    "        print(\"Sampling complete!\")\n",
    "        return sc_coords\n",
    "    \n",
    "    def adjust_coordinates_to_match_structure(self, coords, target_adj, t, timesteps, lr=0.05):\n",
    "        \"\"\"Adjust coordinates to better match target adjacency structure\"\"\"\n",
    "        # Compute current adjacency matrix\n",
    "        distances = torch.cdist(coords, coords, p=2)\n",
    "        weights = torch.exp(-(distances ** 2) / (2 * self.sigma * self.sigma))\n",
    "        weights = weights * (1 - torch.eye(weights.shape[0], device=self.device))\n",
    "        row_sums = weights.sum(dim=1, keepdim=True)\n",
    "        cur_adj = weights / (row_sums + 1e-8)\n",
    "        \n",
    "        # Adjust learning rate based on timestep (smaller adjustments near the end)\n",
    "        lr_scale = 0.1 * (t / timesteps) + 0.01\n",
    "        \n",
    "        # Compute adjustment direction\n",
    "        diff = cur_adj - target_adj\n",
    "        \n",
    "        # Direction vectors between all pairs\n",
    "        coord_i = coords.unsqueeze(1)  # [n, 1, 2]\n",
    "        coord_j = coords.unsqueeze(0)  # [1, n, 2]\n",
    "        directions = coord_i - coord_j  # [n, n, 2]\n",
    "        \n",
    "        # Normalize directions\n",
    "        distances = torch.norm(directions, dim=2, keepdim=True)\n",
    "        norm_directions = directions / (distances + 1e-8)\n",
    "        \n",
    "        # Scale directions by adjacency difference\n",
    "        delta = diff.unsqueeze(2) * norm_directions  # [n, n, 2]\n",
    "        \n",
    "        # Sum influences from all other points\n",
    "        adjustments = -delta.sum(dim=1)  # [n, 2]\n",
    "        \n",
    "        # Apply adjustments with learning rate\n",
    "        adjusted_coords = coords - lr_scale * lr * adjustments\n",
    "        \n",
    "        return adjusted_coords\n",
    "    \n",
    "    def denormalize_coordinates(self, normalized_coords):\n",
    "        \"\"\"Convert normalized coordinates back to original scale\"\"\"\n",
    "        if isinstance(normalized_coords, torch.Tensor):\n",
    "            # Make sure coords_range and coords_min are on the same device\n",
    "            coords_range = self.coords_range.to(normalized_coords.device)\n",
    "            coords_min = self.coords_min.to(normalized_coords.device)\n",
    "            \n",
    "            # Convert from [-1,1] to original scale\n",
    "            original_coords = (normalized_coords + 1) / 2 * coords_range + coords_min\n",
    "            return original_coords\n",
    "        else:\n",
    "            # Handle numpy arrays\n",
    "            coords_range = self.coords_range.cpu().numpy()\n",
    "            coords_min = self.coords_min.cpu().numpy()\n",
    "            original_coords = (normalized_coords + 1) / 2 * coords_range + coords_min\n",
    "            return original_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdata = pd.DataFrame(X_aug)  # Your augmented expression data\n",
    "spcoor = pd.DataFrame(C_aug, columns=['coord_x', 'coord_y'])  # Your augmented coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First convert your data to tensors\n",
    "X_st_aug = torch.tensor(X_aug, dtype=torch.float32)\n",
    "Y_st_aug = torch.tensor(C_aug, dtype=torch.float32)\n",
    "\n",
    "T, D_sc, D_st_aug, fgw_dist = fused_gw_torch(\n",
    "    X_sc=X_sc, X_st=X_st_aug, Y_st=C_aug,\n",
    "    alpha=0.3, k=300, max_iter=200, device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate induced distance matrix\n",
    "D_induced = T @ torch.tensor(D_st_aug, dtype=torch.float32, device=device) @ T.t()\n",
    "D_induced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_induced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames to maintain the same format as original data\n",
    "stdata_aug = pd.DataFrame(X_aug)\n",
    "spcoor_aug = pd.DataFrame(C_aug, columns=['coord_x', 'coord_y'])\n",
    "\n",
    "# Now you can process them as you did with the original data\n",
    "adata_aug = sc.AnnData(stdata_aug)\n",
    "sc.pp.normalize_total(adata_aug)\n",
    "sc.pp.log1p(adata_aug)\n",
    "stdata_processed_aug = pd.DataFrame(adata_aug.X, index=adata_aug.obs_names, columns=adata_aug.var_names)\n",
    "\n",
    "# Convert processed data to tensors\n",
    "X_st_aug = torch.tensor(stdata_processed_aug.values, dtype=torch.float32)\n",
    "# Keep coordinates as numpy array for distance calculation\n",
    "Y_st_aug_np = C_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache before initializing the model\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch's CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# If you're still having memory issues, you can check what's using memory\n",
    "print(f\"GPU memory allocated before model initialization: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "stem_diffusion = STEMDiffusion(\n",
    "    st_gene_expr=X_st_aug,\n",
    "    st_coords=torch.tensor(Y_st_aug_np, dtype=torch.float32),  # Convert to tensor for model\n",
    "    D_st=D_st_aug,  # Tensor version of distance matrix\n",
    "    sc_gene_expr=X_sc,  # Original SC data\n",
    "    D_induced=D_induced,  # Recalculated induced distance matrix\n",
    "    outf='./stem_diffusion_output_aug',  # New output folder for augmented results\n",
    "    device=device,\n",
    "    n_genes=X_st_aug.shape[1],\n",
    "    n_embedding=[512, 256, 128],\n",
    "    hidden_dim=256,\n",
    "    dp=0.1,\n",
    "    n_timesteps=800,\n",
    "    beta_start=1e-4,\n",
    "    beta_end=0.02,\n",
    "    sigma=3.0,\n",
    "    alpha=0.8,\n",
    "    mmdbatch=1000,\n",
    "    batch_size=64  # Adjusted batch size if needed for larger dataset\n",
    ")\n",
    "\n",
    "\n",
    "# Train first encoder component (STEM-inspired) to align ST and SC\n",
    "stem_diffusion.train_encoder(\n",
    "    n_epochs=1000,\n",
    "    ratio_start=0,\n",
    "    ratio_end=1.0  # Gradually increase circle loss weight\n",
    ")\n",
    "\n",
    "# Train diffusion model using trained encoder\n",
    "stem_diffusion.train_diffusion(\n",
    "    n_epochs=2000,\n",
    "    lambda_struct=10.0  # Weight for KL divergence structure loss\n",
    ")\n",
    "\n",
    "# Or use the combined training method\n",
    "# stem_diffusion.train(encoder_epochs=1000, diffusion_epochs=2000)\n",
    "\n",
    "# Generate ST coordinates to evaluate model (using batched approach to save memory)\n",
    "st_gen_coords = stem_diffusion.generate_st_coordinates_batched(batch_size=64)\n",
    "\n",
    "# Visualize and compare with original\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_comparison(original_coords, generated_coords, title=\"Comparison of Original vs Generated ST Coordinates\"):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Plot original coordinates\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(original_coords[:, 0], original_coords[:, 1], alpha=0.7, s=10)\n",
    "    plt.title(\"Original ST Coordinates\")\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Plot generated coordinates\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(generated_coords[:, 0], generated_coords[:, 1], alpha=0.7, s=10)\n",
    "    plt.title(\"Generated ST Coordinates\")\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to numpy if needed\n",
    "if isinstance(Y_st, torch.Tensor):\n",
    "    Y_st_np = Y_st.cpu().numpy()\n",
    "else:\n",
    "    Y_st_np = Y_st\n",
    "\n",
    "if isinstance(st_gen_coords, torch.Tensor):\n",
    "    st_gen_np = st_gen_coords.cpu().numpy()\n",
    "else:\n",
    "    st_gen_np = st_gen_coords\n",
    "\n",
    "# Plot ST comparison\n",
    "plot_comparison(Y_st_np, st_gen_np)\n",
    "\n",
    "# Once ST results look good, generate SC coordinates (also batched)\n",
    "sc_coords = stem_diffusion.sample_sc_coordinates_batched(\n",
    "    batch_size=64,\n",
    "    timesteps=800,\n",
    "    use_structure_guidance=True  # Use D_induced to guide generation\n",
    ")\n",
    "\n",
    "# Convert PyTorch tensor to NumPy array before assigning\n",
    "if isinstance(sc_coords, torch.Tensor):\n",
    "    sc_coords_np = sc_coords.cpu().numpy()\n",
    "else:\n",
    "    sc_coords_np = sc_coords\n",
    "\n",
    "# Now assign the NumPy array to the obsm attribute\n",
    "adata.obsm['stem_diffusion_coords'] = sc_coords_np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))  # your preferred size\n",
    "\n",
    "\n",
    "# Visualization should now work\n",
    "import scanpy as sc\n",
    "sc.pl.embedding(adata, basis='stem_diffusion_coords', color='celltype_mapped_refined',\n",
    "                size=75, title='SC spatial coordinates (STEM-Diffusion Model)',\n",
    "                palette='tab20', legend_loc='right margin', legend_fontsize=10, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ehtesamenv_gains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
